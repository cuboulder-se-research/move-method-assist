[
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "5bc3aa428067dff1f2b9075ff5d1351fb05d4b10",
        "url": "https://github.com/apache/kafka/commit/5bc3aa428067dff1f2b9075ff5d1351fb05d4b10",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public flush(accessor DBAccessor) : void extracted from public flush() : void in class org.apache.kafka.streams.state.internals.RocksDBStore & moved to class org.apache.kafka.streams.state.internals.RocksDBStore.SingleColumnFamilyAccessor",
            "leftSideLocations": [
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java",
                    "startLine": 643,
                    "endLine": 653,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public flush() : void"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java",
                    "startLine": 649,
                    "endLine": 649,
                    "startColumn": 13,
                    "endColumn": 32,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java",
                    "startLine": 961,
                    "endLine": 964,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public flush(accessor DBAccessor) : void"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java",
                    "startLine": 963,
                    "endLine": 963,
                    "startColumn": 13,
                    "endColumn": 42,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java",
                    "startLine": 645,
                    "endLine": 655,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public flush() : void"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java",
                    "startLine": 651,
                    "endLine": 651,
                    "startColumn": 13,
                    "endColumn": 41,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "cfAccessor.flush(dbAccessor)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 566,
        "extraction_results": {
            "success": true,
            "newCommitHash": "5d9207c097766bead70cfb2f4160323d9a750cfc",
            "newBranchName": "extract-flush-flush-c078e51"
        },
        "telemetry": {
            "id": "25740fc3-7908-4513-a8fa-c1b847f84a4a",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 856,
                "lineStart": 89,
                "lineEnd": 944,
                "bodyLineStart": 89,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java",
                "sourceCode": "/**\n * A persistent key-value store based on RocksDB.\n */\npublic class RocksDBStore implements KeyValueStore<Bytes, byte[]>, BatchWritingStore {\n    private static final Logger log = LoggerFactory.getLogger(RocksDBStore.class);\n\n    private static final CompressionType COMPRESSION_TYPE = CompressionType.NO_COMPRESSION;\n    private static final CompactionStyle COMPACTION_STYLE = CompactionStyle.UNIVERSAL;\n    private static final long WRITE_BUFFER_SIZE = 16 * 1024 * 1024L;\n    private static final long BLOCK_CACHE_SIZE = 50 * 1024 * 1024L;\n    private static final long BLOCK_SIZE = 4096L;\n    private static final int MAX_WRITE_BUFFERS = 3;\n    static final String DB_FILE_DIR = \"rocksdb\";\n\n    final String name;\n    private final String parentDir;\n    final Set<KeyValueIterator<Bytes, byte[]>> openIterators = Collections.synchronizedSet(new HashSet<>());\n    private boolean consistencyEnabled = false;\n\n    // VisibleForTesting\n    protected File dbDir;\n    RocksDB db;\n    RocksDBAccessor dbAccessor;\n\n    // the following option objects will be created in openDB and closed in the close() method\n    private RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter userSpecifiedOptions;\n    WriteOptions wOptions;\n    FlushOptions fOptions;\n    private Cache cache;\n    private BloomFilter filter;\n    private Statistics statistics;\n\n    private RocksDBConfigSetter configSetter;\n    private boolean userSpecifiedStatistics = false;\n\n    private final RocksDBMetricsRecorder metricsRecorder;\n    // if true, then open iterators (for range, prefix scan, and other operations) will be\n    // managed automatically (by this store instance). if false, then these iterators must be\n    // managed elsewhere (by the caller of those methods).\n    private final boolean autoManagedIterators;\n\n    protected volatile boolean open = false;\n    protected StateStoreContext context;\n    protected Position position;\n    private OffsetCheckpoint positionCheckpoint;\n\n    public RocksDBStore(final String name,\n                        final String metricsScope) {\n        this(name, DB_FILE_DIR, new RocksDBMetricsRecorder(metricsScope, name));\n    }\n\n    RocksDBStore(final String name,\n                 final String parentDir,\n                 final RocksDBMetricsRecorder metricsRecorder) {\n        this(name, parentDir, metricsRecorder, true);\n    }\n\n    RocksDBStore(final String name,\n                 final String parentDir,\n                 final RocksDBMetricsRecorder metricsRecorder,\n                 final boolean autoManagedIterators) {\n        this.name = name;\n        this.parentDir = parentDir;\n        this.metricsRecorder = metricsRecorder;\n        this.autoManagedIterators = autoManagedIterators;\n    }\n\n    @Deprecated\n    @Override\n    public void init(final ProcessorContext context,\n                     final StateStore root) {\n        if (context instanceof StateStoreContext) {\n            init((StateStoreContext) context, root);\n        } else {\n            throw new UnsupportedOperationException(\n                \"Use RocksDBStore#init(StateStoreContext, StateStore) instead.\"\n            );\n        }\n    }\n\n    @Override\n    public void init(final StateStoreContext context,\n                     final StateStore root) {\n        // open the DB dir\n        metricsRecorder.init(getMetricsImpl(context), context.taskId());\n        openDB(context.appConfigs(), context.stateDir());\n\n        final File positionCheckpointFile = new File(context.stateDir(), name() + \".position\");\n        this.positionCheckpoint = new OffsetCheckpoint(positionCheckpointFile);\n        this.position = StoreQueryUtils.readPositionFromCheckpoint(positionCheckpoint);\n\n        // value getter should always read directly from rocksDB\n        // since it is only for values that are already flushed\n        this.context = context;\n        context.register(\n            root,\n            (RecordBatchingStateRestoreCallback) this::restoreBatch,\n            () -> StoreQueryUtils.checkpointPosition(positionCheckpoint, position)\n        );\n        consistencyEnabled = StreamsConfig.InternalConfig.getBoolean(\n            context.appConfigs(),\n            IQ_CONSISTENCY_OFFSET_VECTOR_ENABLED,\n            false);\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    void openDB(final Map<String, Object> configs, final File stateDir) {\n        // initialize the default rocksdb options\n\n        final DBOptions dbOptions = new DBOptions();\n        final ColumnFamilyOptions columnFamilyOptions = new ColumnFamilyOptions();\n        userSpecifiedOptions = new RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter(dbOptions, columnFamilyOptions);\n\n        final BlockBasedTableConfigWithAccessibleCache tableConfig = new BlockBasedTableConfigWithAccessibleCache();\n        cache = new LRUCache(BLOCK_CACHE_SIZE);\n        tableConfig.setBlockCache(cache);\n        tableConfig.setBlockSize(BLOCK_SIZE);\n\n        filter = new BloomFilter();\n        tableConfig.setFilterPolicy(filter);\n\n        userSpecifiedOptions.optimizeFiltersForHits();\n        userSpecifiedOptions.setTableFormatConfig(tableConfig);\n        userSpecifiedOptions.setWriteBufferSize(WRITE_BUFFER_SIZE);\n        userSpecifiedOptions.setCompressionType(COMPRESSION_TYPE);\n        userSpecifiedOptions.setCompactionStyle(COMPACTION_STYLE);\n        userSpecifiedOptions.setMaxWriteBufferNumber(MAX_WRITE_BUFFERS);\n        userSpecifiedOptions.setCreateIfMissing(true);\n        userSpecifiedOptions.setErrorIfExists(false);\n        userSpecifiedOptions.setInfoLogLevel(InfoLogLevel.ERROR_LEVEL);\n        // this is the recommended way to increase parallelism in RocksDb\n        // note that the current implementation of setIncreaseParallelism affects the number\n        // of compaction threads but not flush threads (the latter remains one). Also,\n        // the parallelism value needs to be at least two because of the code in\n        // https://github.com/facebook/rocksdb/blob/62ad0a9b19f0be4cefa70b6b32876e764b7f3c11/util/options.cc#L580\n        // subtracts one from the value passed to determine the number of compaction threads\n        // (this could be a bug in the RocksDB code and their devs have been contacted).\n        userSpecifiedOptions.setIncreaseParallelism(Math.max(Runtime.getRuntime().availableProcessors(), 2));\n\n        wOptions = new WriteOptions();\n        wOptions.setDisableWAL(true);\n\n        fOptions = new FlushOptions();\n        fOptions.setWaitForFlush(true);\n\n        final Class<RocksDBConfigSetter> configSetterClass =\n                (Class<RocksDBConfigSetter>) configs.get(StreamsConfig.ROCKSDB_CONFIG_SETTER_CLASS_CONFIG);\n\n        if (configSetterClass != null) {\n            configSetter = Utils.newInstance(configSetterClass);\n            configSetter.setConfig(name, userSpecifiedOptions, configs);\n        }\n\n        dbDir = new File(new File(stateDir, parentDir), name);\n        try {\n            Files.createDirectories(dbDir.getParentFile().toPath());\n            Files.createDirectories(dbDir.getAbsoluteFile().toPath());\n        } catch (final IOException fatal) {\n            throw new ProcessorStateException(fatal);\n        }\n\n        // Setup statistics before the database is opened, otherwise the statistics are not updated\n        // with the measurements from Rocks DB\n        setupStatistics(configs, dbOptions);\n        openRocksDB(dbOptions, columnFamilyOptions);\n        open = true;\n\n        addValueProvidersToMetricsRecorder();\n    }\n\n    private void setupStatistics(final Map<String, Object> configs, final DBOptions dbOptions) {\n        statistics = userSpecifiedOptions.statistics();\n        if (statistics == null) {\n            if (RecordingLevel.forName((String) configs.get(METRICS_RECORDING_LEVEL_CONFIG)) == RecordingLevel.DEBUG) {\n                statistics = new Statistics();\n                dbOptions.setStatistics(statistics);\n            }\n            userSpecifiedStatistics = false;\n        } else {\n            userSpecifiedStatistics = true;\n        }\n    }\n\n    private void addValueProvidersToMetricsRecorder() {\n        final TableFormatConfig tableFormatConfig = userSpecifiedOptions.tableFormatConfig();\n        if (tableFormatConfig instanceof BlockBasedTableConfigWithAccessibleCache) {\n            final Cache cache = ((BlockBasedTableConfigWithAccessibleCache) tableFormatConfig).blockCache();\n            metricsRecorder.addValueProviders(name, db, cache, userSpecifiedStatistics ? null : statistics);\n        } else if (tableFormatConfig instanceof BlockBasedTableConfig) {\n            throw new ProcessorStateException(\"The used block-based table format configuration does not expose the \" +\n                    \"block cache. Use the BlockBasedTableConfig instance provided by Options#tableFormatConfig() to configure \" +\n                    \"the block-based table format of RocksDB. Do not provide a new instance of BlockBasedTableConfig to \" +\n                    \"the RocksDB options.\");\n        } else {\n            metricsRecorder.addValueProviders(name, db, null, userSpecifiedStatistics ? null : statistics);\n        }\n    }\n\n    void openRocksDB(final DBOptions dbOptions,\n                     final ColumnFamilyOptions columnFamilyOptions) {\n        final List<ColumnFamilyHandle> columnFamilies = openRocksDB(\n                dbOptions,\n                new ColumnFamilyDescriptor(RocksDB.DEFAULT_COLUMN_FAMILY, columnFamilyOptions)\n        );\n\n        dbAccessor = new SingleColumnFamilyAccessor(columnFamilies.get(0));\n    }\n\n    /**\n     * Open RocksDB while automatically creating any requested column families that don't yet exist.\n     */\n    protected List<ColumnFamilyHandle> openRocksDB(final DBOptions dbOptions,\n                                                   final ColumnFamilyDescriptor defaultColumnFamilyDescriptor,\n                                                   final ColumnFamilyDescriptor... columnFamilyDescriptors) {\n        final String absolutePath = dbDir.getAbsolutePath();\n        final List<ColumnFamilyDescriptor> extraDescriptors = Arrays.asList(columnFamilyDescriptors);\n        final List<ColumnFamilyDescriptor> allDescriptors = new ArrayList<>(1 + columnFamilyDescriptors.length);\n        allDescriptors.add(defaultColumnFamilyDescriptor);\n        allDescriptors.addAll(extraDescriptors);\n\n        try {\n            final Options options = new Options(dbOptions, defaultColumnFamilyDescriptor.getOptions());\n            final List<byte[]> allExisting = RocksDB.listColumnFamilies(options, absolutePath);\n\n            final List<ColumnFamilyDescriptor> existingDescriptors = new LinkedList<>();\n            existingDescriptors.add(defaultColumnFamilyDescriptor);\n            existingDescriptors.addAll(extraDescriptors.stream()\n                    .filter(descriptor -> allExisting.stream().anyMatch(existing -> Arrays.equals(existing, descriptor.getName())))\n                    .collect(Collectors.toList()));\n            final List<ColumnFamilyDescriptor> toCreate = extraDescriptors.stream()\n                    .filter(descriptor -> allExisting.stream().noneMatch(existing -> Arrays.equals(existing, descriptor.getName())))\n                    .collect(Collectors.toList());\n            final List<ColumnFamilyHandle> existingColumnFamilies = new ArrayList<>(existingDescriptors.size());\n            db = RocksDB.open(dbOptions, absolutePath, existingDescriptors, existingColumnFamilies);\n            final List<ColumnFamilyHandle> createdColumnFamilies = db.createColumnFamilies(toCreate);\n\n            return mergeColumnFamilyHandleLists(existingColumnFamilies, createdColumnFamilies, allDescriptors);\n\n        } catch (final RocksDBException e) {\n            throw new ProcessorStateException(\"Error opening store \" + name + \" at location \" + dbDir.toString(), e);\n        }\n    }\n\n    /**\n     * match up the existing and created ColumnFamilyHandles with the existing/created ColumnFamilyDescriptors\n     * so that the order of the resultant List matches the order of the allDescriptors argument\n     */\n    private List<ColumnFamilyHandle> mergeColumnFamilyHandleLists(final List<ColumnFamilyHandle> existingColumnFamilyHandles,\n                                                                  final List<ColumnFamilyHandle> createdColumnFamilyHandles,\n                                                                  final List<ColumnFamilyDescriptor> allDescriptors) throws RocksDBException {\n        final List<ColumnFamilyHandle> columnFamilies = new ArrayList<>(allDescriptors.size());\n        int existing = 0;\n        int created = 0;\n\n        while (existing + created < allDescriptors.size()) {\n            final ColumnFamilyHandle existingHandle = existing < existingColumnFamilyHandles.size() ? existingColumnFamilyHandles.get(existing) : null;\n            final ColumnFamilyHandle createdHandle = created < createdColumnFamilyHandles.size() ? createdColumnFamilyHandles.get(created) : null;\n            if (existingHandle != null && Arrays.equals(existingHandle.getDescriptor().getName(), allDescriptors.get(existing + created).getName())) {\n                columnFamilies.add(existingHandle);\n                existing++;\n            } else if (createdHandle != null && Arrays.equals(createdHandle.getDescriptor().getName(), allDescriptors.get(existing + created).getName())) {\n                columnFamilies.add(createdHandle);\n                created++;\n            } else {\n                throw new IllegalStateException(\"Unable to match up column family handles with descriptors.\");\n            }\n        }\n        return columnFamilies;\n    }\n\n    @Override\n    public String name() {\n        return name;\n    }\n\n    @Override\n    public boolean persistent() {\n        return true;\n    }\n\n    @Override\n    public boolean isOpen() {\n        return open;\n    }\n\n    private void validateStoreOpen() {\n        if (!open) {\n            throw new InvalidStateStoreException(\"Store \" + name + \" is currently closed\");\n        }\n    }\n\n    public Snapshot getSnapshot() {\n        return db.getSnapshot();\n    }\n\n    public void releaseSnapshot(final Snapshot snapshot) {\n        db.releaseSnapshot(snapshot);\n    }\n\n    @Override\n    public synchronized void put(final Bytes key,\n                                 final byte[] value) {\n        Objects.requireNonNull(key, \"key cannot be null\");\n        validateStoreOpen();\n        dbAccessor.put(key.get(), value);\n\n        StoreQueryUtils.updatePosition(position, context);\n    }\n\n    @Override\n    public synchronized byte[] putIfAbsent(final Bytes key,\n                                           final byte[] value) {\n        Objects.requireNonNull(key, \"key cannot be null\");\n        final byte[] originalValue = get(key);\n        if (originalValue == null) {\n            put(key, value);\n        }\n        return originalValue;\n    }\n\n    @Override\n    public void putAll(final List<KeyValue<Bytes, byte[]>> entries) {\n        try (final WriteBatch batch = new WriteBatch()) {\n            dbAccessor.prepareBatch(entries, batch);\n            write(batch);\n            StoreQueryUtils.updatePosition(position, context);\n        } catch (final RocksDBException e) {\n            throw new ProcessorStateException(\"Error while batch writing to store \" + name, e);\n        }\n    }\n\n    @Override\n    public <R> QueryResult<R> query(\n        final Query<R> query,\n        final PositionBound positionBound,\n        final QueryConfig config) {\n\n        return StoreQueryUtils.handleBasicQueries(\n            query,\n            positionBound,\n            config,\n            this,\n            position,\n            context\n        );\n    }\n\n    @Override\n    public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix,\n                                                                                    final PS prefixKeySerializer) {\n        if (!autoManagedIterators) {\n            throw new IllegalStateException(\"Must specify openIterators in call to prefixScan()\");\n        }\n        return doPrefixScan(prefix, prefixKeySerializer, openIterators);\n    }\n\n    <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix,\n                                                                             final PS prefixKeySerializer,\n                                                                             final Set<KeyValueIterator<Bytes, byte[]>> openIterators) {\n        if (autoManagedIterators) {\n            throw new IllegalStateException(\"Cannot specify openIterators when using auto-managed iterators\");\n        }\n        return doPrefixScan(prefix, prefixKeySerializer, openIterators);\n    }\n\n    <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> doPrefixScan(final P prefix,\n                                                                               final PS prefixKeySerializer,\n                                                                               final Set<KeyValueIterator<Bytes, byte[]>> openIterators) {\n        validateStoreOpen();\n        Objects.requireNonNull(prefix, \"prefix cannot be null\");\n        Objects.requireNonNull(prefixKeySerializer, \"prefixKeySerializer cannot be null\");\n        final Bytes prefixBytes = Bytes.wrap(prefixKeySerializer.serialize(null, prefix));\n\n        final ManagedKeyValueIterator<Bytes, byte[]> rocksDbPrefixSeekIterator = dbAccessor.prefixScan(prefixBytes);\n        openIterators.add(rocksDbPrefixSeekIterator);\n        rocksDbPrefixSeekIterator.onClose(() -> openIterators.remove(rocksDbPrefixSeekIterator));\n\n        return rocksDbPrefixSeekIterator;\n    }\n\n    @Override\n    public synchronized byte[] get(final Bytes key) {\n        return get(key, Optional.empty());\n    }\n\n    public synchronized byte[] get(final Bytes key, final ReadOptions readOptions) {\n        return get(key, Optional.of(readOptions));\n    }\n\n    private synchronized byte[] get(final Bytes key, final Optional<ReadOptions> readOptions) {\n        validateStoreOpen();\n        try {\n            return readOptions.isPresent() ? dbAccessor.get(key.get(), readOptions.get()) : dbAccessor.get(key.get());\n        } catch (final RocksDBException e) {\n            // String format is happening in wrapping stores. So formatted message is thrown from wrapping stores.\n            throw new ProcessorStateException(\"Error while getting value for key from store \" + name, e);\n        }\n    }\n\n    @Override\n    public synchronized byte[] delete(final Bytes key) {\n        Objects.requireNonNull(key, \"key cannot be null\");\n        final byte[] oldValue;\n        try {\n            oldValue = dbAccessor.getOnly(key.get());\n        } catch (final RocksDBException e) {\n            // String format is happening in wrapping stores. So formatted message is thrown from wrapping stores.\n            throw new ProcessorStateException(\"Error while getting value for key from store \" + name, e);\n        }\n        put(key, null);\n        return oldValue;\n    }\n\n    void deleteRange(final Bytes keyFrom, final Bytes keyTo) {\n        Objects.requireNonNull(keyFrom, \"keyFrom cannot be null\");\n        Objects.requireNonNull(keyTo, \"keyTo cannot be null\");\n\n        validateStoreOpen();\n\n        // End of key is exclusive, so we increment it by 1 byte to make keyTo inclusive.\n        // RocksDB's deleteRange() does not support a null upper bound so in the event\n        // of overflow from increment(), the operation cannot be performed and an\n        // IndexOutOfBoundsException will be thrown.\n        dbAccessor.deleteRange(keyFrom.get(), Bytes.increment(keyTo).get());\n    }\n\n    @Override\n    public synchronized KeyValueIterator<Bytes, byte[]> range(final Bytes from,\n                                                              final Bytes to) {\n        if (!autoManagedIterators) {\n            throw new IllegalStateException(\"Must specify openIterators in call to range()\");\n        }\n        return range(from, to, true, openIterators);\n    }\n\n    synchronized KeyValueIterator<Bytes, byte[]> range(final Bytes from,\n                                                       final Bytes to,\n                                                       final Set<KeyValueIterator<Bytes, byte[]>> openIterators) {\n        if (autoManagedIterators) {\n            throw new IllegalStateException(\"Cannot specify openIterators when using auto-managed iterators\");\n        }\n        return range(from, to, true, openIterators);\n    }\n\n    @Override\n    public synchronized KeyValueIterator<Bytes, byte[]> reverseRange(final Bytes from,\n                                                                     final Bytes to) {\n        if (!autoManagedIterators) {\n            throw new IllegalStateException(\"Must specify openIterators in call to reverseRange()\");\n        }\n        return range(from, to, false, openIterators);\n    }\n\n    synchronized KeyValueIterator<Bytes, byte[]> reverseRange(final Bytes from,\n                                                              final Bytes to,\n                                                              final Set<KeyValueIterator<Bytes, byte[]>> openIterators) {\n        if (autoManagedIterators) {\n            throw new IllegalStateException(\"Cannot specify openIterators when using auto-managed iterators\");\n        }\n        return range(from, to, false, openIterators);\n    }\n\n    private KeyValueIterator<Bytes, byte[]> range(final Bytes from,\n                                                  final Bytes to,\n                                                  final boolean forward,\n                                                  final Set<KeyValueIterator<Bytes, byte[]>> openIterators) {\n        if (Objects.nonNull(from) && Objects.nonNull(to) && from.compareTo(to) > 0) {\n            log.warn(\"Returning empty iterator for fetch with invalid key range: from > to. \"\n                    + \"This may be due to range arguments set in the wrong order, \" +\n                    \"or serdes that don't preserve ordering when lexicographically comparing the serialized bytes. \" +\n                    \"Note that the built-in numerical serdes do not follow this for negative numbers\");\n            return KeyValueIterators.emptyIterator();\n        }\n\n        validateStoreOpen();\n\n        final ManagedKeyValueIterator<Bytes, byte[]> rocksDBRangeIterator = dbAccessor.range(from, to, forward);\n        openIterators.add(rocksDBRangeIterator);\n        rocksDBRangeIterator.onClose(() -> openIterators.remove(rocksDBRangeIterator));\n\n        return rocksDBRangeIterator;\n    }\n\n    @Override\n    public synchronized KeyValueIterator<Bytes, byte[]> all() {\n        if (!autoManagedIterators) {\n            throw new IllegalStateException(\"Must specify openIterators in call to all()\");\n        }\n        return all(true, openIterators);\n    }\n\n    synchronized KeyValueIterator<Bytes, byte[]> all(final Set<KeyValueIterator<Bytes, byte[]>> openIterators) {\n        if (autoManagedIterators) {\n            throw new IllegalStateException(\"Cannot specify openIterators when using auto-managed iterators\");\n        }\n        return all(true, openIterators);\n    }\n\n    @Override\n    public KeyValueIterator<Bytes, byte[]> reverseAll() {\n        if (!autoManagedIterators) {\n            throw new IllegalStateException(\"Must specify openIterators in call to reverseAll()\");\n        }\n        return all(false, openIterators);\n    }\n\n    KeyValueIterator<Bytes, byte[]> reverseAll(final Set<KeyValueIterator<Bytes, byte[]>> openIterators) {\n        if (autoManagedIterators) {\n            throw new IllegalStateException(\"Cannot specify openIterators when using auto-managed iterators\");\n        }\n        return all(false, openIterators);\n    }\n\n    private KeyValueIterator<Bytes, byte[]> all(final boolean forward,\n                                                final Set<KeyValueIterator<Bytes, byte[]>> openIterators) {\n        validateStoreOpen();\n        final ManagedKeyValueIterator<Bytes, byte[]> rocksDbIterator = dbAccessor.all(forward);\n        openIterators.add(rocksDbIterator);\n        rocksDbIterator.onClose(() -> openIterators.remove(rocksDbIterator));\n        return rocksDbIterator;\n    }\n\n    /**\n     * Return an approximate count of key-value mappings in this store.\n     *\n     * <code>RocksDB</code> cannot return an exact entry count without doing a\n     * full scan, so this method relies on the <code>rocksdb.estimate-num-keys</code>\n     * property to get an approximate count. The returned size also includes\n     * a count of dirty keys in the store's in-memory cache, which may lead to some\n     * double-counting of entries and inflate the estimate.\n     *\n     * @return an approximate count of key-value mappings in the store.\n     */\n    @Override\n    public long approximateNumEntries() {\n        validateStoreOpen();\n        final long numEntries;\n        try {\n            numEntries = dbAccessor.approximateNumEntries();\n        } catch (final RocksDBException e) {\n            throw new ProcessorStateException(\"Error fetching property from store \" + name, e);\n        }\n        if (isOverflowing(numEntries)) {\n            return Long.MAX_VALUE;\n        }\n        return numEntries;\n    }\n\n    private boolean isOverflowing(final long value) {\n        // RocksDB returns an unsigned 8-byte integer, which could overflow long\n        // and manifest as a negative value.\n        return value < 0;\n    }\n\n    @Override\n    public synchronized void flush() {\n        if (db == null) {\n            return;\n        }\n        try {\n            flush1();\n        } catch (final RocksDBException e) {\n            throw new ProcessorStateException(\"Error while executing flush from store \" + name, e);\n        }\n    }\n\n    private void flush1() throws RocksDBException {\n        dbAccessor.flush();\n    }\n\n    @Override\n    public void addToBatch(final KeyValue<byte[], byte[]> record,\n                           final WriteBatchInterface batch) throws RocksDBException {\n        dbAccessor.addToBatch(record.key, record.value, batch);\n    }\n\n    @Override\n    public void write(final WriteBatchInterface batch) throws RocksDBException {\n        if (batch instanceof WriteBatch) {\n            db.write(wOptions, (WriteBatch) batch);\n        } else if (batch instanceof WriteBatchWithIndex) {\n            db.write(wOptions, (WriteBatchWithIndex) batch);\n        } else {\n            log.error(\"Unknown type of batch {}. This is a bug in Kafka Streams. \" +\n                    \"Please file a bug report at https://issues.apache.org/jira/projects/KAFKA.\",\n                    batch.getClass().getCanonicalName());\n            throw new IllegalStateException(\"Unknown type of batch \" + batch.getClass().getCanonicalName());\n        }\n    }\n\n    @Override\n    public synchronized void close() {\n        if (!open) {\n            return;\n        }\n\n        open = false;\n        closeOpenIterators();\n\n        if (configSetter != null) {\n            configSetter.close(name, userSpecifiedOptions);\n            configSetter = null;\n        }\n\n        metricsRecorder.removeValueProviders(name);\n\n        // Important: do not rearrange the order in which the below objects are closed!\n        // Order of closing must follow: ColumnFamilyHandle > RocksDB > DBOptions > ColumnFamilyOptions\n        dbAccessor.close();\n        db.close();\n        userSpecifiedOptions.close();\n        wOptions.close();\n        fOptions.close();\n        filter.close();\n        cache.close();\n        if (statistics != null) {\n            statistics.close();\n        }\n\n        dbAccessor = null;\n        userSpecifiedOptions = null;\n        wOptions = null;\n        fOptions = null;\n        db = null;\n        filter = null;\n        cache = null;\n        statistics = null;\n    }\n\n    private void closeOpenIterators() {\n        final HashSet<KeyValueIterator<Bytes, byte[]>> iterators;\n        synchronized (openIterators) {\n            iterators = new HashSet<>(openIterators);\n        }\n        if (iterators.size() != 0) {\n            log.warn(\"Closing {} open iterators for store {}\", iterators.size(), name);\n            for (final KeyValueIterator<Bytes, byte[]> iterator : iterators) {\n                iterator.close();\n            }\n        }\n    }\n\n    interface RocksDBAccessor {\n\n        void put(final byte[] key,\n                 final byte[] value);\n\n        void prepareBatch(final List<KeyValue<Bytes, byte[]>> entries,\n                          final WriteBatchInterface batch) throws RocksDBException;\n\n        byte[] get(final byte[] key) throws RocksDBException;\n\n        byte[] get(final byte[] key, ReadOptions readOptions) throws RocksDBException;\n\n        /**\n         * In contrast to get(), we don't migrate the key to new CF.\n         * <p>\n         * Use for get() within delete() -- no need to migrate, as it's deleted anyway\n         */\n        byte[] getOnly(final byte[] key) throws RocksDBException;\n\n        ManagedKeyValueIterator<Bytes, byte[]> range(final Bytes from,\n                                              final Bytes to,\n                                              final boolean forward);\n\n        /**\n         * Deletes keys entries in the range ['from', 'to'], including 'from' and excluding 'to'.\n         */\n        void deleteRange(final byte[] from,\n                         final byte[] to);\n\n        ManagedKeyValueIterator<Bytes, byte[]> all(final boolean forward);\n\n        ManagedKeyValueIterator<Bytes, byte[]> prefixScan(final Bytes prefix);\n\n        long approximateNumEntries() throws RocksDBException;\n\n        void flush() throws RocksDBException;\n\n        void addToBatch(final byte[] key,\n                        final byte[] value,\n                        final WriteBatchInterface batch) throws RocksDBException;\n\n        void close();\n    }\n\n    class SingleColumnFamilyAccessor implements RocksDBAccessor {\n        private final ColumnFamilyHandle columnFamily;\n\n        SingleColumnFamilyAccessor(final ColumnFamilyHandle columnFamily) {\n            this.columnFamily = columnFamily;\n        }\n\n        @Override\n        public void put(final byte[] key,\n                        final byte[] value) {\n            if (value == null) {\n                try {\n                    db.delete(columnFamily, wOptions, key);\n                } catch (final RocksDBException e) {\n                    // String format is happening in wrapping stores. So formatted message is thrown from wrapping stores.\n                    throw new ProcessorStateException(\"Error while removing key from store \" + name, e);\n                }\n            } else {\n                try {\n                    db.put(columnFamily, wOptions, key, value);\n                } catch (final RocksDBException e) {\n                    // String format is happening in wrapping stores. So formatted message is thrown from wrapping stores.\n                    throw new ProcessorStateException(\"Error while putting key/value into store \" + name, e);\n                }\n            }\n        }\n\n        @Override\n        public void prepareBatch(final List<KeyValue<Bytes, byte[]>> entries,\n                                 final WriteBatchInterface batch) throws RocksDBException {\n            for (final KeyValue<Bytes, byte[]> entry : entries) {\n                Objects.requireNonNull(entry.key, \"key cannot be null\");\n                addToBatch(entry.key.get(), entry.value, batch);\n            }\n        }\n\n        @Override\n        public byte[] get(final byte[] key) throws RocksDBException {\n            return db.get(columnFamily, key);\n        }\n\n        @Override\n        public byte[] get(final byte[] key, final ReadOptions readOptions) throws RocksDBException {\n            return db.get(columnFamily, readOptions, key);\n        }\n\n        @Override\n        public byte[] getOnly(final byte[] key) throws RocksDBException {\n            return db.get(columnFamily, key);\n        }\n\n        @Override\n        public ManagedKeyValueIterator<Bytes, byte[]> range(final Bytes from,\n                                                     final Bytes to,\n                                                     final boolean forward) {\n            return new RocksDBRangeIterator(\n                    name,\n                    db.newIterator(columnFamily),\n                    from,\n                    to,\n                    forward,\n                    true\n            );\n        }\n\n        @Override\n        public void deleteRange(final byte[] from, final byte[] to) {\n            try {\n                db.deleteRange(columnFamily, wOptions, from, to);\n            } catch (final RocksDBException e) {\n                // String format is happening in wrapping stores. So formatted message is thrown from wrapping stores.\n                throw new ProcessorStateException(\"Error while removing key from store \" + name, e);\n            }\n        }\n\n        @Override\n        public ManagedKeyValueIterator<Bytes, byte[]> all(final boolean forward) {\n            final RocksIterator innerIterWithTimestamp = db.newIterator(columnFamily);\n            if (forward) {\n                innerIterWithTimestamp.seekToFirst();\n            } else {\n                innerIterWithTimestamp.seekToLast();\n            }\n            return new RocksDbIterator(name, innerIterWithTimestamp, forward);\n        }\n\n        @Override\n        public ManagedKeyValueIterator<Bytes, byte[]> prefixScan(final Bytes prefix) {\n            final Bytes to = incrementWithoutOverflow(prefix);\n            return new RocksDBRangeIterator(\n                    name,\n                    db.newIterator(columnFamily),\n                    prefix,\n                    to,\n                    true,\n                    false\n            );\n        }\n\n        @Override\n        public long approximateNumEntries() throws RocksDBException {\n            return db.getLongProperty(columnFamily, \"rocksdb.estimate-num-keys\");\n        }\n\n        @Override\n        public void flush() throws RocksDBException {\n            db.flush(fOptions, columnFamily);\n        }\n\n        @Override\n        public void addToBatch(final byte[] key,\n                               final byte[] value,\n                               final WriteBatchInterface batch) throws RocksDBException {\n            if (value == null) {\n                batch.delete(columnFamily, key);\n            } else {\n                batch.put(columnFamily, key, value);\n            }\n        }\n\n        @Override\n        public void close() {\n            columnFamily.close();\n        }\n    }\n\n    void restoreBatch(final Collection<ConsumerRecord<byte[], byte[]>> records) {\n        try (final WriteBatch batch = new WriteBatch()) {\n            for (final ConsumerRecord<byte[], byte[]> record : records) {\n                ChangelogRecordDeserializationHelper.applyChecksAndUpdatePosition(\n                    record,\n                    consistencyEnabled,\n                    position\n                );\n                // If version headers are not present or version is V0\n                dbAccessor.addToBatch(record.key(), record.value(), batch);\n            }\n            write(batch);\n        } catch (final RocksDBException e) {\n            throw new ProcessorStateException(\"Error restoring batch to store \" + name, e);\n        }\n\n    }\n\n    // for testing\n    public Options getOptions() {\n        return userSpecifiedOptions;\n    }\n\n    @Override\n    public Position getPosition() {\n        return position;\n    }\n\n    /**\n     * Same as {@link Bytes#increment(Bytes)} but {@code null} is returned instead of throwing\n     * {@code IndexOutOfBoundsException} in the event of overflow.\n     *\n     * @param input bytes to increment\n     * @return A new copy of the incremented byte array, or {@code null} if incrementing would\n     *         result in overflow.\n     */\n    static Bytes incrementWithoutOverflow(final Bytes input) {\n        try {\n            return Bytes.increment(input);\n        } catch (final IndexOutOfBoundsException e) {\n            return null;\n        }\n    }\n}",
                "methodCount": 78
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "openDB",
                            "method_signature": "@SuppressWarnings(\"unchecked\") openDB(final Map<String, Object> configs, final File stateDir)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "setupStatistics",
                            "method_signature": "private setupStatistics(final Map<String, Object> configs, final DBOptions dbOptions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addValueProvidersToMetricsRecorder",
                            "method_signature": "private addValueProvidersToMetricsRecorder()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "openRocksDB",
                            "method_signature": " openRocksDB(final DBOptions dbOptions,\n                     final ColumnFamilyOptions columnFamilyOptions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "openRocksDB",
                            "method_signature": "protected openRocksDB(final DBOptions dbOptions,\n                                                   final ColumnFamilyDescriptor defaultColumnFamilyDescriptor,\n                                                   final ColumnFamilyDescriptor... columnFamilyDescriptors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "mergeColumnFamilyHandleLists",
                            "method_signature": "private mergeColumnFamilyHandleLists(final List<ColumnFamilyHandle> existingColumnFamilyHandles,\n                                                                  final List<ColumnFamilyHandle> createdColumnFamilyHandles,\n                                                                  final List<ColumnFamilyDescriptor> allDescriptors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validateStoreOpen",
                            "method_signature": "private validateStoreOpen()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "releaseSnapshot",
                            "method_signature": "public releaseSnapshot(final Snapshot snapshot)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "prefixScan",
                            "method_signature": " prefixScan(final P prefix,\n                                                                             final PS prefixKeySerializer,\n                                                                             final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "doPrefixScan",
                            "method_signature": " doPrefixScan(final P prefix,\n                                                                               final PS prefixKeySerializer,\n                                                                               final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "get",
                            "method_signature": "public synchronized get(final Bytes key, final ReadOptions readOptions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "get",
                            "method_signature": "private synchronized get(final Bytes key, final Optional<ReadOptions> readOptions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteRange",
                            "method_signature": " deleteRange(final Bytes keyFrom, final Bytes keyTo)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "range",
                            "method_signature": "synchronized range(final Bytes from,\n                                                       final Bytes to,\n                                                       final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "reverseRange",
                            "method_signature": "synchronized reverseRange(final Bytes from,\n                                                              final Bytes to,\n                                                              final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "range",
                            "method_signature": "private range(final Bytes from,\n                                                  final Bytes to,\n                                                  final boolean forward,\n                                                  final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "all",
                            "method_signature": "synchronized all(final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "reverseAll",
                            "method_signature": " reverseAll(final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "all",
                            "method_signature": "private all(final boolean forward,\n                                                final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isOverflowing",
                            "method_signature": "private isOverflowing(final long value)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "flush1",
                            "method_signature": "private flush1()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "closeOpenIterators",
                            "method_signature": "private closeOpenIterators()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "restoreBatch",
                            "method_signature": " restoreBatch(final Collection<ConsumerRecord<byte[], byte[]>> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "incrementWithoutOverflow",
                            "method_signature": "static incrementWithoutOverflow(final Bytes input)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "isOverflowing",
                            "method_signature": "private isOverflowing(final long value)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteRange",
                            "method_signature": " deleteRange(final Bytes keyFrom, final Bytes keyTo)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "releaseSnapshot",
                            "method_signature": "public releaseSnapshot(final Snapshot snapshot)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "incrementWithoutOverflow",
                            "method_signature": "static incrementWithoutOverflow(final Bytes input)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "doPrefixScan",
                            "method_signature": " doPrefixScan(final P prefix,\n                                                                               final PS prefixKeySerializer,\n                                                                               final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "flush1",
                            "method_signature": "private flush1()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "openRocksDB",
                            "method_signature": " openRocksDB(final DBOptions dbOptions,\n                     final ColumnFamilyOptions columnFamilyOptions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "all",
                            "method_signature": "private all(final boolean forward,\n                                                final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "range",
                            "method_signature": "private range(final Bytes from,\n                                                  final Bytes to,\n                                                  final boolean forward,\n                                                  final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "get",
                            "method_signature": "public synchronized get(final Bytes key, final ReadOptions readOptions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "openDB",
                            "method_signature": "@SuppressWarnings(\"unchecked\") openDB(final Map<String, Object> configs, final File stateDir)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "reverseAll",
                            "method_signature": " reverseAll(final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addValueProvidersToMetricsRecorder",
                            "method_signature": "private addValueProvidersToMetricsRecorder()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "all",
                            "method_signature": "synchronized all(final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validateStoreOpen",
                            "method_signature": "private validateStoreOpen()",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "isOverflowing",
                            "method_signature": "private isOverflowing(final long value)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteRange",
                            "method_signature": " deleteRange(final Bytes keyFrom, final Bytes keyTo)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "releaseSnapshot",
                            "method_signature": "public releaseSnapshot(final Snapshot snapshot)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "incrementWithoutOverflow",
                            "method_signature": "static incrementWithoutOverflow(final Bytes input)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "doPrefixScan",
                            "method_signature": " doPrefixScan(final P prefix,\n                                                                               final PS prefixKeySerializer,\n                                                                               final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "flush1",
                            "method_signature": "private flush1()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "openRocksDB",
                            "method_signature": " openRocksDB(final DBOptions dbOptions,\n                     final ColumnFamilyOptions columnFamilyOptions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "all",
                            "method_signature": "private all(final boolean forward,\n                                                final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "range",
                            "method_signature": "private range(final Bytes from,\n                                                  final Bytes to,\n                                                  final boolean forward,\n                                                  final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "get",
                            "method_signature": "public synchronized get(final Bytes key, final ReadOptions readOptions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "openDB",
                            "method_signature": "@SuppressWarnings(\"unchecked\") openDB(final Map<String, Object> configs, final File stateDir)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "reverseAll",
                            "method_signature": " reverseAll(final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addValueProvidersToMetricsRecorder",
                            "method_signature": "private addValueProvidersToMetricsRecorder()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "all",
                            "method_signature": "synchronized all(final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validateStoreOpen",
                            "method_signature": "private validateStoreOpen()",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private isOverflowing(final long value)": {
                    "first": {
                        "method_name": "isOverflowing",
                        "method_signature": "private isOverflowing(final long value)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.29004298710595366
                },
                " deleteRange(final Bytes keyFrom, final Bytes keyTo)": {
                    "first": {
                        "method_name": "deleteRange",
                        "method_signature": " deleteRange(final Bytes keyFrom, final Bytes keyTo)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3042104850701185
                },
                "public releaseSnapshot(final Snapshot snapshot)": {
                    "first": {
                        "method_name": "releaseSnapshot",
                        "method_signature": "public releaseSnapshot(final Snapshot snapshot)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4198370588587613
                },
                "static incrementWithoutOverflow(final Bytes input)": {
                    "first": {
                        "method_name": "incrementWithoutOverflow",
                        "method_signature": "static incrementWithoutOverflow(final Bytes input)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4440888130515857
                },
                " doPrefixScan(final P prefix,\n                                                                               final PS prefixKeySerializer,\n                                                                               final Set<KeyValueIterator<Bytes, byte[]>> openIterators)": {
                    "first": {
                        "method_name": "doPrefixScan",
                        "method_signature": " doPrefixScan(final P prefix,\n                                                                               final PS prefixKeySerializer,\n                                                                               final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4449955944444812
                },
                "private flush1()": {
                    "first": {
                        "method_name": "flush1",
                        "method_signature": "private flush1()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.44942936593739335
                },
                " openRocksDB(final DBOptions dbOptions,\n                     final ColumnFamilyOptions columnFamilyOptions)": {
                    "first": {
                        "method_name": "openRocksDB",
                        "method_signature": " openRocksDB(final DBOptions dbOptions,\n                     final ColumnFamilyOptions columnFamilyOptions)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4734198768830058
                },
                "private all(final boolean forward,\n                                                final Set<KeyValueIterator<Bytes, byte[]>> openIterators)": {
                    "first": {
                        "method_name": "all",
                        "method_signature": "private all(final boolean forward,\n                                                final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4788386968464887
                },
                "private range(final Bytes from,\n                                                  final Bytes to,\n                                                  final boolean forward,\n                                                  final Set<KeyValueIterator<Bytes, byte[]>> openIterators)": {
                    "first": {
                        "method_name": "range",
                        "method_signature": "private range(final Bytes from,\n                                                  final Bytes to,\n                                                  final boolean forward,\n                                                  final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4989573922600574
                },
                "public synchronized get(final Bytes key, final ReadOptions readOptions)": {
                    "first": {
                        "method_name": "get",
                        "method_signature": "public synchronized get(final Bytes key, final ReadOptions readOptions)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5088792644482173
                },
                "@SuppressWarnings(\"unchecked\") openDB(final Map<String, Object> configs, final File stateDir)": {
                    "first": {
                        "method_name": "openDB",
                        "method_signature": "@SuppressWarnings(\"unchecked\") openDB(final Map<String, Object> configs, final File stateDir)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5118938007288463
                },
                " reverseAll(final Set<KeyValueIterator<Bytes, byte[]>> openIterators)": {
                    "first": {
                        "method_name": "reverseAll",
                        "method_signature": " reverseAll(final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5379253205255945
                },
                "private addValueProvidersToMetricsRecorder()": {
                    "first": {
                        "method_name": "addValueProvidersToMetricsRecorder",
                        "method_signature": "private addValueProvidersToMetricsRecorder()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5392917323762345
                },
                "synchronized all(final Set<KeyValueIterator<Bytes, byte[]>> openIterators)": {
                    "first": {
                        "method_name": "all",
                        "method_signature": "synchronized all(final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.539709978508523
                },
                "private validateStoreOpen()": {
                    "first": {
                        "method_name": "validateStoreOpen",
                        "method_signature": "private validateStoreOpen()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5412584880882635
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [],
                "llm_response_time": 4518
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "e45e032b8d90435de2f338f77a732c88f8cca66e",
        "url": "https://github.com/apache/kafka/commit/e45e032b8d90435de2f338f77a732c88f8cca66e",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public maybeThrowAsyncError() : void extracted from private deliverMessages() : void in class org.apache.kafka.connect.runtime.WorkerSinkTask & moved to class org.apache.kafka.connect.runtime.errors.WorkerErrantRecordReporter",
            "leftSideLocations": [
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java",
                    "startLine": 595,
                    "endLine": 634,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private deliverMessages() : void"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java",
                    "startLine": 605,
                    "endLine": 606,
                    "startColumn": 17,
                    "endColumn": 57,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java",
                    "startLine": 604,
                    "endLine": 607,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java",
                    "startLine": 604,
                    "endLine": 607,
                    "startColumn": 109,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java",
                    "startLine": 162,
                    "endLine": 166,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public maybeThrowAsyncError() : void"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java",
                    "startLine": 164,
                    "endLine": 164,
                    "startColumn": 13,
                    "endColumn": 103,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java",
                    "startLine": 163,
                    "endLine": 165,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java",
                    "startLine": 163,
                    "endLine": 165,
                    "startColumn": 100,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java",
                    "startLine": 595,
                    "endLine": 633,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private deliverMessages() : void"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java",
                    "startLine": 605,
                    "endLine": 605,
                    "startColumn": 17,
                    "endColumn": 66,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "workerErrantRecordReporter.maybeThrowAsyncError()"
                }
            ],
            "isStatic": false
        },
        "ref_id": 567,
        "extraction_results": {
            "success": true,
            "newCommitHash": "1f1c194190a2c0be33606d4db7188eb04925d32a",
            "newBranchName": "extract-maybeThrowAsyncError-deliverMessages-72b7028"
        },
        "telemetry": {
            "id": "39572661-3997-42c5-8f51-c5bf37f9877f",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 862,
                "lineStart": 72,
                "lineEnd": 933,
                "bodyLineStart": 72,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java",
                "sourceCode": "/**\n * {@link WorkerTask} that uses a {@link SinkTask} to export data from Kafka.\n */\nclass WorkerSinkTask extends WorkerTask {\n    private static final Logger log = LoggerFactory.getLogger(WorkerSinkTask.class);\n\n    private final WorkerConfig workerConfig;\n    private final SinkTask task;\n    private final ClusterConfigState configState;\n    private Map<String, String> taskConfig;\n    private final Converter keyConverter;\n    private final Converter valueConverter;\n    private final HeaderConverter headerConverter;\n    private final TransformationChain<SinkRecord> transformationChain;\n    private final SinkTaskMetricsGroup sinkTaskMetricsGroup;\n    private final boolean isTopicTrackingEnabled;\n    private final Consumer<byte[], byte[]> consumer;\n    private WorkerSinkTaskContext context;\n    private final List<SinkRecord> messageBatch;\n    private final Map<TopicPartition, OffsetAndMetadata> lastCommittedOffsets;\n    private final Map<TopicPartition, OffsetAndMetadata> currentOffsets;\n    private final Map<TopicPartition, OffsetAndMetadata> origOffsets;\n    private RuntimeException rebalanceException;\n    private long nextCommit;\n    private int commitSeqno;\n    private long commitStarted;\n    private int commitFailures;\n    private boolean pausedForRedelivery;\n    private boolean committing;\n    private boolean taskStopped;\n    private final WorkerErrantRecordReporter workerErrantRecordReporter;\n    private final Supplier<List<ErrorReporter>> errorReportersSupplier;\n\n    public WorkerSinkTask(ConnectorTaskId id,\n                          SinkTask task,\n                          TaskStatus.Listener statusListener,\n                          TargetState initialState,\n                          WorkerConfig workerConfig,\n                          ClusterConfigState configState,\n                          ConnectMetrics connectMetrics,\n                          Converter keyConverter,\n                          Converter valueConverter,\n                          ErrorHandlingMetrics errorMetrics,\n                          HeaderConverter headerConverter,\n                          TransformationChain<SinkRecord> transformationChain,\n                          Consumer<byte[], byte[]> consumer,\n                          ClassLoader loader,\n                          Time time,\n                          RetryWithToleranceOperator retryWithToleranceOperator,\n                          WorkerErrantRecordReporter workerErrantRecordReporter,\n                          StatusBackingStore statusBackingStore,\n                          Supplier<List<ErrorReporter>> errorReportersSupplier) {\n        super(id, statusListener, initialState, loader, connectMetrics, errorMetrics,\n                retryWithToleranceOperator, time, statusBackingStore);\n\n        this.workerConfig = workerConfig;\n        this.task = task;\n        this.configState = configState;\n        this.keyConverter = keyConverter;\n        this.valueConverter = valueConverter;\n        this.headerConverter = headerConverter;\n        this.transformationChain = transformationChain;\n        this.messageBatch = new ArrayList<>();\n        this.lastCommittedOffsets = new HashMap<>();\n        this.currentOffsets = new HashMap<>();\n        this.origOffsets = new HashMap<>();\n        this.pausedForRedelivery = false;\n        this.rebalanceException = null;\n        this.nextCommit = time.milliseconds() +\n                workerConfig.getLong(WorkerConfig.OFFSET_COMMIT_INTERVAL_MS_CONFIG);\n        this.committing = false;\n        this.commitSeqno = 0;\n        this.commitStarted = -1;\n        this.commitFailures = 0;\n        this.sinkTaskMetricsGroup = new SinkTaskMetricsGroup(id, connectMetrics);\n        this.sinkTaskMetricsGroup.recordOffsetSequenceNumber(commitSeqno);\n        this.consumer = consumer;\n        this.isTopicTrackingEnabled = workerConfig.getBoolean(TOPIC_TRACKING_ENABLE_CONFIG);\n        this.taskStopped = false;\n        this.workerErrantRecordReporter = workerErrantRecordReporter;\n        this.errorReportersSupplier = errorReportersSupplier;\n    }\n\n    @Override\n    public void initialize(TaskConfig taskConfig) {\n        try {\n            this.taskConfig = taskConfig.originalsStrings();\n            this.context = new WorkerSinkTaskContext(consumer, this, configState);\n        } catch (Throwable t) {\n            log.error(\"{} Task failed initialization and will not be started.\", this, t);\n            onFailure(t);\n        }\n    }\n\n    @Override\n    public void stop() {\n        // Offset commit is handled upon exit in work thread\n        super.stop();\n        consumer.wakeup();\n    }\n\n    @Override\n    protected void close() {\n        // FIXME Kafka needs to add a timeout parameter here for us to properly obey the timeout\n        // passed in\n        try {\n            task.stop();\n        } catch (Throwable t) {\n            log.warn(\"Could not stop task\", t);\n        }\n        taskStopped = true;\n        Utils.closeQuietly(consumer, \"consumer\");\n        Utils.closeQuietly(transformationChain, \"transformation chain\");\n        Utils.closeQuietly(retryWithToleranceOperator, \"retry operator\");\n        Utils.closeQuietly(headerConverter, \"header converter\");\n        /*\n            Setting partition count explicitly to 0 to handle the case,\n            when the task fails, which would cause its consumer to leave the group.\n            This would cause onPartitionsRevoked to be invoked in the rebalance listener, but not onPartitionsAssigned,\n            so the metrics for the task (which are still available for failed tasks until they are explicitly revoked\n            from the worker) would become inaccurate.\n        */\n        sinkTaskMetricsGroup.recordPartitionCount(0);\n    }\n\n    @Override\n    public void removeMetrics() {\n        try {\n            sinkTaskMetricsGroup.close();\n        } finally {\n            super.removeMetrics();\n        }\n    }\n\n    @Override\n    public void transitionTo(TargetState state) {\n        super.transitionTo(state);\n        consumer.wakeup();\n    }\n\n    @Override\n    public void execute() {\n        log.info(\"{} Executing sink task\", this);\n        // Make sure any uncommitted data has been committed and the task has\n        // a chance to clean up its state\n        try (UncheckedCloseable suppressible = this::closeAllPartitions) {\n            while (!isStopping())\n                iteration();\n        } catch (WakeupException e) {\n            log.trace(\"Consumer woken up during initial offset commit attempt, \" \n                + \"but succeeded during a later attempt\");\n        }\n    }\n\n    protected void iteration() {\n        final long offsetCommitIntervalMs = workerConfig.getLong(WorkerConfig.OFFSET_COMMIT_INTERVAL_MS_CONFIG);\n\n        try {\n            long now = time.milliseconds();\n\n            // Maybe commit\n            if (!committing && (context.isCommitRequested() || now >= nextCommit)) {\n                commitOffsets(now, false);\n                nextCommit = now + offsetCommitIntervalMs;\n                context.clearCommitRequest();\n            }\n\n            final long commitTimeoutMs = commitStarted + workerConfig.getLong(WorkerConfig.OFFSET_COMMIT_TIMEOUT_MS_CONFIG);\n\n            // Check for timed out commits\n            if (committing && now >= commitTimeoutMs) {\n                log.warn(\"{} Commit of offsets timed out\", this);\n                commitFailures++;\n                committing = false;\n            }\n\n            // And process messages\n            long timeoutMs = Math.max(nextCommit - now, 0);\n            poll(timeoutMs);\n        } catch (WakeupException we) {\n            log.trace(\"{} Consumer woken up\", this);\n\n            if (isStopping())\n                return;\n\n            if (shouldPause()) {\n                pauseAll();\n                onPause();\n                context.requestCommit();\n            } else if (!pausedForRedelivery) {\n                resumeAll();\n                onResume();\n            }\n        }\n    }\n\n    /**\n     * Respond to a previous commit attempt that may or may not have succeeded. Note that due to our use of async commits,\n     * these invocations may come out of order and thus the need for the commit sequence number.\n     *\n     * @param error            the error resulting from the commit, or null if the commit succeeded without error\n     * @param seqno            the sequence number at the time the commit was requested\n     * @param committedOffsets the offsets that were committed; may be null if the commit did not complete successfully\n     *                         or if no new offsets were committed\n     */\n    private void onCommitCompleted(Throwable error, long seqno, Map<TopicPartition, OffsetAndMetadata> committedOffsets) {\n        if (commitSeqno != seqno) {\n            log.debug(\"{} Received out of order commit callback for sequence number {}, but most recent sequence number is {}\",\n                    this, seqno, commitSeqno);\n            sinkTaskMetricsGroup.recordOffsetCommitSkip();\n        } else {\n            long durationMillis = time.milliseconds() - commitStarted;\n            if (error != null) {\n                log.error(\"{} Commit of offsets threw an unexpected exception for sequence number {}: {}\",\n                        this, seqno, committedOffsets, error);\n                commitFailures++;\n                recordCommitFailure(durationMillis, error);\n            } else {\n                log.debug(\"{} Finished offset commit successfully in {} ms for sequence number {}: {}\",\n                        this, durationMillis, seqno, committedOffsets);\n                if (committedOffsets != null) {\n                    log.trace(\"{} Adding to last committed offsets: {}\", this, committedOffsets);\n                    lastCommittedOffsets.putAll(committedOffsets);\n                    log.debug(\"{} Last committed offsets are now {}\", this, committedOffsets);\n                    sinkTaskMetricsGroup.recordCommittedOffsets(committedOffsets);\n                }\n                commitFailures = 0;\n                recordCommitSuccess(durationMillis);\n            }\n            committing = false;\n        }\n    }\n\n    public int commitFailures() {\n        return commitFailures;\n    }\n\n    /**\n     * Initializes and starts the SinkTask.\n     */\n    @Override\n    protected void initializeAndStart() {\n        SinkConnectorConfig.validate(taskConfig);\n        retryWithToleranceOperator.reporters(errorReportersSupplier.get());\n\n        if (SinkConnectorConfig.hasTopicsConfig(taskConfig)) {\n            List<String> topics = SinkConnectorConfig.parseTopicsList(taskConfig);\n            consumer.subscribe(topics, new HandleRebalance());\n            log.debug(\"{} Initializing and starting task for topics {}\", this, Utils.join(topics, \", \"));\n        } else {\n            String topicsRegexStr = taskConfig.get(SinkTask.TOPICS_REGEX_CONFIG);\n            Pattern pattern = Pattern.compile(topicsRegexStr);\n            consumer.subscribe(pattern, new HandleRebalance());\n            log.debug(\"{} Initializing and starting task for topics regex {}\", this, topicsRegexStr);\n        }\n\n        task.initialize(context);\n        task.start(taskConfig);\n        log.info(\"{} Sink task finished initialization and start\", this);\n    }\n\n    /**\n     * Poll for new messages with the given timeout. Should only be invoked by the worker thread.\n     */\n    protected void poll(long timeoutMs) {\n        rewind();\n        long retryTimeout = context.timeout();\n        if (retryTimeout > 0) {\n            timeoutMs = Math.min(timeoutMs, retryTimeout);\n            context.timeout(-1L);\n        }\n\n        log.trace(\"{} Polling consumer with timeout {} ms\", this, timeoutMs);\n        ConsumerRecords<byte[], byte[]> msgs = pollConsumer(timeoutMs);\n        assert messageBatch.isEmpty() || msgs.isEmpty();\n        log.trace(\"{} Polling returned {} messages\", this, msgs.count());\n\n        convertMessages(msgs);\n        deliverMessages();\n    }\n\n    // Visible for testing\n    boolean isCommitting() {\n        return committing;\n    }\n\n    private void doCommitSync(Map<TopicPartition, OffsetAndMetadata> offsets, int seqno) {\n        log.debug(\"{} Committing offsets synchronously using sequence number {}: {}\", this, seqno, offsets);\n        try {\n            consumer.commitSync(offsets);\n            onCommitCompleted(null, seqno, offsets);\n        } catch (WakeupException e) {\n            // retry the commit to ensure offsets get pushed, then propagate the wakeup up to poll\n            doCommitSync(offsets, seqno);\n            throw e;\n        } catch (KafkaException e) {\n            onCommitCompleted(e, seqno, offsets);\n        }\n    }\n\n    private void doCommitAsync(Map<TopicPartition, OffsetAndMetadata> offsets, final int seqno) {\n        log.debug(\"{} Committing offsets asynchronously using sequence number {}: {}\", this, seqno, offsets);\n        OffsetCommitCallback cb = (tpOffsets, error) -> onCommitCompleted(error, seqno, tpOffsets);\n        consumer.commitAsync(offsets, cb);\n    }\n\n    /**\n     * Starts an offset commit by flushing outstanding messages from the task and then starting\n     * the write commit.\n     */\n    private void doCommit(Map<TopicPartition, OffsetAndMetadata> offsets, boolean closing, int seqno) {\n        if (isCancelled()) {\n            log.debug(\"Skipping final offset commit as task has been cancelled\");\n            return;\n        }\n        if (closing) {\n            doCommitSync(offsets, seqno);\n        } else {\n            doCommitAsync(offsets, seqno);\n        }\n    }\n\n    private void commitOffsets(long now, boolean closing) {\n        commitOffsets(now, closing, consumer.assignment());\n    }\n\n    private void commitOffsets(long now, boolean closing, Collection<TopicPartition> topicPartitions) {\n        log.trace(\"Committing offsets for partitions {}\", topicPartitions);\n        if (workerErrantRecordReporter != null) {\n            log.trace(\"Awaiting reported errors to be completed\");\n            workerErrantRecordReporter.awaitFutures(topicPartitions);\n            log.trace(\"Completed reported errors\");\n        }\n\n        Map<TopicPartition, OffsetAndMetadata> offsetsToCommit = currentOffsets.entrySet().stream()\n            .filter(e -> topicPartitions.contains(e.getKey()))\n            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n\n        if (offsetsToCommit.isEmpty())\n            return;\n\n        committing = true;\n        commitSeqno += 1;\n        commitStarted = now;\n        sinkTaskMetricsGroup.recordOffsetSequenceNumber(commitSeqno);\n\n        Map<TopicPartition, OffsetAndMetadata> lastCommittedOffsetsForPartitions = this.lastCommittedOffsets.entrySet().stream()\n            .filter(e -> offsetsToCommit.containsKey(e.getKey()))\n            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n\n        final Map<TopicPartition, OffsetAndMetadata> taskProvidedOffsets;\n        try {\n            log.trace(\"{} Calling task.preCommit with current offsets: {}\", this, offsetsToCommit);\n            taskProvidedOffsets = task.preCommit(new HashMap<>(offsetsToCommit));\n        } catch (Throwable t) {\n            if (closing) {\n                log.warn(\"{} Offset commit failed during close\", this);\n            } else {\n                log.error(\"{} Offset commit failed, rewinding to last committed offsets\", this, t);\n                for (Map.Entry<TopicPartition, OffsetAndMetadata> entry : lastCommittedOffsetsForPartitions.entrySet()) {\n                    log.debug(\"{} Rewinding topic partition {} to offset {}\", this, entry.getKey(), entry.getValue().offset());\n                    consumer.seek(entry.getKey(), entry.getValue().offset());\n                }\n                currentOffsets.putAll(lastCommittedOffsetsForPartitions);\n            }\n            onCommitCompleted(t, commitSeqno, null);\n            return;\n        } finally {\n            if (closing) {\n                log.trace(\"{} Closing the task before committing the offsets: {}\", this, offsetsToCommit);\n                task.close(topicPartitions);\n            }\n        }\n\n        if (taskProvidedOffsets.isEmpty()) {\n            log.debug(\"{} Skipping offset commit, task opted-out by returning no offsets from preCommit\", this);\n            onCommitCompleted(null, commitSeqno, null);\n            return;\n        }\n\n        Collection<TopicPartition> allAssignedTopicPartitions = consumer.assignment();\n        final Map<TopicPartition, OffsetAndMetadata> committableOffsets = new HashMap<>(lastCommittedOffsetsForPartitions);\n        for (Map.Entry<TopicPartition, OffsetAndMetadata> taskProvidedOffsetEntry : taskProvidedOffsets.entrySet()) {\n            final TopicPartition partition = taskProvidedOffsetEntry.getKey();\n            final OffsetAndMetadata taskProvidedOffset = taskProvidedOffsetEntry.getValue();\n            if (committableOffsets.containsKey(partition)) {\n                long taskOffset = taskProvidedOffset.offset();\n                long currentOffset = offsetsToCommit.get(partition).offset();\n                if (taskOffset <= currentOffset) {\n                    committableOffsets.put(partition, taskProvidedOffset);\n                } else {\n                    log.warn(\"{} Ignoring invalid task provided offset {}/{} -- not yet consumed, taskOffset={} currentOffset={}\",\n                        this, partition, taskProvidedOffset, taskOffset, currentOffset);\n                }\n            } else if (!allAssignedTopicPartitions.contains(partition)) {\n                log.warn(\"{} Ignoring invalid task provided offset {}/{} -- partition not assigned, assignment={}\",\n                        this, partition, taskProvidedOffset, allAssignedTopicPartitions);\n            } else {\n                log.debug(\"{} Ignoring task provided offset {}/{} -- partition not requested, requested={}\",\n                        this, partition, taskProvidedOffset, committableOffsets.keySet());\n            }\n        }\n\n        if (committableOffsets.equals(lastCommittedOffsetsForPartitions)) {\n            log.debug(\"{} Skipping offset commit, no change since last commit\", this);\n            onCommitCompleted(null, commitSeqno, null);\n            return;\n        }\n\n        doCommit(committableOffsets, closing, commitSeqno);\n    }\n\n\n    @Override\n    public String toString() {\n        return \"WorkerSinkTask{\" +\n                \"id=\" + id +\n                '}';\n    }\n\n    private ConsumerRecords<byte[], byte[]> pollConsumer(long timeoutMs) {\n        ConsumerRecords<byte[], byte[]> msgs = consumer.poll(Duration.ofMillis(timeoutMs));\n\n        // Exceptions raised from the task during a rebalance should be rethrown to stop the task and mark it as failed\n        if (rebalanceException != null) {\n            RuntimeException e = rebalanceException;\n            rebalanceException = null;\n            throw e;\n        }\n\n        sinkTaskMetricsGroup.recordRead(msgs.count());\n        return msgs;\n    }\n\n    private void convertMessages(ConsumerRecords<byte[], byte[]> msgs) {\n        for (ConsumerRecord<byte[], byte[]> msg : msgs) {\n            log.trace(\"{} Consuming and converting message in topic '{}' partition {} at offset {} and timestamp {}\",\n                    this, msg.topic(), msg.partition(), msg.offset(), msg.timestamp());\n\n            retryWithToleranceOperator.consumerRecord(msg);\n\n            SinkRecord transRecord = convertAndTransformRecord(msg);\n\n            origOffsets.put(\n                    new TopicPartition(msg.topic(), msg.partition()),\n                    new OffsetAndMetadata(msg.offset() + 1)\n            );\n            if (transRecord != null) {\n                messageBatch.add(transRecord);\n            } else {\n                log.trace(\n                        \"{} Converters and transformations returned null, possibly because of too many retries, so \" +\n                                \"dropping record in topic '{}' partition {} at offset {}\",\n                        this, msg.topic(), msg.partition(), msg.offset()\n                );\n            }\n        }\n        sinkTaskMetricsGroup.recordConsumedOffsets(origOffsets);\n    }\n\n    private SinkRecord convertAndTransformRecord(final ConsumerRecord<byte[], byte[]> msg) {\n        SchemaAndValue keyAndSchema = retryWithToleranceOperator.execute(() -> keyConverter.toConnectData(msg.topic(), msg.headers(), msg.key()),\n                Stage.KEY_CONVERTER, keyConverter.getClass());\n\n        SchemaAndValue valueAndSchema = retryWithToleranceOperator.execute(() -> valueConverter.toConnectData(msg.topic(), msg.headers(), msg.value()),\n                Stage.VALUE_CONVERTER, valueConverter.getClass());\n\n        Headers headers = retryWithToleranceOperator.execute(() -> convertHeadersFor(msg), Stage.HEADER_CONVERTER, headerConverter.getClass());\n\n        if (retryWithToleranceOperator.failed()) {\n            return null;\n        }\n\n        Long timestamp = ConnectUtils.checkAndConvertTimestamp(msg.timestamp());\n        SinkRecord origRecord = new SinkRecord(msg.topic(), msg.partition(),\n                keyAndSchema.schema(), keyAndSchema.value(),\n                valueAndSchema.schema(), valueAndSchema.value(),\n                msg.offset(),\n                timestamp,\n                msg.timestampType(),\n                headers);\n        log.trace(\"{} Applying transformations to record in topic '{}' partition {} at offset {} and timestamp {} with key {} and value {}\",\n                this, msg.topic(), msg.partition(), msg.offset(), timestamp, keyAndSchema.value(), valueAndSchema.value());\n        if (isTopicTrackingEnabled) {\n            recordActiveTopic(origRecord.topic());\n        }\n\n        // Apply the transformations\n        SinkRecord transformedRecord = transformationChain.apply(origRecord);\n        if (transformedRecord == null) {\n            return null;\n        }\n        // Error reporting will need to correlate each sink record with the original consumer record\n        return new InternalSinkRecord(msg, transformedRecord);\n    }\n\n    private Headers convertHeadersFor(ConsumerRecord<byte[], byte[]> record) {\n        Headers result = new ConnectHeaders();\n        org.apache.kafka.common.header.Headers recordHeaders = record.headers();\n        if (recordHeaders != null) {\n            String topic = record.topic();\n            for (org.apache.kafka.common.header.Header recordHeader : recordHeaders) {\n                SchemaAndValue schemaAndValue = headerConverter.toConnectHeader(topic, recordHeader.key(), recordHeader.value());\n                result.add(recordHeader.key(), schemaAndValue);\n            }\n        }\n        return result;\n    }\n\n    protected WorkerErrantRecordReporter workerErrantRecordReporter() {\n        return workerErrantRecordReporter;\n    }\n\n    private void resumeAll() {\n        for (TopicPartition tp : consumer.assignment())\n            if (!context.pausedPartitions().contains(tp))\n                consumer.resume(singleton(tp));\n    }\n\n    private void pauseAll() {\n        consumer.pause(consumer.assignment());\n    }\n\n    private void deliverMessages() {\n        // Finally, deliver this batch to the sink\n        try {\n            // Since we reuse the messageBatch buffer, ensure we give the task its own copy\n            log.trace(\"{} Delivering batch of {} messages to task\", this, messageBatch.size());\n            long start = time.milliseconds();\n            task.put(new ArrayList<>(messageBatch));\n            // if errors raised from the operator were swallowed by the task implementation, an\n            // exception needs to be thrown to kill the task indicating the tolerance was exceeded\n            maybeThrowAsyncError();\n            recordBatch(messageBatch.size());\n            sinkTaskMetricsGroup.recordPut(time.milliseconds() - start);\n            currentOffsets.putAll(origOffsets);\n            origOffsets.clear();\n            messageBatch.clear();\n            // If we had paused all consumer topic partitions to try to redeliver data, then we should resume any that\n            // the task had not explicitly paused\n            if (pausedForRedelivery) {\n                if (!shouldPause())\n                    resumeAll();\n                pausedForRedelivery = false;\n            }\n        } catch (RetriableException e) {\n            log.error(\"{} RetriableException from SinkTask:\", this, e);\n            if (!pausedForRedelivery) {\n                // If we're retrying a previous batch, make sure we've paused all topic partitions so we don't get new data,\n                // but will still be able to poll in order to handle user-requested timeouts, keep group membership, etc.\n                pausedForRedelivery = true;\n                pauseAll();\n            }\n            // Let this exit normally, the batch will be reprocessed on the next loop.\n        } catch (Throwable t) {\n            log.error(\"{} Task threw an uncaught and unrecoverable exception. Task is being killed and will not \"\n                    + \"recover until manually restarted. Error: {}\", this, t.getMessage(), t);\n            throw new ConnectException(\"Exiting WorkerSinkTask due to unrecoverable exception.\", t);\n        }\n    }\n\n    private void maybeThrowAsyncError() {\n        if (retryWithToleranceOperator.failed() && !retryWithToleranceOperator.withinToleranceLimits()) {\n            throw new ConnectException(\"Tolerance exceeded in error handler\",\n                retryWithToleranceOperator.error());\n        }\n    }\n\n    private void rewind() {\n        Map<TopicPartition, Long> offsets = context.offsets();\n        if (offsets.isEmpty()) {\n            return;\n        }\n        for (Map.Entry<TopicPartition, Long> entry: offsets.entrySet()) {\n            TopicPartition tp = entry.getKey();\n            Long offset = entry.getValue();\n            if (offset != null) {\n                log.trace(\"{} Rewind {} to offset {}\", this, tp, offset);\n                consumer.seek(tp, offset);\n                lastCommittedOffsets.put(tp, new OffsetAndMetadata(offset));\n                currentOffsets.put(tp, new OffsetAndMetadata(offset));\n            } else {\n                log.warn(\"{} Cannot rewind {} to null offset\", this, tp);\n            }\n        }\n        context.clearOffsets();\n    }\n\n    private void openPartitions(Collection<TopicPartition> partitions) {\n        task.open(partitions);\n    }\n\n    private void closeAllPartitions() {\n        closePartitions(currentOffsets.keySet(), false);\n    }\n\n    private void closePartitions(Collection<TopicPartition> topicPartitions, boolean lost) {\n        if (!lost) {\n            commitOffsets(time.milliseconds(), true, topicPartitions);\n        } else {\n            log.trace(\"{} Closing the task as partitions have been lost: {}\", this, topicPartitions);\n            task.close(topicPartitions);\n            if (workerErrantRecordReporter != null) {\n                log.trace(\"Cancelling reported errors for {}\", topicPartitions);\n                workerErrantRecordReporter.cancelFutures(topicPartitions);\n                log.trace(\"Cancelled all reported errors for {}\", topicPartitions);\n            }\n            origOffsets.keySet().removeAll(topicPartitions);\n            currentOffsets.keySet().removeAll(topicPartitions);\n        }\n        lastCommittedOffsets.keySet().removeAll(topicPartitions);\n    }\n\n    private void updatePartitionCount() {\n        sinkTaskMetricsGroup.recordPartitionCount(consumer.assignment().size());\n    }\n\n    @Override\n    protected void recordBatch(int size) {\n        super.recordBatch(size);\n        sinkTaskMetricsGroup.recordSend(size);\n    }\n\n    @Override\n    protected void recordCommitSuccess(long duration) {\n        super.recordCommitSuccess(duration);\n        sinkTaskMetricsGroup.recordOffsetCommitSuccess();\n    }\n\n    SinkTaskMetricsGroup sinkTaskMetricsGroup() {\n        return sinkTaskMetricsGroup;\n    }\n\n    // Visible for testing\n    long getNextCommit() {\n        return nextCommit;\n    }\n\n    private class HandleRebalance implements ConsumerRebalanceListener {\n        @Override\n        public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n            log.debug(\"{} Partitions assigned {}\", WorkerSinkTask.this, partitions);\n\n            for (TopicPartition tp : partitions) {\n                long pos = consumer.position(tp);\n                lastCommittedOffsets.put(tp, new OffsetAndMetadata(pos));\n                currentOffsets.put(tp, new OffsetAndMetadata(pos));\n                log.debug(\"{} Assigned topic partition {} with offset {}\", WorkerSinkTask.this, tp, pos);\n            }\n            sinkTaskMetricsGroup.assignedOffsets(currentOffsets);\n\n            boolean wasPausedForRedelivery = pausedForRedelivery;\n            pausedForRedelivery = wasPausedForRedelivery && !messageBatch.isEmpty();\n            if (pausedForRedelivery) {\n                // Re-pause here in case we picked up new partitions in the rebalance\n                pauseAll();\n            } else {\n                // If we paused everything for redelivery and all partitions for the failed deliveries have been revoked, make\n                // sure anything we paused that the task didn't request to be paused *and* which we still own is resumed.\n                // Also make sure our tracking of paused partitions is updated to remove any partitions we no longer own.\n                if (wasPausedForRedelivery) {\n                    resumeAll();\n                }\n                // Ensure that the paused partitions contains only assigned partitions and repause as necessary\n                context.pausedPartitions().retainAll(consumer.assignment());\n                if (shouldPause())\n                    pauseAll();\n                else if (!context.pausedPartitions().isEmpty())\n                    consumer.pause(context.pausedPartitions());\n            }\n            updatePartitionCount();\n            if (partitions.isEmpty()) {\n                return;\n            }\n\n            // Instead of invoking the assignment callback on initialization, we guarantee the consumer is ready upon\n            // task start. Since this callback gets invoked during that initial setup before we've started the task, we\n            // need to guard against invoking the user's callback method during that period.\n            if (rebalanceException == null || rebalanceException instanceof WakeupException) {\n                try {\n                    openPartitions(partitions);\n                    // Rewind should be applied only if openPartitions succeeds.\n                    rewind();\n                } catch (RuntimeException e) {\n                    // The consumer swallows exceptions raised in the rebalance listener, so we need to store\n                    // exceptions and rethrow when poll() returns.\n                    rebalanceException = e;\n                }\n            }\n        }\n\n        @Override\n        public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n            onPartitionsRemoved(partitions, false);\n        }\n\n        @Override\n        public void onPartitionsLost(Collection<TopicPartition> partitions) {\n            onPartitionsRemoved(partitions, true);\n        }\n\n        private void onPartitionsRemoved(Collection<TopicPartition> partitions, boolean lost) {\n            if (taskStopped) {\n                log.trace(\"Skipping partition revocation callback as task has already been stopped\");\n                return;\n            }\n            log.debug(\"{} Partitions {}: {}\", WorkerSinkTask.this, lost ? \"lost\" : \"revoked\", partitions);\n\n            if (partitions.isEmpty())\n                return;\n\n            try {\n                closePartitions(partitions, lost);\n                sinkTaskMetricsGroup.clearOffsets(partitions);\n            } catch (RuntimeException e) {\n                // The consumer swallows exceptions raised in the rebalance listener, so we need to store\n                // exceptions and rethrow when poll() returns.\n                rebalanceException = e;\n            }\n\n            // Make sure we don't have any leftover data since offsets for these partitions will be reset to committed positions\n            messageBatch.removeIf(record -> partitions.contains(new TopicPartition(record.topic(), record.kafkaPartition())));\n        }\n    }\n\n    static class SinkTaskMetricsGroup {\n        private final ConnectorTaskId id;\n        private final ConnectMetrics metrics;\n        private final MetricGroup metricGroup;\n        private final Sensor sinkRecordRead;\n        private final Sensor sinkRecordSend;\n        private final Sensor partitionCount;\n        private final Sensor offsetSeqNum;\n        private final Sensor offsetCompletion;\n        private final Sensor offsetCompletionSkip;\n        private final Sensor putBatchTime;\n        private final Sensor sinkRecordActiveCount;\n        private long activeRecords;\n        private Map<TopicPartition, OffsetAndMetadata> consumedOffsets = new HashMap<>();\n        private Map<TopicPartition, OffsetAndMetadata> committedOffsets = new HashMap<>();\n\n        public SinkTaskMetricsGroup(ConnectorTaskId id, ConnectMetrics connectMetrics) {\n            this.metrics = connectMetrics;\n            this.id = id;\n\n            ConnectMetricsRegistry registry = connectMetrics.registry();\n            metricGroup = connectMetrics\n                                  .group(registry.sinkTaskGroupName(), registry.connectorTagName(), id.connector(), registry.taskTagName(),\n                                         Integer.toString(id.task()));\n            // prevent collisions by removing any previously created metrics in this group.\n            metricGroup.close();\n\n            sinkRecordRead = metricGroup.sensor(\"sink-record-read\");\n            sinkRecordRead.add(metricGroup.metricName(registry.sinkRecordReadRate), new Rate());\n            sinkRecordRead.add(metricGroup.metricName(registry.sinkRecordReadTotal), new CumulativeSum());\n\n            sinkRecordSend = metricGroup.sensor(\"sink-record-send\");\n            sinkRecordSend.add(metricGroup.metricName(registry.sinkRecordSendRate), new Rate());\n            sinkRecordSend.add(metricGroup.metricName(registry.sinkRecordSendTotal), new CumulativeSum());\n\n            sinkRecordActiveCount = metricGroup.sensor(\"sink-record-active-count\");\n            sinkRecordActiveCount.add(metricGroup.metricName(registry.sinkRecordActiveCount), new Value());\n            sinkRecordActiveCount.add(metricGroup.metricName(registry.sinkRecordActiveCountMax), new Max());\n            sinkRecordActiveCount.add(metricGroup.metricName(registry.sinkRecordActiveCountAvg), new Avg());\n\n            partitionCount = metricGroup.sensor(\"partition-count\");\n            partitionCount.add(metricGroup.metricName(registry.sinkRecordPartitionCount), new Value());\n\n            offsetSeqNum = metricGroup.sensor(\"offset-seq-number\");\n            offsetSeqNum.add(metricGroup.metricName(registry.sinkRecordOffsetCommitSeqNum), new Value());\n\n            offsetCompletion = metricGroup.sensor(\"offset-commit-completion\");\n            offsetCompletion.add(metricGroup.metricName(registry.sinkRecordOffsetCommitCompletionRate), new Rate());\n            offsetCompletion.add(metricGroup.metricName(registry.sinkRecordOffsetCommitCompletionTotal), new CumulativeSum());\n\n            offsetCompletionSkip = metricGroup.sensor(\"offset-commit-completion-skip\");\n            offsetCompletionSkip.add(metricGroup.metricName(registry.sinkRecordOffsetCommitSkipRate), new Rate());\n            offsetCompletionSkip.add(metricGroup.metricName(registry.sinkRecordOffsetCommitSkipTotal), new CumulativeSum());\n\n            putBatchTime = metricGroup.sensor(\"put-batch-time\");\n            putBatchTime.add(metricGroup.metricName(registry.sinkRecordPutBatchTimeMax), new Max());\n            putBatchTime.add(metricGroup.metricName(registry.sinkRecordPutBatchTimeAvg), new Avg());\n        }\n\n        void computeSinkRecordLag() {\n            Map<TopicPartition, OffsetAndMetadata> consumed = this.consumedOffsets;\n            Map<TopicPartition, OffsetAndMetadata> committed = this.committedOffsets;\n            activeRecords = 0L;\n            for (Map.Entry<TopicPartition, OffsetAndMetadata> committedOffsetEntry : committed.entrySet()) {\n                final TopicPartition partition = committedOffsetEntry.getKey();\n                final OffsetAndMetadata consumedOffsetMeta = consumed.get(partition);\n                if (consumedOffsetMeta != null) {\n                    final OffsetAndMetadata committedOffsetMeta = committedOffsetEntry.getValue();\n                    long consumedOffset = consumedOffsetMeta.offset();\n                    long committedOffset = committedOffsetMeta.offset();\n                    long diff = consumedOffset - committedOffset;\n                    // Connector tasks can return offsets, so make sure nothing wonky happens\n                    activeRecords += Math.max(diff, 0L);\n                }\n            }\n            sinkRecordActiveCount.record(activeRecords);\n        }\n\n        void close() {\n            metricGroup.close();\n        }\n\n        void recordRead(int batchSize) {\n            sinkRecordRead.record(batchSize);\n        }\n\n        void recordSend(int batchSize) {\n            sinkRecordSend.record(batchSize);\n        }\n\n        void recordPut(long duration) {\n            putBatchTime.record(duration);\n        }\n\n        void recordPartitionCount(int assignedPartitionCount) {\n            partitionCount.record(assignedPartitionCount);\n        }\n\n        void recordOffsetSequenceNumber(int seqNum) {\n            offsetSeqNum.record(seqNum);\n        }\n\n        void recordConsumedOffsets(Map<TopicPartition, OffsetAndMetadata> offsets) {\n            consumedOffsets.putAll(offsets);\n            computeSinkRecordLag();\n        }\n\n        void recordCommittedOffsets(Map<TopicPartition, OffsetAndMetadata> offsets) {\n            committedOffsets = offsets;\n            computeSinkRecordLag();\n        }\n\n        void assignedOffsets(Map<TopicPartition, OffsetAndMetadata> offsets) {\n            consumedOffsets = new HashMap<>(offsets);\n            committedOffsets = offsets;\n            computeSinkRecordLag();\n        }\n\n        void clearOffsets(Collection<TopicPartition> topicPartitions) {\n            consumedOffsets.keySet().removeAll(topicPartitions);\n            committedOffsets.keySet().removeAll(topicPartitions);\n            computeSinkRecordLag();\n        }\n\n        void recordOffsetCommitSuccess() {\n            offsetCompletion.record(1.0);\n        }\n\n        void recordOffsetCommitSkip() {\n            offsetCompletionSkip.record(1.0);\n        }\n\n        protected MetricGroup metricGroup() {\n            return metricGroup;\n        }\n    }\n}",
                "methodCount": 56
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 4,
                "candidates": [
                    {
                        "lineStart": 332,
                        "lineEnd": 350,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method poll to class SinkTaskMetricsGroup",
                        "description": "Move method poll to org.apache.kafka.connect.runtime.WorkerSinkTask.SinkTaskMetricsGroup\nRationale: The 'poll' method deals with polling consumer records, managing timeouts, logging, and delivering messages, all of which are monitoring and management actions. The SinkTaskMetricsGroup class already handles metrics and records related activities such as reading, sending, and batch timing of records. Thus, placing the 'poll' method within this class would centralize related functions, providing better cohesion and maintainability. The 'Converter' class, on the other hand, focuses on transforming data formats, which is unrelated to the polling and managing tasks that 'poll' accomplishes.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 371,
                        "lineEnd": 375,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method doCommitAsync to class SinkTaskMetricsGroup",
                        "description": "Move method doCommitAsync to org.apache.kafka.connect.runtime.WorkerSinkTask.SinkTaskMetricsGroup\nRationale: The doCommitAsync method is closely related to offset management, which is a core responsibility of the SinkTaskMetricsGroup class. This class already handles the recording of consumed and committed offsets, making it a fitting location for the asynchronous commit operation as well. Moving the method here aligns with the single responsibility principle, ensuring that all offset-related logic is centralized in one place, enhancing maintainability and coherence.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 393,
                        "lineEnd": 395,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method commitOffsets to class SinkTaskMetricsGroup",
                        "description": "Move method commitOffsets to org.apache.kafka.connect.runtime.WorkerSinkTask.SinkTaskMetricsGroup\nRationale: The `commitOffsets` method appears to be related to offset management, which is a concept extensively handled by the `SinkTaskMetricsGroup` class. This class is already managing consumed and committed offsets, providing metrics on offset lag, and recording offset-related events. Placing `commitOffsets` in this class would align with its existing responsibilities and ensure that all offset management functionalities are centralized, making the codebase more cohesive and maintainable.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 584,
                        "lineEnd": 588,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method resumeAll to class SinkTaskMetricsGroup",
                        "description": "Move method resumeAll to org.apache.kafka.connect.runtime.WorkerSinkTask.SinkTaskMetricsGroup\nRationale: The method resumeAll() is closely related to the topic partition and consumer assignment operations, which are operations managed by SinkTaskMetricsGroup. This class already deals with topic partitions via its methods such as recordConsumedOffsets and clearOffsets. Therefore, it is more appropriate for handling consumer assignment and partition management.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "iteration",
                            "method_signature": "protected iteration()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onCommitCompleted",
                            "method_signature": "private onCommitCompleted(Throwable error, long seqno, Map<TopicPartition, OffsetAndMetadata> committedOffsets)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "poll",
                            "method_signature": "protected poll(long timeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "doCommitSync",
                            "method_signature": "private doCommitSync(Map<TopicPartition, OffsetAndMetadata> offsets, int seqno)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "doCommitAsync",
                            "method_signature": "private doCommitAsync(Map<TopicPartition, OffsetAndMetadata> offsets, final int seqno)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "doCommit",
                            "method_signature": "private doCommit(Map<TopicPartition, OffsetAndMetadata> offsets, boolean closing, int seqno)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffsets",
                            "method_signature": "private commitOffsets(long now, boolean closing)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffsets",
                            "method_signature": "private commitOffsets(long now, boolean closing, Collection<TopicPartition> topicPartitions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollConsumer",
                            "method_signature": "private pollConsumer(long timeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "convertMessages",
                            "method_signature": "private convertMessages(ConsumerRecords<byte[], byte[]> msgs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "convertAndTransformRecord",
                            "method_signature": "private convertAndTransformRecord(final ConsumerRecord<byte[], byte[]> msg)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "convertHeadersFor",
                            "method_signature": "private convertHeadersFor(ConsumerRecord<byte[], byte[]> record)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resumeAll",
                            "method_signature": "private resumeAll()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pauseAll",
                            "method_signature": "private pauseAll()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deliverMessages",
                            "method_signature": "private deliverMessages()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeThrowAsyncError",
                            "method_signature": "private maybeThrowAsyncError()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "rewind",
                            "method_signature": "private rewind()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "openPartitions",
                            "method_signature": "private openPartitions(Collection<TopicPartition> partitions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "closeAllPartitions",
                            "method_signature": "private closeAllPartitions()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "closePartitions",
                            "method_signature": "private closePartitions(Collection<TopicPartition> topicPartitions, boolean lost)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updatePartitionCount",
                            "method_signature": "private updatePartitionCount()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onPartitionsRemoved",
                            "method_signature": "private onPartitionsRemoved(Collection<TopicPartition> partitions, boolean lost)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "computeSinkRecordLag",
                            "method_signature": " computeSinkRecordLag()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": " close()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordRead",
                            "method_signature": " recordRead(int batchSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordSend",
                            "method_signature": " recordSend(int batchSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordPut",
                            "method_signature": " recordPut(long duration)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordPartitionCount",
                            "method_signature": " recordPartitionCount(int assignedPartitionCount)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordOffsetSequenceNumber",
                            "method_signature": " recordOffsetSequenceNumber(int seqNum)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordConsumedOffsets",
                            "method_signature": " recordConsumedOffsets(Map<TopicPartition, OffsetAndMetadata> offsets)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordCommittedOffsets",
                            "method_signature": " recordCommittedOffsets(Map<TopicPartition, OffsetAndMetadata> offsets)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assignedOffsets",
                            "method_signature": " assignedOffsets(Map<TopicPartition, OffsetAndMetadata> offsets)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "clearOffsets",
                            "method_signature": " clearOffsets(Collection<TopicPartition> topicPartitions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordOffsetCommitSuccess",
                            "method_signature": " recordOffsetCommitSuccess()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordOffsetCommitSkip",
                            "method_signature": " recordOffsetCommitSkip()",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "clearOffsets",
                            "method_signature": " clearOffsets(Collection<TopicPartition> topicPartitions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "doCommitAsync",
                            "method_signature": "private doCommitAsync(Map<TopicPartition, OffsetAndMetadata> offsets, final int seqno)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffsets",
                            "method_signature": "private commitOffsets(long now, boolean closing)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordConsumedOffsets",
                            "method_signature": " recordConsumedOffsets(Map<TopicPartition, OffsetAndMetadata> offsets)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordPartitionCount",
                            "method_signature": " recordPartitionCount(int assignedPartitionCount)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordOffsetSequenceNumber",
                            "method_signature": " recordOffsetSequenceNumber(int seqNum)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resumeAll",
                            "method_signature": "private resumeAll()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordRead",
                            "method_signature": " recordRead(int batchSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordSend",
                            "method_signature": " recordSend(int batchSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordPut",
                            "method_signature": " recordPut(long duration)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordOffsetCommitSuccess",
                            "method_signature": " recordOffsetCommitSuccess()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordOffsetCommitSkip",
                            "method_signature": " recordOffsetCommitSkip()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "poll",
                            "method_signature": "protected poll(long timeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": " close()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordCommittedOffsets",
                            "method_signature": " recordCommittedOffsets(Map<TopicPartition, OffsetAndMetadata> offsets)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "clearOffsets",
                            "method_signature": " clearOffsets(Collection<TopicPartition> topicPartitions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "doCommitAsync",
                            "method_signature": "private doCommitAsync(Map<TopicPartition, OffsetAndMetadata> offsets, final int seqno)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffsets",
                            "method_signature": "private commitOffsets(long now, boolean closing)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordConsumedOffsets",
                            "method_signature": " recordConsumedOffsets(Map<TopicPartition, OffsetAndMetadata> offsets)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordPartitionCount",
                            "method_signature": " recordPartitionCount(int assignedPartitionCount)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordOffsetSequenceNumber",
                            "method_signature": " recordOffsetSequenceNumber(int seqNum)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resumeAll",
                            "method_signature": "private resumeAll()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordRead",
                            "method_signature": " recordRead(int batchSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordSend",
                            "method_signature": " recordSend(int batchSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordPut",
                            "method_signature": " recordPut(long duration)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordOffsetCommitSuccess",
                            "method_signature": " recordOffsetCommitSuccess()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordOffsetCommitSkip",
                            "method_signature": " recordOffsetCommitSkip()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "poll",
                            "method_signature": "protected poll(long timeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": " close()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordCommittedOffsets",
                            "method_signature": " recordCommittedOffsets(Map<TopicPartition, OffsetAndMetadata> offsets)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                " clearOffsets(Collection<TopicPartition> topicPartitions)": {
                    "first": {
                        "method_name": "clearOffsets",
                        "method_signature": " clearOffsets(Collection<TopicPartition> topicPartitions)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4026617912428347
                },
                "private doCommitAsync(Map<TopicPartition, OffsetAndMetadata> offsets, final int seqno)": {
                    "first": {
                        "method_name": "doCommitAsync",
                        "method_signature": "private doCommitAsync(Map<TopicPartition, OffsetAndMetadata> offsets, final int seqno)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.40608710106837076
                },
                "private commitOffsets(long now, boolean closing)": {
                    "first": {
                        "method_name": "commitOffsets",
                        "method_signature": "private commitOffsets(long now, boolean closing)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.422955116290468
                },
                " recordConsumedOffsets(Map<TopicPartition, OffsetAndMetadata> offsets)": {
                    "first": {
                        "method_name": "recordConsumedOffsets",
                        "method_signature": " recordConsumedOffsets(Map<TopicPartition, OffsetAndMetadata> offsets)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.42911760384822434
                },
                " recordPartitionCount(int assignedPartitionCount)": {
                    "first": {
                        "method_name": "recordPartitionCount",
                        "method_signature": " recordPartitionCount(int assignedPartitionCount)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.45913107340220083
                },
                " recordOffsetSequenceNumber(int seqNum)": {
                    "first": {
                        "method_name": "recordOffsetSequenceNumber",
                        "method_signature": " recordOffsetSequenceNumber(int seqNum)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.45913107340220083
                },
                "private resumeAll()": {
                    "first": {
                        "method_name": "resumeAll",
                        "method_signature": "private resumeAll()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.45941217337101226
                },
                " recordRead(int batchSize)": {
                    "first": {
                        "method_name": "recordRead",
                        "method_signature": " recordRead(int batchSize)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.46058523361723297
                },
                " recordSend(int batchSize)": {
                    "first": {
                        "method_name": "recordSend",
                        "method_signature": " recordSend(int batchSize)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.46058523361723297
                },
                " recordPut(long duration)": {
                    "first": {
                        "method_name": "recordPut",
                        "method_signature": " recordPut(long duration)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.46058523361723297
                },
                " recordOffsetCommitSuccess()": {
                    "first": {
                        "method_name": "recordOffsetCommitSuccess",
                        "method_signature": " recordOffsetCommitSuccess()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5029497263213352
                },
                " recordOffsetCommitSkip()": {
                    "first": {
                        "method_name": "recordOffsetCommitSkip",
                        "method_signature": " recordOffsetCommitSkip()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5029497263213352
                },
                "protected poll(long timeoutMs)": {
                    "first": {
                        "method_name": "poll",
                        "method_signature": "protected poll(long timeoutMs)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5037279246152485
                },
                " close()": {
                    "first": {
                        "method_name": "close",
                        "method_signature": " close()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5061356517001
                },
                " recordCommittedOffsets(Map<TopicPartition, OffsetAndMetadata> offsets)": {
                    "first": {
                        "method_name": "recordCommittedOffsets",
                        "method_signature": " recordCommittedOffsets(Map<TopicPartition, OffsetAndMetadata> offsets)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5079742013420163
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "protected poll(long timeoutMs)",
                    "private doCommitAsync(Map<TopicPartition, OffsetAndMetadata> offsets, final int seqno)",
                    "private commitOffsets(long now, boolean closing)",
                    "private resumeAll()"
                ],
                "llm_response_time": 4759
            },
            "targetClassMap": {
                "poll": {
                    "target_classes": [
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.25808208409099265
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.25808208409099265
                        },
                        {
                            "class_name": "HeaderConverter",
                            "similarity_score": 0.24740390281223112
                        },
                        {
                            "class_name": "SinkTaskMetricsGroup",
                            "similarity_score": 0.38627778387552847
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SinkTaskMetricsGroup",
                        "Converter",
                        "Converter"
                    ],
                    "llm_response_time": 2797,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                },
                "doCommitAsync": {
                    "target_classes": [
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.028617427596496936
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.028617427596496936
                        },
                        {
                            "class_name": "HeaderConverter",
                            "similarity_score": 0.016359952408967662
                        },
                        {
                            "class_name": "SinkTaskMetricsGroup",
                            "similarity_score": 0.3670651741928988
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SinkTaskMetricsGroup",
                        "Converter",
                        "Converter"
                    ],
                    "llm_response_time": 2324,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                },
                "commitOffsets": {
                    "target_classes": [
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.03926674661733455
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.03926674661733455
                        },
                        {
                            "class_name": "HeaderConverter",
                            "similarity_score": 0.014591156645246276
                        },
                        {
                            "class_name": "SinkTaskMetricsGroup",
                            "similarity_score": 0.36830141598201066
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SinkTaskMetricsGroup",
                        "Converter",
                        "Converter"
                    ],
                    "llm_response_time": 2702,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "resumeAll": {
                    "target_classes": [
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.07224030685898176
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.07224030685898176
                        },
                        {
                            "class_name": "HeaderConverter",
                            "similarity_score": 0.06039860389867787
                        },
                        {
                            "class_name": "SinkTaskMetricsGroup",
                            "similarity_score": 0.35492079126761683
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SinkTaskMetricsGroup",
                        "Converter",
                        "Converter"
                    ],
                    "llm_response_time": 2509,
                    "similarity_computation_time": 3,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "e7fa0edd6351c0949b9082c626597046e9def854",
        "url": "https://github.com/apache/kafka/commit/e7fa0edd6351c0949b9082c626597046e9def854",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public getOrMaybeCreateGroup(groupType Group.GroupType, groupId String) : Group extracted from public testDeleteGroupAllOffsets(groupType Group.GroupType) : void in class org.apache.kafka.coordinator.group.OffsetMetadataManagerTest & moved to class org.apache.kafka.coordinator.group.OffsetMetadataManagerTest.OffsetMetadataManagerTestContext",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2345,
                    "endLine": 2380,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testDeleteGroupAllOffsets(groupType Group.GroupType) : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2350,
                    "endLine": 2350,
                    "startColumn": 13,
                    "endColumn": 26,
                    "codeElementType": "SWITCH_CASE",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2356,
                    "endLine": 2356,
                    "startColumn": 13,
                    "endColumn": 27,
                    "codeElementType": "SWITCH_CASE",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2362,
                    "endLine": 2362,
                    "startColumn": 13,
                    "endColumn": 21,
                    "codeElementType": "SWITCH_CASE",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2363,
                    "endLine": 2363,
                    "startColumn": 17,
                    "endColumn": 88,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2351,
                    "endLine": 2354,
                    "startColumn": 17,
                    "endColumn": 19,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2357,
                    "endLine": 2360,
                    "startColumn": 17,
                    "endColumn": 19,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2349,
                    "endLine": 2364,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "SWITCH_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 184,
                    "endLine": 202,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public getOrMaybeCreateGroup(groupType Group.GroupType, groupId String) : Group"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 189,
                    "endLine": 189,
                    "startColumn": 17,
                    "endColumn": 30,
                    "codeElementType": "SWITCH_CASE",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 194,
                    "endLine": 194,
                    "startColumn": 17,
                    "endColumn": 31,
                    "codeElementType": "SWITCH_CASE",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 199,
                    "endLine": 199,
                    "startColumn": 17,
                    "endColumn": 25,
                    "codeElementType": "SWITCH_CASE",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 200,
                    "endLine": 200,
                    "startColumn": 21,
                    "endColumn": 92,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 190,
                    "endLine": 193,
                    "startColumn": 21,
                    "endColumn": 23,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 195,
                    "endLine": 198,
                    "startColumn": 21,
                    "endColumn": 23,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 188,
                    "endLine": 201,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "SWITCH_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2391,
                    "endLine": 2412,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testDeleteGroupAllOffsets(groupType Group.GroupType) : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2395,
                    "endLine": 2395,
                    "startColumn": 9,
                    "endColumn": 56,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "context.getOrMaybeCreateGroup(groupType,\"foo\")"
                }
            ],
            "isStatic": false
        },
        "ref_id": 568,
        "extraction_results": {
            "success": true,
            "newCommitHash": "fd74defe96b3f3fc1d3616cb408a0bfdb81ca73a",
            "newBranchName": "extract-getOrMaybeCreateGroup-testDeleteGroupAllOffsets-6ccc19c"
        },
        "telemetry": {
            "id": "2a079d46-87de-4f9b-8a37-7c65db804426",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 2871,
                "lineStart": 92,
                "lineEnd": 2962,
                "bodyLineStart": 92,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                "sourceCode": "public class OffsetMetadataManagerTest {\n    static class OffsetMetadataManagerTestContext {\n        public static class Builder {\n            private final MockTime time = new MockTime();\n            private final MockCoordinatorTimer<Void, Record> timer = new MockCoordinatorTimer<>(time);\n            private final LogContext logContext = new LogContext();\n            private final SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n            private GroupMetadataManager groupMetadataManager = null;\n            private MetadataImage metadataImage = null;\n            private GroupCoordinatorConfig config = null;\n            private GroupCoordinatorMetricsShard metrics = mock(GroupCoordinatorMetricsShard.class);\n\n            Builder withOffsetMetadataMaxSize(int offsetMetadataMaxSize) {\n                config = GroupCoordinatorConfigTest.createGroupCoordinatorConfig(offsetMetadataMaxSize, 60000L, 24 * 60 * 1000);\n                return this;\n            }\n\n            Builder withOffsetsRetentionMs(long offsetsRetentionMs) {\n                config = GroupCoordinatorConfigTest.createGroupCoordinatorConfig(4096, 60000L, offsetsRetentionMs);\n                return this;\n            }\n\n            Builder withGroupMetadataManager(GroupMetadataManager groupMetadataManager) {\n                this.groupMetadataManager = groupMetadataManager;\n                return this;\n            }\n\n            OffsetMetadataManagerTestContext build() {\n                if (metadataImage == null) metadataImage = MetadataImage.EMPTY;\n                if (config == null) {\n                    config = GroupCoordinatorConfigTest.createGroupCoordinatorConfig(4096, 60000L, 24 * 60 * 1000);\n                }\n\n                if (groupMetadataManager == null) {\n                    groupMetadataManager = new GroupMetadataManager.Builder()\n                        .withTime(time)\n                        .withTimer(timer)\n                        .withSnapshotRegistry(snapshotRegistry)\n                        .withLogContext(logContext)\n                        .withMetadataImage(metadataImage)\n                        .withConsumerGroupAssignors(Collections.singletonList(new RangeAssignor()))\n                        .withGroupCoordinatorMetricsShard(metrics)\n                        .build();\n                }\n\n                OffsetMetadataManager offsetMetadataManager = new OffsetMetadataManager.Builder()\n                    .withTime(time)\n                    .withLogContext(logContext)\n                    .withSnapshotRegistry(snapshotRegistry)\n                    .withMetadataImage(metadataImage)\n                    .withGroupMetadataManager(groupMetadataManager)\n                    .withGroupCoordinatorConfig(config)\n                    .withGroupCoordinatorMetricsShard(metrics)\n                    .build();\n\n                return new OffsetMetadataManagerTestContext(\n                    time,\n                    timer,\n                    snapshotRegistry,\n                    metrics,\n                    groupMetadataManager,\n                    offsetMetadataManager\n                );\n            }\n        }\n\n        final MockTime time;\n        final MockCoordinatorTimer<Void, Record> timer;\n        final SnapshotRegistry snapshotRegistry;\n        final GroupCoordinatorMetricsShard metrics;\n        final GroupMetadataManager groupMetadataManager;\n        final OffsetMetadataManager offsetMetadataManager;\n\n        long lastCommittedOffset = 0L;\n        long lastWrittenOffset = 0L;\n\n        OffsetMetadataManagerTestContext(\n            MockTime time,\n            MockCoordinatorTimer<Void, Record> timer,\n            SnapshotRegistry snapshotRegistry,\n            GroupCoordinatorMetricsShard metrics,\n            GroupMetadataManager groupMetadataManager,\n            OffsetMetadataManager offsetMetadataManager\n        ) {\n            this.time = time;\n            this.timer = timer;\n            this.snapshotRegistry = snapshotRegistry;\n            this.metrics = metrics;\n            this.groupMetadataManager = groupMetadataManager;\n            this.offsetMetadataManager = offsetMetadataManager;\n        }\n\n        public void commit() {\n            long lastCommittedOffset = this.lastCommittedOffset;\n            this.lastCommittedOffset = lastWrittenOffset;\n            snapshotRegistry.deleteSnapshotsUpTo(lastCommittedOffset);\n        }\n\n        public CoordinatorResult<OffsetCommitResponseData, Record> commitOffset(\n            OffsetCommitRequestData request\n        ) {\n            return commitOffset(ApiKeys.OFFSET_COMMIT.latestVersion(), request);\n        }\n\n        public CoordinatorResult<OffsetCommitResponseData, Record> commitOffset(\n            short version,\n            OffsetCommitRequestData request\n        ) {\n            RequestContext context = new RequestContext(\n                new RequestHeader(\n                    ApiKeys.OFFSET_COMMIT,\n                    version,\n                    \"client\",\n                    0\n                ),\n                \"1\",\n                InetAddress.getLoopbackAddress(),\n                KafkaPrincipal.ANONYMOUS,\n                ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n                SecurityProtocol.PLAINTEXT,\n                ClientInformation.EMPTY,\n                false\n            );\n\n            CoordinatorResult<OffsetCommitResponseData, Record> result = offsetMetadataManager.commitOffset(\n                context,\n                request\n            );\n\n            result.records().forEach(this::replay);\n            return result;\n        }\n\n        public CoordinatorResult<TxnOffsetCommitResponseData, Record> commitTransactionalOffset(\n            TxnOffsetCommitRequestData request\n        ) {\n            RequestContext context = new RequestContext(\n                new RequestHeader(\n                    ApiKeys.TXN_OFFSET_COMMIT,\n                    ApiKeys.TXN_OFFSET_COMMIT.latestVersion(),\n                    \"client\",\n                    0\n                ),\n                \"1\",\n                InetAddress.getLoopbackAddress(),\n                KafkaPrincipal.ANONYMOUS,\n                ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n                SecurityProtocol.PLAINTEXT,\n                ClientInformation.EMPTY,\n                false\n            );\n\n            CoordinatorResult<TxnOffsetCommitResponseData, Record> result = offsetMetadataManager.commitTransactionalOffset(\n                context,\n                request\n            );\n\n            result.records().forEach(record -> replay(\n                request.producerId(),\n                record\n            ));\n\n            return result;\n        }\n\n        public CoordinatorResult<OffsetDeleteResponseData, Record> deleteOffsets(\n            OffsetDeleteRequestData request\n        ) {\n            CoordinatorResult<OffsetDeleteResponseData, Record> result = offsetMetadataManager.deleteOffsets(request);\n            result.records().forEach(this::replay);\n            return result;\n        }\n\n        public int deleteAllOffsets(\n            String groupId,\n            List<Record> records\n        ) {\n            List<Record> addedRecords = new ArrayList<>();\n            int numDeletedOffsets = offsetMetadataManager.deleteAllOffsets(groupId, addedRecords);\n            addedRecords.forEach(this::replay);\n\n            records.addAll(addedRecords);\n            return numDeletedOffsets;\n        }\n\n        public boolean cleanupExpiredOffsets(String groupId, List<Record> records) {\n            List<Record> addedRecords = new ArrayList<>();\n            boolean isOffsetsEmptyForGroup = offsetMetadataManager.cleanupExpiredOffsets(groupId, addedRecords);\n            addedRecords.forEach(this::replay);\n\n            records.addAll(addedRecords);\n            return isOffsetsEmptyForGroup;\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        ) {\n            return fetchOffsets(\n                groupId,\n                null,\n                -1,\n                topics,\n                committedOffset\n            );\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        ) {\n            OffsetFetchResponseData.OffsetFetchResponseGroup response = offsetMetadataManager.fetchOffsets(\n                new OffsetFetchRequestData.OffsetFetchRequestGroup()\n                    .setGroupId(groupId)\n                    .setMemberId(memberId)\n                    .setMemberEpoch(memberEpoch)\n                    .setTopics(topics),\n                committedOffset\n            );\n            assertEquals(groupId, response.groupId());\n            return response.topics();\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        ) {\n            return fetchAllOffsets(\n                groupId,\n                null,\n                -1,\n                committedOffset\n            );\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchAllOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            long committedOffset\n        ) {\n            OffsetFetchResponseData.OffsetFetchResponseGroup response = offsetMetadataManager.fetchAllOffsets(\n                new OffsetFetchRequestData.OffsetFetchRequestGroup()\n                    .setGroupId(groupId)\n                    .setMemberId(memberId)\n                    .setMemberEpoch(memberEpoch),\n                committedOffset\n            );\n            assertEquals(groupId, response.groupId());\n            return response.topics();\n        }\n\n        public List<MockCoordinatorTimer.ExpiredTimeout<Void, Record>> sleep(long ms) {\n            time.sleep(ms);\n            List<MockCoordinatorTimer.ExpiredTimeout<Void, Record>> timeouts = timer.poll();\n            timeouts.forEach(timeout -> {\n                if (timeout.result.replayRecords()) {\n                    timeout.result.records().forEach(this::replay);\n                }\n            });\n            return timeouts;\n        }\n\n        public void commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        ) {\n            commitOffset(\n                groupId,\n                topic,\n                partition,\n                offset,\n                leaderEpoch,\n                time.milliseconds()\n            );\n        }\n\n        public void commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        ) {\n            commitOffset(\n                RecordBatch.NO_PRODUCER_ID,\n                groupId,\n                topic,\n                partition,\n                offset,\n                leaderEpoch,\n                commitTimestamp\n            );\n        }\n\n        public void commitOffset(\n            long producerId,\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        ) {\n            replay(producerId, RecordHelpers.newOffsetCommitRecord(\n                groupId,\n                topic,\n                partition,\n                new OffsetAndMetadata(\n                    offset,\n                    OptionalInt.of(leaderEpoch),\n                    \"metadata\",\n                    commitTimestamp,\n                    OptionalLong.empty()\n                ),\n                MetadataVersion.latestTesting()\n            ));\n        }\n\n        public void deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        ) {\n            replay(RecordHelpers.newOffsetCommitTombstoneRecord(\n                groupId,\n                topic,\n                partition\n            ));\n        }\n\n        private ApiMessage messageOrNull(ApiMessageAndVersion apiMessageAndVersion) {\n            if (apiMessageAndVersion == null) {\n                return null;\n            } else {\n                return apiMessageAndVersion.message();\n            }\n        }\n\n        private void replay(\n            Record record\n        ) {\n            replay(\n                RecordBatch.NO_PRODUCER_ID,\n                record\n            );\n        }\n\n        private void replay(\n            long producerId,\n            Record record\n        ) {\n            snapshotRegistry.getOrCreateSnapshot(lastWrittenOffset);\n\n            ApiMessageAndVersion key = record.key();\n            ApiMessageAndVersion value = record.value();\n\n            if (key == null) {\n                throw new IllegalStateException(\"Received a null key in \" + record);\n            }\n\n            switch (key.version()) {\n                case OffsetCommitKey.HIGHEST_SUPPORTED_VERSION:\n                    offsetMetadataManager.replay(\n                        lastWrittenOffset,\n                        producerId,\n                        (OffsetCommitKey) key.message(),\n                        (OffsetCommitValue) messageOrNull(value)\n                    );\n                    break;\n\n                default:\n                    throw new IllegalStateException(\"Received an unknown record type \" + key.version()\n                        + \" in \" + record);\n            }\n\n            lastWrittenOffset++;\n        }\n\n        private void replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        ) {\n            snapshotRegistry.getOrCreateSnapshot(lastWrittenOffset);\n            offsetMetadataManager.replayEndTransactionMarker(producerId, result);\n            lastWrittenOffset++;\n        }\n\n        public void testOffsetDeleteWith(\n            String groupId,\n            String topic,\n            int partition,\n            Errors expectedError\n        ) {\n            final OffsetDeleteRequestData.OffsetDeleteRequestTopicCollection requestTopicCollection =\n                new OffsetDeleteRequestData.OffsetDeleteRequestTopicCollection(Collections.singletonList(\n                    new OffsetDeleteRequestData.OffsetDeleteRequestTopic()\n                        .setName(topic)\n                        .setPartitions(Collections.singletonList(\n                            new OffsetDeleteRequestData.OffsetDeleteRequestPartition().setPartitionIndex(partition)\n                        ))\n                ).iterator());\n\n            final OffsetDeleteResponseData.OffsetDeleteResponsePartitionCollection expectedResponsePartitionCollection =\n                new OffsetDeleteResponseData.OffsetDeleteResponsePartitionCollection();\n            if (hasOffset(groupId, topic, partition)) {\n                expectedResponsePartitionCollection.add(\n                    new OffsetDeleteResponseData.OffsetDeleteResponsePartition()\n                        .setPartitionIndex(partition)\n                        .setErrorCode(expectedError.code())\n                );\n            }\n\n            final OffsetDeleteResponseData.OffsetDeleteResponseTopicCollection expectedResponseTopicCollection =\n                new OffsetDeleteResponseData.OffsetDeleteResponseTopicCollection(Collections.singletonList(\n                    new OffsetDeleteResponseData.OffsetDeleteResponseTopic()\n                        .setName(topic)\n                        .setPartitions(expectedResponsePartitionCollection)\n                ).iterator());\n\n            List<Record> expectedRecords = Collections.emptyList();\n            if (hasOffset(groupId, topic, partition) && expectedError == Errors.NONE) {\n                expectedRecords = Collections.singletonList(\n                    RecordHelpers.newOffsetCommitTombstoneRecord(groupId, topic, partition)\n                );\n            }\n\n            final CoordinatorResult<OffsetDeleteResponseData, Record> coordinatorResult = deleteOffsets(\n                new OffsetDeleteRequestData()\n                    .setGroupId(groupId)\n                    .setTopics(requestTopicCollection)\n            );\n\n            assertEquals(new OffsetDeleteResponseData().setTopics(expectedResponseTopicCollection), coordinatorResult.response());\n            assertEquals(expectedRecords, coordinatorResult.records());\n        }\n\n        public boolean hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        ) {\n            return offsetMetadataManager.offset(groupId, topic, partition) != null;\n        }\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testOffsetCommitWithUnknownGroup(short version) {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        Class<? extends Throwable> expectedType;\n        if (version >= 9) {\n            expectedType = GroupIdNotFoundException.class;\n        } else {\n            expectedType = IllegalGenerationException.class;\n        }\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(expectedType, () -> context.commitOffset(\n            version,\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithDeadGroup() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a dead group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n        group.transitionTo(ClassicGroupState.DEAD);\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(CoordinatorNotAvailableException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithUnknownMemberId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(UnknownMemberIdException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithIllegalGeneration() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(IllegalGenerationException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithUnknownInstanceId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member without static id.\n        group.add(mkGenericMember(\"member\", Optional.empty()));\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(UnknownMemberIdException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGroupInstanceId(\"instanceid\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithFencedInstanceId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member with static id.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(UnknownMemberIdException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGroupInstanceId(\"old-instance-id\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWhileInCompletingRebalanceState() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(RebalanceInProgressException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(1)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithoutMemberIdAndGeneration() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(UnknownMemberIdException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithRetentionTime() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n        group.transitionTo(ClassicGroupState.STABLE);\n\n        CoordinatorResult<OffsetCommitResponseData, Record> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(1)\n                .setRetentionTimeMs(1234L)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(RecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.empty(),\n                    \"\",\n                    context.time.milliseconds(),\n                    OptionalLong.of(context.time.milliseconds() + 1234L)\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitMaintainsSession() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        ClassicGroupMember member = mkGenericMember(\"member\", Optional.empty());\n        group.add(member);\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n        group.transitionTo(ClassicGroupState.STABLE);\n\n        // Schedule session timeout. This would be normally done when\n        // the group transitions to stable.\n        context.groupMetadataManager.rescheduleClassicGroupMemberHeartbeat(group, member);\n\n        // Advance time by half of the session timeout. No timeouts are\n        // expired.\n        assertEquals(Collections.emptyList(), context.sleep(5000 / 2));\n\n        // Commit.\n        context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(1)\n                .setRetentionTimeMs(1234L)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n        );\n\n        // Advance time by half of the session timeout. No timeouts are\n        // expired.\n        assertEquals(Collections.emptyList(), context.sleep(5000 / 2));\n\n        // Advance time by half of the session timeout again. The timeout should\n        // expire and the member is removed from the group.\n        List<MockCoordinatorTimer.ExpiredTimeout<Void, Record>> timeouts =\n            context.sleep(5000 / 2);\n        assertEquals(1, timeouts.size());\n        assertFalse(group.hasMemberId(member.memberId()));\n    }\n\n    @Test\n    public void testSimpleGroupOffsetCommit() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        CoordinatorResult<OffsetCommitResponseData, Record> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(RecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.empty(),\n                    \"\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n\n        // A generic should have been created.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            false\n        );\n        assertNotNull(group);\n        assertEquals(\"foo\", group.groupId());\n    }\n\n    @Test\n    public void testSimpleGroupOffsetCommitWithInstanceId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        CoordinatorResult<OffsetCommitResponseData, Record> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                // Instance id should be ignored.\n                .setGroupInstanceId(\"instance-id\")\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(RecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.empty(),\n                    \"\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testConsumerGroupOffsetCommitWithUnknownMemberId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(UnknownMemberIdException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testConsumerGroupOffsetCommitWithStaleMemberEpoch() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setTargetMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .build()\n        );\n\n        OffsetCommitRequestData request = new OffsetCommitRequestData()\n            .setGroupId(\"foo\")\n            .setMemberId(\"member\")\n            .setGenerationIdOrMemberEpoch(9)\n            .setTopics(Collections.singletonList(\n                new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                    .setName(\"bar\")\n                    .setPartitions(Collections.singletonList(\n                        new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                            .setPartitionIndex(0)\n                            .setCommittedOffset(100L)\n                    ))\n            ));\n\n        // Verify that a smaller epoch is rejected.\n        assertThrows(StaleMemberEpochException.class, () -> context.commitOffset(request));\n\n        // Verify that a larger epoch is rejected.\n        request.setGenerationIdOrMemberEpoch(11);\n        assertThrows(StaleMemberEpochException.class, () -> context.commitOffset(request));\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testConsumerGroupOffsetCommitWithVersionSmallerThanVersion9(short version) {\n        // All the newer versions are fine.\n        if (version >= 9) return;\n        // Version 0 does not support MemberId and GenerationIdOrMemberEpoch fields.\n        if (version == 0) return;\n\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setTargetMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .build()\n        );\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(UnsupportedVersionException.class, () -> context.commitOffset(\n            version,\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(9)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testConsumerGroupOffsetCommitFromAdminClient() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        CoordinatorResult<OffsetCommitResponseData, Record> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(RecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.empty(),\n                    \"\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testConsumerGroupOffsetCommit() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setTargetMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .build()\n        );\n\n        CoordinatorResult<OffsetCommitResponseData, Record> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                                .setCommitTimestamp(context.time.milliseconds())\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(RecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.of(10),\n                    \"metadata\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testConsumerGroupOffsetCommitWithOffsetMetadataTooLarge() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .withOffsetMetadataMaxSize(5)\n            .build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setTargetMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .build()\n        );\n\n        CoordinatorResult<OffsetCommitResponseData, Record> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Arrays.asList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"toolarge\")\n                                .setCommitTimestamp(context.time.milliseconds()),\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(1)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"small\")\n                                .setCommitTimestamp(context.time.milliseconds())\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Arrays.asList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.OFFSET_METADATA_TOO_LARGE.code()),\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(1)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(RecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                1,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.of(10),\n                    \"small\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testConsumerGroupTransactionalOffsetCommit() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setTargetMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .build()\n        );\n\n        CoordinatorResult<TxnOffsetCommitResponseData, Record> result = context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(10)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new TxnOffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitResponseData.TxnOffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitResponseData.TxnOffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(RecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.of(10),\n                    \"metadata\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testConsumerGroupTransactionalOffsetCommitWithUnknownGroupId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        assertThrows(IllegalGenerationException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(10)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testConsumerGroupTransactionalOffsetCommitWithUnknownMemberId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        assertThrows(UnknownMemberIdException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(10)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testConsumerGroupTransactionalOffsetCommitWithStaleMemberEpoch() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setTargetMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .build()\n        );\n\n        assertThrows(IllegalGenerationException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(100)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testGenericGroupTransactionalOffsetCommit() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        ClassicGroupMember member = mkGenericMember(\"member\", Optional.empty());\n        group.add(member);\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n        group.transitionTo(ClassicGroupState.STABLE);\n\n        CoordinatorResult<TxnOffsetCommitResponseData, Record> result = context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(1)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new TxnOffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitResponseData.TxnOffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitResponseData.TxnOffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(RecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.of(10),\n                    \"metadata\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testGenericGroupTransactionalOffsetCommitWithUnknownGroupId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        assertThrows(IllegalGenerationException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(10)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testGenericGroupTransactionalOffsetCommitWithUnknownMemberId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        assertThrows(UnknownMemberIdException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(10)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testGenericGroupTransactionalOffsetCommitWithIllegalGenerationId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        ClassicGroupMember member = mkGenericMember(\"member\", Optional.empty());\n        group.add(member);\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n        group.transitionTo(ClassicGroupState.STABLE);\n\n        assertThrows(IllegalGenerationException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(100)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testGenericGroupFetchOffsetsWithDeadGroup() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a dead group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"group\",\n            true\n        );\n        group.transitionTo(ClassicGroupState.DEAD);\n\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> request = Arrays.asList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Arrays.asList(0, 1)),\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"bar\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        List<OffsetFetchResponseData.OffsetFetchResponseTopics> expectedResponse = Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Collections.singletonList(\n                    mkInvalidOffsetPartitionResponse(0)\n                ))\n        );\n\n        assertEquals(expectedResponse, context.fetchOffsets(\"group\", request, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchOffsetsWithUnknownGroup() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> request = Arrays.asList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Arrays.asList(0, 1)),\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"bar\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        List<OffsetFetchResponseData.OffsetFetchResponseTopics> expectedResponse = Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Collections.singletonList(\n                    mkInvalidOffsetPartitionResponse(0)\n                ))\n        );\n\n        assertEquals(expectedResponse, context.fetchOffsets(\"group\", request, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchOffsetsAtDifferentCommittedOffset() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\"group\", true);\n\n        assertEquals(0, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n        assertEquals(1, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 1, 110L, 1);\n        assertEquals(2, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"bar\", 0, 200L, 1);\n        assertEquals(3, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 1, 111L, 2);\n        assertEquals(4, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"bar\", 1, 210L, 2);\n        assertEquals(5, context.lastWrittenOffset);\n\n        // Always use the same request.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> request = Arrays.asList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Arrays.asList(0, 1)),\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"bar\")\n                .setPartitionIndexes(Arrays.asList(0, 1))\n        );\n\n        // Fetching with 0 should return all invalid offsets.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, 0L));\n\n        // Fetching with 1 should return data up to offset 1.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkInvalidOffsetPartitionResponse(1)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, 1L));\n\n        // Fetching with 2 should return data up to offset 2.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, 2L));\n\n        // Fetching with 3 should return data up to offset 3.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, 3L));\n\n        // Fetching with 4 should return data up to offset 4.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 2, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, 4L));\n\n        // Fetching with 5 should return data up to offset 5.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 2, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 210L, 2, \"metadata\")\n                ))\n        ), context.fetchOffsets(\"group\", request, 5L));\n\n        // Fetching with Long.MAX_VALUE should return all offsets.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 2, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 210L, 2, \"metadata\")\n                ))\n        ), context.fetchOffsets(\"group\", request, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchOffsetsWithPendingTransactionalOffsets() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\"group\", true);\n\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n        context.commitOffset(\"group\", \"foo\", 1, 110L, 1);\n        context.commitOffset(\"group\", \"bar\", 0, 200L, 1);\n\n        context.commit();\n\n        assertEquals(3, context.lastWrittenOffset);\n        assertEquals(3, context.lastCommittedOffset);\n\n        context.commitOffset(10L, \"group\", \"foo\", 1, 111L, 1, context.time.milliseconds());\n        context.commitOffset(10L, \"group\", \"bar\", 0, 201L, 1, context.time.milliseconds());\n        // Note that bar-1 does not exist in the initial commits. UNSTABLE_OFFSET_COMMIT errors\n        // must be returned in this case too.\n        context.commitOffset(10L, \"group\", \"bar\", 1, 211L, 1, context.time.milliseconds());\n\n        // Always use the same request.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> request = Arrays.asList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Arrays.asList(0, 1)),\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"bar\")\n                .setPartitionIndexes(Arrays.asList(0, 1))\n        );\n\n        // Fetching offsets with \"require stable\" (Long.MAX_VALUE) should return the committed offset for\n        // foo-0 and the UNSTABLE_OFFSET_COMMIT error for foo-1, bar-0 and bar-1.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, Errors.UNSTABLE_OFFSET_COMMIT)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, Errors.UNSTABLE_OFFSET_COMMIT),\n                    mkOffsetPartitionResponse(1, Errors.UNSTABLE_OFFSET_COMMIT)\n                ))\n        ), context.fetchOffsets(\"group\", request, Long.MAX_VALUE));\n\n        // Fetching offsets without \"require stable\" (lastCommittedOffset) should return the committed\n        // offset for foo-0, foo-1 and bar-0 and the INVALID_OFFSET for bar-1.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, context.lastCommittedOffset));\n\n        // Commit the ongoing transaction.\n        context.replayEndTransactionMarker(10L, TransactionResult.COMMIT);\n\n        // Fetching offsets with \"require stable\" (Long.MAX_VALUE) should not return any errors now.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 201L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 211L, 1, \"metadata\")\n                ))\n        ), context.fetchOffsets(\"group\", request, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testGenericGroupFetchAllOffsetsWithDeadGroup() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a dead group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"group\",\n            true\n        );\n        group.transitionTo(ClassicGroupState.DEAD);\n\n        assertEquals(Collections.emptyList(), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchAllOffsetsWithUnknownGroup() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        assertEquals(Collections.emptyList(), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchAllOffsetsAtDifferentCommittedOffset() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\"group\", true);\n\n        assertEquals(0, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n        assertEquals(1, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 1, 110L, 1);\n        assertEquals(2, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"bar\", 0, 200L, 1);\n        assertEquals(3, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 1, 111L, 2);\n        assertEquals(4, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"bar\", 1, 210L, 2);\n        assertEquals(5, context.lastWrittenOffset);\n\n        // Fetching with 0 should no offsets.\n        assertEquals(Collections.emptyList(), context.fetchAllOffsets(\"group\", 0L));\n\n        // Fetching with 1 should return data up to offset 1.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", 1L));\n\n        // Fetching with 2 should return data up to offset 2.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", 2L));\n\n        // Fetching with 3 should return data up to offset 3.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", 3L));\n\n        // Fetching with 4 should return data up to offset 4.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 2, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", 4L));\n\n        // Fetching with Long.MAX_VALUE should return all offsets.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 210L, 2, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 2, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchAllOffsetsWithPendingTransactionalOffsets() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\"group\", true);\n\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n        context.commitOffset(\"group\", \"foo\", 1, 110L, 1);\n        context.commitOffset(\"group\", \"bar\", 0, 200L, 1);\n\n        context.commit();\n\n        assertEquals(3, context.lastWrittenOffset);\n        assertEquals(3, context.lastCommittedOffset);\n\n        context.commitOffset(10L, \"group\", \"foo\", 1, 111L, 1, context.time.milliseconds());\n        context.commitOffset(10L, \"group\", \"bar\", 0, 201L, 1, context.time.milliseconds());\n        // Note that bar-1 does not exist in the initial commits. The API does not return it at all until\n        // the transaction is committed.\n        context.commitOffset(10L, \"group\", \"bar\", 1, 211L, 1, context.time.milliseconds());\n\n\n        // Fetching offsets with \"require stable\" (Long.MAX_VALUE) should return the committed offset for\n        // foo-0 and the UNSTABLE_OFFSET_COMMIT error for foo-1 and bar-0.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, Errors.UNSTABLE_OFFSET_COMMIT)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, Errors.UNSTABLE_OFFSET_COMMIT)\n                ))\n        ), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n\n        // Fetching offsets without \"require stable\" (lastCommittedOffset) should the committed\n        // offset for the foo-0, foo-1 and bar-0.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", context.lastCommittedOffset));\n\n        // Commit the ongoing transaction.\n        context.replayEndTransactionMarker(10L, TransactionResult.COMMIT);\n\n        // Fetching offsets with \"require stable\" (Long.MAX_VALUE) should not return any errors now.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 201L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 211L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testConsumerGroupOffsetFetchWithMemberIdAndEpoch() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        // Create consumer group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\"group\", true);\n        // Create member.\n        group.getOrMaybeCreateMember(\"member\", true);\n        // Commit offset.\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n\n        // Fetch offsets case.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics = Collections.singletonList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        assertEquals(Collections.singletonList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Collections.singletonList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\")\n                ))\n        ), context.fetchOffsets(\"group\", \"member\", 0, topics, Long.MAX_VALUE));\n\n        // Fetch all offsets case.\n        assertEquals(Collections.singletonList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Collections.singletonList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", \"member\", 0, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testConsumerGroupOffsetFetchFromAdminClient() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        // Create consumer group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\"group\", true);\n        // Create member.\n        group.getOrMaybeCreateMember(\"member\", true);\n        // Commit offset.\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n\n        // Fetch offsets case.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics = Collections.singletonList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        assertEquals(Collections.singletonList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Collections.singletonList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\")\n                ))\n        ), context.fetchOffsets(\"group\", topics, Long.MAX_VALUE));\n\n        // Fetch all offsets case.\n        assertEquals(Collections.singletonList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Collections.singletonList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testConsumerGroupOffsetFetchWithUnknownMemberId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\"group\", true);\n\n        // Fetch offsets case.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics = Collections.singletonList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        // Fetch offsets cases.\n        assertThrows(UnknownMemberIdException.class,\n            () -> context.fetchOffsets(\"group\", \"\", 0, topics, Long.MAX_VALUE));\n        assertThrows(UnknownMemberIdException.class,\n            () -> context.fetchOffsets(\"group\", \"member\", 0, topics, Long.MAX_VALUE));\n\n        // Fetch all offsets cases.\n        assertThrows(UnknownMemberIdException.class,\n            () -> context.fetchAllOffsets(\"group\", \"\", 0, Long.MAX_VALUE));\n        assertThrows(UnknownMemberIdException.class,\n            () -> context.fetchAllOffsets(\"group\", \"member\", 0, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testConsumerGroupOffsetFetchWithStaleMemberEpoch() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\"group\", true);\n        group.getOrMaybeCreateMember(\"member\", true);\n\n        // Fetch offsets case.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics = Collections.singletonList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        // Fetch offsets case.\n        assertThrows(StaleMemberEpochException.class,\n            () -> context.fetchOffsets(\"group\", \"member\", 10, topics, Long.MAX_VALUE));\n\n        // Fetch all offsets case.\n        assertThrows(StaleMemberEpochException.class,\n            () -> context.fetchAllOffsets(\"group\", \"member\", 10, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testGenericGroupOffsetDelete() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n        context.commitOffset(\"foo\", \"bar\", 0, 100L, 0);\n        group.setSubscribedTopics(Optional.of(Collections.emptySet()));\n        context.testOffsetDeleteWith(\"foo\", \"bar\", 0, Errors.NONE);\n    }\n\n    @Test\n    public void testGenericGroupOffsetDeleteWithErrors() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n        group.setSubscribedTopics(Optional.of(Collections.singleton(\"bar\")));\n        context.commitOffset(\"foo\", \"bar\", 0, 100L, 0);\n\n        // Delete the offset whose topic partition doesn't exist.\n        context.testOffsetDeleteWith(\"foo\", \"bar1\", 0, Errors.NONE);\n        // Delete the offset from the topic that the group is subscribed to.\n        context.testOffsetDeleteWith(\"foo\", \"bar\", 0, Errors.GROUP_SUBSCRIBED_TO_TOPIC);\n    }\n\n    @Test\n    public void testConsumerGroupOffsetDelete() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n        context.commitOffset(\"foo\", \"bar\", 0, 100L, 0);\n        assertFalse(group.isSubscribedToTopic(\"bar\"));\n        context.testOffsetDeleteWith(\"foo\", \"bar\", 0, Errors.NONE);\n    }\n\n    @Test\n    public void testConsumerGroupOffsetDeleteWithErrors() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n        MetadataImage image = new GroupMetadataManagerTest.MetadataImageBuilder()\n            .addTopic(Uuid.randomUuid(), \"foo\", 1)\n            .addRacks()\n            .build();\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n        group.computeSubscriptionMetadata(\n            null,\n            member1,\n            image.topics(),\n            image.cluster()\n        );\n        group.updateMember(member1);\n        context.commitOffset(\"foo\", \"bar\", 0, 100L, 0);\n        assertTrue(group.isSubscribedToTopic(\"bar\"));\n\n        // Delete the offset whose topic partition doesn't exist.\n        context.testOffsetDeleteWith(\"foo\", \"bar1\", 0, Errors.NONE);\n        // Delete the offset from the topic that the group is subscribed to.\n        context.testOffsetDeleteWith(\"foo\", \"bar\", 0, Errors.GROUP_SUBSCRIBED_TO_TOPIC);\n    }\n\n    @ParameterizedTest\n    @EnumSource(Group.GroupType.class)\n    public void testDeleteGroupAllOffsets(Group.GroupType groupType) {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        getOrMaybeCreateGroup(groupType, context);\n        context.commitOffset(\"foo\", \"bar-0\", 0, 100L, 0);\n        context.commitOffset(\"foo\", \"bar-0\", 1, 100L, 0);\n        context.commitOffset(\"foo\", \"bar-1\", 0, 100L, 0);\n\n        List<Record> expectedRecords = Arrays.asList(\n            RecordHelpers.newOffsetCommitTombstoneRecord(\"foo\", \"bar-1\", 0),\n            RecordHelpers.newOffsetCommitTombstoneRecord(\"foo\", \"bar-0\", 0),\n            RecordHelpers.newOffsetCommitTombstoneRecord(\"foo\", \"bar-0\", 1)\n        );\n\n        List<Record> records = new ArrayList<>();\n        int numDeleteOffsets = context.deleteAllOffsets(\"foo\", records);\n\n        assertEquals(expectedRecords, records);\n        assertEquals(3, numDeleteOffsets);\n    }\n\n    private void getOrMaybeCreateGroup(Group.GroupType groupType, OffsetMetadataManagerTestContext context) {\n        switch (groupType) {\n            case CLASSIC:\n                context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n                    \"foo\",\n                    true\n                );\n                break;\n            case CONSUMER:\n                context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n                    \"foo\",\n                    true\n                );\n                break;\n            default:\n                throw new IllegalArgumentException(\"Invalid group type: \" + groupType);\n        }\n    }\n\n    @Test\n    public void testCleanupExpiredOffsetsGroupHasNoOffsets() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .build();\n\n        List<Record> records = new ArrayList<>();\n        assertTrue(context.cleanupExpiredOffsets(\"unknown-group-id\", records));\n        assertEquals(Collections.emptyList(), records);\n    }\n\n    @Test\n    public void testCleanupExpiredOffsetsGroupDoesNotExist() {\n        GroupMetadataManager groupMetadataManager = mock(GroupMetadataManager.class);\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .withGroupMetadataManager(groupMetadataManager)\n            .build();\n\n        when(groupMetadataManager.group(\"unknown-group-id\")).thenThrow(GroupIdNotFoundException.class);\n        context.commitOffset(\"unknown-group-id\", \"topic\", 0, 100L, 0);\n        assertThrows(GroupIdNotFoundException.class, () -> context.cleanupExpiredOffsets(\"unknown-group-id\", new ArrayList<>()));\n    }\n\n    @Test\n    public void testCleanupExpiredOffsetsEmptyOffsetExpirationCondition() {\n        GroupMetadataManager groupMetadataManager = mock(GroupMetadataManager.class);\n        Group group = mock(Group.class);\n\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .withGroupMetadataManager(groupMetadataManager)\n            .build();\n\n        context.commitOffset(\"group-id\", \"topic\", 0, 100L, 0);\n\n        when(groupMetadataManager.group(\"group-id\")).thenReturn(group);\n        when(group.offsetExpirationCondition()).thenReturn(Optional.empty());\n\n        List<Record> records = new ArrayList<>();\n        assertFalse(context.cleanupExpiredOffsets(\"group-id\", records));\n        assertEquals(Collections.emptyList(), records);\n    }\n\n    @Test\n    public void testCleanupExpiredOffsets() {\n        GroupMetadataManager groupMetadataManager = mock(GroupMetadataManager.class);\n        Group group = mock(Group.class);\n\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .withGroupMetadataManager(groupMetadataManager)\n            .withOffsetsRetentionMs(1000)\n            .build();\n\n        long commitTimestamp = context.time.milliseconds();\n\n        context.commitOffset(\"group-id\", \"firstTopic\", 0, 100L, 0, commitTimestamp);\n        context.commitOffset(\"group-id\", \"secondTopic\", 0, 100L, 0, commitTimestamp);\n        context.commitOffset(\"group-id\", \"secondTopic\", 1, 100L, 0, commitTimestamp + 500);\n\n        context.time.sleep(1000);\n\n        // firstTopic-0: group is still subscribed to firstTopic. Do not expire.\n        // secondTopic-0: should expire as offset retention has passed.\n        // secondTopic-1: has not passed offset retention. Do not expire.\n        List<Record> expectedRecords = Collections.singletonList(\n            RecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"secondTopic\", 0)\n        );\n\n        when(groupMetadataManager.group(\"group-id\")).thenReturn(group);\n        when(group.offsetExpirationCondition()).thenReturn(Optional.of(\n            new OffsetExpirationConditionImpl(offsetAndMetadata -> offsetAndMetadata.commitTimestampMs)));\n        when(group.isSubscribedToTopic(\"firstTopic\")).thenReturn(true);\n        when(group.isSubscribedToTopic(\"secondTopic\")).thenReturn(false);\n\n        List<Record> records = new ArrayList<>();\n        assertFalse(context.cleanupExpiredOffsets(\"group-id\", records));\n        assertEquals(expectedRecords, records);\n\n        // Expire secondTopic-1.\n        context.time.sleep(500);\n        expectedRecords = Collections.singletonList(\n            RecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"secondTopic\", 1)\n        );\n\n        records = new ArrayList<>();\n        assertFalse(context.cleanupExpiredOffsets(\"group-id\", records));\n        assertEquals(expectedRecords, records);\n\n        // Add 2 more commits, then expire all.\n        when(group.isSubscribedToTopic(\"firstTopic\")).thenReturn(false);\n        context.commitOffset(\"group-id\", \"firstTopic\", 1, 100L, 0, commitTimestamp + 500);\n        context.commitOffset(\"group-id\", \"secondTopic\", 0, 101L, 0, commitTimestamp + 500);\n\n        expectedRecords = Arrays.asList(\n            RecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"firstTopic\", 0),\n            RecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"firstTopic\", 1),\n            RecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"secondTopic\", 0)\n        );\n\n        records = new ArrayList<>();\n        assertTrue(context.cleanupExpiredOffsets(\"group-id\", records));\n        assertEquals(expectedRecords, records);\n    }\n\n    static private OffsetFetchResponseData.OffsetFetchResponsePartitions mkOffsetPartitionResponse(\n        int partition,\n        long offset,\n        int leaderEpoch,\n        String metadata\n    ) {\n        return new OffsetFetchResponseData.OffsetFetchResponsePartitions()\n            .setPartitionIndex(partition)\n            .setCommittedOffset(offset)\n            .setCommittedLeaderEpoch(leaderEpoch)\n            .setMetadata(metadata);\n    }\n\n    static private OffsetFetchResponseData.OffsetFetchResponsePartitions mkInvalidOffsetPartitionResponse(int partition) {\n        return new OffsetFetchResponseData.OffsetFetchResponsePartitions()\n            .setPartitionIndex(partition)\n            .setCommittedOffset(INVALID_OFFSET)\n            .setCommittedLeaderEpoch(-1)\n            .setMetadata(\"\");\n    }\n\n    static private OffsetFetchResponseData.OffsetFetchResponsePartitions mkOffsetPartitionResponse(int partition, Errors error) {\n        return new OffsetFetchResponseData.OffsetFetchResponsePartitions()\n            .setPartitionIndex(partition)\n            .setErrorCode(error.code())\n            .setCommittedOffset(INVALID_OFFSET)\n            .setCommittedLeaderEpoch(-1)\n            .setMetadata(\"\");\n    }\n\n    @Test\n    public void testReplay() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        verifyReplay(context, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            0L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyReplay(context, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            1L,\n            200L,\n            OptionalInt.of(10),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyReplay(context, \"foo\", \"bar\", 1, new OffsetAndMetadata(\n            2L,\n            200L,\n            OptionalInt.of(10),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyReplay(context, \"foo\", \"bar\", 1, new OffsetAndMetadata(\n            3L,\n            300L,\n            OptionalInt.of(10),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.of(12345L)\n        ));\n    }\n\n    @Test\n    public void testTransactionalReplay() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        verifyTransactionalReplay(context, 5, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            0L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyTransactionalReplay(context, 5, \"foo\", \"bar\", 1, new OffsetAndMetadata(\n            1L,\n            101L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyTransactionalReplay(context, 5, \"bar\", \"zar\", 0, new OffsetAndMetadata(\n            2L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyTransactionalReplay(context, 5, \"bar\", \"zar\", 1, new OffsetAndMetadata(\n            3L,\n            101L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyTransactionalReplay(context, 6, \"foo\", \"bar\", 2, new OffsetAndMetadata(\n            4L,\n            102L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyTransactionalReplay(context, 6, \"foo\", \"bar\", 3, new OffsetAndMetadata(\n            5L,\n            102L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n    }\n\n    @Test\n    public void testReplayWithTombstone() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Verify replay adds the offset the map.\n        verifyReplay(context, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            0L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Create a tombstone record and replay it to delete the record.\n        context.replay(RecordHelpers.newOffsetCommitTombstoneRecord(\n            \"foo\",\n            \"bar\",\n            0\n        ));\n\n        // Verify that the offset is gone.\n        assertNull(context.offsetMetadataManager.offset(\"foo\", \"bar\", 0));\n    }\n\n    @Test\n    public void testReplayTransactionEndMarkerWithCommit() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Add regular offset commit.\n        verifyReplay(context, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            0L,\n            99L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Add pending transactional commit for producer id 5.\n        verifyTransactionalReplay(context, 5L, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            1L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Add pending transactional commit for producer id 6.\n        verifyTransactionalReplay(context, 6L, \"foo\", \"bar\", 1, new OffsetAndMetadata(\n            2L,\n            200L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Replaying an end marker with an unknown producer id should not fail.\n        context.replayEndTransactionMarker(1L, TransactionResult.COMMIT);\n\n        // Replaying an end marker to commit transaction of producer id 5.\n        context.replayEndTransactionMarker(5L, TransactionResult.COMMIT);\n\n        // The pending offset is removed...\n        assertNull(context.offsetMetadataManager.pendingTransactionalOffset(\n            5L,\n            \"foo\",\n            \"bar\",\n            0\n        ));\n\n        // ... and added to the main offset storage.\n        assertEquals(new OffsetAndMetadata(\n            1L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ), context.offsetMetadataManager.offset(\n            \"foo\",\n            \"bar\",\n            0\n        ));\n\n        // Replaying an end marker to abort transaction of producer id 6.\n        context.replayEndTransactionMarker(6L, TransactionResult.ABORT);\n\n        // The pending offset is removed from the pending offsets and\n        // it is not added to the main offset storage.\n        assertNull(context.offsetMetadataManager.pendingTransactionalOffset(\n            6L,\n            \"foo\",\n            \"bar\",\n            1\n        ));\n        assertNull(context.offsetMetadataManager.offset(\n            \"foo\",\n            \"bar\",\n            1\n        ));\n    }\n\n    @Test\n    public void testReplayTransactionEndMarkerKeepsTheMostRecentCommittedOffset() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Add pending transactional offset commit for producer id 5.\n        verifyTransactionalReplay(context, 5L, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            0L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Add regular offset commit.\n        verifyReplay(context, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            1L,\n            101L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Replaying an end marker to commit transaction of producer id 5.\n        context.replayEndTransactionMarker(5L, TransactionResult.COMMIT);\n\n        // The pending offset is removed...\n        assertNull(context.offsetMetadataManager.pendingTransactionalOffset(\n            5L,\n            \"foo\",\n            \"bar\",\n            0\n        ));\n\n        // ... but it is not added to the main storage because the regular\n        // committed offset is more recent.\n        assertEquals(new OffsetAndMetadata(\n            1L,\n            101L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ), context.offsetMetadataManager.offset(\n            \"foo\",\n            \"bar\",\n            0\n        ));\n    }\n\n    @Test\n    public void testOffsetCommitsSensor() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n        group.transitionTo(ClassicGroupState.STABLE);\n\n        CoordinatorResult<OffsetCommitResponseData, Record> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(1)\n                .setRetentionTimeMs(1234L)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Arrays.asList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L),\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(1)\n                                .setCommittedOffset(150L)\n                        ))\n                ))\n        );\n\n        verify(context.metrics).record(OFFSET_COMMITS_SENSOR_NAME, 2);\n    }\n\n    @Test\n    public void testOffsetsExpiredSensor() {\n        GroupMetadataManager groupMetadataManager = mock(GroupMetadataManager.class);\n        Group group = mock(Group.class);\n\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .withGroupMetadataManager(groupMetadataManager)\n            .withOffsetsRetentionMs(1000)\n            .build();\n\n        long commitTimestamp = context.time.milliseconds();\n\n        context.commitOffset(\"group-id\", \"firstTopic\", 0, 100L, 0, commitTimestamp);\n        context.commitOffset(\"group-id\", \"secondTopic\", 0, 100L, 0, commitTimestamp);\n        context.commitOffset(\"group-id\", \"secondTopic\", 1, 100L, 0, commitTimestamp + 500);\n\n        context.time.sleep(1000);\n\n        // firstTopic-0: group is still subscribed to firstTopic. Do not expire.\n        // secondTopic-0: should expire as offset retention has passed.\n        // secondTopic-1: has not passed offset retention. Do not expire.\n        List<Record> expectedRecords = Collections.singletonList(\n            RecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"secondTopic\", 0)\n        );\n\n        when(groupMetadataManager.group(\"group-id\")).thenReturn(group);\n        when(group.offsetExpirationCondition()).thenReturn(Optional.of(\n            new OffsetExpirationConditionImpl(offsetAndMetadata -> offsetAndMetadata.commitTimestampMs)));\n        when(group.isSubscribedToTopic(\"firstTopic\")).thenReturn(true);\n        when(group.isSubscribedToTopic(\"secondTopic\")).thenReturn(false);\n\n        List<Record> records = new ArrayList<>();\n        assertFalse(context.cleanupExpiredOffsets(\"group-id\", records));\n        assertEquals(expectedRecords, records);\n\n        // Expire secondTopic-1.\n        context.time.sleep(500);\n\n        records = new ArrayList<>();\n        assertFalse(context.cleanupExpiredOffsets(\"group-id\", records));\n        verify(context.metrics, times(2)).record(OFFSET_EXPIRED_SENSOR_NAME, 1);\n\n        // Add 2 more commits, then expire all.\n        when(group.isSubscribedToTopic(\"firstTopic\")).thenReturn(false);\n        context.commitOffset(\"group-id\", \"firstTopic\", 1, 100L, 0, commitTimestamp + 500);\n        context.commitOffset(\"group-id\", \"secondTopic\", 0, 101L, 0, commitTimestamp + 500);\n\n        records = new ArrayList<>();\n        assertTrue(context.cleanupExpiredOffsets(\"group-id\", records));\n        verify(context.metrics).record(OFFSET_EXPIRED_SENSOR_NAME, 3);\n    }\n\n    @Test\n    public void testOffsetDeletionsSensor() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\"foo\", true);\n\n        context.commitOffset(\"foo\", \"bar\", 0, 100L, 0);\n        context.commitOffset(\"foo\", \"bar\", 1, 150L, 0);\n        group.setSubscribedTopics(Optional.of(Collections.emptySet()));\n\n        OffsetDeleteRequestData.OffsetDeleteRequestTopicCollection requestTopicCollection =\n            new OffsetDeleteRequestData.OffsetDeleteRequestTopicCollection(Collections.singletonList(\n                new OffsetDeleteRequestData.OffsetDeleteRequestTopic()\n                    .setName(\"bar\")\n                    .setPartitions(Arrays.asList(\n                        new OffsetDeleteRequestData.OffsetDeleteRequestPartition().setPartitionIndex(0),\n                        new OffsetDeleteRequestData.OffsetDeleteRequestPartition().setPartitionIndex(1)\n                    ))\n            ).iterator());\n\n        context.deleteOffsets(\n            new OffsetDeleteRequestData()\n                .setGroupId(\"foo\")\n                .setTopics(requestTopicCollection)\n        );\n\n        verify(context.metrics).record(OFFSET_DELETIONS_SENSOR_NAME, 2);\n    }\n\n    private void verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    ) {\n        context.replay(RecordHelpers.newOffsetCommitRecord(\n            groupId,\n            topic,\n            partition,\n            offsetAndMetadata,\n            MetadataImage.EMPTY.features().metadataVersion()\n        ));\n\n        assertEquals(offsetAndMetadata, context.offsetMetadataManager.offset(\n            groupId,\n            topic,\n            partition\n        ));\n    }\n\n    private void verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    ) {\n        context.replay(producerId, RecordHelpers.newOffsetCommitRecord(\n            groupId,\n            topic,\n            partition,\n            offsetAndMetadata,\n            MetadataImage.EMPTY.features().metadataVersion()\n        ));\n\n        assertEquals(offsetAndMetadata, context.offsetMetadataManager.pendingTransactionalOffset(\n            producerId,\n            groupId,\n            topic,\n            partition\n        ));\n    }\n\n    private ClassicGroupMember mkGenericMember(\n        String memberId,\n        Optional<String> groupInstanceId\n    ) {\n        return new ClassicGroupMember(\n            memberId,\n            groupInstanceId,\n            \"client-id\",\n            \"host\",\n            5000,\n            5000,\n            \"consumer\",\n            new JoinGroupRequestData.JoinGroupRequestProtocolCollection(\n                Collections.singletonList(new JoinGroupRequestData.JoinGroupRequestProtocol()\n                    .setName(\"range\")\n                    .setMetadata(new byte[0])\n                ).iterator()\n            )\n        );\n    }\n}",
                "methodCount": 89
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 11,
                "candidates": [
                    {
                        "lineStart": 189,
                        "lineEnd": 193,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method commitOffset to class OffsetMetadataManager",
                        "description": "Move method commitOffset to org.apache.kafka.coordinator.group.OffsetMetadataManager\nRationale: The `commitOffset` method is directly related to handling OffsetCommitRequestData and produces CoordinatorResults, which are central to the functionality of the `OffsetMetadataManager`. This class also already contains methods that deal with validating and committing offsets, such as `validateOffsetCommit` and `commitTransactionalOffset`. Additionally, its responsibilities include managing offsets and handling requests related to offset commits, making it the most appropriate class for the `commitOffset` method.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 375,
                        "lineEnd": 392,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method commitOffset to class OffsetMetadataManager",
                        "description": "Move method commitOffset to org.apache.kafka.coordinator.group.OffsetMetadataManager\nRationale: The commitOffset() method is concerned with managing offsets, which is directly related to the responsibilities of OffsetMetadataManager. The OffsetMetadataManager class already contains methods that handle offset commits and manages the state of offsets for different groups, topics, and partitions. This aligns perfectly with the functionality of the commitOffset() method, which simplifies offset management tasks such as committing offsets and updating the state of these offsets in the system.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 358,
                        "lineEnd": 373,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method commitOffset to class MockTime",
                        "description": "Move method commitOffset to org.apache.kafka.common.utils.MockTime\nRationale: The commitOffset() method relies on the time.milliseconds() method, which is part of the MockTime class. This strong dependency suggests that the commitOffset() method is closely related to the functionality provided by MockTime. Moving commitOffset() to MockTime would ensure encapsulation of time-related operations in one place, making the codebase easier to maintain and understand. Additionally, MockTime is designed to handle operations that involve manual advancement of time, which aligns well with the purpose of the commitOffset() method, as it likely involves operations dependent on precise timing.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 285,
                        "lineEnd": 297,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method fetchOffsets to class OffsetMetadataManager",
                        "description": "Move method fetchOffsets to org.apache.kafka.coordinator.group.OffsetMetadataManager\nRationale: The `fetchOffsets` method is directly involved with the offset fetching logic related to groups, which fits within the responsibility of the `OffsetMetadataManager`. This class already contains other methods related to offset management such as validation of offset fetch requests and offset commits, making it the natural home for the `fetchOffsets` method. In contrast, `MockTime` handles time management and `GroupCoordinatorMetricsShard` is focused on metrics collection, neither of which align with the functionality of fetching offsets for groups.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 318,
                        "lineEnd": 328,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method fetchAllOffsets to class OffsetMetadataManager",
                        "description": "Move method fetchAllOffsets to org.apache.kafka.coordinator.group.OffsetMetadataManager\nRationale: The `fetchAllOffsets` method deals with fetching offsets based on group IDs and committed offsets, which directly correlates with the responsibility of the `OffsetMetadataManager` class. This class already contains methods that manage offsets, such as `fetchOffsets`, `deleteOffsets`, `commitOffset`, and validation methods. Also, `OffsetMetadataManager` is tightly coupled with `GroupMetadataManager`, `SnapshotRegistry`, and the `Offsets` class, which are essential components for managing offset metadata.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 2895,
                        "lineEnd": 2915,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method verifyReplay to class OffsetMetadataManagerTestContext",
                        "description": "Move method verifyReplay to org.apache.kafka.coordinator.group.OffsetMetadataManagerTest.OffsetMetadataManagerTestContext\nRationale: The method 'verifyReplay' primarily interacts with instances and methods from the OffsetMetadataManagerTestContext class and not directly with its current class. It uses the OffsetMetadataManagerTestContext for both replaying records and verifying offsets. Hence, it belongs to OffsetMetadataManagerTestContext where it can directly utilize and validate its state and behavior.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 2917,
                        "lineEnd": 2939,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method verifyTransactionalReplay to class OffsetMetadataManagerTestContext",
                        "description": "Move method verifyTransactionalReplay to org.apache.kafka.coordinator.group.OffsetMetadataManagerTest.OffsetMetadataManagerTestContext\nRationale: The method verifyTransactionalReplay is tightly coupled with the OffsetMetadataManagerTestContext class. It uses context.replay() and validates the results using context.offsetMetadataManager.pendingTransactionalOffset(). Both of these methods are part of the functionality provided by OffsetMetadataManagerTestContext. This makes it more appropriate to keep this method as part of OffsetMetadataManagerTestContext, ensuring that the method has access to the necessary context and the related components.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 536,
                        "lineEnd": 542,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method hasOffset to class OffsetMetadataManager",
                        "description": "Move method hasOffset to org.apache.kafka.coordinator.group.OffsetMetadataManager\nRationale: The method `hasOffset` directly interacts with the `offset` method of the `OffsetMetadataManager` class to check if an offset exists for a provided group, topic, and partition. Since it primarily operates on the offset data, it logically belongs to the `OffsetMetadataManager` class. Additionally, placing this method within `OffsetMetadataManager` maintains encapsulation and adheres to the single responsibility principle by keeping all offset-related operations together.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 108,
                        "lineEnd": 111,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method withOffsetsRetentionMs to class GroupCoordinatorConfig",
                        "description": "Move method withOffsetsRetentionMs to org.apache.kafka.coordinator.group.GroupCoordinatorConfig\nRationale: The withOffsetsRetentionMs method is closely related to the configuration settings of the GroupCoordinatorConfig class, as it is responsible for setting the offsets retention time which is a property managed by this class. Placing this method inside GroupCoordinatorConfig enhances cohesion by keeping the configuration logic centralized. Additionally, moving this method here reduces the dependency on external classes to manage configuration instantiation and parameter settings.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 103,
                        "lineEnd": 106,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method withOffsetMetadataMaxSize to class GroupCoordinatorConfig",
                        "description": "Move method withOffsetMetadataMaxSize to org.apache.kafka.coordinator.group.GroupCoordinatorConfig\nRationale: The method 'withOffsetMetadataMaxSize' is configuring parameters specific to the GroupCoordinatorConfig class, such as 'offsetMetadataMaxSize'. This method manipulates the configuration directly relevant to GroupCoordinatorConfig, and therefore, it logically belongs within that class. Moving the method to GroupCoordinatorConfig will centralize configuration logic, making it clearer, easier to maintain, and more coherent.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 430,
                        "lineEnd": 436,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method messageOrNull to class ApiMessageAndVersion",
                        "description": "Move method messageOrNull to org.apache.kafka.server.common.ApiMessageAndVersion\nRationale: The method messageOrNull(ApiMessageAndVersion apiMessageAndVersion) deals directly with the ApiMessageAndVersion class by accessing its message method. This implies a strong relation where the method is presumably providing a utility function specific to ApiMessageAndVersion objects. Moving it to ApiMessageAndVersion consolidates all related functionalities within one class, improving encapsulation and cohesion.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "withOffsetMetadataMaxSize",
                            "method_signature": " withOffsetMetadataMaxSize(int offsetMetadataMaxSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withOffsetsRetentionMs",
                            "method_signature": " withOffsetsRetentionMs(long offsetsRetentionMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withGroupMetadataManager",
                            "method_signature": " withGroupMetadataManager(GroupMetadataManager groupMetadataManager)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": " build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commit",
                            "method_signature": "public commit()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            short version,\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitTransactionalOffset",
                            "method_signature": "public commitTransactionalOffset(\n            TxnOffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteOffsets",
                            "method_signature": "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteAllOffsets",
                            "method_signature": "public deleteAllOffsets(\n            String groupId,\n            List<Record> records\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "cleanupExpiredOffsets",
                            "method_signature": "public cleanupExpiredOffsets(String groupId, List<Record> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public fetchOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public fetchAllOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sleep",
                            "method_signature": "public sleep(long ms)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            long producerId,\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteOffset",
                            "method_signature": "public deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            Record record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            long producerId,\n            Record record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testOffsetDeleteWith",
                            "method_signature": "public testOffsetDeleteWith(\n            String groupId,\n            String topic,\n            int partition,\n            Errors expectedError\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasOffset",
                            "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testOffsetCommitWithUnknownGroup",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testOffsetCommitWithUnknownGroup(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testConsumerGroupOffsetCommitWithVersionSmallerThanVersion9",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testConsumerGroupOffsetCommitWithVersionSmallerThanVersion9(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testDeleteGroupAllOffsets",
                            "method_signature": "@ParameterizedTest\n    @EnumSource(Group.GroupType.class)\n    public testDeleteGroupAllOffsets(Group.GroupType groupType)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getOrMaybeCreateGroup",
                            "method_signature": "private getOrMaybeCreateGroup(Group.GroupType groupType, OffsetMetadataManagerTestContext context)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "mkOffsetPartitionResponse",
                            "method_signature": "static private mkOffsetPartitionResponse(\n        int partition,\n        long offset,\n        int leaderEpoch,\n        String metadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "mkInvalidOffsetPartitionResponse",
                            "method_signature": "static private mkInvalidOffsetPartitionResponse(int partition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "mkOffsetPartitionResponse",
                            "method_signature": "static private mkOffsetPartitionResponse(int partition, Errors error)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyReplay",
                            "method_signature": "private verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyTransactionalReplay",
                            "method_signature": "private verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "mkGenericMember",
                            "method_signature": "private mkGenericMember(\n        String memberId,\n        Optional<String> groupInstanceId\n    )",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "verifyTransactionalReplay",
                            "method_signature": "private verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyReplay",
                            "method_signature": "private verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            Record record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasOffset",
                            "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withOffsetMetadataMaxSize",
                            "method_signature": " withOffsetMetadataMaxSize(int offsetMetadataMaxSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteOffset",
                            "method_signature": "public deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withOffsetsRetentionMs",
                            "method_signature": " withOffsetsRetentionMs(long offsetsRetentionMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteOffsets",
                            "method_signature": "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "verifyTransactionalReplay",
                            "method_signature": "private verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyReplay",
                            "method_signature": "private verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            Record record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasOffset",
                            "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withOffsetMetadataMaxSize",
                            "method_signature": " withOffsetMetadataMaxSize(int offsetMetadataMaxSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteOffset",
                            "method_signature": "public deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withOffsetsRetentionMs",
                            "method_signature": " withOffsetsRetentionMs(long offsetsRetentionMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteOffsets",
                            "method_signature": "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )": {
                    "first": {
                        "method_name": "verifyTransactionalReplay",
                        "method_signature": "private verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.17700207178686908
                },
                "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )": {
                    "first": {
                        "method_name": "replayEndTransactionMarker",
                        "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.17992824182661685
                },
                "private verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )": {
                    "first": {
                        "method_name": "verifyReplay",
                        "method_signature": "private verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.1808396379628798
                },
                "private replay(\n            Record record\n        )": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "private replay(\n            Record record\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.1877122740011324
                },
                "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )": {
                    "first": {
                        "method_name": "hasOffset",
                        "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.19491347759936603
                },
                " withOffsetMetadataMaxSize(int offsetMetadataMaxSize)": {
                    "first": {
                        "method_name": "withOffsetMetadataMaxSize",
                        "method_signature": " withOffsetMetadataMaxSize(int offsetMetadataMaxSize)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.19566351235320667
                },
                "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        )": {
                    "first": {
                        "method_name": "commitOffset",
                        "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.20294075034565348
                },
                "public fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )": {
                    "first": {
                        "method_name": "fetchOffsets",
                        "method_signature": "public fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2073997305938372
                },
                "public deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        )": {
                    "first": {
                        "method_name": "deleteOffset",
                        "method_signature": "public deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.21238367323571578
                },
                "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )": {
                    "first": {
                        "method_name": "commitOffset",
                        "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.21445714946446603
                },
                "public commitOffset(\n            OffsetCommitRequestData request\n        )": {
                    "first": {
                        "method_name": "commitOffset",
                        "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.22022156181641017
                },
                "public fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        )": {
                    "first": {
                        "method_name": "fetchAllOffsets",
                        "method_signature": "public fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.22064468411356009
                },
                " withOffsetsRetentionMs(long offsetsRetentionMs)": {
                    "first": {
                        "method_name": "withOffsetsRetentionMs",
                        "method_signature": " withOffsetsRetentionMs(long offsetsRetentionMs)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.23639940689664912
                },
                "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)": {
                    "first": {
                        "method_name": "messageOrNull",
                        "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2366464419842501
                },
                "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )": {
                    "first": {
                        "method_name": "deleteOffsets",
                        "method_signature": "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.24805272351352323
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "private replay(\n            Record record\n        )",
                    "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                    "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                    "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        )",
                    "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                    "public fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                    "public fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        )",
                    "private verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                    "private verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                    "public deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                    "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )",
                    "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                    " withOffsetsRetentionMs(long offsetsRetentionMs)",
                    " withOffsetMetadataMaxSize(int offsetMetadataMaxSize)",
                    "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)"
                ],
                "llm_response_time": 12236
            },
            "targetClassMap": {
                "replay": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 1999,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "replayEndTransactionMarker": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2842,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "commitOffset": {
                    "target_classes": [
                        {
                            "class_name": "MockTime",
                            "similarity_score": 0.27134238459003635
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "MockTime"
                    ],
                    "llm_response_time": 2974,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "fetchOffsets": {
                    "target_classes": [
                        {
                            "class_name": "MockTime",
                            "similarity_score": 0.29140866251201486
                        },
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.19588008878590737
                        },
                        {
                            "class_name": "GroupCoordinatorMetricsShard",
                            "similarity_score": 0.20744234793771407
                        },
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.1837561824056427
                        },
                        {
                            "class_name": "OffsetMetadataManager",
                            "similarity_score": 0.22313034595199158
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "OffsetMetadataManager",
                        "GroupCoordinatorMetricsShard",
                        "MockTime"
                    ],
                    "llm_response_time": 5662,
                    "similarity_computation_time": 10,
                    "similarity_metric": "cosine"
                },
                "fetchAllOffsets": {
                    "target_classes": [
                        {
                            "class_name": "MockTime",
                            "similarity_score": 0.3215270791515118
                        },
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.21612519088609763
                        },
                        {
                            "class_name": "GroupCoordinatorMetricsShard",
                            "similarity_score": 0.22888246234613863
                        },
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.20274822339052034
                        },
                        {
                            "class_name": "OffsetMetadataManager",
                            "similarity_score": 0.24619188662949326
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "OffsetMetadataManager",
                        "GroupCoordinatorMetricsShard",
                        "MockTime"
                    ],
                    "llm_response_time": 5940,
                    "similarity_computation_time": 6,
                    "similarity_metric": "cosine"
                },
                "verifyReplay": {
                    "target_classes": [
                        {
                            "class_name": "OffsetMetadataManagerTestContext",
                            "similarity_score": 0.4127327425271779
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "OffsetMetadataManagerTestContext"
                    ],
                    "llm_response_time": 2190,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                },
                "verifyTransactionalReplay": {
                    "target_classes": [
                        {
                            "class_name": "OffsetMetadataManagerTestContext",
                            "similarity_score": 0.4216995944551269
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "OffsetMetadataManagerTestContext"
                    ],
                    "llm_response_time": 2510,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "deleteOffset": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3125,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "deleteOffsets": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 4269,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "hasOffset": {
                    "target_classes": [
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.21711431383910185
                        },
                        {
                            "class_name": "OffsetMetadataManager",
                            "similarity_score": 0.2884348722688805
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "OffsetMetadataManager",
                        "GroupMetadataManager"
                    ],
                    "llm_response_time": 8600,
                    "similarity_computation_time": 6,
                    "similarity_metric": "cosine"
                },
                "withOffsetsRetentionMs": {
                    "target_classes": [
                        {
                            "class_name": "GroupCoordinatorConfig",
                            "similarity_score": 0.08434089989487624
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GroupCoordinatorConfig"
                    ],
                    "llm_response_time": 2141,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "withOffsetMetadataMaxSize": {
                    "target_classes": [
                        {
                            "class_name": "GroupCoordinatorConfig",
                            "similarity_score": 0.2926722396852698
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GroupCoordinatorConfig"
                    ],
                    "llm_response_time": 1736,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "messageOrNull": {
                    "target_classes": [
                        {
                            "class_name": "ApiMessageAndVersion",
                            "similarity_score": 0.6106401198187058
                        },
                        {
                            "class_name": "MockTime",
                            "similarity_score": 0.5914470146353319
                        },
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.4220009252215372
                        },
                        {
                            "class_name": "GroupCoordinatorMetricsShard",
                            "similarity_score": 0.4615361902486966
                        },
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.38725094003252886
                        },
                        {
                            "class_name": "OffsetMetadataManager",
                            "similarity_score": 0.49188657722411167
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "ApiMessageAndVersion",
                        "OffsetMetadataManager",
                        "MockTime"
                    ],
                    "llm_response_time": 4257,
                    "similarity_computation_time": 7,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "0472db2cd39d4f53c3855063117bdae15f4d0d86",
        "url": "https://github.com/apache/kafka/commit/0472db2cd39d4f53c3855063117bdae15f4d0d86",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public combineFutures(futures List<CompletableFuture<T>>, init Supplier<T>, add BiConsumer<T,T>) : CompletableFuture<T> extracted from public consumerGroupDescribe(context RequestContext, groupIds List<String>) : CompletableFuture<List<ConsumerGroupDescribeResponseData.DescribedGroup>> in class org.apache.kafka.coordinator.group.GroupCoordinatorService & moved to class org.apache.kafka.server.util.FutureUtils",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 560,
                    "endLine": 619,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public consumerGroupDescribe(context RequestContext, groupIds List<String>) : CompletableFuture<List<ConsumerGroupDescribeResponseData.DescribedGroup>>"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 613,
                    "endLine": 613,
                    "startColumn": 9,
                    "endColumn": 119,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 617,
                    "endLine": 617,
                    "startColumn": 13,
                    "endColumn": 24,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 616,
                    "endLine": 616,
                    "startColumn": 13,
                    "endColumn": 66,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 614,
                    "endLine": 618,
                    "startColumn": 42,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 614,
                    "endLine": 618,
                    "startColumn": 9,
                    "endColumn": 12,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 106,
                    "endLine": 127,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public combineFutures(futures List<CompletableFuture<T>>, init Supplier<T>, add BiConsumer<T,T>) : CompletableFuture<T>"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 121,
                    "endLine": 121,
                    "startColumn": 9,
                    "endColumn": 119,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 125,
                    "endLine": 125,
                    "startColumn": 13,
                    "endColumn": 24,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 124,
                    "endLine": 124,
                    "startColumn": 13,
                    "endColumn": 71,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 122,
                    "endLine": 126,
                    "startColumn": 42,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 122,
                    "endLine": 126,
                    "startColumn": 9,
                    "endColumn": 12,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 534,
                    "endLine": 583,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public consumerGroupDescribe(context RequestContext, groupIds List<String>) : CompletableFuture<List<ConsumerGroupDescribeResponseData.DescribedGroup>>"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 582,
                    "endLine": 582,
                    "startColumn": 16,
                    "endColumn": 81,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "FutureUtils.combineFutures(futures,ArrayList::new,List::addAll)"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 123,
                    "endLine": 123,
                    "startColumn": 13,
                    "endColumn": 38,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": true
        },
        "ref_id": 569,
        "extraction_results": {
            "success": true,
            "newCommitHash": "194327a1030ebdb592a03af369090178521cf665",
            "newBranchName": "extract-combineFutures-consumerGroupDescribe-016bd68"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "0472db2cd39d4f53c3855063117bdae15f4d0d86",
        "url": "https://github.com/apache/kafka/commit/0472db2cd39d4f53c3855063117bdae15f4d0d86",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public combineFutures(futures List<CompletableFuture<T>>, init Supplier<T>, add BiConsumer<T,T>) : CompletableFuture<T> extracted from public describeGroups(context RequestContext, groupIds List<String>) : CompletableFuture<List<DescribeGroupsResponseData.DescribedGroup>> in class org.apache.kafka.coordinator.group.GroupCoordinatorService & moved to class org.apache.kafka.server.util.FutureUtils",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 621,
                    "endLine": 682,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public describeGroups(context RequestContext, groupIds List<String>) : CompletableFuture<List<DescribeGroupsResponseData.DescribedGroup>>"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 676,
                    "endLine": 676,
                    "startColumn": 9,
                    "endColumn": 119,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 680,
                    "endLine": 680,
                    "startColumn": 13,
                    "endColumn": 24,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 679,
                    "endLine": 679,
                    "startColumn": 13,
                    "endColumn": 66,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 677,
                    "endLine": 681,
                    "startColumn": 42,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 677,
                    "endLine": 681,
                    "startColumn": 9,
                    "endColumn": 12,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 106,
                    "endLine": 127,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public combineFutures(futures List<CompletableFuture<T>>, init Supplier<T>, add BiConsumer<T,T>) : CompletableFuture<T>"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 121,
                    "endLine": 121,
                    "startColumn": 9,
                    "endColumn": 119,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 125,
                    "endLine": 125,
                    "startColumn": 13,
                    "endColumn": 24,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 124,
                    "endLine": 124,
                    "startColumn": 13,
                    "endColumn": 71,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 122,
                    "endLine": 126,
                    "startColumn": 42,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 122,
                    "endLine": 126,
                    "startColumn": 9,
                    "endColumn": 12,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 585,
                    "endLine": 636,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public describeGroups(context RequestContext, groupIds List<String>) : CompletableFuture<List<DescribeGroupsResponseData.DescribedGroup>>"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 635,
                    "endLine": 635,
                    "startColumn": 16,
                    "endColumn": 81,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "FutureUtils.combineFutures(futures,ArrayList::new,List::addAll)"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 123,
                    "endLine": 123,
                    "startColumn": 13,
                    "endColumn": 38,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": true
        },
        "ref_id": 570,
        "extraction_results": {
            "success": true,
            "newCommitHash": "4188c4e8760841b9d5334cfe509818e000a01ad0",
            "newBranchName": "extract-combineFutures-describeGroups-016bd68"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "0472db2cd39d4f53c3855063117bdae15f4d0d86",
        "url": "https://github.com/apache/kafka/commit/0472db2cd39d4f53c3855063117bdae15f4d0d86",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public combineFutures(futures List<CompletableFuture<T>>, init Supplier<T>, add BiConsumer<T,T>) : CompletableFuture<T> extracted from public deleteGroups(context RequestContext, groupIds List<String>, bufferSupplier BufferSupplier) : CompletableFuture<DeleteGroupsResponseData.DeletableGroupResultCollection> in class org.apache.kafka.coordinator.group.GroupCoordinatorService & moved to class org.apache.kafka.server.util.FutureUtils",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 684,
                    "endLine": 745,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public deleteGroups(context RequestContext, groupIds List<String>, bufferSupplier BufferSupplier) : CompletableFuture<DeleteGroupsResponseData.DeletableGroupResultCollection>"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 733,
                    "endLine": 733,
                    "startColumn": 9,
                    "endColumn": 119,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 743,
                    "endLine": 743,
                    "startColumn": 13,
                    "endColumn": 24,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 736,
                    "endLine": 742,
                    "startColumn": 13,
                    "endColumn": 15,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 734,
                    "endLine": 744,
                    "startColumn": 43,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 734,
                    "endLine": 744,
                    "startColumn": 9,
                    "endColumn": 12,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 106,
                    "endLine": 127,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public combineFutures(futures List<CompletableFuture<T>>, init Supplier<T>, add BiConsumer<T,T>) : CompletableFuture<T>"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 121,
                    "endLine": 121,
                    "startColumn": 9,
                    "endColumn": 119,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 125,
                    "endLine": 125,
                    "startColumn": 13,
                    "endColumn": 24,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 124,
                    "endLine": 124,
                    "startColumn": 13,
                    "endColumn": 71,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 122,
                    "endLine": 126,
                    "startColumn": 42,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 122,
                    "endLine": 126,
                    "startColumn": 9,
                    "endColumn": 12,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 638,
                    "endLine": 694,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public deleteGroups(context RequestContext, groupIds List<String>, bufferSupplier BufferSupplier) : CompletableFuture<DeleteGroupsResponseData.DeletableGroupResultCollection>"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 690,
                    "endLine": 693,
                    "startColumn": 16,
                    "endColumn": 108,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "FutureUtils.combineFutures(futures,DeleteGroupsResponseData.DeletableGroupResultCollection::new,(accumulator,newResults) -> newResults.forEach(result -> accumulator.add(result.duplicate())))"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 123,
                    "endLine": 123,
                    "startColumn": 13,
                    "endColumn": 38,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": true
        },
        "ref_id": 571,
        "extraction_results": {
            "success": true,
            "newCommitHash": "05e534aa47ab77057acd03014cfa4c015157adbd",
            "newBranchName": "extract-combineFutures-deleteGroups-016bd68"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4f0a40590833a141c78341ce95ffc782747c5ac8",
        "url": "https://github.com/apache/kafka/commit/4f0a40590833a141c78341ce95ffc782747c5ac8",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public tasksMax() : int extracted from public connectorTaskConfigs(connName String, connConfig ConnectorConfig) : List<Map<String,String>> in class org.apache.kafka.connect.runtime.Worker & moved to class org.apache.kafka.connect.runtime.ConnectorConfig",
            "leftSideLocations": [
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java",
                    "startLine": 377,
                    "endLine": 414,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public connectorTaskConfigs(connName String, connConfig ConnectorConfig) : List<Map<String,String>>"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java",
                    "startLine": 392,
                    "endLine": 392,
                    "startColumn": 13,
                    "endColumn": 80,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java",
                    "startLine": 295,
                    "endLine": 297,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public tasksMax() : int"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java",
                    "startLine": 296,
                    "endLine": 296,
                    "startColumn": 9,
                    "endColumn": 41,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java",
                    "startLine": 377,
                    "endLine": 422,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public connectorTaskConfigs(connName String, connConfig ConnectorConfig) : List<Map<String,String>>"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java",
                    "startLine": 392,
                    "endLine": 392,
                    "startColumn": 28,
                    "endColumn": 49,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "connConfig.tasksMax()"
                }
            ],
            "isStatic": false
        },
        "ref_id": 572,
        "extraction_results": {
            "success": true,
            "newCommitHash": "617c4dda13dda70600b65f8dc16964d46f996a68",
            "newBranchName": "extract-tasksMax-connectorTaskConfigs-8da6508"
        },
        "telemetry": {
            "id": "85251c00-1b16-4293-9f60-29716143538c",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 2193,
                "lineStart": 128,
                "lineEnd": 2320,
                "bodyLineStart": 128,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java",
                "sourceCode": "/**\n * <p>\n * Worker runs a (dynamic) set of tasks in a set of threads, doing the work of actually moving\n * data to/from Kafka.\n * </p>\n * <p>\n * Since each task has a dedicated thread, this is mainly just a container for them.\n * </p>\n */\npublic class Worker {\n\n    public static final long CONNECTOR_GRACEFUL_SHUTDOWN_TIMEOUT_MS = TimeUnit.SECONDS.toMillis(5);\n    public static final long EXECUTOR_SHUTDOWN_TERMINATION_TIMEOUT_MS = TimeUnit.SECONDS.toMillis(1);\n\n    private static final Logger log = LoggerFactory.getLogger(Worker.class);\n\n    protected Herder herder;\n    private final ExecutorService executor;\n    private final Time time;\n    private final String workerId;\n    //kafka cluster id\n    private final String kafkaClusterId;\n    private final Plugins plugins;\n    private final ConnectMetrics metrics;\n    private final WorkerMetricsGroup workerMetricsGroup;\n    private ConnectorStatusMetricsGroup connectorStatusMetricsGroup;\n    private final WorkerConfig config;\n    private final Converter internalKeyConverter;\n    private final Converter internalValueConverter;\n    private final OffsetBackingStore globalOffsetBackingStore;\n\n    private final ConcurrentMap<String, WorkerConnector> connectors = new ConcurrentHashMap<>();\n    private final ConcurrentMap<ConnectorTaskId, WorkerTask<?, ?>> tasks = new ConcurrentHashMap<>();\n    private Optional<SourceTaskOffsetCommitter> sourceTaskOffsetCommitter = Optional.empty();\n    private final WorkerConfigTransformer workerConfigTransformer;\n    private final ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy;\n    private final Function<Map<String, Object>, Admin> adminFactory;\n\n    public Worker(\n        String workerId,\n        Time time,\n        Plugins plugins,\n        WorkerConfig config,\n        OffsetBackingStore globalOffsetBackingStore,\n        ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy) {\n        this(workerId, time, plugins, config, globalOffsetBackingStore, Executors.newCachedThreadPool(), connectorClientConfigOverridePolicy, Admin::create);\n    }\n\n    @SuppressWarnings(\"this-escape\")\n    Worker(\n            String workerId,\n            Time time,\n            Plugins plugins,\n            WorkerConfig config,\n            OffsetBackingStore globalOffsetBackingStore,\n            ExecutorService executorService,\n            ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n            Function<Map<String, Object>, Admin> adminFactory\n    ) {\n        this.kafkaClusterId = config.kafkaClusterId();\n        this.metrics = new ConnectMetrics(workerId, config, time, kafkaClusterId);\n        this.executor = executorService;\n        this.workerId = workerId;\n        this.time = time;\n        this.plugins = plugins;\n        this.config = config;\n        this.connectorClientConfigOverridePolicy = connectorClientConfigOverridePolicy;\n        this.workerMetricsGroup = new WorkerMetricsGroup(this.connectors, this.tasks, metrics);\n\n        Map<String, String> internalConverterConfig = Collections.singletonMap(JsonConverterConfig.SCHEMAS_ENABLE_CONFIG, \"false\");\n        this.internalKeyConverter = plugins.newInternalConverter(true, JsonConverter.class.getName(), internalConverterConfig);\n        this.internalValueConverter = plugins.newInternalConverter(false, JsonConverter.class.getName(), internalConverterConfig);\n\n        this.globalOffsetBackingStore = globalOffsetBackingStore;\n\n        this.workerConfigTransformer = initConfigTransformer();\n        this.adminFactory = adminFactory;\n    }\n\n    private WorkerConfigTransformer initConfigTransformer() {\n        final List<String> providerNames = config.getList(WorkerConfig.CONFIG_PROVIDERS_CONFIG);\n        Map<String, ConfigProvider> providerMap = new HashMap<>();\n        for (String providerName : providerNames) {\n            ConfigProvider configProvider = plugins.newConfigProvider(\n                    config,\n                    WorkerConfig.CONFIG_PROVIDERS_CONFIG + \".\" + providerName,\n                    ClassLoaderUsage.PLUGINS\n            );\n            providerMap.put(providerName, configProvider);\n        }\n        return new WorkerConfigTransformer(this, providerMap);\n    }\n\n    public WorkerConfigTransformer configTransformer() {\n        return workerConfigTransformer;\n    }\n\n    protected Herder herder() {\n        return herder;\n    }\n\n    /**\n     * Start worker.\n     */\n    public void start() {\n        log.info(\"Worker starting\");\n\n        globalOffsetBackingStore.start();\n\n        sourceTaskOffsetCommitter = config.exactlyOnceSourceEnabled()\n                ? Optional.empty()\n                : Optional.of(new SourceTaskOffsetCommitter(config));\n\n        connectorStatusMetricsGroup = new ConnectorStatusMetricsGroup(metrics, tasks, herder);\n\n        log.info(\"Worker started\");\n    }\n\n    /**\n     * Stop worker.\n     */\n    public void stop() {\n        log.info(\"Worker stopping\");\n\n        long started = time.milliseconds();\n        long limit = started + config.getLong(WorkerConfig.TASK_SHUTDOWN_GRACEFUL_TIMEOUT_MS_CONFIG);\n\n        if (!connectors.isEmpty()) {\n            log.warn(\"Shutting down connectors {} uncleanly; herder should have shut down connectors before the Worker is stopped\", connectors.keySet());\n            stopAndAwaitConnectors();\n        }\n\n        if (!tasks.isEmpty()) {\n            log.warn(\"Shutting down tasks {} uncleanly; herder should have shut down tasks before the Worker is stopped\", tasks.keySet());\n            stopAndAwaitTasks();\n        }\n\n        long timeoutMs = limit - time.milliseconds();\n        sourceTaskOffsetCommitter.ifPresent(committer -> committer.close(timeoutMs));\n\n        globalOffsetBackingStore.stop();\n        metrics.stop();\n\n        log.info(\"Worker stopped\");\n\n        workerMetricsGroup.close();\n        if (connectorStatusMetricsGroup != null) {\n            connectorStatusMetricsGroup.close();\n        }\n\n        workerConfigTransformer.close();\n        ThreadUtils.shutdownExecutorServiceQuietly(executor, EXECUTOR_SHUTDOWN_TERMINATION_TIMEOUT_MS, TimeUnit.MILLISECONDS);\n    }\n\n    /**\n     * Start a connector managed by this worker.\n     *\n     * @param connName the connector name.\n     * @param connProps the properties of the connector.\n     * @param ctx the connector runtime context.\n     * @param statusListener a listener for the runtime status transitions of the connector.\n     * @param initialState the initial state of the connector.\n     * @param onConnectorStateChange invoked when the initial state change of the connector is completed\n     */\n    public void startConnector(\n            String connName,\n            Map<String, String> connProps,\n            CloseableConnectorContext ctx,\n            ConnectorStatus.Listener statusListener,\n            TargetState initialState,\n            Callback<TargetState> onConnectorStateChange\n    ) {\n        final ConnectorStatus.Listener connectorStatusListener = workerMetricsGroup.wrapStatusListener(statusListener);\n        try (LoggingContext loggingContext = LoggingContext.forConnector(connName)) {\n            if (connectors.containsKey(connName)) {\n                onConnectorStateChange.onCompletion(\n                        new ConnectException(\"Connector with name \" + connName + \" already exists\"),\n                        null);\n                return;\n            }\n\n            final WorkerConnector workerConnector;\n            final String connClass = connProps.get(ConnectorConfig.CONNECTOR_CLASS_CONFIG);\n            ClassLoader connectorLoader = plugins.connectorLoader(connClass);\n            try (LoaderSwap loaderSwap = plugins.withClassLoader(connectorLoader)) {\n                log.info(\"Creating connector {} of type {}\", connName, connClass);\n                final Connector connector = plugins.newConnector(connClass);\n                final ConnectorConfig connConfig;\n                final CloseableOffsetStorageReader offsetReader;\n                final ConnectorOffsetBackingStore offsetStore;\n                if (ConnectUtils.isSinkConnector(connector)) {\n                    connConfig = new SinkConnectorConfig(plugins, connProps);\n                    offsetReader = null;\n                    offsetStore = null;\n                } else {\n                    SourceConnectorConfig sourceConfig = new SourceConnectorConfig(plugins, connProps, config.topicCreationEnable());\n                    connConfig = sourceConfig;\n\n                    // Set up the offset backing store for this connector instance\n                    offsetStore = config.exactlyOnceSourceEnabled()\n                            ? offsetStoreForExactlyOnceSourceConnector(sourceConfig, connName, connector, null)\n                            : offsetStoreForRegularSourceConnector(sourceConfig, connName, connector, null);\n                    offsetStore.configure(config);\n                    offsetReader = new OffsetStorageReaderImpl(offsetStore, connName, internalKeyConverter, internalValueConverter);\n                }\n                workerConnector = new WorkerConnector(\n                        connName, connector, connConfig, ctx, metrics, connectorStatusListener, offsetReader, offsetStore, connectorLoader);\n                log.info(\"Instantiated connector {} with version {} of type {}\", connName, connector.version(), connector.getClass());\n                workerConnector.transitionTo(initialState, onConnectorStateChange);\n            } catch (Throwable t) {\n                log.error(\"Failed to start connector {}\", connName, t);\n                connectorStatusListener.onFailure(connName, t);\n                onConnectorStateChange.onCompletion(t, null);\n                return;\n            }\n\n            WorkerConnector existing = connectors.putIfAbsent(connName, workerConnector);\n            if (existing != null) {\n                onConnectorStateChange.onCompletion(\n                        new ConnectException(\"Connector with name \" + connName + \" already exists\"),\n                        null);\n                // Don't need to do any cleanup of the WorkerConnector instance (such as calling\n                // shutdown() on it) here because it hasn't actually started running yet\n                return;\n            }\n\n            executor.submit(plugins.withClassLoader(connectorLoader, workerConnector));\n\n            log.info(\"Finished creating connector {}\", connName);\n        }\n    }\n\n    /**\n     * Return true if the connector associated with this worker is a sink connector.\n     *\n     * @param connName the connector name.\n     * @return true if the connector belongs to the worker and is a sink connector.\n     * @throws ConnectException if the worker does not manage a connector with the given name.\n     */\n    public boolean isSinkConnector(String connName) {\n        WorkerConnector workerConnector = connectors.get(connName);\n        if (workerConnector == null)\n            throw new ConnectException(\"Connector \" + connName + \" not found in this worker.\");\n\n        try (LoaderSwap loaderSwap = plugins.withClassLoader(workerConnector.loader())) {\n            return workerConnector.isSinkConnector();\n        }\n    }\n\n    /**\n     * Get a list of updated task properties for the tasks of this connector.\n     *\n     * @param connName the connector name.\n     * @return a list of updated tasks properties.\n     */\n    public List<Map<String, String>> connectorTaskConfigs(String connName, ConnectorConfig connConfig) {\n        List<Map<String, String>> result = new ArrayList<>();\n        try (LoggingContext loggingContext = LoggingContext.forConnector(connName)) {\n            log.trace(\"Reconfiguring connector tasks for {}\", connName);\n\n            WorkerConnector workerConnector = connectors.get(connName);\n            if (workerConnector == null)\n                throw new ConnectException(\"Connector \" + connName + \" not found in this worker.\");\n\n            int maxTasks = tasksMax(connConfig);\n            Map<String, String> connOriginals = connConfig.originalsStrings();\n\n            Connector connector = workerConnector.connector();\n            try (LoaderSwap loaderSwap = plugins.withClassLoader(workerConnector.loader())) {\n                String taskClassName = connector.taskClass().getName();\n                for (Map<String, String> taskProps : connector.taskConfigs(maxTasks)) {\n                    // Ensure we don't modify the connector's copy of the config\n                    Map<String, String> taskConfig = new HashMap<>(taskProps);\n                    taskConfig.put(TaskConfig.TASK_CLASS_CONFIG, taskClassName);\n                    if (connOriginals.containsKey(SinkTask.TOPICS_CONFIG)) {\n                        taskConfig.put(SinkTask.TOPICS_CONFIG, connOriginals.get(SinkTask.TOPICS_CONFIG));\n                    }\n                    if (connOriginals.containsKey(SinkTask.TOPICS_REGEX_CONFIG)) {\n                        taskConfig.put(SinkTask.TOPICS_REGEX_CONFIG, connOriginals.get(SinkTask.TOPICS_REGEX_CONFIG));\n                    }\n                    result.add(taskConfig);\n                }\n            }\n        }\n\n        return result;\n    }\n\n    private int tasksMax(ConnectorConfig connConfig) {\n        int maxTasks = connConfig.getInt(ConnectorConfig.TASKS_MAX_CONFIG);\n        return maxTasks;\n    }\n\n    /**\n     * Stop a connector managed by this worker.\n     *\n     * @param connName the connector name.\n     */\n    private void stopConnector(String connName) {\n        try (LoggingContext loggingContext = LoggingContext.forConnector(connName)) {\n            WorkerConnector workerConnector = connectors.get(connName);\n            log.info(\"Stopping connector {}\", connName);\n\n            if (workerConnector == null) {\n                log.warn(\"Ignoring stop request for unowned connector {}\", connName);\n                return;\n            }\n\n            try (LoaderSwap loaderSwap = plugins.withClassLoader(workerConnector.loader())) {\n                workerConnector.shutdown();\n            }\n        }\n    }\n\n    private void stopConnectors(Collection<String> ids) {\n        // Herder is responsible for stopping connectors. This is an internal method to sequentially\n        // stop connectors that have not explicitly been stopped.\n        for (String connector: ids)\n            stopConnector(connector);\n    }\n\n    private void awaitStopConnector(String connName, long timeout) {\n        try (LoggingContext loggingContext = LoggingContext.forConnector(connName)) {\n            WorkerConnector connector = connectors.remove(connName);\n            if (connector == null) {\n                log.warn(\"Ignoring await stop request for non-present connector {}\", connName);\n                return;\n            }\n\n            if (!connector.awaitShutdown(timeout)) {\n                log.error(\"Connector '{}' failed to properly shut down, has become unresponsive, and \"\n                        + \"may be consuming external resources. Correct the configuration for \"\n                        + \"this connector or remove the connector. After fixing the connector, it \"\n                        + \"may be necessary to restart this worker to release any consumed \"\n                        + \"resources.\", connName);\n                connector.cancel();\n            } else {\n                log.debug(\"Graceful stop of connector {} succeeded.\", connName);\n            }\n        }\n    }\n\n    private void awaitStopConnectors(Collection<String> ids) {\n        long now = time.milliseconds();\n        long deadline = now + CONNECTOR_GRACEFUL_SHUTDOWN_TIMEOUT_MS;\n        for (String id : ids) {\n            long remaining = Math.max(0, deadline - time.milliseconds());\n            awaitStopConnector(id, remaining);\n        }\n    }\n\n    /**\n     * Stop asynchronously all the worker's connectors and await their termination.\n     */\n    public void stopAndAwaitConnectors() {\n        stopAndAwaitConnectors(new ArrayList<>(connectors.keySet()));\n    }\n\n    /**\n     * Stop asynchronously a collection of connectors that belong to this worker and await their\n     * termination.\n     *\n     * @param ids the collection of connectors to be stopped.\n     */\n    public void stopAndAwaitConnectors(Collection<String> ids) {\n        stopConnectors(ids);\n        awaitStopConnectors(ids);\n    }\n\n    /**\n     * Stop a connector that belongs to this worker and await its termination.\n     *\n     * @param connName the name of the connector to be stopped.\n     */\n    public void stopAndAwaitConnector(String connName) {\n        stopConnector(connName);\n        awaitStopConnectors(Collections.singletonList(connName));\n    }\n\n    /**\n     * Get the IDs of the connectors currently running in this worker.\n     *\n     * @return the set of connector IDs.\n     */\n    public Set<String> connectorNames() {\n        return connectors.keySet();\n    }\n\n    /**\n     * Return true if a connector with the given name is managed by this worker and is currently running.\n     *\n     * @param connName the connector name.\n     * @return true if the connector is running, false if the connector is not running or is not manages by this worker.\n     */\n    public boolean isRunning(String connName) {\n        WorkerConnector workerConnector = connectors.get(connName);\n        return workerConnector != null && workerConnector.isRunning();\n    }\n\n    /**\n     * Start a sink task managed by this worker.\n     *\n     * @param id the task ID.\n     * @param configState the most recent {@link ClusterConfigState} known to the worker\n     * @param connProps the connector properties.\n     * @param taskProps the tasks properties.\n     * @param statusListener a listener for the runtime status transitions of the task.\n     * @param initialState the initial state of the connector.\n     * @return true if the task started successfully.\n     */\n    public boolean startSinkTask(\n            ConnectorTaskId id,\n            ClusterConfigState configState,\n            Map<String, String> connProps,\n            Map<String, String> taskProps,\n            TaskStatus.Listener statusListener,\n            TargetState initialState\n    ) {\n        return startTask(id, connProps, taskProps, statusListener,\n                new SinkTaskBuilder(id, configState, statusListener, initialState));\n    }\n\n    /**\n     * Start a source task managed by this worker using older behavior that does not provide exactly-once support.\n     *\n     * @param id the task ID.\n     * @param configState the most recent {@link ClusterConfigState} known to the worker\n     * @param connProps the connector properties.\n     * @param taskProps the tasks properties.\n     * @param statusListener a listener for the runtime status transitions of the task.\n     * @param initialState the initial state of the connector.\n     * @return true if the task started successfully.\n     */\n    public boolean startSourceTask(\n            ConnectorTaskId id,\n            ClusterConfigState configState,\n            Map<String, String> connProps,\n            Map<String, String> taskProps,\n            TaskStatus.Listener statusListener,\n            TargetState initialState\n    ) {\n        return startTask(id, connProps, taskProps, statusListener,\n                new SourceTaskBuilder(id, configState, statusListener, initialState));\n    }\n\n    /**\n     * Start a source task with exactly-once support managed by this worker.\n     *\n     * @param id the task ID.\n     * @param configState the most recent {@link ClusterConfigState} known to the worker\n     * @param connProps the connector properties.\n     * @param taskProps the tasks properties.\n     * @param statusListener a listener for the runtime status transitions of the task.\n     * @param initialState the initial state of the connector.\n     * @param preProducerCheck a preflight check that should be performed before the task initializes its transactional producer.\n     * @param postProducerCheck a preflight check that should be performed after the task initializes its transactional producer,\n     *                          but before producing any source records or offsets.\n     * @return true if the task started successfully.\n     */\n    public boolean startExactlyOnceSourceTask(\n            ConnectorTaskId id,\n            ClusterConfigState configState,\n            Map<String, String> connProps,\n            Map<String, String> taskProps,\n            TaskStatus.Listener statusListener,\n            TargetState initialState,\n            Runnable preProducerCheck,\n            Runnable postProducerCheck\n    ) {\n        return startTask(id, connProps, taskProps, statusListener,\n                new ExactlyOnceSourceTaskBuilder(id, configState, statusListener, initialState, preProducerCheck, postProducerCheck));\n    }\n\n    /**\n     * Start a task managed by this worker.\n     *\n     * @param id the task ID.\n     * @param connProps the connector properties.\n     * @param taskProps the tasks properties.\n     * @param statusListener a listener for the runtime status transitions of the task.\n     * @param taskBuilder the {@link TaskBuilder} used to create the {@link WorkerTask} that manages the lifecycle of the task.\n     * @return true if the task started successfully.\n     */\n    private boolean startTask(\n            ConnectorTaskId id,\n            Map<String, String> connProps,\n            Map<String, String> taskProps,\n            TaskStatus.Listener statusListener,\n            TaskBuilder<?, ?> taskBuilder\n    ) {\n        final WorkerTask<?, ?> workerTask;\n        final TaskStatus.Listener taskStatusListener = workerMetricsGroup.wrapStatusListener(statusListener);\n        try (LoggingContext loggingContext = LoggingContext.forTask(id)) {\n            log.info(\"Creating task {}\", id);\n\n            if (tasks.containsKey(id))\n                throw new ConnectException(\"Task already exists in this worker: \" + id);\n\n            connectorStatusMetricsGroup.recordTaskAdded(id);\n            String connType = connProps.get(ConnectorConfig.CONNECTOR_CLASS_CONFIG);\n            ClassLoader connectorLoader = plugins.connectorLoader(connType);\n\n            try (LoaderSwap loaderSwap = plugins.withClassLoader(connectorLoader)) {\n                final ConnectorConfig connConfig = new ConnectorConfig(plugins, connProps);\n                final TaskConfig taskConfig = new TaskConfig(taskProps);\n                final Class<? extends Task> taskClass = taskConfig.getClass(TaskConfig.TASK_CLASS_CONFIG).asSubclass(Task.class);\n                final Task task = plugins.newTask(taskClass);\n                log.info(\"Instantiated task {} with version {} of type {}\", id, task.version(), taskClass.getName());\n\n                // By maintaining connector's specific class loader for this thread here, we first\n                // search for converters within the connector dependencies.\n                // If any of these aren't found, that means the connector didn't configure specific converters,\n                // so we should instantiate based upon the worker configuration\n                Converter keyConverter = plugins.newConverter(connConfig, WorkerConfig.KEY_CONVERTER_CLASS_CONFIG, ClassLoaderUsage\n                                                                                                                           .CURRENT_CLASSLOADER);\n                Converter valueConverter = plugins.newConverter(connConfig, WorkerConfig.VALUE_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.CURRENT_CLASSLOADER);\n                HeaderConverter headerConverter = plugins.newHeaderConverter(connConfig, WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG,\n                                                                             ClassLoaderUsage.CURRENT_CLASSLOADER);\n                if (keyConverter == null) {\n                    keyConverter = plugins.newConverter(config, WorkerConfig.KEY_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS);\n                    log.info(\"Set up the key converter {} for task {} using the worker config\", keyConverter.getClass(), id);\n                } else {\n                    log.info(\"Set up the key converter {} for task {} using the connector config\", keyConverter.getClass(), id);\n                }\n                if (valueConverter == null) {\n                    valueConverter = plugins.newConverter(config, WorkerConfig.VALUE_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS);\n                    log.info(\"Set up the value converter {} for task {} using the worker config\", valueConverter.getClass(), id);\n                } else {\n                    log.info(\"Set up the value converter {} for task {} using the connector config\", valueConverter.getClass(), id);\n                }\n                if (headerConverter == null) {\n                    headerConverter = plugins.newHeaderConverter(config, WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG, ClassLoaderUsage\n                                                                                                                             .PLUGINS);\n                    log.info(\"Set up the header converter {} for task {} using the worker config\", headerConverter.getClass(), id);\n                } else {\n                    log.info(\"Set up the header converter {} for task {} using the connector config\", headerConverter.getClass(), id);\n                }\n\n                workerTask = taskBuilder\n                        .withTask(task)\n                        .withConnectorConfig(connConfig)\n                        .withKeyConverter(keyConverter)\n                        .withValueConverter(valueConverter)\n                        .withHeaderConverter(headerConverter)\n                        .withClassloader(connectorLoader)\n                        .build();\n\n                workerTask.initialize(taskConfig);\n            } catch (Throwable t) {\n                log.error(\"Failed to start task {}\", id, t);\n                connectorStatusMetricsGroup.recordTaskRemoved(id);\n                taskStatusListener.onFailure(id, t);\n                return false;\n            }\n\n            WorkerTask<?, ?> existing = tasks.putIfAbsent(id, workerTask);\n            if (existing != null)\n                throw new ConnectException(\"Task already exists in this worker: \" + id);\n\n            executor.submit(plugins.withClassLoader(connectorLoader, workerTask));\n            if (workerTask instanceof WorkerSourceTask) {\n                sourceTaskOffsetCommitter.ifPresent(committer -> committer.schedule(id, (WorkerSourceTask) workerTask));\n            }\n            return true;\n        }\n    }\n\n    /**\n     * Using the admin principal for this connector, perform a round of zombie fencing that disables transactional producers\n     * for the specified number of source tasks from sending any more records.\n     * @param connName the name of the connector\n     * @param numTasks the number of tasks to fence out\n     * @param connProps the configuration of the connector; may not be null\n     * @return a {@link KafkaFuture} that will complete when the producers have all been fenced out, or the attempt has failed\n     */\n    public KafkaFuture<Void> fenceZombies(String connName, int numTasks, Map<String, String> connProps) {\n        log.debug(\"Fencing out {} task producers for source connector {}\", numTasks, connName);\n        try (LoggingContext loggingContext = LoggingContext.forConnector(connName)) {\n            String connType = connProps.get(ConnectorConfig.CONNECTOR_CLASS_CONFIG);\n            ClassLoader connectorLoader = plugins.connectorLoader(connType);\n            try (LoaderSwap loaderSwap = plugins.withClassLoader(connectorLoader)) {\n                final SourceConnectorConfig connConfig = new SourceConnectorConfig(plugins, connProps, config.topicCreationEnable());\n                final Class<? extends Connector> connClass = plugins.connectorClass(\n                        connConfig.getString(ConnectorConfig.CONNECTOR_CLASS_CONFIG));\n\n                Map<String, Object> adminConfig = adminConfigs(\n                        connName,\n                        \"connector-worker-adminclient-\" + connName,\n                        config,\n                        connConfig,\n                        connClass,\n                        connectorClientConfigOverridePolicy,\n                        kafkaClusterId,\n                        ConnectorType.SOURCE);\n                final Admin admin = adminFactory.apply(adminConfig);\n\n                try {\n                    Collection<String> transactionalIds = IntStream.range(0, numTasks)\n                            .mapToObj(i -> new ConnectorTaskId(connName, i))\n                            .map(this::taskTransactionalId)\n                            .collect(Collectors.toList());\n                    FenceProducersOptions fencingOptions = new FenceProducersOptions()\n                            .timeoutMs((int) RestServer.DEFAULT_REST_REQUEST_TIMEOUT_MS);\n                    return admin.fenceProducers(transactionalIds, fencingOptions).all().whenComplete((ignored, error) -> {\n                        if (error == null)\n                            log.debug(\"Finished fencing out {} task producers for source connector {}\", numTasks, connName);\n                        Utils.closeQuietly(admin, \"Zombie fencing admin for connector \" + connName);\n                    });\n                } catch (Exception e) {\n                    Utils.closeQuietly(admin, \"Zombie fencing admin for connector \" + connName);\n                    throw e;\n                }\n            }\n        }\n    }\n\n    static Map<String, Object> exactlyOnceSourceTaskProducerConfigs(ConnectorTaskId id,\n                                                              WorkerConfig config,\n                                                              ConnectorConfig connConfig,\n                                                              Class<? extends Connector>  connectorClass,\n                                                              ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                                              String clusterId) {\n        Map<String, Object> result = baseProducerConfigs(id.connector(), \"connector-producer-\" + id, config, connConfig, connectorClass, connectorClientConfigOverridePolicy, clusterId);\n        // The base producer properties forcibly disable idempotence; remove it from those properties\n        // if not explicitly requested by the user\n        boolean connectorProducerIdempotenceConfigured = connConfig.originals().containsKey(\n                ConnectorConfig.CONNECTOR_CLIENT_PRODUCER_OVERRIDES_PREFIX + ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG\n        );\n        if (!connectorProducerIdempotenceConfigured) {\n            boolean workerProducerIdempotenceConfigured = config.originals().containsKey(\n                    \"producer.\" + ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG\n            );\n            if (!workerProducerIdempotenceConfigured) {\n                result.remove(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG);\n            }\n        }\n        ConnectUtils.ensureProperty(\n                result, ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\",\n                \"for connectors when exactly-once source support is enabled\",\n                false\n        );\n        String transactionalId = taskTransactionalId(config.groupId(), id.connector(), id.task());\n        ConnectUtils.ensureProperty(\n                result, ProducerConfig.TRANSACTIONAL_ID_CONFIG, transactionalId,\n                \"for connectors when exactly-once source support is enabled\",\n                true\n        );\n        return result;\n    }\n\n    static Map<String, Object> baseProducerConfigs(String connName,\n                                               String defaultClientId,\n                                               WorkerConfig config,\n                                               ConnectorConfig connConfig,\n                                               Class<? extends Connector>  connectorClass,\n                                               ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                               String clusterId) {\n        Map<String, Object> producerProps = new HashMap<>();\n        producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, config.bootstrapServers());\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.ByteArraySerializer\");\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.ByteArraySerializer\");\n        // These settings will execute infinite retries on retriable exceptions. They *may* be overridden via configs passed to the worker,\n        // but this may compromise the delivery guarantees of Kafka Connect.\n        producerProps.put(ProducerConfig.MAX_BLOCK_MS_CONFIG, Long.toString(Long.MAX_VALUE));\n        // By default, Connect disables idempotent behavior for all producers, even though idempotence became\n        // default for Kafka producers. This is to ensure Connect continues to work with many Kafka broker versions, including older brokers that do not support\n        // idempotent producers or require explicit steps to enable them (e.g. adding the IDEMPOTENT_WRITE ACL to brokers older than 2.8).\n        // These settings might change when https://cwiki.apache.org/confluence/display/KAFKA/KIP-318%3A+Make+Kafka+Connect+Source+idempotent\n        // gets approved and scheduled for release.\n        producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"false\");\n        producerProps.put(ProducerConfig.ACKS_CONFIG, \"all\");\n        producerProps.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, \"1\");\n        producerProps.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, Integer.toString(Integer.MAX_VALUE));\n        producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, defaultClientId);\n        // User-specified overrides\n        producerProps.putAll(config.originalsWithPrefix(\"producer.\"));\n        //add client metrics.context properties\n        ConnectUtils.addMetricsContextProperties(producerProps, config, clusterId);\n\n        // Connector-specified overrides\n        Map<String, Object> producerOverrides =\n            connectorClientConfigOverrides(connName, connConfig, connectorClass, ConnectorConfig.CONNECTOR_CLIENT_PRODUCER_OVERRIDES_PREFIX,\n                                           ConnectorType.SOURCE, ConnectorClientConfigRequest.ClientType.PRODUCER,\n                                           connectorClientConfigOverridePolicy);\n        producerProps.putAll(producerOverrides);\n\n        return producerProps;\n    }\n\n    static Map<String, Object> exactlyOnceSourceOffsetsConsumerConfigs(String connName,\n                                                                       String defaultClientId,\n                                                                       WorkerConfig config,\n                                                                       ConnectorConfig connConfig,\n                                                                       Class<? extends Connector> connectorClass,\n                                                                       ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                                                       String clusterId) {\n        Map<String, Object> result = baseConsumerConfigs(\n                connName, defaultClientId, config, connConfig, connectorClass,\n                connectorClientConfigOverridePolicy, clusterId, ConnectorType.SOURCE);\n        ConnectUtils.ensureProperty(\n                result, ConsumerConfig.ISOLATION_LEVEL_CONFIG, IsolationLevel.READ_COMMITTED.name().toLowerCase(Locale.ROOT),\n                \"for source connectors' offset consumers when exactly-once source support is enabled\",\n                false\n        );\n        return result;\n    }\n\n    static Map<String, Object> regularSourceOffsetsConsumerConfigs(String connName,\n                                                                   String defaultClientId,\n                                                                   WorkerConfig config,\n                                                                   ConnectorConfig connConfig,\n                                                                   Class<? extends Connector> connectorClass,\n                                                                   ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                                                   String clusterId) {\n        Map<String, Object> result = baseConsumerConfigs(\n                connName, defaultClientId, config, connConfig, connectorClass,\n                connectorClientConfigOverridePolicy, clusterId, ConnectorType.SOURCE);\n        // Users can disable this if they want to since the task isn't exactly-once anyways\n        result.putIfAbsent(\n                ConsumerConfig.ISOLATION_LEVEL_CONFIG,\n                IsolationLevel.READ_COMMITTED.toString().toLowerCase(Locale.ROOT));\n        return result;\n    }\n\n    static Map<String, Object> baseConsumerConfigs(String connName,\n                                               String defaultClientId,\n                                               WorkerConfig config,\n                                               ConnectorConfig connConfig,\n                                               Class<? extends Connector> connectorClass,\n                                               ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                               String clusterId,\n                                               ConnectorType connectorType) {\n        // Include any unknown worker configs so consumer configs can be set globally on the worker\n        // and through to the task\n        Map<String, Object> consumerProps = new HashMap<>();\n\n        consumerProps.put(ConsumerConfig.GROUP_ID_CONFIG, SinkUtils.consumerGroupId(connName));\n        consumerProps.put(ConsumerConfig.CLIENT_ID_CONFIG, defaultClientId);\n        consumerProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, config.bootstrapServers());\n        consumerProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \"false\");\n        consumerProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.ByteArrayDeserializer\");\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.ByteArrayDeserializer\");\n\n        consumerProps.putAll(config.originalsWithPrefix(\"consumer.\"));\n        //add client metrics.context properties\n        ConnectUtils.addMetricsContextProperties(consumerProps, config, clusterId);\n        // Connector-specified overrides\n        Map<String, Object> consumerOverrides =\n            connectorClientConfigOverrides(connName, connConfig, connectorClass, ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX,\n                                           connectorType, ConnectorClientConfigRequest.ClientType.CONSUMER,\n                                           connectorClientConfigOverridePolicy);\n        consumerProps.putAll(consumerOverrides);\n\n        return consumerProps;\n    }\n\n    static Map<String, Object> adminConfigs(String connName,\n                                            String defaultClientId,\n                                            WorkerConfig config,\n                                            ConnectorConfig connConfig,\n                                            Class<? extends Connector> connectorClass,\n                                            ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                            String clusterId,\n                                            ConnectorType connectorType) {\n        Map<String, Object> adminProps = new HashMap<>();\n        // Use the top-level worker configs to retain backwards compatibility with older releases which\n        // did not require a prefix for connector admin client configs in the worker configuration file\n        // Ignore configs that begin with \"admin.\" since those will be added next (with the prefix stripped)\n        // and those that begin with \"producer.\" and \"consumer.\", since we know they aren't intended for\n        // the admin client\n        Map<String, Object> nonPrefixedWorkerConfigs = config.originals().entrySet().stream()\n                .filter(e -> !e.getKey().startsWith(\"admin.\")\n                        && !e.getKey().startsWith(\"producer.\")\n                        && !e.getKey().startsWith(\"consumer.\"))\n                .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n        adminProps.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, config.bootstrapServers());\n        adminProps.put(AdminClientConfig.CLIENT_ID_CONFIG, defaultClientId);\n        adminProps.putAll(nonPrefixedWorkerConfigs);\n\n        // Admin client-specific overrides in the worker config\n        adminProps.putAll(config.originalsWithPrefix(\"admin.\"));\n\n        // Connector-specified overrides\n        Map<String, Object> adminOverrides =\n                connectorClientConfigOverrides(connName, connConfig, connectorClass, ConnectorConfig.CONNECTOR_CLIENT_ADMIN_OVERRIDES_PREFIX,\n                        connectorType, ConnectorClientConfigRequest.ClientType.ADMIN,\n                        connectorClientConfigOverridePolicy);\n        adminProps.putAll(adminOverrides);\n\n        //add client metrics.context properties\n        ConnectUtils.addMetricsContextProperties(adminProps, config, clusterId);\n\n        return adminProps;\n    }\n\n    private static Map<String, Object> connectorClientConfigOverrides(String connName,\n                                                                      ConnectorConfig connConfig,\n                                                                      Class<? extends Connector> connectorClass,\n                                                                      String clientConfigPrefix,\n                                                                      ConnectorType connectorType,\n                                                                      ConnectorClientConfigRequest.ClientType clientType,\n                                                                      ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy) {\n        Map<String, Object> clientOverrides = connConfig.originalsWithPrefix(clientConfigPrefix);\n        ConnectorClientConfigRequest connectorClientConfigRequest = new ConnectorClientConfigRequest(\n            connName,\n            connectorType,\n            connectorClass,\n            clientOverrides,\n            clientType\n        );\n        List<ConfigValue> configValues = connectorClientConfigOverridePolicy.validate(connectorClientConfigRequest);\n        List<ConfigValue> errorConfigs = configValues.stream().\n            filter(configValue -> configValue.errorMessages().size() > 0).collect(Collectors.toList());\n        // These should be caught when the herder validates the connector configuration, but just in case\n        if (errorConfigs.size() > 0) {\n            throw new ConnectException(\"Client Config Overrides not allowed \" + errorConfigs);\n        }\n        return clientOverrides;\n    }\n\n    private String taskTransactionalId(ConnectorTaskId id) {\n        return taskTransactionalId(config.groupId(), id.connector(), id.task());\n    }\n\n    /**\n     * @return the {@link ProducerConfig#TRANSACTIONAL_ID_CONFIG transactional ID} to use for a task that writes\n     * records and/or offsets in a transaction. Not to be confused with {@link DistributedConfig#transactionalProducerId()},\n     * which is not used by tasks at all, but instead, by the worker itself.\n     */\n    public static String taskTransactionalId(String groupId, String connector, int taskId) {\n        return String.format(\"%s-%s-%d\", groupId, connector, taskId);\n    }\n\n    ErrorHandlingMetrics errorHandlingMetrics(ConnectorTaskId id) {\n        return new ErrorHandlingMetrics(id, metrics);\n    }\n\n    private List<ErrorReporter<ConsumerRecord<byte[], byte[]>>> sinkTaskReporters(ConnectorTaskId id, SinkConnectorConfig connConfig,\n                                                     ErrorHandlingMetrics errorHandlingMetrics,\n                                                     Class<? extends Connector> connectorClass) {\n        ArrayList<ErrorReporter<ConsumerRecord<byte[], byte[]>>> reporters = new ArrayList<>();\n        LogReporter<ConsumerRecord<byte[], byte[]>> logReporter = new LogReporter.Sink(id, connConfig, errorHandlingMetrics);\n        reporters.add(logReporter);\n\n        // check if topic for dead letter queue exists\n        String topic = connConfig.dlqTopicName();\n        if (topic != null && !topic.isEmpty()) {\n            Map<String, Object> producerProps = baseProducerConfigs(id.connector(), \"connector-dlq-producer-\" + id, config, connConfig, connectorClass,\n                                                                connectorClientConfigOverridePolicy, kafkaClusterId);\n            Map<String, Object> adminProps = adminConfigs(id.connector(), \"connector-dlq-adminclient-\", config, connConfig, connectorClass, connectorClientConfigOverridePolicy, kafkaClusterId, ConnectorType.SINK);\n            DeadLetterQueueReporter reporter = DeadLetterQueueReporter.createAndSetup(adminProps, id, connConfig, producerProps, errorHandlingMetrics);\n\n            reporters.add(reporter);\n        }\n\n        return reporters;\n    }\n\n    private List<ErrorReporter<SourceRecord>> sourceTaskReporters(ConnectorTaskId id, ConnectorConfig connConfig,\n                                                       ErrorHandlingMetrics errorHandlingMetrics) {\n        List<ErrorReporter<SourceRecord>> reporters = new ArrayList<>();\n        LogReporter<SourceRecord> logReporter = new LogReporter.Source(id, connConfig, errorHandlingMetrics);\n        reporters.add(logReporter);\n\n        return reporters;\n    }\n\n    private WorkerErrantRecordReporter createWorkerErrantRecordReporter(\n        SinkConnectorConfig connConfig,\n        RetryWithToleranceOperator<ConsumerRecord<byte[], byte[]>> retryWithToleranceOperator,\n        Converter keyConverter,\n        Converter valueConverter,\n        HeaderConverter headerConverter\n    ) {\n        // check if errant record reporter topic is configured\n        if (connConfig.enableErrantRecordReporter()) {\n            return new WorkerErrantRecordReporter(retryWithToleranceOperator, keyConverter, valueConverter, headerConverter);\n        }\n        return null;\n    }\n\n    private void stopTask(ConnectorTaskId taskId) {\n        try (LoggingContext loggingContext = LoggingContext.forTask(taskId)) {\n            WorkerTask<?, ?> task = tasks.get(taskId);\n            if (task == null) {\n                log.warn(\"Ignoring stop request for unowned task {}\", taskId);\n                return;\n            }\n\n            log.info(\"Stopping task {}\", task.id());\n            if (task instanceof WorkerSourceTask)\n                sourceTaskOffsetCommitter.ifPresent(committer -> committer.remove(task.id()));\n\n            try (LoaderSwap loaderSwap = plugins.withClassLoader(task.loader())) {\n                task.stop();\n            }\n        }\n    }\n\n    private void stopTasks(Collection<ConnectorTaskId> ids) {\n        // Herder is responsible for stopping tasks. This is an internal method to sequentially\n        // stop the tasks that have not explicitly been stopped.\n        for (ConnectorTaskId taskId : ids) {\n            stopTask(taskId);\n        }\n    }\n\n    private void awaitStopTask(ConnectorTaskId taskId, long timeout) {\n        try (LoggingContext loggingContext = LoggingContext.forTask(taskId)) {\n            WorkerTask<?, ?> task = tasks.remove(taskId);\n            if (task == null) {\n                log.warn(\"Ignoring await stop request for non-present task {}\", taskId);\n                return;\n            }\n\n            if (!task.awaitStop(timeout)) {\n                log.error(\"Graceful stop of task {} failed.\", task.id());\n                task.cancel();\n            } else {\n                log.debug(\"Graceful stop of task {} succeeded.\", task.id());\n            }\n\n            try {\n                task.removeMetrics();\n            } finally {\n                connectorStatusMetricsGroup.recordTaskRemoved(taskId);\n            }\n        }\n    }\n\n    private void awaitStopTasks(Collection<ConnectorTaskId> ids) {\n        long now = time.milliseconds();\n        long deadline = now + config.getLong(WorkerConfig.TASK_SHUTDOWN_GRACEFUL_TIMEOUT_MS_CONFIG);\n        for (ConnectorTaskId id : ids) {\n            long remaining = Math.max(0, deadline - time.milliseconds());\n            awaitStopTask(id, remaining);\n        }\n    }\n\n    /**\n     * Stop asynchronously all the worker's tasks and await their termination.\n     */\n    public void stopAndAwaitTasks() {\n        stopAndAwaitTasks(new ArrayList<>(tasks.keySet()));\n    }\n\n    /**\n     * Stop asynchronously a collection of tasks that belong to this worker and await their termination.\n     *\n     * @param ids the collection of tasks to be stopped.\n     */\n    public void stopAndAwaitTasks(Collection<ConnectorTaskId> ids) {\n        stopTasks(ids);\n        awaitStopTasks(ids);\n    }\n\n    /**\n     * Stop a task that belongs to this worker and await its termination.\n     *\n     * @param taskId the ID of the task to be stopped.\n     */\n    public void stopAndAwaitTask(ConnectorTaskId taskId) {\n        stopTask(taskId);\n        awaitStopTasks(Collections.singletonList(taskId));\n    }\n\n    /**\n     * Get the IDs of the tasks currently running in this worker.\n     */\n    public Set<ConnectorTaskId> taskIds() {\n        return tasks.keySet();\n    }\n\n    public Converter getInternalKeyConverter() {\n        return internalKeyConverter;\n    }\n\n    public Converter getInternalValueConverter() {\n        return internalValueConverter;\n    }\n\n    public Plugins getPlugins() {\n        return plugins;\n    }\n\n    public String workerId() {\n        return workerId;\n    }\n\n    /**\n     * Returns whether this worker is configured to allow source connectors to create the topics\n     * that they use with custom configurations, if these topics don't already exist.\n     *\n     * @return true if topic creation by source connectors is allowed; false otherwise\n     */\n    public boolean isTopicCreationEnabled() {\n        return config.topicCreationEnable();\n    }\n\n    /**\n     * Get the {@link ConnectMetrics} that uses Kafka Metrics and manages the JMX reporter.\n     * @return the Connect-specific metrics; never null\n     */\n    public ConnectMetrics metrics() {\n        return metrics;\n    }\n\n    public void setTargetState(String connName, TargetState state, Callback<TargetState> stateChangeCallback) {\n        log.info(\"Setting connector {} state to {}\", connName, state);\n\n        WorkerConnector workerConnector = connectors.get(connName);\n        if (workerConnector != null) {\n            try (LoaderSwap loaderSwap = plugins.withClassLoader(workerConnector.loader())) {\n                workerConnector.transitionTo(state, stateChangeCallback);\n            }\n        }\n\n        for (Map.Entry<ConnectorTaskId, WorkerTask<?, ?>> taskEntry : tasks.entrySet()) {\n            if (taskEntry.getKey().connector().equals(connName)) {\n                WorkerTask<?, ?> workerTask = taskEntry.getValue();\n                try (LoaderSwap loaderSwap = plugins.withClassLoader(workerTask.loader())) {\n                    workerTask.transitionTo(state);\n                }\n            }\n        }\n    }\n\n    /**\n     * Get the current offsets for a connector. This method is asynchronous and the passed callback is completed when the\n     * request finishes processing.\n     *\n     * @param connName the name of the connector whose offsets are to be retrieved\n     * @param connectorConfig the connector's configurations\n     * @param cb callback to invoke upon completion of the request\n     */\n    public void connectorOffsets(String connName, Map<String, String> connectorConfig, Callback<ConnectorOffsets> cb) {\n        String connectorClassOrAlias = connectorConfig.get(ConnectorConfig.CONNECTOR_CLASS_CONFIG);\n        ClassLoader connectorLoader = plugins.connectorLoader(connectorClassOrAlias);\n\n        try (LoaderSwap loaderSwap = plugins.withClassLoader(connectorLoader)) {\n            Connector connector = plugins.newConnector(connectorClassOrAlias);\n            if (ConnectUtils.isSinkConnector(connector)) {\n                log.debug(\"Fetching offsets for sink connector: {}\", connName);\n                sinkConnectorOffsets(connName, connector, connectorConfig, cb);\n            } else {\n                log.debug(\"Fetching offsets for source connector: {}\", connName);\n                sourceConnectorOffsets(connName, connector, connectorConfig, cb);\n            }\n        }\n    }\n\n    /**\n     * Get the current consumer group offsets for a sink connector.\n     * <p>\n     * Visible for testing.\n     *\n     * @param connName the name of the sink connector whose offsets are to be retrieved\n     * @param connector the sink connector\n     * @param connectorConfig the sink connector's configurations\n     * @param cb callback to invoke upon completion of the request\n     */\n    void sinkConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                              Callback<ConnectorOffsets> cb) {\n        Map<String, Object> adminConfig = adminConfigs(\n                connName,\n                \"connector-worker-adminclient-\" + connName,\n                config,\n                new SinkConnectorConfig(plugins, connectorConfig),\n                connector.getClass(),\n                connectorClientConfigOverridePolicy,\n                kafkaClusterId,\n                ConnectorType.SINK);\n        String groupId = (String) baseConsumerConfigs(\n                connName, \"connector-consumer-\", config, new SinkConnectorConfig(plugins, connectorConfig),\n                connector.getClass(), connectorClientConfigOverridePolicy, kafkaClusterId, ConnectorType.SINK).get(ConsumerConfig.GROUP_ID_CONFIG);\n        Admin admin = adminFactory.apply(adminConfig);\n        try {\n            ListConsumerGroupOffsetsOptions listOffsetsOptions = new ListConsumerGroupOffsetsOptions()\n                    .timeoutMs((int) RestServer.DEFAULT_REST_REQUEST_TIMEOUT_MS);\n            ListConsumerGroupOffsetsResult listConsumerGroupOffsetsResult = admin.listConsumerGroupOffsets(groupId, listOffsetsOptions);\n            listConsumerGroupOffsetsResult.partitionsToOffsetAndMetadata().whenComplete((result, error) -> {\n                if (error != null) {\n                    log.error(\"Failed to retrieve consumer group offsets for sink connector {}\", connName, error);\n                    cb.onCompletion(new ConnectException(\"Failed to retrieve consumer group offsets for sink connector \" + connName, error), null);\n                } else {\n                    ConnectorOffsets offsets = SinkUtils.consumerGroupOffsetsToConnectorOffsets(result);\n                    cb.onCompletion(null, offsets);\n                }\n                Utils.closeQuietly(admin, \"Offset fetch admin for sink connector \" + connName);\n            });\n        } catch (Throwable t) {\n            Utils.closeQuietly(admin, \"Offset fetch admin for sink connector \" + connName);\n            log.error(\"Failed to retrieve consumer group offsets for sink connector {}\", connName, t);\n            cb.onCompletion(new ConnectException(\"Failed to retrieve consumer group offsets for sink connector \" + connName, t), null);\n        }\n    }\n\n    /**\n     * Get the current offsets for a source connector.\n     *\n     * @param connName the name of the source connector whose offsets are to be retrieved\n     * @param connector the source connector\n     * @param connectorConfig the source connector's configurations\n     * @param cb callback to invoke upon completion of the request\n     */\n    private void sourceConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                                        Callback<ConnectorOffsets> cb) {\n        SourceConnectorConfig sourceConfig = new SourceConnectorConfig(plugins, connectorConfig, config.topicCreationEnable());\n        ConnectorOffsetBackingStore offsetStore = config.exactlyOnceSourceEnabled()\n                ? offsetStoreForExactlyOnceSourceConnector(sourceConfig, connName, connector, null)\n                : offsetStoreForRegularSourceConnector(sourceConfig, connName, connector, null);\n        CloseableOffsetStorageReader offsetReader = new OffsetStorageReaderImpl(offsetStore, connName, internalKeyConverter, internalValueConverter);\n        sourceConnectorOffsets(connName, offsetStore, offsetReader, cb);\n    }\n\n    // Visible for testing\n    void sourceConnectorOffsets(String connName, ConnectorOffsetBackingStore offsetStore,\n                                CloseableOffsetStorageReader offsetReader, Callback<ConnectorOffsets> cb) {\n        executor.submit(() -> {\n            try {\n                offsetStore.configure(config);\n                offsetStore.start();\n                Set<Map<String, Object>> connectorPartitions = offsetStore.connectorPartitions(connName);\n                List<ConnectorOffset> connectorOffsets = offsetReader.offsets(connectorPartitions).entrySet().stream()\n                        .map(entry -> new ConnectorOffset(entry.getKey(), entry.getValue()))\n                        .collect(Collectors.toList());\n                cb.onCompletion(null, new ConnectorOffsets(connectorOffsets));\n            } catch (Throwable t) {\n                log.error(\"Failed to retrieve offsets for source connector {}\", connName, t);\n                cb.onCompletion(ConnectUtils.maybeWrap(t, \"Failed to retrieve offsets for source connector \" + connName), null);\n            } finally {\n                Utils.closeQuietly(offsetReader, \"Offset reader for connector \" + connName);\n                Utils.closeQuietly(offsetStore::stop, \"Offset store for connector \" + connName);\n            }\n        });\n    }\n\n    /**\n     * Modify (alter / reset) a connector's offsets.\n     *\n     * @param connName the name of the connector whose offsets are to be modified\n     * @param connectorConfig the connector's configurations\n     * @param offsets a mapping from partitions (either source partitions for source connectors, or Kafka topic\n     *                partitions for sink connectors) to offsets that need to be written; this should be {@code null}\n     *                for offsets reset requests\n     * @param cb callback to invoke upon completion\n     */\n    public void modifyConnectorOffsets(String connName, Map<String, String> connectorConfig,\n                                      Map<Map<String, ?>, Map<String, ?>> offsets, Callback<Message> cb) {\n        String connectorClassOrAlias = connectorConfig.get(ConnectorConfig.CONNECTOR_CLASS_CONFIG);\n        ClassLoader connectorLoader = plugins.connectorLoader(connectorClassOrAlias);\n        Connector connector;\n\n        try (LoaderSwap loaderSwap = plugins.withClassLoader(connectorLoader)) {\n            connector = plugins.newConnector(connectorClassOrAlias);\n            if (ConnectUtils.isSinkConnector(connector)) {\n                log.debug(\"Modifying offsets for sink connector: {}\", connName);\n                modifySinkConnectorOffsets(connName, connector, connectorConfig, offsets, connectorLoader, cb);\n            } else {\n                log.debug(\"Modifying offsets for source connector: {}\", connName);\n                modifySourceConnectorOffsets(connName, connector, connectorConfig, offsets, connectorLoader, cb);\n            }\n        }\n    }\n\n    /**\n     * Modify (alter / reset) a sink connector's consumer group offsets.\n     * <p>\n     * Visible for testing.\n     *\n     * @param connName the name of the sink connector whose offsets are to be modified\n     * @param connector an instance of the sink connector\n     * @param connectorConfig the sink connector's configuration\n     * @param offsets a mapping from topic partitions to offsets that need to be written; this should be {@code null}\n     *                for offsets reset requests\n     * @param connectorLoader the connector plugin's classloader to be used as the thread context classloader\n     * @param cb callback to invoke upon completion\n     */\n    void modifySinkConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                                    Map<Map<String, ?>, Map<String, ?>> offsets, ClassLoader connectorLoader, Callback<Message> cb) {\n        executor.submit(plugins.withClassLoader(connectorLoader, () -> {\n            try {\n                Timer timer = time.timer(Duration.ofMillis(RestServer.DEFAULT_REST_REQUEST_TIMEOUT_MS));\n                boolean isReset = offsets == null;\n                SinkConnectorConfig sinkConnectorConfig = new SinkConnectorConfig(plugins, connectorConfig);\n                Class<? extends Connector> sinkConnectorClass = connector.getClass();\n                Map<String, Object> adminConfig = adminConfigs(\n                        connName,\n                        \"connector-worker-adminclient-\" + connName,\n                        config,\n                        sinkConnectorConfig,\n                        sinkConnectorClass,\n                        connectorClientConfigOverridePolicy,\n                        kafkaClusterId,\n                        ConnectorType.SINK);\n\n                String groupId = (String) baseConsumerConfigs(\n                        connName, \"connector-consumer-\", config, sinkConnectorConfig,\n                        sinkConnectorClass, connectorClientConfigOverridePolicy, kafkaClusterId, ConnectorType.SINK).get(ConsumerConfig.GROUP_ID_CONFIG);\n\n                Admin admin = adminFactory.apply(adminConfig);\n\n                try {\n                    Map<TopicPartition, Long> offsetsToWrite;\n                    if (isReset) {\n                        offsetsToWrite = new HashMap<>();\n                        ListConsumerGroupOffsetsOptions listConsumerGroupOffsetsOptions = new ListConsumerGroupOffsetsOptions()\n                                .timeoutMs((int) timer.remainingMs());\n                        try {\n                            admin.listConsumerGroupOffsets(groupId, listConsumerGroupOffsetsOptions)\n                                    .partitionsToOffsetAndMetadata()\n                                    .get(timer.remainingMs(), TimeUnit.MILLISECONDS)\n                                    .forEach((topicPartition, offsetAndMetadata) -> offsetsToWrite.put(topicPartition, null));\n\n                            timer.update();\n                            log.debug(\"Found the following topic partitions (to reset offsets) for sink connector {} and consumer group ID {}: {}\",\n                                    connName, groupId, offsetsToWrite.keySet());\n                        } catch (Exception e) {\n                            Utils.closeQuietly(admin, \"Offset reset admin for sink connector \" + connName);\n                            log.error(\"Failed to list offsets prior to resetting offsets for sink connector {}\", connName, e);\n                            cb.onCompletion(new ConnectException(\"Failed to list offsets prior to resetting offsets for sink connector \" + connName, e), null);\n                            return;\n                        }\n                    } else {\n                        offsetsToWrite = SinkUtils.parseSinkConnectorOffsets(offsets);\n                    }\n\n                    boolean alterOffsetsResult;\n                    try {\n                        alterOffsetsResult = ((SinkConnector) connector).alterOffsets(connectorConfig, offsetsToWrite);\n                    } catch (UnsupportedOperationException e) {\n                        log.error(\"Failed to modify offsets for connector {} because it doesn't support external modification of offsets\",\n                                connName, e);\n                        throw new ConnectException(\"Failed to modify offsets for connector \" + connName + \" because it doesn't support external \" +\n                                \"modification of offsets\", e);\n                    }\n                    updateTimerAndCheckExpiry(timer, \"Timed out while calling the 'alterOffsets' method for sink connector \" + connName);\n\n                    if (isReset) {\n                        resetSinkConnectorOffsets(connName, groupId, admin, cb, alterOffsetsResult, timer);\n                    } else {\n                        alterSinkConnectorOffsets(connName, groupId, admin, offsetsToWrite, cb, alterOffsetsResult, timer);\n                    }\n                } catch (Throwable t) {\n                    Utils.closeQuietly(admin, \"Offset modification admin for sink connector \" + connName);\n                    throw t;\n                }\n            } catch (Throwable t) {\n                log.error(\"Failed to modify offsets for sink connector {}\", connName, t);\n                cb.onCompletion(ConnectUtils.maybeWrap(t, \"Failed to modify offsets for sink connector \" + connName), null);\n            }\n        }));\n    }\n\n    /**\n     * Alter a sink connector's consumer group offsets. This is done via calls to {@link Admin#alterConsumerGroupOffsets}\n     * and / or {@link Admin#deleteConsumerGroupOffsets}.\n     *\n     * @param connName the name of the sink connector whose offsets are to be altered\n     * @param groupId the sink connector's consumer group ID\n     * @param admin the {@link Admin admin client} to be used for altering the consumer group offsets; will be closed after use\n     * @param offsetsToWrite a mapping from topic partitions to offsets that need to be written; may not be null or empty\n     * @param cb callback to invoke upon completion\n     * @param alterOffsetsResult the result of the call to {@link SinkConnector#alterOffsets} for the connector\n     * @param timer {@link Timer} to bound the total runtime of admin client requests\n     */\n    private void alterSinkConnectorOffsets(String connName, String groupId, Admin admin, Map<TopicPartition, Long> offsetsToWrite,\n                                           Callback<Message> cb, boolean alterOffsetsResult, Timer timer) {\n        List<KafkaFuture<Void>> adminFutures = new ArrayList<>();\n\n        Map<TopicPartition, OffsetAndMetadata> offsetsToAlter = offsetsToWrite.entrySet()\n                .stream()\n                .filter(entry -> entry.getValue() != null)\n                .collect(Collectors.toMap(Map.Entry::getKey, e -> new OffsetAndMetadata(e.getValue())));\n\n        if (!offsetsToAlter.isEmpty()) {\n            log.debug(\"Committing the following consumer group offsets using an admin client for sink connector {}: {}.\",\n                    connName, offsetsToAlter);\n            AlterConsumerGroupOffsetsOptions alterConsumerGroupOffsetsOptions = new AlterConsumerGroupOffsetsOptions()\n                    .timeoutMs((int) timer.remainingMs());\n            AlterConsumerGroupOffsetsResult alterConsumerGroupOffsetsResult = admin.alterConsumerGroupOffsets(groupId, offsetsToAlter,\n                    alterConsumerGroupOffsetsOptions);\n\n            adminFutures.add(alterConsumerGroupOffsetsResult.all());\n        }\n\n        Set<TopicPartition> partitionsToReset = offsetsToWrite.entrySet()\n                .stream()\n                .filter(entry -> entry.getValue() == null)\n                .map(Map.Entry::getKey)\n                .collect(Collectors.toSet());\n\n        if (!partitionsToReset.isEmpty()) {\n            log.debug(\"Deleting the consumer group offsets for the following topic partitions using an admin client for sink connector {}: {}.\",\n                    connName, partitionsToReset);\n            DeleteConsumerGroupOffsetsOptions deleteConsumerGroupOffsetsOptions = new DeleteConsumerGroupOffsetsOptions()\n                    .timeoutMs((int) timer.remainingMs());\n            DeleteConsumerGroupOffsetsResult deleteConsumerGroupOffsetsResult = admin.deleteConsumerGroupOffsets(groupId, partitionsToReset,\n                    deleteConsumerGroupOffsetsOptions);\n\n            adminFutures.add(deleteConsumerGroupOffsetsResult.all());\n        }\n\n        @SuppressWarnings(\"rawtypes\")\n        KafkaFuture<Void> compositeAdminFuture = KafkaFuture.allOf(adminFutures.toArray(new KafkaFuture[0]));\n\n        compositeAdminFuture.whenComplete((ignored, error) -> {\n            if (error != null) {\n                // When a consumer group is non-empty, only group members can commit offsets. An attempt to alter offsets via the admin client\n                // will result in an UnknownMemberIdException if the consumer group is non-empty (i.e. if the sink tasks haven't stopped\n                // completely or if the connector is resumed while the alter offsets request is being processed). Similarly, an attempt to\n                // delete consumer group offsets for a non-empty consumer group will result in a GroupSubscribedToTopicException\n                if (error instanceof UnknownMemberIdException || error instanceof GroupSubscribedToTopicException) {\n                    String errorMsg = \"Failed to alter consumer group offsets for connector \" + connName + \" either because its tasks \" +\n                            \"haven't stopped completely yet or the connector was resumed before the request to alter its offsets could be successfully \" +\n                            \"completed. If the connector is in a stopped state, this operation can be safely retried. If it doesn't eventually succeed, the \" +\n                            \"Connect cluster may need to be restarted to get rid of the zombie sink tasks.\";\n                    log.error(errorMsg, error);\n                    cb.onCompletion(new ConnectException(errorMsg, error), null);\n                } else {\n                    log.error(\"Failed to alter consumer group offsets for connector {}\", connName, error);\n                    cb.onCompletion(new ConnectException(\"Failed to alter consumer group offsets for connector \" + connName, error), null);\n                }\n            } else {\n                completeModifyOffsetsCallback(alterOffsetsResult, false, cb);\n            }\n        }).whenComplete((ignored, ignoredError) -> {\n            // errors originating from the original future are handled in the prior whenComplete invocation which isn't expected to throw\n            // an exception itself, and we can thus ignore the error here\n            Utils.closeQuietly(admin, \"Offset alter admin for sink connector \" + connName);\n        });\n    }\n\n    /**\n     * Reset a sink connector's consumer group offsets. This is done by deleting the consumer group via a call to\n     * {@link Admin#deleteConsumerGroups}\n     *\n     * @param connName the name of the sink connector whose offsets are to be reset\n     * @param groupId the sink connector's consumer group ID\n     * @param admin the {@link Admin admin client} to be used for resetting the consumer group offsets; will be closed after use\n     * @param cb callback to invoke upon completion\n     * @param alterOffsetsResult the result of the call to {@link SinkConnector#alterOffsets} for the connector\n     * @param timer {@link Timer} to bound the total runtime of admin client requests\n     */\n    private void resetSinkConnectorOffsets(String connName, String groupId, Admin admin, Callback<Message> cb, boolean alterOffsetsResult, Timer timer) {\n        DeleteConsumerGroupsOptions deleteConsumerGroupsOptions = new DeleteConsumerGroupsOptions().timeoutMs((int) timer.remainingMs());\n\n        admin.deleteConsumerGroups(Collections.singleton(groupId), deleteConsumerGroupsOptions)\n                .all()\n                .whenComplete((ignored, error) -> {\n                    // We treat GroupIdNotFoundException as a non-error here because resetting a connector's offsets is expected to be an idempotent operation\n                    // and the consumer group could have already been deleted in a prior offsets reset request\n                    if (error != null && !(error instanceof GroupIdNotFoundException)) {\n                        // When a consumer group is non-empty, attempts to delete it via the admin client result in a GroupNotEmptyException. This can occur\n                        // if the sink tasks haven't stopped completely or if the connector is resumed while the reset offsets request is being processed\n                        if (error instanceof GroupNotEmptyException) {\n                            String errorMsg = \"Failed to reset consumer group offsets for connector \" + connName + \" either because its tasks \" +\n                                    \"haven't stopped completely yet or the connector was resumed before the request to reset its offsets could be successfully \" +\n                                    \"completed. If the connector is in a stopped state, this operation can be safely retried. If it doesn't eventually succeed, the \" +\n                                    \"Connect cluster may need to be restarted to get rid of the zombie sink tasks.\";\n                            log.error(errorMsg, error);\n                            cb.onCompletion(new ConnectException(errorMsg, error), null);\n                        } else {\n                            log.error(\"Failed to reset consumer group offsets for sink connector {}\", connName, error);\n                            cb.onCompletion(new ConnectException(\"Failed to reset consumer group offsets for sink connector \" + connName, error), null);\n                        }\n                    } else {\n                        completeModifyOffsetsCallback(alterOffsetsResult, true, cb);\n                    }\n                }).whenComplete((ignored, ignoredError) -> {\n                    // errors originating from the original future are handled in the prior whenComplete invocation which isn't expected to throw\n                    // an exception itself, and we can thus ignore the error here\n                    Utils.closeQuietly(admin, \"Offset reset admin for sink connector \" + connName);\n                });\n    }\n\n    /**\n     * Modify (alter / reset) a source connector's offsets.\n     *\n     * @param connName the name of the source connector whose offsets are to be modified\n     * @param connector an instance of the source connector\n     * @param connectorConfig the source connector's configuration\n     * @param offsets a mapping from partitions to offsets that need to be written; this should be {@code null} for\n     *                offsets reset requests\n     * @param connectorLoader the connector plugin's classloader to be used as the thread context classloader\n     * @param cb callback to invoke upon completion\n     */\n    private void modifySourceConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                                              Map<Map<String, ?>, Map<String, ?>> offsets, ClassLoader connectorLoader, Callback<Message> cb) {\n        SourceConnectorConfig sourceConfig = new SourceConnectorConfig(plugins, connectorConfig, config.topicCreationEnable());\n        Map<String, Object> producerProps = config.exactlyOnceSourceEnabled()\n                ? exactlyOnceSourceTaskProducerConfigs(new ConnectorTaskId(connName, 0), config, sourceConfig,\n                connector.getClass(), connectorClientConfigOverridePolicy, kafkaClusterId)\n                : baseProducerConfigs(connName, \"connector-offset-producer-\" + connName, config, sourceConfig,\n                connector.getClass(), connectorClientConfigOverridePolicy, kafkaClusterId);\n        KafkaProducer<byte[], byte[]> producer = new KafkaProducer<>(producerProps);\n\n        ConnectorOffsetBackingStore offsetStore = config.exactlyOnceSourceEnabled()\n                ? offsetStoreForExactlyOnceSourceConnector(sourceConfig, connName, connector, producer)\n                : offsetStoreForRegularSourceConnector(sourceConfig, connName, connector, producer);\n        offsetStore.configure(config);\n\n        OffsetStorageWriter offsetWriter = new OffsetStorageWriter(offsetStore, connName, internalKeyConverter, internalValueConverter);\n        modifySourceConnectorOffsets(connName, connector, connectorConfig, offsets, offsetStore, producer, offsetWriter, connectorLoader, cb);\n    }\n\n    // Visible for testing\n    void modifySourceConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                                      Map<Map<String, ?>, Map<String, ?>> offsets, ConnectorOffsetBackingStore offsetStore,\n                                      KafkaProducer<byte[], byte[]> producer, OffsetStorageWriter offsetWriter,\n                                      ClassLoader connectorLoader, Callback<Message> cb) {\n        executor.submit(plugins.withClassLoader(connectorLoader, () -> {\n            try {\n                Timer timer = time.timer(Duration.ofMillis(RestServer.DEFAULT_REST_REQUEST_TIMEOUT_MS));\n                // This reads to the end of the offsets topic and can be a potentially time-consuming operation\n                offsetStore.start();\n                updateTimerAndCheckExpiry(timer, \"Timed out while trying to read to the end of the offsets topic prior to modifying \" +\n                        \"offsets for source connector \" + connName);\n                Map<Map<String, ?>, Map<String, ?>> offsetsToWrite;\n\n                // If the offsets argument is null, it indicates an offsets reset operation - i.e. a null offset should\n                // be written for every source partition of the connector\n                boolean isReset;\n                if (offsets == null) {\n                    isReset = true;\n                    offsetsToWrite = new HashMap<>();\n                    offsetStore.connectorPartitions(connName).forEach(partition -> offsetsToWrite.put(partition, null));\n                    log.debug(\"Found the following partitions (to reset offsets) for source connector {}: {}\", connName, offsetsToWrite.keySet());\n                } else {\n                    isReset = false;\n                    offsetsToWrite = offsets;\n                }\n\n                Map<Map<String, ?>, Map<String, ?>> normalizedOffsets = normalizeSourceConnectorOffsets(offsetsToWrite);\n\n                boolean alterOffsetsResult;\n                try {\n                    alterOffsetsResult = ((SourceConnector) connector).alterOffsets(connectorConfig, normalizedOffsets);\n                } catch (UnsupportedOperationException e) {\n                    log.error(\"Failed to modify offsets for connector {} because it doesn't support external modification of offsets\",\n                            connName, e);\n                    throw new ConnectException(\"Failed to modify offsets for connector \" + connName + \" because it doesn't support external \" +\n                            \"modification of offsets\", e);\n                }\n                updateTimerAndCheckExpiry(timer, \"Timed out while calling the 'alterOffsets' method for source connector \" + connName);\n\n                // This should only occur for an offsets reset request when there are no source partitions found for the source connector in the\n                // offset store - either because there was a prior attempt to reset offsets or if there are no offsets committed by this source\n                // connector so far\n                if (normalizedOffsets.isEmpty()) {\n                    log.info(\"No offsets found for source connector {} - this can occur due to a prior attempt to reset offsets or if the \" +\n                            \"source connector hasn't committed any offsets yet\", connName);\n                    completeModifyOffsetsCallback(alterOffsetsResult, isReset, cb);\n                    return;\n                }\n\n                // The modifySourceConnectorOffsets method should only be called after all the connector's tasks have been stopped, and it's\n                // safe to write offsets via an offset writer\n                normalizedOffsets.forEach(offsetWriter::offset);\n\n                // We can call begin flush without a timeout because this newly created single-purpose offset writer can't do concurrent\n                // offset writes. We can also ignore the return value since it returns false if and only if there is no data to be flushed,\n                // and we've just put some data in the previous statement\n                offsetWriter.beginFlush();\n\n                if (config.exactlyOnceSourceEnabled()) {\n                    producer.initTransactions();\n                    producer.beginTransaction();\n                }\n                log.debug(\"Committing the following offsets for source connector {}: {}\", connName, normalizedOffsets);\n                FutureCallback<Void> offsetWriterCallback = new FutureCallback<>();\n                offsetWriter.doFlush(offsetWriterCallback);\n                if (config.exactlyOnceSourceEnabled()) {\n                    producer.commitTransaction();\n                }\n\n                try {\n                    offsetWriterCallback.get(timer.remainingMs(), TimeUnit.MILLISECONDS);\n                } catch (ExecutionException e) {\n                    throw new ConnectException(\"Failed to modify offsets for source connector \" + connName, e.getCause());\n                } catch (TimeoutException e) {\n                    throw new ConnectException(\"Timed out while attempting to modify offsets for source connector \" + connName, e);\n                } catch (InterruptedException e) {\n                    throw new ConnectException(\"Unexpectedly interrupted while attempting to modify offsets for source connector \" + connName, e);\n                }\n\n                completeModifyOffsetsCallback(alterOffsetsResult, isReset, cb);\n            } catch (Throwable t) {\n                log.error(\"Failed to modify offsets for source connector {}\", connName, t);\n                cb.onCompletion(ConnectUtils.maybeWrap(t, \"Failed to modify offsets for source connector \" + connName), null);\n            } finally {\n                Utils.closeQuietly(offsetStore::stop, \"Offset store for offset modification request for connector \" + connName);\n            }\n        }));\n    }\n\n    /**\n     * \"Normalize\" source connector offsets by serializing and deserializing them using the internal {@link JsonConverter}.\n     * This is done in order to prevent type mismatches between the offsets passed to {@link SourceConnector#alterOffsets(Map, Map)}\n     * and the offsets that connectors and tasks retrieve via an instance of {@link OffsetStorageReader}.\n     * <p>\n     * Visible for testing.\n     *\n     * @param originalOffsets the offsets that are to be normalized\n     * @return the normalized offsets\n     */\n    @SuppressWarnings(\"unchecked\")\n    Map<Map<String, ?>, Map<String, ?>> normalizeSourceConnectorOffsets(Map<Map<String, ?>, Map<String, ?>> originalOffsets) {\n        Map<Map<String, ?>, Map<String, ?>> normalizedOffsets = new HashMap<>();\n        for (Map.Entry<Map<String, ?>, Map<String, ?>> entry : originalOffsets.entrySet()) {\n            OffsetUtils.validateFormat(entry.getKey());\n            OffsetUtils.validateFormat(entry.getValue());\n            byte[] serializedKey = internalKeyConverter.fromConnectData(\"\", null, entry.getKey());\n            byte[] serializedValue = internalValueConverter.fromConnectData(\"\", null, entry.getValue());\n            Object deserializedKey = internalKeyConverter.toConnectData(\"\", serializedKey).value();\n            Object deserializedValue = internalValueConverter.toConnectData(\"\", serializedValue).value();\n            normalizedOffsets.put((Map<String, ?>) deserializedKey, (Map<String, ?>) deserializedValue);\n        }\n\n        return normalizedOffsets;\n    }\n\n    /**\n     * Update the provided timer, check if it's expired and throw a {@link ConnectException} with the provided error\n     * message if it is.\n     *\n     * @param timer {@link Timer} to check\n     * @param errorMessageIfExpired error message indicating the cause for the timer expiry\n     * @throws ConnectException if the timer has expired\n     */\n    private void updateTimerAndCheckExpiry(Timer timer, String errorMessageIfExpired) {\n        timer.update();\n        if (timer.isExpired()) {\n            log.error(errorMessageIfExpired);\n            throw new ConnectException(errorMessageIfExpired);\n        }\n    }\n\n    /**\n     * Complete the alter / reset offsets callback with a potential-success or a definite-success message.\n     *\n     * @param alterOffsetsResult the result of the call to {@link SinkConnector#alterOffsets} / {@link SourceConnector#alterOffsets}\n     * @param isReset whether this callback if for an offsets reset operation\n     * @param cb the callback to complete\n     *\n     * @see <a href=\"https://cwiki.apache.org/confluence/display/KAFKA/KIP-875%3A+First-class+offsets+support+in+Kafka+Connect\">KIP-875</a>\n     */\n    private void completeModifyOffsetsCallback(boolean alterOffsetsResult, boolean isReset, Callback<Message> cb) {\n        String modificationType = isReset ? \"reset\" : \"altered\";\n        if (alterOffsetsResult) {\n            cb.onCompletion(null, new Message(\"The offsets for this connector have been \" + modificationType + \" successfully\"));\n        } else {\n            cb.onCompletion(null, new Message(\"The Connect framework-managed offsets for this connector have been \" +\n                    modificationType + \" successfully. However, if this connector manages offsets externally, they will need to be \" +\n                    \"manually \" + modificationType + \" in the system that the connector uses.\"));\n        }\n    }\n\n    ConnectorStatusMetricsGroup connectorStatusMetricsGroup() {\n        return connectorStatusMetricsGroup;\n    }\n\n    WorkerMetricsGroup workerMetricsGroup() {\n        return workerMetricsGroup;\n    }\n\n    abstract class TaskBuilder<T, R extends ConnectRecord<R>> {\n\n        private final ConnectorTaskId id;\n        private final ClusterConfigState configState;\n        private final TaskStatus.Listener statusListener;\n        private final TargetState initialState;\n\n        private Task task = null;\n        private ConnectorConfig connectorConfig = null;\n        private Converter keyConverter = null;\n        private Converter valueConverter = null;\n        private HeaderConverter headerConverter = null;\n        private ClassLoader classLoader = null;\n\n        public TaskBuilder(ConnectorTaskId id,\n                           ClusterConfigState configState,\n                           TaskStatus.Listener statusListener,\n                           TargetState initialState) {\n            this.id = id;\n            this.configState = configState;\n            this.statusListener = statusListener;\n            this.initialState = initialState;\n        }\n\n        public TaskBuilder<T, R> withTask(Task task) {\n            this.task = task;\n            return this;\n        }\n\n        public TaskBuilder<T, R> withConnectorConfig(ConnectorConfig connectorConfig) {\n            this.connectorConfig = connectorConfig;\n            return this;\n        }\n\n        public TaskBuilder<T, R> withKeyConverter(Converter keyConverter) {\n            this.keyConverter = keyConverter;\n            return this;\n        }\n\n        public TaskBuilder<T, R> withValueConverter(Converter valueConverter) {\n            this.valueConverter = valueConverter;\n            return this;\n        }\n\n        public TaskBuilder<T, R> withHeaderConverter(HeaderConverter headerConverter) {\n            this.headerConverter = headerConverter;\n            return this;\n        }\n\n        public TaskBuilder<T, R> withClassloader(ClassLoader classLoader) {\n            this.classLoader = classLoader;\n            return this;\n        }\n\n        public WorkerTask<T, R> build() {\n            Objects.requireNonNull(task, \"Task cannot be null\");\n            Objects.requireNonNull(connectorConfig, \"Connector config used by task cannot be null\");\n            Objects.requireNonNull(keyConverter, \"Key converter used by task cannot be null\");\n            Objects.requireNonNull(valueConverter, \"Value converter used by task cannot be null\");\n            Objects.requireNonNull(headerConverter, \"Header converter used by task cannot be null\");\n            Objects.requireNonNull(classLoader, \"Classloader used by task cannot be null\");\n\n            ErrorHandlingMetrics errorHandlingMetrics = errorHandlingMetrics(id);\n            final Class<? extends Connector> connectorClass = plugins.connectorClass(\n                    connectorConfig.getString(ConnectorConfig.CONNECTOR_CLASS_CONFIG));\n\n            RetryWithToleranceOperator<T> retryWithToleranceOperator = new RetryWithToleranceOperator<>(connectorConfig.errorRetryTimeout(),\n                    connectorConfig.errorMaxDelayInMillis(), connectorConfig.errorToleranceType(), Time.SYSTEM, errorHandlingMetrics);\n\n            TransformationChain<T, R> transformationChain = new TransformationChain<>(connectorConfig.<R>transformationStages(), retryWithToleranceOperator);\n            log.info(\"Initializing: {}\", transformationChain);\n\n            return doBuild(task, id, configState, statusListener, initialState,\n                    connectorConfig, keyConverter, valueConverter, headerConverter, classLoader,\n                    retryWithToleranceOperator, transformationChain,\n                    errorHandlingMetrics, connectorClass);\n        }\n\n        abstract WorkerTask<T, R> doBuild(\n                Task task,\n                ConnectorTaskId id,\n                ClusterConfigState configState,\n                TaskStatus.Listener statusListener,\n                TargetState initialState,\n                ConnectorConfig connectorConfig,\n                Converter keyConverter,\n                Converter valueConverter,\n                HeaderConverter headerConverter,\n                ClassLoader classLoader,\n                RetryWithToleranceOperator<T> retryWithToleranceOperator,\n                TransformationChain<T, R> transformationChain,\n                ErrorHandlingMetrics errorHandlingMetrics,\n                Class<? extends Connector> connectorClass\n        );\n\n    }\n\n    class SinkTaskBuilder extends TaskBuilder<ConsumerRecord<byte[], byte[]>, SinkRecord> {\n        public SinkTaskBuilder(ConnectorTaskId id,\n                               ClusterConfigState configState,\n                               TaskStatus.Listener statusListener,\n                               TargetState initialState) {\n            super(id, configState, statusListener, initialState);\n        }\n\n        @Override\n        public WorkerTask<ConsumerRecord<byte[], byte[]>, SinkRecord> doBuild(\n                Task task,\n                ConnectorTaskId id,\n                ClusterConfigState configState,\n                TaskStatus.Listener statusListener,\n                TargetState initialState,\n                ConnectorConfig connectorConfig,\n                Converter keyConverter,\n                Converter valueConverter,\n                HeaderConverter headerConverter,\n                ClassLoader classLoader,\n                RetryWithToleranceOperator<ConsumerRecord<byte[], byte[]>> retryWithToleranceOperator,\n                TransformationChain<ConsumerRecord<byte[], byte[]>, SinkRecord> transformationChain,\n                ErrorHandlingMetrics errorHandlingMetrics,\n                Class<? extends Connector> connectorClass\n        ) {\n            SinkConnectorConfig sinkConfig = new SinkConnectorConfig(plugins, connectorConfig.originalsStrings());\n            WorkerErrantRecordReporter workerErrantRecordReporter = createWorkerErrantRecordReporter(sinkConfig, retryWithToleranceOperator,\n                    keyConverter, valueConverter, headerConverter);\n\n            Map<String, Object> consumerProps = baseConsumerConfigs(\n                    id.connector(),  \"connector-consumer-\" + id, config, connectorConfig, connectorClass,\n                    connectorClientConfigOverridePolicy, kafkaClusterId, ConnectorType.SINK);\n            KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(consumerProps);\n\n            return new WorkerSinkTask(id, (SinkTask) task, statusListener, initialState, config, configState, metrics, keyConverter,\n                    valueConverter, errorHandlingMetrics, headerConverter, transformationChain, consumer, classLoader, time,\n                    retryWithToleranceOperator, workerErrantRecordReporter, herder.statusBackingStore(),\n                    () -> sinkTaskReporters(id, sinkConfig, errorHandlingMetrics, connectorClass));\n        }\n    }\n\n    class SourceTaskBuilder extends TaskBuilder<SourceRecord, SourceRecord> {\n        public SourceTaskBuilder(ConnectorTaskId id,\n                               ClusterConfigState configState,\n                               TaskStatus.Listener statusListener,\n                               TargetState initialState) {\n            super(id, configState, statusListener, initialState);\n        }\n\n        @Override\n        public WorkerTask<SourceRecord, SourceRecord> doBuild(\n                Task task,\n                ConnectorTaskId id,\n                ClusterConfigState configState,\n                TaskStatus.Listener statusListener,\n                TargetState initialState,\n                ConnectorConfig connectorConfig,\n                Converter keyConverter,\n                Converter valueConverter,\n                HeaderConverter headerConverter,\n                ClassLoader classLoader,\n                RetryWithToleranceOperator<SourceRecord> retryWithToleranceOperator,\n                TransformationChain<SourceRecord, SourceRecord> transformationChain,\n                ErrorHandlingMetrics errorHandlingMetrics,\n                Class<? extends Connector> connectorClass\n        ) {\n            SourceConnectorConfig sourceConfig = new SourceConnectorConfig(plugins,\n                    connectorConfig.originalsStrings(), config.topicCreationEnable());\n\n            Map<String, Object> producerProps = baseProducerConfigs(id.connector(), \"connector-producer-\" + id, config, sourceConfig, connectorClass,\n                    connectorClientConfigOverridePolicy, kafkaClusterId);\n            KafkaProducer<byte[], byte[]> producer = new KafkaProducer<>(producerProps);\n\n            TopicAdmin topicAdmin = null;\n            final boolean topicCreationEnabled = sourceConnectorTopicCreationEnabled(sourceConfig);\n            if (topicCreationEnabled || regularSourceTaskUsesConnectorSpecificOffsetsStore(sourceConfig)) {\n                Map<String, Object> adminOverrides = adminConfigs(id.connector(), \"connector-adminclient-\" + id, config,\n                        sourceConfig, connectorClass, connectorClientConfigOverridePolicy, kafkaClusterId, ConnectorType.SOURCE);\n                topicAdmin = new TopicAdmin(adminOverrides);\n            }\n\n            Map<String, TopicCreationGroup> topicCreationGroups = topicCreationEnabled\n                    ? TopicCreationGroup.configuredGroups(sourceConfig)\n                    : null;\n\n            // Set up the offset backing store for this task instance\n            ConnectorOffsetBackingStore offsetStore = offsetStoreForRegularSourceTask(\n                    id, sourceConfig, connectorClass, producer, producerProps, topicAdmin);\n            offsetStore.configure(config);\n\n            CloseableOffsetStorageReader offsetReader = new OffsetStorageReaderImpl(offsetStore, id.connector(), internalKeyConverter, internalValueConverter);\n            OffsetStorageWriter offsetWriter = new OffsetStorageWriter(offsetStore, id.connector(), internalKeyConverter, internalValueConverter);\n\n            // Note we pass the configState as it performs dynamic transformations under the covers\n            return new WorkerSourceTask(id, (SourceTask) task, statusListener, initialState, keyConverter, valueConverter, errorHandlingMetrics,\n                    headerConverter, transformationChain, producer, topicAdmin, topicCreationGroups,\n                    offsetReader, offsetWriter, offsetStore, config, configState, metrics, classLoader, time,\n                    retryWithToleranceOperator, herder.statusBackingStore(), executor, () -> sourceTaskReporters(id, sourceConfig, errorHandlingMetrics));\n        }\n    }\n\n    class ExactlyOnceSourceTaskBuilder extends TaskBuilder<SourceRecord, SourceRecord> {\n        private final Runnable preProducerCheck;\n        private final Runnable postProducerCheck;\n\n        public ExactlyOnceSourceTaskBuilder(ConnectorTaskId id,\n                                            ClusterConfigState configState,\n                                            TaskStatus.Listener statusListener,\n                                            TargetState initialState,\n                                            Runnable preProducerCheck,\n                                            Runnable postProducerCheck) {\n            super(id, configState, statusListener, initialState);\n            this.preProducerCheck = preProducerCheck;\n            this.postProducerCheck = postProducerCheck;\n        }\n\n        @Override\n        public WorkerTask<SourceRecord, SourceRecord> doBuild(\n                Task task,\n                ConnectorTaskId id,\n                ClusterConfigState configState,\n                TaskStatus.Listener statusListener,\n                TargetState initialState,\n                ConnectorConfig connectorConfig,\n                Converter keyConverter,\n                Converter valueConverter,\n                HeaderConverter headerConverter,\n                ClassLoader classLoader,\n                RetryWithToleranceOperator<SourceRecord> retryWithToleranceOperator,\n                TransformationChain<SourceRecord, SourceRecord> transformationChain,\n                ErrorHandlingMetrics errorHandlingMetrics,\n                Class<? extends Connector> connectorClass\n        ) {\n            SourceConnectorConfig sourceConfig = new SourceConnectorConfig(plugins,\n                    connectorConfig.originalsStrings(), config.topicCreationEnable());\n            Map<String, Object> producerProps = exactlyOnceSourceTaskProducerConfigs(\n                    id, config, sourceConfig, connectorClass,\n                    connectorClientConfigOverridePolicy, kafkaClusterId);\n            KafkaProducer<byte[], byte[]> producer = new KafkaProducer<>(producerProps);\n\n            // Create a topic admin that the task will use for its offsets topic and, potentially, automatic topic creation\n            Map<String, Object> adminOverrides = adminConfigs(id.connector(), \"connector-adminclient-\" + id, config,\n                    sourceConfig, connectorClass, connectorClientConfigOverridePolicy, kafkaClusterId, ConnectorType.SOURCE);\n            TopicAdmin topicAdmin = new TopicAdmin(adminOverrides);\n\n            Map<String, TopicCreationGroup> topicCreationGroups = sourceConnectorTopicCreationEnabled(sourceConfig)\n                    ? TopicCreationGroup.configuredGroups(sourceConfig)\n                    : null;\n\n            // Set up the offset backing store for this task instance\n            ConnectorOffsetBackingStore offsetStore = offsetStoreForExactlyOnceSourceTask(\n                    id, sourceConfig, connectorClass, producer, producerProps, topicAdmin);\n            offsetStore.configure(config);\n\n            CloseableOffsetStorageReader offsetReader = new OffsetStorageReaderImpl(offsetStore, id.connector(), internalKeyConverter, internalValueConverter);\n            OffsetStorageWriter offsetWriter = new OffsetStorageWriter(offsetStore, id.connector(), internalKeyConverter, internalValueConverter);\n\n            // Note we pass the configState as it performs dynamic transformations under the covers\n            return new ExactlyOnceWorkerSourceTask(id, (SourceTask) task, statusListener, initialState, keyConverter, valueConverter,\n                    headerConverter, transformationChain, producer, topicAdmin, topicCreationGroups,\n                    offsetReader, offsetWriter, offsetStore, config, configState, metrics, errorHandlingMetrics, classLoader, time, retryWithToleranceOperator,\n                    herder.statusBackingStore(), sourceConfig, executor, preProducerCheck, postProducerCheck,\n                    () -> sourceTaskReporters(id, sourceConfig, errorHandlingMetrics));\n        }\n    }\n\n    /**\n     * Builds and returns an offset backing store for a regular source connector (i.e. when exactly-once support for source connectors is disabled).\n     * The offset backing store will either be just the worker's global offset backing store (if the connector doesn't define a connector-specific\n     * offset topic via its configs), just a connector-specific offset backing store (if the connector defines a connector-specific offsets\n     * topic which appears to be the same as the worker's global offset topic) or a combination of both the worker's global offset backing store\n     * and a connector-specific offset backing store.\n     * <p>\n     * Visible for testing.\n     * @param sourceConfig the source connector's config\n     * @param connName the source connector's name\n     * @param connector the source connector\n     * @param producer the Kafka producer for the offset backing store; may be {@code null} if a read-only offset backing store is required\n     * @return An offset backing store for a regular source connector\n     */\n    ConnectorOffsetBackingStore offsetStoreForRegularSourceConnector(\n            SourceConnectorConfig sourceConfig,\n            String connName,\n            Connector connector,\n            Producer<byte[], byte[]> producer\n    ) {\n        String connectorSpecificOffsetsTopic = sourceConfig.offsetsTopic();\n\n        Map<String, Object> producerProps = baseProducerConfigs(connName, \"connector-producer-\" + connName, config, sourceConfig, connector.getClass(),\n                connectorClientConfigOverridePolicy, kafkaClusterId);\n\n        // We use a connector-specific store (i.e., a dedicated KafkaOffsetBackingStore for this connector)\n        // if the worker supports per-connector offsets topics (which may be the case in distributed but not standalone mode, for example)\n        // and if the connector is explicitly configured with an offsets topic\n        final boolean usesConnectorSpecificStore = connectorSpecificOffsetsTopic != null\n                && config.connectorOffsetsTopicsPermitted();\n\n        if (usesConnectorSpecificStore) {\n            Map<String, Object> consumerProps = regularSourceOffsetsConsumerConfigs(\n                        connName, \"connector-consumer-\" + connName, config, sourceConfig, connector.getClass(),\n                        connectorClientConfigOverridePolicy, kafkaClusterId);\n            KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(consumerProps);\n\n            Map<String, Object> adminOverrides = adminConfigs(connName, \"connector-adminclient-\" + connName, config,\n                    sourceConfig, connector.getClass(), connectorClientConfigOverridePolicy, kafkaClusterId, ConnectorType.SOURCE);\n\n            TopicAdmin admin = new TopicAdmin(adminOverrides);\n\n            KafkaOffsetBackingStore connectorStore = producer == null\n                    ? KafkaOffsetBackingStore.readOnlyStore(connectorSpecificOffsetsTopic, consumer, admin, internalKeyConverter)\n                    : KafkaOffsetBackingStore.readWriteStore(connectorSpecificOffsetsTopic, producer, consumer, admin, internalKeyConverter);\n\n            // If the connector's offsets topic is the same as the worker-global offsets topic, there's no need to construct\n            // an offset store that has a primary and a secondary store which both read from that same topic.\n            // So, if the user has explicitly configured the connector with a connector-specific offsets topic\n            // but we know that that topic is the same as the worker-global offsets topic, we ignore the worker-global\n            // offset store and build a store backed exclusively by a connector-specific offsets store.\n            // It may seem reasonable to instead build a store backed exclusively by the worker-global offset store, but that\n            // would prevent users from being able to customize the config properties used for the Kafka clients that\n            // access the offsets topic, and we would not be able to establish reasonable defaults like setting\n            // isolation.level=read_committed for the offsets topic consumer for this connector\n            if (sameOffsetTopicAsWorker(connectorSpecificOffsetsTopic, producerProps)) {\n                return ConnectorOffsetBackingStore.withOnlyConnectorStore(\n                        () -> LoggingContext.forConnector(connName),\n                        connectorStore,\n                        connectorSpecificOffsetsTopic,\n                        admin\n                );\n            } else {\n                return ConnectorOffsetBackingStore.withConnectorAndWorkerStores(\n                        () -> LoggingContext.forConnector(connName),\n                        globalOffsetBackingStore,\n                        connectorStore,\n                        connectorSpecificOffsetsTopic,\n                        admin\n                );\n            }\n        } else {\n            Utils.closeQuietly(producer, \"Unused producer for offset store\");\n            return ConnectorOffsetBackingStore.withOnlyWorkerStore(\n                    () -> LoggingContext.forConnector(connName),\n                    globalOffsetBackingStore,\n                    config.offsetsTopic()\n            );\n        }\n    }\n\n    /**\n     * Builds and returns an offset backing store for an exactly-once source connector. The offset backing store will either be just\n     * a connector-specific offset backing store (if the connector's offsets topic is the same as the worker's global offset topic)\n     * or a combination of both the worker's global offset backing store and a connector-specific offset backing store.\n     * <p>\n     * Visible for testing.\n     * @param sourceConfig the source connector's config\n     * @param connName the source connector's name\n     * @param connector the source connector\n     * @param producer the Kafka producer for the offset backing store; may be {@code null} if a read-only offset backing store is required\n     * @return An offset backing store for an exactly-once source connector\n     */\n    ConnectorOffsetBackingStore offsetStoreForExactlyOnceSourceConnector(\n            SourceConnectorConfig sourceConfig,\n            String connName,\n            Connector connector,\n            Producer<byte[], byte[]> producer\n    ) {\n        String connectorSpecificOffsetsTopic = Optional.ofNullable(sourceConfig.offsetsTopic()).orElse(config.offsetsTopic());\n\n        Map<String, Object> producerProps = baseProducerConfigs(connName, \"connector-producer-\" + connName, config, sourceConfig, connector.getClass(),\n                connectorClientConfigOverridePolicy, kafkaClusterId);\n\n        Map<String, Object> consumerProps = exactlyOnceSourceOffsetsConsumerConfigs(\n                    connName, \"connector-consumer-\" + connName, config, sourceConfig, connector.getClass(),\n                    connectorClientConfigOverridePolicy, kafkaClusterId);\n        KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(consumerProps);\n\n        Map<String, Object> adminOverrides = adminConfigs(connName, \"connector-adminclient-\" + connName, config,\n                sourceConfig, connector.getClass(), connectorClientConfigOverridePolicy, kafkaClusterId, ConnectorType.SOURCE);\n\n        TopicAdmin admin = new TopicAdmin(adminOverrides);\n\n        KafkaOffsetBackingStore connectorStore = producer == null\n                ? KafkaOffsetBackingStore.readOnlyStore(connectorSpecificOffsetsTopic, consumer, admin, internalKeyConverter)\n                : KafkaOffsetBackingStore.readWriteStore(connectorSpecificOffsetsTopic, producer, consumer, admin, internalKeyConverter);\n\n        // If the connector's offsets topic is the same as the worker-global offsets topic, there's no need to construct\n        // an offset store that has a primary and a secondary store which both read from that same topic.\n        // So, even if the user has explicitly configured the connector with a connector-specific offsets topic,\n        // if we know that that topic is the same as the worker-global offsets topic, we ignore the worker-global\n        // offset store and build a store backed exclusively by a connector-specific offsets store.\n        // It may seem reasonable to instead build a store backed exclusively by the worker-global offset store, but that\n        // would prevent users from being able to customize the config properties used for the Kafka clients that\n        // access the offsets topic, and may lead to confusion for them when tasks are created for the connector\n        // since they will all have their own dedicated offsets stores anyways\n        if (sameOffsetTopicAsWorker(connectorSpecificOffsetsTopic, producerProps)) {\n            return ConnectorOffsetBackingStore.withOnlyConnectorStore(\n                    () -> LoggingContext.forConnector(connName),\n                    connectorStore,\n                    connectorSpecificOffsetsTopic,\n                    admin\n            );\n        } else {\n            return ConnectorOffsetBackingStore.withConnectorAndWorkerStores(\n                    () -> LoggingContext.forConnector(connName),\n                    globalOffsetBackingStore,\n                    connectorStore,\n                    connectorSpecificOffsetsTopic,\n                    admin\n            );\n        }\n    }\n\n    // Visible for testing\n    ConnectorOffsetBackingStore offsetStoreForRegularSourceTask(\n            ConnectorTaskId id,\n            SourceConnectorConfig sourceConfig,\n            Class<? extends Connector> connectorClass,\n            Producer<byte[], byte[]> producer,\n            Map<String, Object> producerProps,\n            TopicAdmin topicAdmin\n    ) {\n        String connectorSpecificOffsetsTopic = sourceConfig.offsetsTopic();\n\n        if (regularSourceTaskUsesConnectorSpecificOffsetsStore(sourceConfig)) {\n            Objects.requireNonNull(topicAdmin, \"Source tasks require a non-null topic admin when configured to use their own offsets topic\");\n\n            Map<String, Object> consumerProps = regularSourceOffsetsConsumerConfigs(\n                    id.connector(), \"connector-consumer-\" + id, config, sourceConfig, connectorClass,\n                    connectorClientConfigOverridePolicy, kafkaClusterId);\n            KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(consumerProps);\n\n            KafkaOffsetBackingStore connectorStore =\n                    KafkaOffsetBackingStore.readWriteStore(sourceConfig.offsetsTopic(), producer, consumer, topicAdmin, internalKeyConverter);\n\n            // If the connector's offsets topic is the same as the worker-global offsets topic, there's no need to construct\n            // an offset store that has a primary and a secondary store which both read from that same topic.\n            // So, if the user has (implicitly or explicitly) configured the connector with a connector-specific offsets topic\n            // but we know that that topic is the same as the worker-global offsets topic, we ignore the worker-global\n            // offset store and build a store backed exclusively by a connector-specific offsets store.\n            // It may seem reasonable to instead build a store backed exclusively by the worker-global offset store, but that\n            // would prevent users from being able to customize the config properties used for the Kafka clients that\n            // access the offsets topic, and we would not be able to establish reasonable defaults like setting\n            // isolation.level=read_committed for the offsets topic consumer for this task\n            if (sameOffsetTopicAsWorker(sourceConfig.offsetsTopic(), producerProps)) {\n                return ConnectorOffsetBackingStore.withOnlyConnectorStore(\n                        () -> LoggingContext.forTask(id),\n                        connectorStore,\n                        connectorSpecificOffsetsTopic,\n                        topicAdmin\n                );\n            } else {\n                return ConnectorOffsetBackingStore.withConnectorAndWorkerStores(\n                        () -> LoggingContext.forTask(id),\n                        globalOffsetBackingStore,\n                        connectorStore,\n                        connectorSpecificOffsetsTopic,\n                        topicAdmin\n                );\n            }\n        } else {\n            return ConnectorOffsetBackingStore.withOnlyWorkerStore(\n                    () -> LoggingContext.forTask(id),\n                    globalOffsetBackingStore,\n                    config.offsetsTopic()\n            );\n        }\n    }\n\n    // Visible for testing\n    ConnectorOffsetBackingStore offsetStoreForExactlyOnceSourceTask(\n            ConnectorTaskId id,\n            SourceConnectorConfig sourceConfig,\n            Class<? extends Connector> connectorClass,\n            Producer<byte[], byte[]> producer,\n            Map<String, Object> producerProps,\n            TopicAdmin topicAdmin\n    ) {\n        Objects.requireNonNull(topicAdmin, \"Source tasks require a non-null topic admin when exactly-once support is enabled\");\n\n        Map<String, Object> consumerProps = exactlyOnceSourceOffsetsConsumerConfigs(\n                id.connector(), \"connector-consumer-\" + id, config, sourceConfig, connectorClass,\n                connectorClientConfigOverridePolicy, kafkaClusterId);\n        KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(consumerProps);\n\n        String connectorOffsetsTopic = Optional.ofNullable(sourceConfig.offsetsTopic()).orElse(config.offsetsTopic());\n\n        KafkaOffsetBackingStore connectorStore =\n                KafkaOffsetBackingStore.readWriteStore(connectorOffsetsTopic, producer, consumer, topicAdmin, internalKeyConverter);\n\n        // If the connector's offsets topic is the same as the worker-global offsets topic, there's no need to construct\n        // an offset store that has a primary and a secondary store which both read from that same topic.\n        // So, if the user has (implicitly or explicitly) configured the connector with a connector-specific offsets topic\n        // but we know that that topic is the same as the worker-global offsets topic, we ignore the worker-global\n        // offset store and build a store backed exclusively by a connector-specific offsets store.\n        // We cannot under any circumstances build an offset store backed exclusively by the worker-global offset store\n        // as that would prevent us from being able to write source records and source offset information for the task\n        // with the same producer, and therefore, in the same transaction.\n        if (sameOffsetTopicAsWorker(connectorOffsetsTopic, producerProps)) {\n            return ConnectorOffsetBackingStore.withOnlyConnectorStore(\n                    () -> LoggingContext.forTask(id),\n                    connectorStore,\n                    connectorOffsetsTopic,\n                    topicAdmin\n            );\n        } else {\n            return ConnectorOffsetBackingStore.withConnectorAndWorkerStores(\n                    () -> LoggingContext.forTask(id),\n                    globalOffsetBackingStore,\n                    connectorStore,\n                    connectorOffsetsTopic,\n                    topicAdmin\n            );\n        }\n    }\n\n    /**\n     * Gives a best-effort guess for whether the given offsets topic is the same topic as the worker-global offsets topic.\n     * Even if the name of the topic is the same as the name of the worker's offsets topic, the two may still be different topics\n     * if the connector is configured to produce to a different Kafka cluster than the one that hosts the worker's offsets topic.\n     * @param offsetsTopic the name of the offsets topic for the connector\n     * @param producerProps the producer configuration for the connector\n     * @return whether it appears that the connector's offsets topic is the same topic as the worker-global offsets topic.\n     * If {@code true}, it is guaranteed that the two are the same;\n     * if {@code false}, it is likely but not guaranteed that the two are not the same\n     */\n    private boolean sameOffsetTopicAsWorker(String offsetsTopic, Map<String, Object> producerProps) {\n        // We can check the offset topic name and the Kafka cluster's bootstrap servers,\n        // although this isn't exact and can lead to some false negatives if the user\n        // provides an overridden bootstrap servers value for their producer that is different than\n        // the worker's but still resolves to the same Kafka cluster used by the worker.\n        // At the moment this is probably adequate, especially since we don't want to put\n        // a network ping to a remote Kafka cluster inside the herder's tick thread (which is where this\n        // logic takes place right now) in case that takes a while.\n        Set<String> workerBootstrapServers = new HashSet<>(config.getList(BOOTSTRAP_SERVERS_CONFIG));\n        Set<String> producerBootstrapServers = new HashSet<>();\n        try {\n            String rawBootstrapServers = producerProps.getOrDefault(BOOTSTRAP_SERVERS_CONFIG, \"\").toString();\n            @SuppressWarnings(\"unchecked\")\n            List<String> parsedBootstrapServers = (List<String>) ConfigDef.parseType(BOOTSTRAP_SERVERS_CONFIG, rawBootstrapServers, ConfigDef.Type.LIST);\n            producerBootstrapServers.addAll(parsedBootstrapServers);\n        } catch (Exception e) {\n            // Should never happen by this point, but if it does, make sure to present a readable error message to the user\n            throw new ConnectException(\"Failed to parse bootstrap servers property in producer config\", e);\n        }\n        return offsetsTopic.equals(config.offsetsTopic())\n                && workerBootstrapServers.equals(producerBootstrapServers);\n    }\n\n    private boolean regularSourceTaskUsesConnectorSpecificOffsetsStore(SourceConnectorConfig sourceConfig) {\n        // We use a connector-specific store (i.e., a dedicated KafkaOffsetBackingStore for this task)\n        // if the worker supports per-connector offsets topics (which may be the case in distributed mode but not standalone, for example)\n        // and the user has explicitly specified an offsets topic for the connector\n        return sourceConfig.offsetsTopic() != null && config.connectorOffsetsTopicsPermitted();\n    }\n\n    private boolean sourceConnectorTopicCreationEnabled(SourceConnectorConfig sourceConfig) {\n        return config.topicCreationEnable() && sourceConfig.usesTopicCreation();\n    }\n\n    static class ConnectorStatusMetricsGroup {\n        private final ConnectMetrics connectMetrics;\n        private final ConnectMetricsRegistry registry;\n        private final ConcurrentMap<String, MetricGroup> connectorStatusMetrics = new ConcurrentHashMap<>();\n        private final Herder herder;\n        private final ConcurrentMap<ConnectorTaskId, WorkerTask<?, ?>> tasks;\n\n\n        protected ConnectorStatusMetricsGroup(\n                ConnectMetrics connectMetrics, ConcurrentMap<ConnectorTaskId, WorkerTask<?, ?>> tasks, Herder herder) {\n            this.connectMetrics = connectMetrics;\n            this.registry = connectMetrics.registry();\n            this.tasks = tasks;\n            this.herder = herder;\n        }\n\n        protected ConnectMetrics.LiteralSupplier<Long> taskCounter(String connName) {\n            return now -> tasks.keySet()\n                .stream()\n                .filter(taskId -> taskId.connector().equals(connName))\n                .count();\n        }\n\n        protected ConnectMetrics.LiteralSupplier<Long> taskStatusCounter(String connName, TaskStatus.State state) {\n            return now -> tasks.values()\n                .stream()\n                .filter(task ->\n                    task.id().connector().equals(connName) &&\n                    herder.taskStatus(task.id()).state().equalsIgnoreCase(state.toString()))\n                .count();\n        }\n\n        protected synchronized void recordTaskAdded(ConnectorTaskId connectorTaskId) {\n            if (connectorStatusMetrics.containsKey(connectorTaskId.connector())) {\n                return;\n            }\n\n            String connName = connectorTaskId.connector();\n\n            MetricGroup metricGroup = connectMetrics.group(registry.workerGroupName(),\n                registry.connectorTagName(), connName);\n\n            metricGroup.addValueMetric(registry.connectorTotalTaskCount, taskCounter(connName));\n            for (Map.Entry<MetricNameTemplate, TaskStatus.State> statusMetric : registry.connectorStatusMetrics\n                .entrySet()) {\n                metricGroup.addValueMetric(statusMetric.getKey(), taskStatusCounter(connName,\n                    statusMetric.getValue()));\n            }\n            connectorStatusMetrics.put(connectorTaskId.connector(), metricGroup);\n        }\n\n        protected synchronized void recordTaskRemoved(ConnectorTaskId connectorTaskId) {\n            // Unregister connector task count metric if we remove the last task of the connector\n            if (tasks.keySet().stream().noneMatch(id -> id.connector().equals(connectorTaskId.connector()))) {\n                connectorStatusMetrics.get(connectorTaskId.connector()).close();\n                connectorStatusMetrics.remove(connectorTaskId.connector());\n            }\n        }\n\n        protected synchronized void close() {\n            for (MetricGroup metricGroup: connectorStatusMetrics.values()) {\n                metricGroup.close();\n            }\n        }\n\n        protected MetricGroup metricGroup(String connectorId) {\n            return connectorStatusMetrics.get(connectorId);\n        }\n    }\n\n}",
                "methodCount": 97
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 7,
                "candidates": [
                    {
                        "lineStart": 2245,
                        "lineEnd": 2247,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method sourceConnectorTopicCreationEnabled to class WorkerConfig",
                        "description": "Move method sourceConnectorTopicCreationEnabled to org.apache.kafka.connect.runtime.WorkerConfig\nRationale: The method `sourceConnectorTopicCreationEnabled` primarily checks if topic creation is enabled by calling `config.topicCreationEnable()`, which is a method within the `WorkerConfig` class. Thus, it's logical to relocate the method to `WorkerConfig` to maintain encapsulation and reduce dependencies between classes. The functionality directly pertains to the configuration managed within `WorkerConfig`.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 415,
                        "lineEnd": 418,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method tasksMax to class ConnectorStatusMetricsGroup",
                        "description": "Move method tasksMax to org.apache.kafka.connect.runtime.Worker.ConnectorStatusMetricsGroup\nRationale: The tasksMax method queries the maximum number of tasks for a given connector configuration, which aligns closely with the responsibilities of the ConnectorStatusMetricsGroup class. This class is responsible for managing metrics related to connector status and tasks, including counting and recording task statuses. Moving the tasksMax method to this class will help consolidate task-related methods in a single class, improving maintainability and cohesion.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 948,
                        "lineEnd": 950,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method taskTransactionalId to class WorkerConfig",
                        "description": "Move method taskTransactionalId to org.apache.kafka.connect.runtime.WorkerConfig\nRationale: The method taskTransactionalId() relies on accessing the groupId() method from the same configuration context, which aligns with the responsibilities of WorkerConfig. This class already handles various properties and configurations needed for the Worker, making it a suitable location for this method. Placing taskTransactionalId() in WorkerConfig maintains cohesion by keeping related configuration logic together.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 961,
                        "lineEnd": 963,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method errorHandlingMetrics to class ConnectMetrics",
                        "description": "Move method errorHandlingMetrics to org.apache.kafka.connect.runtime.ConnectMetrics\nRationale: The method `errorHandlingMetrics` directly references both `ConnectorTaskId` and `metrics`, where `metrics` is an instance variable of `ConnectMetrics`. Moving this method to `ConnectMetrics` ensures that it has direct access to `metrics` without the need for additional dependencies or breaking encapsulation.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 2272,
                        "lineEnd": 2279,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method taskStatusCounter to class Herder",
                        "description": "Move method taskStatusCounter to org.apache.kafka.connect.runtime.Herder\nRationale: The method taskStatusCounter() directly interacts with instances that heavily rely on connector names and task statuses, which are managed by the Herder interface. Moving this method to the Herder class centralizes the logic related to task statuses and connectors, ensuring better cohesion and making Herder responsible for state tracking and management as its described purpose.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 2265,
                        "lineEnd": 2270,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method taskCounter to class Herder",
                        "description": "Move method taskCounter to org.apache.kafka.connect.runtime.Herder\nRationale: The method taskCounter(String connName) is responsible for counting the tasks associated with a particular connector by name. The Herder interface is designed to manage and monitor the state of workers and connectors, which includes tasks. The taskCounter method fits naturally into the Herder class as it provides metrics relevant to connector management, and it can benefit from the Herder's existing methods that interact with task and connector information.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 2314,
                        "lineEnd": 2316,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method metricGroup to class Herder",
                        "description": "Move method metricGroup to org.apache.kafka.connect.runtime.Herder\nRationale: The metricGroup() method is closely related to obtaining metrics for connectors, which plays a crucial role in managing connector statuses and their lifecycle. The Herder interface is responsible for tracking and managing workers and connectors, and is inherently involved in overseeing connector statuses and their metrics. Moving the metricGroup() method to the Herder class consolidates all connector-related status tracking and management within a single cohesive interface, thus enhancing code clarity and cohesion.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "initConfigTransformer",
                            "method_signature": "private initConfigTransformer()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "start",
                            "method_signature": "public start()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stop",
                            "method_signature": "public stop()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "startConnector",
                            "method_signature": "public startConnector(\n            String connName,\n            Map<String, String> connProps,\n            CloseableConnectorContext ctx,\n            ConnectorStatus.Listener statusListener,\n            TargetState initialState,\n            Callback<TargetState> onConnectorStateChange\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isSinkConnector",
                            "method_signature": "public isSinkConnector(String connName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "connectorTaskConfigs",
                            "method_signature": "public connectorTaskConfigs(String connName, ConnectorConfig connConfig)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "tasksMax",
                            "method_signature": "private tasksMax(ConnectorConfig connConfig)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stopConnector",
                            "method_signature": "private stopConnector(String connName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stopConnectors",
                            "method_signature": "private stopConnectors(Collection<String> ids)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitStopConnector",
                            "method_signature": "private awaitStopConnector(String connName, long timeout)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitStopConnectors",
                            "method_signature": "private awaitStopConnectors(Collection<String> ids)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stopAndAwaitConnectors",
                            "method_signature": "public stopAndAwaitConnectors()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stopAndAwaitConnectors",
                            "method_signature": "public stopAndAwaitConnectors(Collection<String> ids)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stopAndAwaitConnector",
                            "method_signature": "public stopAndAwaitConnector(String connName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "connectorNames",
                            "method_signature": "public connectorNames()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isRunning",
                            "method_signature": "public isRunning(String connName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "startSinkTask",
                            "method_signature": "public startSinkTask(\n            ConnectorTaskId id,\n            ClusterConfigState configState,\n            Map<String, String> connProps,\n            Map<String, String> taskProps,\n            TaskStatus.Listener statusListener,\n            TargetState initialState\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "startSourceTask",
                            "method_signature": "public startSourceTask(\n            ConnectorTaskId id,\n            ClusterConfigState configState,\n            Map<String, String> connProps,\n            Map<String, String> taskProps,\n            TaskStatus.Listener statusListener,\n            TargetState initialState\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "startExactlyOnceSourceTask",
                            "method_signature": "public startExactlyOnceSourceTask(\n            ConnectorTaskId id,\n            ClusterConfigState configState,\n            Map<String, String> connProps,\n            Map<String, String> taskProps,\n            TaskStatus.Listener statusListener,\n            TargetState initialState,\n            Runnable preProducerCheck,\n            Runnable postProducerCheck\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "startTask",
                            "method_signature": "private startTask(\n            ConnectorTaskId id,\n            Map<String, String> connProps,\n            Map<String, String> taskProps,\n            TaskStatus.Listener statusListener,\n            TaskBuilder<?, ?> taskBuilder\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fenceZombies",
                            "method_signature": "public fenceZombies(String connName, int numTasks, Map<String, String> connProps)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "exactlyOnceSourceTaskProducerConfigs",
                            "method_signature": "static exactlyOnceSourceTaskProducerConfigs(ConnectorTaskId id,\n                                                              WorkerConfig config,\n                                                              ConnectorConfig connConfig,\n                                                              Class<? extends Connector>  connectorClass,\n                                                              ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                                              String clusterId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "baseProducerConfigs",
                            "method_signature": "static baseProducerConfigs(String connName,\n                                               String defaultClientId,\n                                               WorkerConfig config,\n                                               ConnectorConfig connConfig,\n                                               Class<? extends Connector>  connectorClass,\n                                               ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                               String clusterId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "exactlyOnceSourceOffsetsConsumerConfigs",
                            "method_signature": "static exactlyOnceSourceOffsetsConsumerConfigs(String connName,\n                                                                       String defaultClientId,\n                                                                       WorkerConfig config,\n                                                                       ConnectorConfig connConfig,\n                                                                       Class<? extends Connector> connectorClass,\n                                                                       ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                                                       String clusterId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "regularSourceOffsetsConsumerConfigs",
                            "method_signature": "static regularSourceOffsetsConsumerConfigs(String connName,\n                                                                   String defaultClientId,\n                                                                   WorkerConfig config,\n                                                                   ConnectorConfig connConfig,\n                                                                   Class<? extends Connector> connectorClass,\n                                                                   ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                                                   String clusterId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "baseConsumerConfigs",
                            "method_signature": "static baseConsumerConfigs(String connName,\n                                               String defaultClientId,\n                                               WorkerConfig config,\n                                               ConnectorConfig connConfig,\n                                               Class<? extends Connector> connectorClass,\n                                               ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                               String clusterId,\n                                               ConnectorType connectorType)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "adminConfigs",
                            "method_signature": "static adminConfigs(String connName,\n                                            String defaultClientId,\n                                            WorkerConfig config,\n                                            ConnectorConfig connConfig,\n                                            Class<? extends Connector> connectorClass,\n                                            ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                            String clusterId,\n                                            ConnectorType connectorType)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "connectorClientConfigOverrides",
                            "method_signature": "private static connectorClientConfigOverrides(String connName,\n                                                                      ConnectorConfig connConfig,\n                                                                      Class<? extends Connector> connectorClass,\n                                                                      String clientConfigPrefix,\n                                                                      ConnectorType connectorType,\n                                                                      ConnectorClientConfigRequest.ClientType clientType,\n                                                                      ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "taskTransactionalId",
                            "method_signature": "private taskTransactionalId(ConnectorTaskId id)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "taskTransactionalId",
                            "method_signature": "public static taskTransactionalId(String groupId, String connector, int taskId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "errorHandlingMetrics",
                            "method_signature": " errorHandlingMetrics(ConnectorTaskId id)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sinkTaskReporters",
                            "method_signature": "private sinkTaskReporters(ConnectorTaskId id, SinkConnectorConfig connConfig,\n                                                     ErrorHandlingMetrics errorHandlingMetrics,\n                                                     Class<? extends Connector> connectorClass)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sourceTaskReporters",
                            "method_signature": "private sourceTaskReporters(ConnectorTaskId id, ConnectorConfig connConfig,\n                                                       ErrorHandlingMetrics errorHandlingMetrics)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createWorkerErrantRecordReporter",
                            "method_signature": "private createWorkerErrantRecordReporter(\n        SinkConnectorConfig connConfig,\n        RetryWithToleranceOperator<ConsumerRecord<byte[], byte[]>> retryWithToleranceOperator,\n        Converter keyConverter,\n        Converter valueConverter,\n        HeaderConverter headerConverter\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stopTask",
                            "method_signature": "private stopTask(ConnectorTaskId taskId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stopTasks",
                            "method_signature": "private stopTasks(Collection<ConnectorTaskId> ids)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitStopTask",
                            "method_signature": "private awaitStopTask(ConnectorTaskId taskId, long timeout)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitStopTasks",
                            "method_signature": "private awaitStopTasks(Collection<ConnectorTaskId> ids)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stopAndAwaitTasks",
                            "method_signature": "public stopAndAwaitTasks()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stopAndAwaitTasks",
                            "method_signature": "public stopAndAwaitTasks(Collection<ConnectorTaskId> ids)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stopAndAwaitTask",
                            "method_signature": "public stopAndAwaitTask(ConnectorTaskId taskId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "taskIds",
                            "method_signature": "public taskIds()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isTopicCreationEnabled",
                            "method_signature": "public isTopicCreationEnabled()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "setTargetState",
                            "method_signature": "public setTargetState(String connName, TargetState state, Callback<TargetState> stateChangeCallback)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "connectorOffsets",
                            "method_signature": "public connectorOffsets(String connName, Map<String, String> connectorConfig, Callback<ConnectorOffsets> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sinkConnectorOffsets",
                            "method_signature": " sinkConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                              Callback<ConnectorOffsets> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sourceConnectorOffsets",
                            "method_signature": "private sourceConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                                        Callback<ConnectorOffsets> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sourceConnectorOffsets",
                            "method_signature": " sourceConnectorOffsets(String connName, ConnectorOffsetBackingStore offsetStore,\n                                CloseableOffsetStorageReader offsetReader, Callback<ConnectorOffsets> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "modifyConnectorOffsets",
                            "method_signature": "public modifyConnectorOffsets(String connName, Map<String, String> connectorConfig,\n                                      Map<Map<String, ?>, Map<String, ?>> offsets, Callback<Message> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "modifySinkConnectorOffsets",
                            "method_signature": " modifySinkConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                                    Map<Map<String, ?>, Map<String, ?>> offsets, ClassLoader connectorLoader, Callback<Message> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "alterSinkConnectorOffsets",
                            "method_signature": "private alterSinkConnectorOffsets(String connName, String groupId, Admin admin, Map<TopicPartition, Long> offsetsToWrite,\n                                           Callback<Message> cb, boolean alterOffsetsResult, Timer timer)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resetSinkConnectorOffsets",
                            "method_signature": "private resetSinkConnectorOffsets(String connName, String groupId, Admin admin, Callback<Message> cb, boolean alterOffsetsResult, Timer timer)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "modifySourceConnectorOffsets",
                            "method_signature": "private modifySourceConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                                              Map<Map<String, ?>, Map<String, ?>> offsets, ClassLoader connectorLoader, Callback<Message> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "modifySourceConnectorOffsets",
                            "method_signature": " modifySourceConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                                      Map<Map<String, ?>, Map<String, ?>> offsets, ConnectorOffsetBackingStore offsetStore,\n                                      KafkaProducer<byte[], byte[]> producer, OffsetStorageWriter offsetWriter,\n                                      ClassLoader connectorLoader, Callback<Message> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "normalizeSourceConnectorOffsets",
                            "method_signature": "@SuppressWarnings(\"unchecked\") normalizeSourceConnectorOffsets(Map<Map<String, ?>, Map<String, ?>> originalOffsets)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateTimerAndCheckExpiry",
                            "method_signature": "private updateTimerAndCheckExpiry(Timer timer, String errorMessageIfExpired)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeModifyOffsetsCallback",
                            "method_signature": "private completeModifyOffsetsCallback(boolean alterOffsetsResult, boolean isReset, Callback<Message> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withTask",
                            "method_signature": "public withTask(Task task)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withConnectorConfig",
                            "method_signature": "public withConnectorConfig(ConnectorConfig connectorConfig)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withKeyConverter",
                            "method_signature": "public withKeyConverter(Converter keyConverter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withValueConverter",
                            "method_signature": "public withValueConverter(Converter valueConverter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withHeaderConverter",
                            "method_signature": "public withHeaderConverter(HeaderConverter headerConverter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withClassloader",
                            "method_signature": "public withClassloader(ClassLoader classLoader)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "offsetStoreForRegularSourceConnector",
                            "method_signature": " offsetStoreForRegularSourceConnector(\n            SourceConnectorConfig sourceConfig,\n            String connName,\n            Connector connector,\n            Producer<byte[], byte[]> producer\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "offsetStoreForExactlyOnceSourceConnector",
                            "method_signature": " offsetStoreForExactlyOnceSourceConnector(\n            SourceConnectorConfig sourceConfig,\n            String connName,\n            Connector connector,\n            Producer<byte[], byte[]> producer\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "offsetStoreForRegularSourceTask",
                            "method_signature": " offsetStoreForRegularSourceTask(\n            ConnectorTaskId id,\n            SourceConnectorConfig sourceConfig,\n            Class<? extends Connector> connectorClass,\n            Producer<byte[], byte[]> producer,\n            Map<String, Object> producerProps,\n            TopicAdmin topicAdmin\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "offsetStoreForExactlyOnceSourceTask",
                            "method_signature": " offsetStoreForExactlyOnceSourceTask(\n            ConnectorTaskId id,\n            SourceConnectorConfig sourceConfig,\n            Class<? extends Connector> connectorClass,\n            Producer<byte[], byte[]> producer,\n            Map<String, Object> producerProps,\n            TopicAdmin topicAdmin\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sameOffsetTopicAsWorker",
                            "method_signature": "private sameOffsetTopicAsWorker(String offsetsTopic, Map<String, Object> producerProps)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "regularSourceTaskUsesConnectorSpecificOffsetsStore",
                            "method_signature": "private regularSourceTaskUsesConnectorSpecificOffsetsStore(SourceConnectorConfig sourceConfig)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sourceConnectorTopicCreationEnabled",
                            "method_signature": "private sourceConnectorTopicCreationEnabled(SourceConnectorConfig sourceConfig)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "taskCounter",
                            "method_signature": "protected taskCounter(String connName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "taskStatusCounter",
                            "method_signature": "protected taskStatusCounter(String connName, TaskStatus.State state)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordTaskAdded",
                            "method_signature": "protected synchronized recordTaskAdded(ConnectorTaskId connectorTaskId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordTaskRemoved",
                            "method_signature": "protected synchronized recordTaskRemoved(ConnectorTaskId connectorTaskId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "protected synchronized close()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "metricGroup",
                            "method_signature": "protected metricGroup(String connectorId)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "taskCounter",
                            "method_signature": "protected taskCounter(String connName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "taskStatusCounter",
                            "method_signature": "protected taskStatusCounter(String connName, TaskStatus.State state)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "metricGroup",
                            "method_signature": "protected metricGroup(String connectorId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sourceConnectorTopicCreationEnabled",
                            "method_signature": "private sourceConnectorTopicCreationEnabled(SourceConnectorConfig sourceConfig)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "taskTransactionalId",
                            "method_signature": "private taskTransactionalId(ConnectorTaskId id)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "errorHandlingMetrics",
                            "method_signature": " errorHandlingMetrics(ConnectorTaskId id)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "exactlyOnceSourceOffsetsConsumerConfigs",
                            "method_signature": "static exactlyOnceSourceOffsetsConsumerConfigs(String connName,\n                                                                       String defaultClientId,\n                                                                       WorkerConfig config,\n                                                                       ConnectorConfig connConfig,\n                                                                       Class<? extends Connector> connectorClass,\n                                                                       ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                                                       String clusterId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "tasksMax",
                            "method_signature": "private tasksMax(ConnectorConfig connConfig)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sourceTaskReporters",
                            "method_signature": "private sourceTaskReporters(ConnectorTaskId id, ConnectorConfig connConfig,\n                                                       ErrorHandlingMetrics errorHandlingMetrics)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withConnectorConfig",
                            "method_signature": "public withConnectorConfig(ConnectorConfig connectorConfig)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withKeyConverter",
                            "method_signature": "public withKeyConverter(Converter keyConverter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withValueConverter",
                            "method_signature": "public withValueConverter(Converter valueConverter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withHeaderConverter",
                            "method_signature": "public withHeaderConverter(HeaderConverter headerConverter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withClassloader",
                            "method_signature": "public withClassloader(ClassLoader classLoader)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "taskCounter",
                            "method_signature": "protected taskCounter(String connName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "taskStatusCounter",
                            "method_signature": "protected taskStatusCounter(String connName, TaskStatus.State state)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "metricGroup",
                            "method_signature": "protected metricGroup(String connectorId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sourceConnectorTopicCreationEnabled",
                            "method_signature": "private sourceConnectorTopicCreationEnabled(SourceConnectorConfig sourceConfig)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "taskTransactionalId",
                            "method_signature": "private taskTransactionalId(ConnectorTaskId id)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "errorHandlingMetrics",
                            "method_signature": " errorHandlingMetrics(ConnectorTaskId id)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "exactlyOnceSourceOffsetsConsumerConfigs",
                            "method_signature": "static exactlyOnceSourceOffsetsConsumerConfigs(String connName,\n                                                                       String defaultClientId,\n                                                                       WorkerConfig config,\n                                                                       ConnectorConfig connConfig,\n                                                                       Class<? extends Connector> connectorClass,\n                                                                       ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                                                       String clusterId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "tasksMax",
                            "method_signature": "private tasksMax(ConnectorConfig connConfig)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sourceTaskReporters",
                            "method_signature": "private sourceTaskReporters(ConnectorTaskId id, ConnectorConfig connConfig,\n                                                       ErrorHandlingMetrics errorHandlingMetrics)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withConnectorConfig",
                            "method_signature": "public withConnectorConfig(ConnectorConfig connectorConfig)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withKeyConverter",
                            "method_signature": "public withKeyConverter(Converter keyConverter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withValueConverter",
                            "method_signature": "public withValueConverter(Converter valueConverter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withHeaderConverter",
                            "method_signature": "public withHeaderConverter(HeaderConverter headerConverter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withClassloader",
                            "method_signature": "public withClassloader(ClassLoader classLoader)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "protected taskCounter(String connName)": {
                    "first": {
                        "method_name": "taskCounter",
                        "method_signature": "protected taskCounter(String connName)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.19003285668326841
                },
                "protected taskStatusCounter(String connName, TaskStatus.State state)": {
                    "first": {
                        "method_name": "taskStatusCounter",
                        "method_signature": "protected taskStatusCounter(String connName, TaskStatus.State state)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.19381993703809114
                },
                "public build()": {
                    "first": {
                        "method_name": "build",
                        "method_signature": "public build()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2283753052143277
                },
                "protected metricGroup(String connectorId)": {
                    "first": {
                        "method_name": "metricGroup",
                        "method_signature": "protected metricGroup(String connectorId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2395856894817648
                },
                "private sourceConnectorTopicCreationEnabled(SourceConnectorConfig sourceConfig)": {
                    "first": {
                        "method_name": "sourceConnectorTopicCreationEnabled",
                        "method_signature": "private sourceConnectorTopicCreationEnabled(SourceConnectorConfig sourceConfig)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.24543519642527495
                },
                "private taskTransactionalId(ConnectorTaskId id)": {
                    "first": {
                        "method_name": "taskTransactionalId",
                        "method_signature": "private taskTransactionalId(ConnectorTaskId id)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.25516890474978476
                },
                " errorHandlingMetrics(ConnectorTaskId id)": {
                    "first": {
                        "method_name": "errorHandlingMetrics",
                        "method_signature": " errorHandlingMetrics(ConnectorTaskId id)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.26404643555017665
                },
                "static exactlyOnceSourceOffsetsConsumerConfigs(String connName,\n                                                                       String defaultClientId,\n                                                                       WorkerConfig config,\n                                                                       ConnectorConfig connConfig,\n                                                                       Class<? extends Connector> connectorClass,\n                                                                       ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                                                       String clusterId)": {
                    "first": {
                        "method_name": "exactlyOnceSourceOffsetsConsumerConfigs",
                        "method_signature": "static exactlyOnceSourceOffsetsConsumerConfigs(String connName,\n                                                                       String defaultClientId,\n                                                                       WorkerConfig config,\n                                                                       ConnectorConfig connConfig,\n                                                                       Class<? extends Connector> connectorClass,\n                                                                       ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                                                       String clusterId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.27724909049258634
                },
                "private tasksMax(ConnectorConfig connConfig)": {
                    "first": {
                        "method_name": "tasksMax",
                        "method_signature": "private tasksMax(ConnectorConfig connConfig)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2884071803738167
                },
                "private sourceTaskReporters(ConnectorTaskId id, ConnectorConfig connConfig,\n                                                       ErrorHandlingMetrics errorHandlingMetrics)": {
                    "first": {
                        "method_name": "sourceTaskReporters",
                        "method_signature": "private sourceTaskReporters(ConnectorTaskId id, ConnectorConfig connConfig,\n                                                       ErrorHandlingMetrics errorHandlingMetrics)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.30699269596265
                },
                "public withConnectorConfig(ConnectorConfig connectorConfig)": {
                    "first": {
                        "method_name": "withConnectorConfig",
                        "method_signature": "public withConnectorConfig(ConnectorConfig connectorConfig)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.31080353556995544
                },
                "public withKeyConverter(Converter keyConverter)": {
                    "first": {
                        "method_name": "withKeyConverter",
                        "method_signature": "public withKeyConverter(Converter keyConverter)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.31080353556995544
                },
                "public withValueConverter(Converter valueConverter)": {
                    "first": {
                        "method_name": "withValueConverter",
                        "method_signature": "public withValueConverter(Converter valueConverter)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.31080353556995544
                },
                "public withHeaderConverter(HeaderConverter headerConverter)": {
                    "first": {
                        "method_name": "withHeaderConverter",
                        "method_signature": "public withHeaderConverter(HeaderConverter headerConverter)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.31080353556995544
                },
                "public withClassloader(ClassLoader classLoader)": {
                    "first": {
                        "method_name": "withClassloader",
                        "method_signature": "public withClassloader(ClassLoader classLoader)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.31080353556995544
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "private sourceConnectorTopicCreationEnabled(SourceConnectorConfig sourceConfig)",
                    "private tasksMax(ConnectorConfig connConfig)",
                    "private taskTransactionalId(ConnectorTaskId id)",
                    " errorHandlingMetrics(ConnectorTaskId id)",
                    "protected taskStatusCounter(String connName, TaskStatus.State state)",
                    "protected taskCounter(String connName)",
                    "protected metricGroup(String connectorId)",
                    "public withConnectorConfig(ConnectorConfig connectorConfig)",
                    "public withKeyConverter(Converter keyConverter)",
                    "public withValueConverter(Converter valueConverter)",
                    "public withHeaderConverter(HeaderConverter headerConverter)",
                    "public withClassloader(ClassLoader classLoader)",
                    "public build()"
                ],
                "llm_response_time": 6972
            },
            "targetClassMap": {
                "sourceConnectorTopicCreationEnabled": {
                    "target_classes": [
                        {
                            "class_name": "Herder",
                            "similarity_score": 0.03791780158451814
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.12016297604277221
                        },
                        {
                            "class_name": "ConnectorStatusMetricsGroup",
                            "similarity_score": 0.40931462414438785
                        },
                        {
                            "class_name": "WorkerConfig",
                            "similarity_score": 0.14579712273792464
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.04941997354933901
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.04941997354933901
                        },
                        {
                            "class_name": "OffsetBackingStore",
                            "similarity_score": 0.016574838603294898
                        },
                        {
                            "class_name": "ConnectorClientConfigOverridePolicy",
                            "similarity_score": 0.022689215826037595
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "WorkerConfig",
                        "ConnectorStatusMetricsGroup",
                        "Time"
                    ],
                    "llm_response_time": 5012,
                    "similarity_computation_time": 4,
                    "similarity_metric": "cosine"
                },
                "tasksMax": {
                    "target_classes": [
                        {
                            "class_name": "ConnectorConfig",
                            "similarity_score": 0.33106591951688563
                        },
                        {
                            "class_name": "Herder",
                            "similarity_score": 0.029299557795755153
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.12463721647779649
                        },
                        {
                            "class_name": "Plugins",
                            "similarity_score": 0.3434051313734477
                        },
                        {
                            "class_name": "ConnectMetrics",
                            "similarity_score": 0.2128322068442647
                        },
                        {
                            "class_name": "WorkerMetricsGroup",
                            "similarity_score": 0.40962283318350806
                        },
                        {
                            "class_name": "ConnectorStatusMetricsGroup",
                            "similarity_score": 0.40883108632154813
                        },
                        {
                            "class_name": "WorkerConfig",
                            "similarity_score": 0.18807424735078157
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.03828687583764507
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.03828687583764507
                        },
                        {
                            "class_name": "OffsetBackingStore",
                            "similarity_score": 0.014008295367013146
                        },
                        {
                            "class_name": "WorkerConfigTransformer",
                            "similarity_score": 0.4161082220485403
                        },
                        {
                            "class_name": "ConnectorClientConfigOverridePolicy",
                            "similarity_score": 0.019175887291829265
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "ConnectorStatusMetricsGroup",
                        "WorkerConfigTransformer",
                        "WorkerMetricsGroup"
                    ],
                    "llm_response_time": 3183,
                    "similarity_computation_time": 7,
                    "similarity_metric": "cosine"
                },
                "taskTransactionalId": {
                    "target_classes": [
                        {
                            "class_name": "Herder",
                            "similarity_score": 0.0335843385462875
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.12016297604277221
                        },
                        {
                            "class_name": "ConnectorStatusMetricsGroup",
                            "similarity_score": 0.40931462414438785
                        },
                        {
                            "class_name": "WorkerConfig",
                            "similarity_score": 0.21792833082931892
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.045301642420227425
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.045301642420227425
                        },
                        {
                            "class_name": "OffsetBackingStore",
                            "similarity_score": 0.016574838603294898
                        },
                        {
                            "class_name": "ConnectorClientConfigOverridePolicy",
                            "similarity_score": 0.022689215826037595
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "WorkerConfig",
                        "ConnectorStatusMetricsGroup",
                        "Time"
                    ],
                    "llm_response_time": 4003,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                },
                "errorHandlingMetrics": {
                    "target_classes": [
                        {
                            "class_name": "Herder",
                            "similarity_score": 0.03425903339240541
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.1439351116361389
                        },
                        {
                            "class_name": "ConnectMetrics",
                            "similarity_score": 0.1990862997205606
                        },
                        {
                            "class_name": "ConnectorStatusMetricsGroup",
                            "similarity_score": 0.36608345225794353
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.052093226107062324
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.052093226107062324
                        },
                        {
                            "class_name": "OffsetBackingStore",
                            "similarity_score": 0.017471413945365305
                        },
                        {
                            "class_name": "ConnectorClientConfigOverridePolicy",
                            "similarity_score": 0.023916533444472508
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "ConnectMetrics",
                        "ConnectorStatusMetricsGroup",
                        "Time"
                    ],
                    "llm_response_time": 3971,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                },
                "taskStatusCounter": {
                    "target_classes": [
                        {
                            "class_name": "Herder",
                            "similarity_score": 0.030651329352501994
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Herder"
                    ],
                    "llm_response_time": 1747,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "taskCounter": {
                    "target_classes": [
                        {
                            "class_name": "Herder",
                            "similarity_score": 0.020772590192419116
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Herder"
                    ],
                    "llm_response_time": 2281,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "metricGroup": {
                    "target_classes": [
                        {
                            "class_name": "Herder",
                            "similarity_score": 0.03028099353583279
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Herder"
                    ],
                    "llm_response_time": 2170,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "withConnectorConfig": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 4738,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withKeyConverter": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 1943,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withValueConverter": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2942,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withHeaderConverter": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 1883,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withClassloader": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3554,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "build": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 4955,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "d24abe0edebad37e554adea47408c3063037f744",
        "url": "https://github.com/apache/kafka/commit/d24abe0edebad37e554adea47408c3063037f744",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public assertNoOrEmptyResult(timeouts List<MockCoordinatorTimer.ExpiredTimeout<Void,Record>>) : void extracted from public testLastJoiningMembersAreKickedOutWhenRejoiningGroupWithMaxSize() : void in class org.apache.kafka.coordinator.group.GroupMetadataManagerTest & moved to class org.apache.kafka.coordinator.group.GroupMetadataManagerTestContext",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTest.java",
                    "startLine": 5158,
                    "endLine": 5217,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testLastJoiningMembersAreKickedOutWhenRejoiningGroupWithMaxSize() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTest.java",
                    "startLine": 10901,
                    "endLine": 10901,
                    "startColumn": 9,
                    "endColumn": 42,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTest.java",
                    "startLine": 10902,
                    "endLine": 10902,
                    "startColumn": 9,
                    "endColumn": 81,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTest.java",
                    "startLine": 10902,
                    "endLine": 10902,
                    "startColumn": 37,
                    "endColumn": 79,
                    "codeElementType": "LAMBDA_EXPRESSION_BODY",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 106,
                    "endLine": 109,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public assertNoOrEmptyResult(timeouts List<MockCoordinatorTimer.ExpiredTimeout<Void,Record>>) : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 107,
                    "endLine": 107,
                    "startColumn": 9,
                    "endColumn": 42,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 108,
                    "endLine": 108,
                    "startColumn": 9,
                    "endColumn": 81,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 108,
                    "endLine": 108,
                    "startColumn": 37,
                    "endColumn": 79,
                    "codeElementType": "LAMBDA_EXPRESSION_BODY",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTest.java",
                    "startLine": 4040,
                    "endLine": 4097,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testLastJoiningMembersAreKickedOutWhenRejoiningGroupWithMaxSize() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTest.java",
                    "startLine": 4085,
                    "endLine": 4085,
                    "startColumn": 9,
                    "endColumn": 84,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "GroupMetadataManagerTestContext.assertNoOrEmptyResult(context.sleep(10000))"
                }
            ],
            "isStatic": true
        },
        "ref_id": 573,
        "extraction_results": {
            "success": true,
            "newCommitHash": "d67ed1ff166294681b2b562d9437a2ba61f11db9",
            "newBranchName": "extract-assertNoOrEmptyResult-testLastJoiningMembersAreKickedOutWhenRejoiningGroupWithMaxSize-be6653c"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "7dbdc15c668dfb5a4a91c79f339c22fb7178c368",
        "url": "https://github.com/apache/kafka/commit/7dbdc15c668dfb5a4a91c79f339c22fb7178c368",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public maybeCheckpoint() : void extracted from package pollAndUpdate() : void in class org.apache.kafka.streams.processor.internals.GlobalStreamThread.StateConsumer & moved to class org.apache.kafka.streams.processor.internals.GlobalStateUpdateTask",
            "leftSideLocations": [
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                    "startLine": 265,
                    "endLine": 275,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package pollAndUpdate() : void"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                    "startLine": 270,
                    "endLine": 270,
                    "startColumn": 13,
                    "endColumn": 50,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                    "startLine": 273,
                    "endLine": 273,
                    "startColumn": 17,
                    "endColumn": 33,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                    "startLine": 272,
                    "endLine": 272,
                    "startColumn": 17,
                    "endColumn": 46,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                    "startLine": 271,
                    "endLine": 274,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                    "startLine": 271,
                    "endLine": 271,
                    "startColumn": 17,
                    "endColumn": 49,
                    "codeElementType": "INFIX_EXPRESSION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                    "startLine": 271,
                    "endLine": 274,
                    "startColumn": 51,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateUpdateTask.java",
                    "startLine": 163,
                    "endLine": 170,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public maybeCheckpoint() : void"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateUpdateTask.java",
                    "startLine": 165,
                    "endLine": 165,
                    "startColumn": 9,
                    "endColumn": 46,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateUpdateTask.java",
                    "startLine": 168,
                    "endLine": 168,
                    "startColumn": 13,
                    "endColumn": 29,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateUpdateTask.java",
                    "startLine": 167,
                    "endLine": 167,
                    "startColumn": 13,
                    "endColumn": 26,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateUpdateTask.java",
                    "startLine": 166,
                    "endLine": 169,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateUpdateTask.java",
                    "startLine": 166,
                    "endLine": 166,
                    "startColumn": 13,
                    "endColumn": 45,
                    "codeElementType": "INFIX_EXPRESSION",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateUpdateTask.java",
                    "startLine": 166,
                    "endLine": 169,
                    "startColumn": 129,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                    "startLine": 256,
                    "endLine": 262,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package pollAndUpdate() : void"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                    "startLine": 261,
                    "endLine": 261,
                    "startColumn": 13,
                    "endColumn": 46,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "stateMaintainer.maybeCheckpoint()"
                }
            ],
            "isStatic": false
        },
        "ref_id": 574,
        "extraction_results": {
            "success": true,
            "newCommitHash": "1d75be12be5762c23b7a31c0b37559f1ba8a719b",
            "newBranchName": "extract-maybeCheckpoint-pollAndUpdate-2c0cab3"
        },
        "telemetry": {
            "id": "2265b31c-a50c-4882-8229-4197cccca9c0",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 460,
                "lineStart": 54,
                "lineEnd": 513,
                "bodyLineStart": 54,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                "sourceCode": "/**\n * This is the thread responsible for keeping all Global State Stores updated.\n * It delegates most of the responsibility to the internal class StateConsumer\n */\npublic class GlobalStreamThread extends Thread {\n\n    private final Logger log;\n    private final LogContext logContext;\n    private final StreamsConfig config;\n    private final Consumer<byte[], byte[]> globalConsumer;\n    private final StateDirectory stateDirectory;\n    private final Time time;\n    private final ThreadCache cache;\n    private final StreamsMetricsImpl streamsMetrics;\n    private final ProcessorTopology topology;\n    private final AtomicLong cacheSize;\n    private volatile StreamsException startupException;\n    private java.util.function.Consumer<Throwable> streamsUncaughtExceptionHandler;\n    private volatile long fetchDeadlineClientInstanceId = -1;\n    private volatile KafkaFutureImpl<Uuid> clientInstanceIdFuture = new KafkaFutureImpl<>();\n\n    /**\n     * The states that the global stream thread can be in\n     *\n     * <pre>\n     *                +-------------+\n     *          +<--- | Created (0) |\n     *          |     +-----+-------+\n     *          |           |\n     *          |           v\n     *          |     +-----+-------+\n     *          +<--- | Running (1) |\n     *          |     +-----+-------+\n     *          |           |\n     *          |           v\n     *          |     +-----+-------+\n     *          +---> | Pending     |\n     *                | Shutdown (2)|\n     *                +-----+-------+\n     *                      |\n     *                      v\n     *                +-----+-------+\n     *                | Dead (3)    |\n     *                +-------------+\n     * </pre>\n     *\n     * Note the following:\n     * <ul>\n     *     <li>Any state can go to PENDING_SHUTDOWN. That is because streams can be closed at any time.</li>\n     *     <li>State PENDING_SHUTDOWN may want to transit itself. In this case we will forbid the transition but will not treat as an error.</li>\n     * </ul>\n     */\n    public enum State implements ThreadStateTransitionValidator {\n        CREATED(1, 2), RUNNING(2), PENDING_SHUTDOWN(3), DEAD;\n\n        private final Set<Integer> validTransitions = new HashSet<>();\n\n        State(final Integer... validTransitions) {\n            this.validTransitions.addAll(Arrays.asList(validTransitions));\n        }\n\n        public boolean isRunning() {\n            return equals(RUNNING);\n        }\n\n        public boolean inErrorState() {\n            return equals(DEAD) || equals(PENDING_SHUTDOWN);\n        }\n\n        @Override\n        public boolean isValidTransition(final ThreadStateTransitionValidator newState) {\n            final State tmpState = (State) newState;\n            return validTransitions.contains(tmpState.ordinal());\n        }\n    }\n\n    private volatile State state = State.CREATED;\n    private final Object stateLock = new Object();\n    private StreamThread.StateListener stateListener = null;\n    private final String logPrefix;\n    private final StateRestoreListener stateRestoreListener;\n\n    /**\n     * Set the {@link StreamThread.StateListener} to be notified when state changes. Note this API is internal to\n     * Kafka Streams and is not intended to be used by an external application.\n     */\n    public void setStateListener(final StreamThread.StateListener listener) {\n        stateListener = listener;\n    }\n\n    /**\n     * @return The state this instance is in\n     */\n    public State state() {\n        // we do not need to use the stat lock since the variable is volatile\n        return state;\n    }\n\n    /**\n     * Sets the state\n     *\n     * @param newState New state\n     */\n    private void setState(final State newState) {\n        final State oldState = state;\n\n        synchronized (stateLock) {\n            if (state == State.PENDING_SHUTDOWN && newState == State.PENDING_SHUTDOWN) {\n                // when the state is already in PENDING_SHUTDOWN, its transition to itself\n                // will be refused but we do not throw exception here\n                return;\n            } else if (state == State.DEAD) {\n                // when the state is already in NOT_RUNNING, all its transitions\n                // will be refused but we do not throw exception here\n                return;\n            } else if (!state.isValidTransition(newState)) {\n                log.error(\"Unexpected state transition from {} to {}\", oldState, newState);\n                throw new StreamsException(logPrefix + \"Unexpected state transition from \" + oldState + \" to \" + newState);\n            } else {\n                log.info(\"State transition from {} to {}\", oldState, newState);\n            }\n\n            state = newState;\n        }\n\n        if (stateListener != null) {\n            stateListener.onChange(this, state, oldState);\n        }\n    }\n\n    public boolean stillRunning() {\n        synchronized (stateLock) {\n            return state.isRunning();\n        }\n    }\n\n    public boolean inErrorState() {\n        synchronized (stateLock) {\n            return state.inErrorState();\n        }\n    }\n\n    public boolean stillInitializing() {\n        synchronized (stateLock) {\n            return state.equals(CREATED);\n        }\n    }\n\n    public GlobalStreamThread(final ProcessorTopology topology,\n                              final StreamsConfig config,\n                              final Consumer<byte[], byte[]> globalConsumer,\n                              final StateDirectory stateDirectory,\n                              final long cacheSizeBytes,\n                              final StreamsMetricsImpl streamsMetrics,\n                              final Time time,\n                              final String threadClientId,\n                              final StateRestoreListener stateRestoreListener,\n                              final java.util.function.Consumer<Throwable> streamsUncaughtExceptionHandler) {\n        super(threadClientId);\n        this.time = time;\n        this.config = config;\n        this.topology = topology;\n        this.globalConsumer = globalConsumer;\n        this.stateDirectory = stateDirectory;\n        this.streamsMetrics = streamsMetrics;\n        this.logPrefix = String.format(\"global-stream-thread [%s] \", threadClientId);\n        this.logContext = new LogContext(logPrefix);\n        this.log = logContext.logger(getClass());\n        this.cache = new ThreadCache(logContext, cacheSizeBytes, this.streamsMetrics);\n        this.stateRestoreListener = stateRestoreListener;\n        this.streamsUncaughtExceptionHandler = streamsUncaughtExceptionHandler;\n        this.cacheSize = new AtomicLong(-1L);\n    }\n\n    static class StateConsumer {\n        private final Consumer<byte[], byte[]> globalConsumer;\n        private final GlobalStateMaintainer stateMaintainer;\n        private final Time time;\n        private final Duration pollTime;\n        private final long flushInterval;\n        private final Logger log;\n\n        private long lastFlush;\n\n        StateConsumer(final LogContext logContext,\n                      final Consumer<byte[], byte[]> globalConsumer,\n                      final GlobalStateMaintainer stateMaintainer,\n                      final Time time,\n                      final Duration pollTime,\n                      final long flushInterval) {\n            this.log = logContext.logger(getClass());\n            this.globalConsumer = globalConsumer;\n            this.stateMaintainer = stateMaintainer;\n            this.time = time;\n            this.pollTime = pollTime;\n            this.flushInterval = flushInterval;\n        }\n\n        /**\n         * @throws IllegalStateException If store gets registered after initialized is already finished\n         * @throws StreamsException      if the store's change log does not contain the partition\n         */\n        void initialize() {\n            final Map<TopicPartition, Long> partitionOffsets = stateMaintainer.initialize();\n            globalConsumer.assign(partitionOffsets.keySet());\n            for (final Map.Entry<TopicPartition, Long> entry : partitionOffsets.entrySet()) {\n                globalConsumer.seek(entry.getKey(), entry.getValue());\n            }\n            lastFlush = time.milliseconds();\n        }\n\n        void pollAndUpdate() {\n            final ConsumerRecords<byte[], byte[]> received = globalConsumer.poll(pollTime);\n            for (final ConsumerRecord<byte[], byte[]> record : received) {\n                stateMaintainer.update(record);\n            }\n            final long now = time.milliseconds();\n            maybeCheckpoint(now);\n        }\n\n        private void maybeCheckpoint(long now) {\n            if (now - flushInterval >= lastFlush) {\n                stateMaintainer.flushState();\n                lastFlush = now;\n            }\n        }\n\n        public void close(final boolean wipeStateStore) throws IOException {\n            try {\n                globalConsumer.close();\n            } catch (final RuntimeException e) {\n                // just log an error if the consumer throws an exception during close\n                // so we can always attempt to close the state stores.\n                log.error(\"Failed to close global consumer due to the following error:\", e);\n            }\n\n            stateMaintainer.close(wipeStateStore);\n        }\n    }\n\n    @Override\n    public void run() {\n        final StateConsumer stateConsumer = initialize();\n\n        if (stateConsumer == null) {\n            // during initialization, the caller thread would wait for the state consumer\n            // to restore the global state store before transiting to RUNNING state and return;\n            // if an error happens during the restoration process, the stateConsumer will be null\n            // and in this case we will transit the state to PENDING_SHUTDOWN and DEAD immediately.\n            // the exception will be thrown in the caller thread during start() function.\n            setState(State.PENDING_SHUTDOWN);\n            setState(State.DEAD);\n\n            log.error(\"Error happened during initialization of the global state store; this thread has shutdown.\");\n            streamsMetrics.removeAllThreadLevelSensors(getName());\n            streamsMetrics.removeAllThreadLevelMetrics(getName());\n\n            return;\n        }\n        setState(RUNNING);\n\n        boolean wipeStateStore = false;\n        try {\n            while (stillRunning()) {\n                final long size = cacheSize.getAndSet(-1L);\n                if (size != -1L) {\n                    cache.resize(size);\n                }\n                stateConsumer.pollAndUpdate();\n\n                if (fetchDeadlineClientInstanceId != -1) {\n                    if (fetchDeadlineClientInstanceId >= time.milliseconds()) {\n                        try {\n                            // we pass in a timeout of zero, to just trigger the \"get instance id\" background RPC,\n                            // we don't want to block the global thread that can do useful work in the meantime\n                            clientInstanceIdFuture.complete(globalConsumer.clientInstanceId(Duration.ZERO));\n                            fetchDeadlineClientInstanceId = -1;\n                        } catch (final IllegalStateException disabledError) {\n                            // if telemetry is disabled on a client, we swallow the error,\n                            // to allow returning a partial result for all other clients\n                            clientInstanceIdFuture.complete(null);\n                            fetchDeadlineClientInstanceId = -1;\n                        } catch (final TimeoutException swallow) {\n                            // swallow\n                        } catch (final Exception error) {\n                            clientInstanceIdFuture.completeExceptionally(error);\n                            fetchDeadlineClientInstanceId = -1;\n                        }\n                    } else {\n                        clientInstanceIdFuture.completeExceptionally(\n                            new TimeoutException(\"Could not retrieve global consumer client instance id.\")\n                        );\n                        fetchDeadlineClientInstanceId = -1;\n                    }\n                }\n            }\n        } catch (final InvalidOffsetException recoverableException) {\n            wipeStateStore = true;\n            log.error(\n                \"Updating global state failed due to inconsistent local state. Will attempt to clean up the local state. You can restart KafkaStreams to recover from this error.\",\n                recoverableException\n            );\n            final StreamsException e = new StreamsException(\n                \"Updating global state failed. You can restart KafkaStreams to launch a new GlobalStreamThread to recover from this error.\",\n                recoverableException\n            );\n            this.streamsUncaughtExceptionHandler.accept(e);\n        } catch (final Exception e) {\n            log.error(\"Error happened while maintaining global state store. The streams application or client will now close to ERROR.\", e);\n            this.streamsUncaughtExceptionHandler.accept(e);\n        } finally {\n            // set the state to pending shutdown first as it may be called due to error;\n            // its state may already be PENDING_SHUTDOWN so it will return false but we\n            // intentionally do not check the returned flag\n            setState(State.PENDING_SHUTDOWN);\n\n            log.info(\"Shutting down\");\n\n            try {\n                stateConsumer.close(wipeStateStore);\n            } catch (final IOException e) {\n                log.error(\"Failed to close state maintainer due to the following error:\", e);\n            }\n\n            streamsMetrics.removeAllThreadLevelSensors(getName());\n            streamsMetrics.removeAllThreadLevelMetrics(getName());\n\n            setState(DEAD);\n\n            log.info(\"Shutdown complete\");\n        }\n    }\n\n    public void setUncaughtExceptionHandler(final java.util.function.Consumer<Throwable> streamsUncaughtExceptionHandler) {\n        this.streamsUncaughtExceptionHandler = streamsUncaughtExceptionHandler;\n    }\n\n    public void resize(final long cacheSize) {\n        this.cacheSize.set(cacheSize);\n    }\n\n    private StateConsumer initialize() {\n        StateConsumer stateConsumer = null;\n        try {\n            final GlobalStateManager stateMgr = new GlobalStateManagerImpl(\n                logContext,\n                time,\n                topology,\n                globalConsumer,\n                stateDirectory,\n                stateRestoreListener,\n                config\n            );\n\n            final GlobalProcessorContextImpl globalProcessorContext = new GlobalProcessorContextImpl(\n                config,\n                stateMgr,\n                streamsMetrics,\n                cache,\n                time\n            );\n            stateMgr.setGlobalProcessorContext(globalProcessorContext);\n\n            stateConsumer = new StateConsumer(\n                logContext,\n                globalConsumer,\n                new GlobalStateUpdateTask(\n                    logContext,\n                    topology,\n                    globalProcessorContext,\n                    stateMgr,\n                    config.defaultDeserializationExceptionHandler()\n                ),\n                time,\n                Duration.ofMillis(config.getLong(StreamsConfig.POLL_MS_CONFIG)),\n                config.getLong(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG)\n            );\n\n            try {\n                stateConsumer.initialize();\n            } catch (final InvalidOffsetException recoverableException) {\n                log.error(\n                    \"Bootstrapping global state failed due to inconsistent local state. Will attempt to clean up the local state. You can restart KafkaStreams to recover from this error.\",\n                    recoverableException\n                );\n\n                closeStateConsumer(stateConsumer, true);\n\n                throw new StreamsException(\n                    \"Bootstrapping global state failed. You can restart KafkaStreams to recover from this error.\",\n                    recoverableException\n                );\n            }\n\n            return stateConsumer;\n        } catch (final StreamsException fatalException) {\n            closeStateConsumer(stateConsumer, false);\n            startupException = fatalException;\n        } catch (final Exception fatalException) {\n            closeStateConsumer(stateConsumer, false);\n            startupException = new StreamsException(\"Exception caught during initialization of GlobalStreamThread\", fatalException);\n        }\n        return null;\n    }\n\n    private void closeStateConsumer(final StateConsumer stateConsumer, final boolean wipeStateStore) {\n        if (stateConsumer != null) {\n            try {\n                stateConsumer.close(wipeStateStore);\n            } catch (final IOException e) {\n                log.error(\"Failed to close state consumer due to the following error:\", e);\n            }\n        }\n    }\n\n    @Override\n    public synchronized void start() {\n        super.start();\n        while (stillInitializing()) {\n            Utils.sleep(1);\n            if (startupException != null) {\n                throw startupException;\n            }\n        }\n\n        if (inErrorState()) {\n            throw new IllegalStateException(\"Initialization for the global stream thread failed\");\n        }\n    }\n\n    public void shutdown() {\n        // one could call shutdown() multiple times, so ignore subsequent calls\n        // if already shutting down or dead\n        setState(PENDING_SHUTDOWN);\n    }\n\n    public Map<MetricName, Metric> consumerMetrics() {\n        return Collections.unmodifiableMap(globalConsumer.metrics());\n    }\n\n    // this method is NOT thread-safe (we rely on the callee to be `synchronized`)\n    public KafkaFuture<Uuid> globalConsumerInstanceId(final Duration timeout) {\n        boolean setDeadline = false;\n\n        if (clientInstanceIdFuture.isDone()) {\n            if (clientInstanceIdFuture.isCompletedExceptionally()) {\n                clientInstanceIdFuture = new KafkaFutureImpl<>();\n                setDeadline = true;\n            }\n        } else {\n            setDeadline = true;\n        }\n\n        if (setDeadline) {\n            fetchDeadlineClientInstanceId = time.milliseconds() + timeout.toMillis();\n        }\n\n        return clientInstanceIdFuture;\n    }\n}",
                "methodCount": 25
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 12,
                "candidates": [
                    {
                        "lineStart": 394,
                        "lineEnd": 456,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method initialize to class State",
                        "description": "Move method initialize to org.apache.kafka.streams.processor.internals.GlobalStreamThread.State\nRationale: The method initialize() involves the setup of various state-related components, including GlobalStateManager, GlobalProcessorContextImpl, and StateConsumer. The creation and initialization of these components are tightly linked to the state management logic. Therefore, moving this method to the 'State' class aligns with its primary responsibility of handling state transitions and state management. It will centralize the state-related initialization logic within the class responsible for state, making the code more cohesive and easier to maintain.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 458,
                        "lineEnd": 466,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method closeStateConsumer to class State",
                        "description": "Move method closeStateConsumer to org.apache.kafka.streams.processor.internals.GlobalStreamThread.State\nRationale: The method closeStateConsumer() interacts directly with a StateConsumer instance and closes it, which relates to the life cycle of a state-aware component. The State class is designed to manage different states, including transitions and lifecycle aspects. Thus, encapsulating state consumer-related operations within the State class aligns with the general principle of cohesion, where related functionalities should exist together. Relocating the method to the State class enhances manageability and improves the logical grouping of state-related functionalities.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 273,
                        "lineEnd": 278,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method maybeCheckpoint to class GlobalStateMaintainer",
                        "description": "Move method maybeCheckpoint to org.apache.kafka.streams.processor.internals.GlobalStateMaintainer\nRationale: The `maybeCheckpoint` method primarily interacts with the `stateMaintainer.flushState()` method and updates the `lastFlush` variable in the context of maintaining global state. Therefore, it makes sense to move this method to the `GlobalStateMaintainer` class, since its logic is closely tied to the responsibilities and functionality provided by this class. The method's functionality aligns more with managing and maintaining state, which falls under the purview of the `GlobalStateMaintainer`.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 493,
                        "lineEnd": 511,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method globalConsumerInstanceId to class Time",
                        "description": "Move method globalConsumerInstanceId to org.apache.kafka.common.utils.Time\nRationale: The method 'globalConsumerInstanceId' uses the 'time' object for calculating deadlines and relies on timing functionality (specifically 'time.milliseconds()' and 'timeout.toMillis()'). The 'Time' class abstracts clock time and includes methods related to timing operations, which are central to the functionality of 'globalConsumerInstanceId'. Therefore, it makes sense to move this method to 'Time' to better encapsulate timing-related functionalities and improve class cohesion.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 489,
                        "lineEnd": 491,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method consumerMetrics to class State",
                        "description": "Move method consumerMetrics to org.apache.kafka.streams.processor.internals.GlobalStreamThread.State\nRationale: The method consumerMetrics() deals with metrics, which could be associated with the different states of an entity, in this case, a consumer's state. Moving the method to the State class makes logical sense because it enables tracking and retrieval of metrics as the state transitions occur. Therefore, the State class is an appropriate place for consumerMetrics() as it aligns with state-based monitoring and metric collection.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 483,
                        "lineEnd": 487,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method shutdown to class State",
                        "description": "Move method shutdown to org.apache.kafka.streams.processor.internals.GlobalStreamThread.State\nRationale: The method `shutdown()` is fundamentally dealing with thread states and state transitions. It sets the state to `PENDING_SHUTDOWN`, indicating that it is primarily related to state management. Therefore, moving the method to the `State` class is most appropriate, as it is directly concerned with the state transitions of the system.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 280,
                        "lineEnd": 290,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method close to class GlobalStateMaintainer",
                        "description": "Move method close to org.apache.kafka.streams.processor.internals.GlobalStateMaintainer\nRationale: The provided close method interacts with both globalConsumer and stateMaintainer objects. Specifically, it calls stateMaintainer's close method, passing the boolean wipeStateStore. Since this method manages the state and involves closing the state store, it would be more cohesive and appropriate within the GlobalStateMaintainer class, which is designed to handle global state stores. This move enhances encapsulation, ensuring that all state management logic resides within a single class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 390,
                        "lineEnd": 392,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method resize to class StateListener",
                        "description": "Move method resize to org.apache.kafka.streams.processor.internals.StreamThread.StateListener\nRationale: The method 'resize' is setting a cache size, which is likely to be related to a state management mechanism. The StateListener interface listens to state changes and could meaningfully incorporate cache resizing logic within its implementations, making it the most appropriate class to move the method to. Also, the name suggests that it is expected to manage or listen to changes in state which could include adjusting resources like caches.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 183,
                        "lineEnd": 187,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method stillRunning to class State",
                        "description": "Move method stillRunning to org.apache.kafka.streams.processor.internals.GlobalStreamThread.State\nRationale: The stillRunning() method is highly cohesive with the State class because it utilizes the isRunning() method of the State enumeration. Moving stillRunning() to the State class maintains logical cohesion and ensures that state checks related to thread running status are encapsulated within the State class. The method also manipulates the stateLock object, which should be managed in the context of the State class to maintain thread safety and consistency.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 195,
                        "lineEnd": 199,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method stillInitializing to class State",
                        "description": "Move method stillInitializing to org.apache.kafka.streams.processor.internals.GlobalStreamThread.State\nRationale: The method `stillInitializing()` checks if the current state is equal to the `CREATED` state. This check aligns directly with the purpose of the `State` class, which manages different lifecycle states and their transitions. Additionally, the synchronization on `stateLock` and the state comparison indicate that the method is a natural fit for managing state transitions related functions within the `State` class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 189,
                        "lineEnd": 193,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method inErrorState to class State",
                        "description": "Move method inErrorState to org.apache.kafka.streams.processor.internals.GlobalStreamThread.State\nRationale: The method `inErrorState` directly relates to the state of a thread, and the `State` enum already has an `inErrorState` method. This indicates that evaluating an error state is a primary concern of the `State` class. The synchronized block using `stateLock` suggests concurrency control over the `state` object, reinforcing the appropriateness of placing this method within the `State` class. Moving it here ensures all state-related logic, including error state determination, is encapsulated within a single, cohesive unit.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 251,
                        "lineEnd": 262,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method initialize to class GlobalStateMaintainer",
                        "description": "Move method initialize to org.apache.kafka.streams.processor.internals.GlobalStateMaintainer\nRationale: The `initialize()` method makes use of the `stateMaintainer.initialize()` method to get partition offsets and performs operations on a global consumer, indicating its strong coupling with global state management and maintenance tasks. Thus, moving it to the `GlobalStateMaintainer` class aligns with its responsibilities of maintaining and initializing global state stores, ensuring separation of concerns and higher cohesion.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "isRunning",
                            "method_signature": "public isRunning()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "inErrorState",
                            "method_signature": "public inErrorState()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stillRunning",
                            "method_signature": "public stillRunning()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "inErrorState",
                            "method_signature": "public inErrorState()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stillInitializing",
                            "method_signature": "public stillInitializing()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "initialize",
                            "method_signature": " initialize()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollAndUpdate",
                            "method_signature": " pollAndUpdate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeCheckpoint",
                            "method_signature": "private maybeCheckpoint(long now)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "public close(final boolean wipeStateStore)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resize",
                            "method_signature": "public resize(final long cacheSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "initialize",
                            "method_signature": "private initialize()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "closeStateConsumer",
                            "method_signature": "private closeStateConsumer(final StateConsumer stateConsumer, final boolean wipeStateStore)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "shutdown",
                            "method_signature": "public shutdown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "consumerMetrics",
                            "method_signature": "public consumerMetrics()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "globalConsumerInstanceId",
                            "method_signature": "public globalConsumerInstanceId(final Duration timeout)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "shutdown",
                            "method_signature": "public shutdown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "inErrorState",
                            "method_signature": "public inErrorState()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "consumerMetrics",
                            "method_signature": "public consumerMetrics()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resize",
                            "method_signature": "public resize(final long cacheSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isRunning",
                            "method_signature": "public isRunning()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stillRunning",
                            "method_signature": "public stillRunning()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stillInitializing",
                            "method_signature": "public stillInitializing()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "inErrorState",
                            "method_signature": "public inErrorState()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "initialize",
                            "method_signature": " initialize()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeCheckpoint",
                            "method_signature": "private maybeCheckpoint(long now)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollAndUpdate",
                            "method_signature": " pollAndUpdate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "public close(final boolean wipeStateStore)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "initialize",
                            "method_signature": "private initialize()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "globalConsumerInstanceId",
                            "method_signature": "public globalConsumerInstanceId(final Duration timeout)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "closeStateConsumer",
                            "method_signature": "private closeStateConsumer(final StateConsumer stateConsumer, final boolean wipeStateStore)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "shutdown",
                            "method_signature": "public shutdown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "inErrorState",
                            "method_signature": "public inErrorState()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "consumerMetrics",
                            "method_signature": "public consumerMetrics()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resize",
                            "method_signature": "public resize(final long cacheSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isRunning",
                            "method_signature": "public isRunning()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stillRunning",
                            "method_signature": "public stillRunning()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stillInitializing",
                            "method_signature": "public stillInitializing()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "inErrorState",
                            "method_signature": "public inErrorState()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "initialize",
                            "method_signature": " initialize()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeCheckpoint",
                            "method_signature": "private maybeCheckpoint(long now)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollAndUpdate",
                            "method_signature": " pollAndUpdate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "public close(final boolean wipeStateStore)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "initialize",
                            "method_signature": "private initialize()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "globalConsumerInstanceId",
                            "method_signature": "public globalConsumerInstanceId(final Duration timeout)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "closeStateConsumer",
                            "method_signature": "private closeStateConsumer(final StateConsumer stateConsumer, final boolean wipeStateStore)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "public shutdown()": {
                    "first": {
                        "method_name": "shutdown",
                        "method_signature": "public shutdown()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2749226404049458
                },
                "public inErrorState()": {
                    "first": {
                        "method_name": "inErrorState",
                        "method_signature": "public inErrorState()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5094467402237999
                },
                "public consumerMetrics()": {
                    "first": {
                        "method_name": "consumerMetrics",
                        "method_signature": "public consumerMetrics()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3636251681094007
                },
                "public resize(final long cacheSize)": {
                    "first": {
                        "method_name": "resize",
                        "method_signature": "public resize(final long cacheSize)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.37843196171358623
                },
                "public isRunning()": {
                    "first": {
                        "method_name": "isRunning",
                        "method_signature": "public isRunning()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.40907760256101827
                },
                "public stillRunning()": {
                    "first": {
                        "method_name": "stillRunning",
                        "method_signature": "public stillRunning()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5078848062967465
                },
                "public stillInitializing()": {
                    "first": {
                        "method_name": "stillInitializing",
                        "method_signature": "public stillInitializing()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5078848062967465
                },
                " initialize()": {
                    "first": {
                        "method_name": "initialize",
                        "method_signature": " initialize()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5103691774573709
                },
                "private maybeCheckpoint(long now)": {
                    "first": {
                        "method_name": "maybeCheckpoint",
                        "method_signature": "private maybeCheckpoint(long now)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5125346719514966
                },
                " pollAndUpdate()": {
                    "first": {
                        "method_name": "pollAndUpdate",
                        "method_signature": " pollAndUpdate()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5239823068789906
                },
                "public close(final boolean wipeStateStore)": {
                    "first": {
                        "method_name": "close",
                        "method_signature": "public close(final boolean wipeStateStore)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5600783070384077
                },
                "private initialize()": {
                    "first": {
                        "method_name": "initialize",
                        "method_signature": "private initialize()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5896075843766644
                },
                "public globalConsumerInstanceId(final Duration timeout)": {
                    "first": {
                        "method_name": "globalConsumerInstanceId",
                        "method_signature": "public globalConsumerInstanceId(final Duration timeout)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.647063611496997
                },
                "private closeStateConsumer(final StateConsumer stateConsumer, final boolean wipeStateStore)": {
                    "first": {
                        "method_name": "closeStateConsumer",
                        "method_signature": "private closeStateConsumer(final StateConsumer stateConsumer, final boolean wipeStateStore)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6793117689246415
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "private initialize()",
                    "private closeStateConsumer(final StateConsumer stateConsumer, final boolean wipeStateStore)",
                    "private maybeCheckpoint(long now)",
                    "public globalConsumerInstanceId(final Duration timeout)",
                    "public consumerMetrics()",
                    "public shutdown()",
                    "public close(final boolean wipeStateStore)",
                    "public resize(final long cacheSize)",
                    "public stillRunning()",
                    "public stillInitializing()",
                    "public inErrorState()",
                    "public inErrorState()",
                    " initialize()",
                    "public isRunning()"
                ],
                "llm_response_time": 5320
            },
            "targetClassMap": {
                "initialize": {
                    "target_classes": [
                        {
                            "class_name": "GlobalStateMaintainer",
                            "similarity_score": 0.2912758640177315
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.43148008839966806
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GlobalStateMaintainer",
                        "Time"
                    ],
                    "llm_response_time": 3786,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "closeStateConsumer": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.28071617046412006
                        },
                        {
                            "class_name": "State",
                            "similarity_score": 0.18407603655783492
                        },
                        {
                            "class_name": "StateListener",
                            "similarity_score": 0.20965696734438363
                        },
                        {
                            "class_name": "StateRestoreListener",
                            "similarity_score": 0.16240744038683613
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "State",
                        "StateListener",
                        "Time"
                    ],
                    "llm_response_time": 6231,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "maybeCheckpoint": {
                    "target_classes": [
                        {
                            "class_name": "GlobalStateMaintainer",
                            "similarity_score": 0.2389760596996216
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.16939262038071365
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GlobalStateMaintainer",
                        "Time"
                    ],
                    "llm_response_time": 3731,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "globalConsumerInstanceId": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.2882343356171844
                        },
                        {
                            "class_name": "State",
                            "similarity_score": 0.18364151982084145
                        },
                        {
                            "class_name": "StateListener",
                            "similarity_score": 0.09692876233453128
                        },
                        {
                            "class_name": "StateRestoreListener",
                            "similarity_score": 0.122090900665225
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time",
                        "State",
                        "StateRestoreListener"
                    ],
                    "llm_response_time": 3611,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "consumerMetrics": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.14045294101448405
                        },
                        {
                            "class_name": "State",
                            "similarity_score": 0.15406237963881292
                        },
                        {
                            "class_name": "StateListener",
                            "similarity_score": 0.098058067569092
                        },
                        {
                            "class_name": "StateRestoreListener",
                            "similarity_score": 0.019149359211884053
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "State",
                        "StateListener",
                        "Time"
                    ],
                    "llm_response_time": 4036,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "shutdown": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.08813182095966561
                        },
                        {
                            "class_name": "State",
                            "similarity_score": 0.07266159337809273
                        },
                        {
                            "class_name": "StateListener",
                            "similarity_score": 0.0698856557814612
                        },
                        {
                            "class_name": "StateRestoreListener",
                            "similarity_score": 0.03275444175868964
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "State",
                        "StateListener",
                        "Time"
                    ],
                    "llm_response_time": 3664,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "close": {
                    "target_classes": [
                        {
                            "class_name": "GlobalStateMaintainer",
                            "similarity_score": 0.2822767727858068
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.34029856810846043
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GlobalStateMaintainer",
                        "Time"
                    ],
                    "llm_response_time": 2587,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "resize": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.17098618906111104
                        },
                        {
                            "class_name": "State",
                            "similarity_score": 0.1268749008790224
                        },
                        {
                            "class_name": "StateListener",
                            "similarity_score": 0.13074409009212268
                        },
                        {
                            "class_name": "StateRestoreListener",
                            "similarity_score": 0.057448077635652156
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "StateListener",
                        "State",
                        "Time"
                    ],
                    "llm_response_time": 3072,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "stillRunning": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.182846319218817
                        },
                        {
                            "class_name": "State",
                            "similarity_score": 0.19854927197906458
                        },
                        {
                            "class_name": "StateListener",
                            "similarity_score": 0.11935247900657213
                        },
                        {
                            "class_name": "StateRestoreListener",
                            "similarity_score": 0.025172486405083787
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "State",
                        "StateListener",
                        "Time"
                    ],
                    "llm_response_time": 3118,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "stillInitializing": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.182846319218817
                        },
                        {
                            "class_name": "State",
                            "similarity_score": 0.19854927197906458
                        },
                        {
                            "class_name": "StateListener",
                            "similarity_score": 0.11935247900657213
                        },
                        {
                            "class_name": "StateRestoreListener",
                            "similarity_score": 0.025172486405083787
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "State",
                        "StateListener",
                        "Time"
                    ],
                    "llm_response_time": 3812,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "inErrorState": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.182846319218817
                        },
                        {
                            "class_name": "State",
                            "similarity_score": 0.2051675810450334
                        },
                        {
                            "class_name": "StateListener",
                            "similarity_score": 0.11935247900657213
                        },
                        {
                            "class_name": "StateRestoreListener",
                            "similarity_score": 0.025172486405083787
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "State",
                        "StateListener",
                        "Time"
                    ],
                    "llm_response_time": 3015,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "isRunning": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 4406,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "b6183a41342c765daec9c88f9d2723a221131960",
        "url": "https://github.com/apache/kafka/commit/b6183a41342c765daec9c88f9d2723a221131960",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public fromArgs(args String[]) : ConsumerGroupCommandOptions extracted from public testDescribeGroupCliWithGroupDescribe(quorum String) : void in class org.apache.kafka.tools.consumer.group.AuthorizerIntegrationTest & moved to class org.apache.kafka.tools.consumer.group.ConsumerGroupCommandOptions",
            "leftSideLocations": [
                {
                    "filePath": "tools/src/test/java/org/apache/kafka/tools/consumer/group/AuthorizerIntegrationTest.java",
                    "startLine": 36,
                    "endLine": 46,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testDescribeGroupCliWithGroupDescribe(quorum String) : void"
                },
                {
                    "filePath": "tools/src/test/java/org/apache/kafka/tools/consumer/group/AuthorizerIntegrationTest.java",
                    "startLine": 42,
                    "endLine": 42,
                    "startColumn": 9,
                    "endColumn": 127,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "tools/src/main/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandOptions.java",
                    "startLine": 120,
                    "endLine": 124,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public fromArgs(args String[]) : ConsumerGroupCommandOptions"
                },
                {
                    "filePath": "tools/src/main/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandOptions.java",
                    "startLine": 121,
                    "endLine": 121,
                    "startColumn": 9,
                    "endColumn": 82,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "tools/src/test/java/org/apache/kafka/tools/consumer/group/AuthorizerIntegrationTest.java",
                    "startLine": 33,
                    "endLine": 44,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testDescribeGroupCliWithGroupDescribe(quorum String) : void"
                },
                {
                    "filePath": "tools/src/test/java/org/apache/kafka/tools/consumer/group/AuthorizerIntegrationTest.java",
                    "startLine": 40,
                    "endLine": 40,
                    "startColumn": 44,
                    "endColumn": 89,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ConsumerGroupCommandOptions.fromArgs(cgcArgs)"
                },
                {
                    "filePath": "tools/src/main/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandOptions.java",
                    "startLine": 122,
                    "endLine": 122,
                    "startColumn": 9,
                    "endColumn": 26,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "tools/src/main/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandOptions.java",
                    "startLine": 123,
                    "endLine": 123,
                    "startColumn": 9,
                    "endColumn": 21,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": true
        },
        "ref_id": 575,
        "extraction_results": {
            "success": true,
            "newCommitHash": "a7fa737b2cc8ece8e16aef27445f356bd1a14c8e",
            "newBranchName": "extract-fromArgs-testDescribeGroupCliWithGroupDescribe-34d365f"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "b6183a41342c765daec9c88f9d2723a221131960",
        "url": "https://github.com/apache/kafka/commit/b6183a41342c765daec9c88f9d2723a221131960",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public fromArgs(args String[]) : ConsumerGroupCommandOptions extracted from package getConsumerGroupService(args String[]) : ConsumerGroupCommand.ConsumerGroupService in class org.apache.kafka.tools.consumer.group.ConsumerGroupCommandTest & moved to class org.apache.kafka.tools.consumer.group.ConsumerGroupCommandOptions",
            "leftSideLocations": [
                {
                    "filePath": "tools/src/test/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandTest.java",
                    "startLine": 132,
                    "endLine": 141,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package getConsumerGroupService(args String[]) : ConsumerGroupCommand.ConsumerGroupService"
                },
                {
                    "filePath": "tools/src/test/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandTest.java",
                    "startLine": 134,
                    "endLine": 137,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "tools/src/test/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandTest.java",
                    "startLine": 133,
                    "endLine": 133,
                    "startColumn": 9,
                    "endColumn": 124,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "tools/src/main/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandOptions.java",
                    "startLine": 120,
                    "endLine": 124,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public fromArgs(args String[]) : ConsumerGroupCommandOptions"
                },
                {
                    "filePath": "tools/src/main/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandOptions.java",
                    "startLine": 123,
                    "endLine": 123,
                    "startColumn": 9,
                    "endColumn": 21,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "tools/src/main/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandOptions.java",
                    "startLine": 121,
                    "endLine": 121,
                    "startColumn": 9,
                    "endColumn": 82,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "tools/src/test/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandTest.java",
                    "startLine": 130,
                    "endLine": 139,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package getConsumerGroupService(args String[]) : ConsumerGroupCommand.ConsumerGroupService"
                },
                {
                    "filePath": "tools/src/test/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandTest.java",
                    "startLine": 131,
                    "endLine": 131,
                    "startColumn": 44,
                    "endColumn": 86,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ConsumerGroupCommandOptions.fromArgs(args)"
                },
                {
                    "filePath": "tools/src/main/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandOptions.java",
                    "startLine": 122,
                    "endLine": 122,
                    "startColumn": 9,
                    "endColumn": 26,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": true
        },
        "ref_id": 576,
        "extraction_results": {
            "success": true,
            "newCommitHash": "7af27c6d699e8995dba400666005069a2daa5be0",
            "newBranchName": "extract-fromArgs-getConsumerGroupService-34d365f"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "b6183a41342c765daec9c88f9d2723a221131960",
        "url": "https://github.com/apache/kafka/commit/b6183a41342c765daec9c88f9d2723a221131960",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public fromArgs(args String[]) : ConsumerGroupCommandOptions extracted from private prepareConsumerGroupService() : ConsumerGroupCommand.ConsumerGroupService in class org.apache.kafka.tools.consumer.group.SaslClientsWithInvalidCredentialsTest & moved to class org.apache.kafka.tools.consumer.group.ConsumerGroupCommandOptions",
            "leftSideLocations": [
                {
                    "filePath": "tools/src/test/java/org/apache/kafka/tools/consumer/group/SaslClientsWithInvalidCredentialsTest.java",
                    "startLine": 158,
                    "endLine": 169,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private prepareConsumerGroupService() : ConsumerGroupCommand.ConsumerGroupService"
                },
                {
                    "filePath": "tools/src/test/java/org/apache/kafka/tools/consumer/group/SaslClientsWithInvalidCredentialsTest.java",
                    "startLine": 167,
                    "endLine": 167,
                    "startColumn": 9,
                    "endColumn": 127,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "tools/src/main/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandOptions.java",
                    "startLine": 120,
                    "endLine": 124,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public fromArgs(args String[]) : ConsumerGroupCommandOptions"
                },
                {
                    "filePath": "tools/src/main/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandOptions.java",
                    "startLine": 121,
                    "endLine": 121,
                    "startColumn": 9,
                    "endColumn": 82,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "tools/src/test/java/org/apache/kafka/tools/consumer/group/SaslClientsWithInvalidCredentialsTest.java",
                    "startLine": 156,
                    "endLine": 167,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private prepareConsumerGroupService() : ConsumerGroupCommand.ConsumerGroupService"
                },
                {
                    "filePath": "tools/src/test/java/org/apache/kafka/tools/consumer/group/SaslClientsWithInvalidCredentialsTest.java",
                    "startLine": 165,
                    "endLine": 165,
                    "startColumn": 44,
                    "endColumn": 89,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ConsumerGroupCommandOptions.fromArgs(cgcArgs)"
                },
                {
                    "filePath": "tools/src/main/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandOptions.java",
                    "startLine": 122,
                    "endLine": 122,
                    "startColumn": 9,
                    "endColumn": 26,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "tools/src/main/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandOptions.java",
                    "startLine": 123,
                    "endLine": 123,
                    "startColumn": 9,
                    "endColumn": 21,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": true
        },
        "ref_id": 577,
        "extraction_results": {
            "success": true,
            "newCommitHash": "1b78c84f6d0cc6ae9d5dad0f09e845a861e84805",
            "newBranchName": "extract-fromArgs-prepareConsumerGroupService-34d365f"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "71bcac3b6ada9872dcdec48f72372d6c5b041c0a",
        "url": "https://github.com/apache/kafka/commit/71bcac3b6ada9872dcdec48f72372d6c5b041c0a",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public castToStringObjectMap(inputMap Map<?,?>) : Map<String,Object> extracted from public AbstractConfig(definition ConfigDef, originals Map<?,?>, configProviderProps Map<String,?>, doLog boolean) in class org.apache.kafka.common.config.AbstractConfig & moved to class org.apache.kafka.common.utils.Utils",
            "leftSideLocations": [
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/config/AbstractConfig.java",
                    "startLine": 65,
                    "endLine": 121,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public AbstractConfig(definition ConfigDef, originals Map<?,?>, configProviderProps Map<String,?>, doLog boolean)"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/config/AbstractConfig.java",
                    "startLine": 109,
                    "endLine": 109,
                    "startColumn": 17,
                    "endColumn": 113,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/config/AbstractConfig.java",
                    "startLine": 107,
                    "endLine": 109,
                    "startColumn": 9,
                    "endColumn": 113,
                    "codeElementType": "ENHANCED_FOR_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/config/AbstractConfig.java",
                    "startLine": 108,
                    "endLine": 109,
                    "startColumn": 13,
                    "endColumn": 113,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java",
                    "startLine": 1508,
                    "endLine": 1525,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public castToStringObjectMap(inputMap Map<?,?>) : Map<String,Object>"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java",
                    "startLine": 1521,
                    "endLine": 1521,
                    "startColumn": 17,
                    "endColumn": 118,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java",
                    "startLine": 1516,
                    "endLine": 1523,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "ENHANCED_FOR_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java",
                    "startLine": 1517,
                    "endLine": 1522,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/config/AbstractConfig.java",
                    "startLine": 70,
                    "endLine": 123,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public AbstractConfig(definition ConfigDef, originals Map<?,?>, configProviderProps Map<String,?>, doLog boolean)"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/config/AbstractConfig.java",
                    "startLine": 111,
                    "endLine": 111,
                    "startColumn": 43,
                    "endColumn": 81,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "Utils.castToStringObjectMap(originals)"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java",
                    "startLine": 1515,
                    "endLine": 1515,
                    "startColumn": 9,
                    "endColumn": 66,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java",
                    "startLine": 1518,
                    "endLine": 1518,
                    "startColumn": 17,
                    "endColumn": 52,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java",
                    "startLine": 1519,
                    "endLine": 1519,
                    "startColumn": 17,
                    "endColumn": 46,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java",
                    "startLine": 1524,
                    "endLine": 1524,
                    "startColumn": 9,
                    "endColumn": 20,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java",
                    "startLine": 1517,
                    "endLine": 1520,
                    "startColumn": 51,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java",
                    "startLine": 1520,
                    "endLine": 1522,
                    "startColumn": 20,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java",
                    "startLine": 1516,
                    "endLine": 1523,
                    "startColumn": 59,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": true
        },
        "ref_id": 578,
        "extraction_results": {
            "success": false
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "a1ca788c993e3b217a0904c3f4768f0682dc840b",
        "url": "https://github.com/apache/kafka/commit/a1ca788c993e3b217a0904c3f4768f0682dc840b",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method package adminCall(callable Callable<T>, errMsg Supplier<String>) : T extracted from private listTopicAclBindings() : Optional<Collection<AclBinding>> in class org.apache.kafka.connect.mirror.MirrorSourceConnector & moved to class org.apache.kafka.connect.mirror.MirrorUtils",
            "leftSideLocations": [
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorSourceConnector.java",
                    "startLine": 525,
                    "endLine": 546,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private listTopicAclBindings() : Optional<Collection<AclBinding>>"
                },
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorSourceConnector.java",
                    "startLine": 542,
                    "endLine": 542,
                    "startColumn": 17,
                    "endColumn": 25,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorSourceConnector.java",
                    "startLine": 540,
                    "endLine": 540,
                    "startColumn": 17,
                    "endColumn": 41,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorUtils.java",
                    "startLine": 327,
                    "endLine": 342,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "package adminCall(callable Callable<T>, errMsg Supplier<String>) : T"
                },
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorUtils.java",
                    "startLine": 338,
                    "endLine": 338,
                    "startColumn": 13,
                    "endColumn": 21,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorUtils.java",
                    "startLine": 330,
                    "endLine": 330,
                    "startColumn": 13,
                    "endColumn": 36,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorSourceConnector.java",
                    "startLine": 543,
                    "endLine": 569,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private listTopicAclBindings() : Optional<Collection<AclBinding>>"
                },
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorSourceConnector.java",
                    "startLine": 545,
                    "endLine": 568,
                    "startColumn": 16,
                    "endColumn": 10,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "adminCall(() -> {\n  Collection<AclBinding> bindings;\n  try {\n    bindings=sourceAdminClient.describeAcls(ANY_TOPIC_ACL).values().get();\n  }\n catch (  ExecutionException e) {\n    if (e.getCause() instanceof SecurityDisabledException) {\n      if (noAclAuthorizer.compareAndSet(false,true)) {\n        log.info(\"No ACL authorizer is configured on the source Kafka cluster, so no topic ACL syncing will take place. \" + \"Consider disabling topic ACL syncing by setting \" + SYNC_TOPIC_ACLS_ENABLED + \" to 'false'.\");\n      }\n else {\n        log.debug(\"Source-side ACL authorizer still not found; skipping topic ACL sync\");\n      }\n      return Optional.empty();\n    }\n else {\n      throw e;\n    }\n  }\n  return Optional.of(bindings);\n}\n,() -> \"describe ACLs on \" + config.sourceClusterAlias() + \" cluster\")"
                },
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorUtils.java",
                    "startLine": 332,
                    "endLine": 332,
                    "startColumn": 13,
                    "endColumn": 44,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorUtils.java",
                    "startLine": 336,
                    "endLine": 336,
                    "startColumn": 17,
                    "endColumn": 109,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorUtils.java",
                    "startLine": 340,
                    "endLine": 340,
                    "startColumn": 13,
                    "endColumn": 43,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorUtils.java",
                    "startLine": 329,
                    "endLine": 341,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "TRY_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorUtils.java",
                    "startLine": 335,
                    "endLine": 337,
                    "startColumn": 67,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorUtils.java",
                    "startLine": 333,
                    "endLine": 337,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorUtils.java",
                    "startLine": 331,
                    "endLine": 339,
                    "startColumn": 11,
                    "endColumn": 10,
                    "codeElementType": "CATCH_CLAUSE",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorUtils.java",
                    "startLine": 339,
                    "endLine": 341,
                    "startColumn": 11,
                    "endColumn": 10,
                    "codeElementType": "CATCH_CLAUSE",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": true
        },
        "ref_id": 579,
        "extraction_results": {
            "success": true,
            "newCommitHash": "edafad4bfaf5f07b8323dc99a2100711712341ed",
            "newBranchName": "extract-adminCall-listTopicAclBindings-f7eb962"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "ebeef4eb64a36d46565629d2a370eeb1635a7293",
        "url": "https://github.com/apache/kafka/commit/ebeef4eb64a36d46565629d2a370eeb1635a7293",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method private shouldInitialize() : boolean extracted from public resetInitializingPositions() : void in class org.apache.kafka.clients.consumer.internals.SubscriptionState & moved to class org.apache.kafka.clients.consumer.internals.SubscriptionState.TopicPartitionState",
            "leftSideLocations": [
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java",
                    "startLine": 763,
                    "endLine": 776,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public resetInitializingPositions() : void"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java",
                    "startLine": 766,
                    "endLine": 766,
                    "startColumn": 17,
                    "endColumn": 75,
                    "codeElementType": "IF_STATEMENT_CONDITION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java",
                    "startLine": 1102,
                    "endLine": 1110,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "private shouldInitialize() : boolean"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java",
                    "startLine": 1109,
                    "endLine": 1109,
                    "startColumn": 13,
                    "endColumn": 94,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java",
                    "startLine": 764,
                    "endLine": 789,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public resetInitializingPositions() : void"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java",
                    "startLine": 779,
                    "endLine": 779,
                    "startColumn": 17,
                    "endColumn": 50,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "partitionState.shouldInitialize()"
                }
            ],
            "isStatic": false
        },
        "ref_id": 580,
        "extraction_results": {
            "success": true,
            "newCommitHash": "fa9a5d390256649011d80f3a1cd77fecfe7dda76",
            "newBranchName": "extract-shouldInitialize-resetInitializingPositions-a1ca788"
        },
        "telemetry": {
            "id": "3db662ee-b006-4b1c-9da7-c5f8221c9fc2",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1250,
                "lineStart": 54,
                "lineEnd": 1303,
                "bodyLineStart": 54,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java",
                "sourceCode": "/**\n * A class for tracking the topics, partitions, and offsets for the consumer. A partition\n * is \"assigned\" either directly with {@link #assignFromUser(Set)} (manual assignment)\n * or with {@link #assignFromSubscribed(Collection)} (automatic assignment from subscription).\n * <p>\n * Once assigned, the partition is not considered \"fetchable\" until its initial position has\n * been set with {@link #seekValidated(TopicPartition, FetchPosition)}. Fetchable partitions\n * track a position which is the last offset that has been returned to the user. You can\n * suspend fetching from a partition through {@link #pause(TopicPartition)} without affecting the consumed\n * position. The partition will remain unfetchable until the {@link #resume(TopicPartition)} is\n * used. You can also query the pause state independently with {@link #isPaused(TopicPartition)}.\n * <p>\n * Note that pause state as well as the consumed positions are not preserved when partition\n * assignment is changed whether directly by the user or through a group rebalance.\n * <p>\n * Thread Safety: this class is thread-safe.\n */\npublic class SubscriptionState {\n    private static final String SUBSCRIPTION_EXCEPTION_MESSAGE =\n            \"Subscription to topics, partitions and pattern are mutually exclusive\";\n\n    private final Logger log;\n\n    private enum SubscriptionType {\n        NONE, AUTO_TOPICS, AUTO_PATTERN, USER_ASSIGNED\n    }\n\n    /* the type of subscription */\n    private SubscriptionType subscriptionType;\n\n    /* the pattern user has requested */\n    private Pattern subscribedPattern;\n\n    /* the list of topics the user has requested */\n    private Set<String> subscription;\n\n    /* The list of topics the group has subscribed to. This may include some topics which are not part\n     * of `subscription` for the leader of a group since it is responsible for detecting metadata changes\n     * which require a group rebalance. */\n    private Set<String> groupSubscription;\n\n    /* the partitions that are currently assigned, note that the order of partition matters (see FetchBuilder for more details) */\n    private final PartitionStates<TopicPartitionState> assignment;\n\n    /* Default offset reset strategy */\n    private final OffsetResetStrategy defaultResetStrategy;\n\n    /* User-provided listener to be invoked when assignment changes */\n    private Optional<ConsumerRebalanceListener> rebalanceListener;\n\n    private int assignmentId = 0;\n\n    @Override\n    public synchronized String toString() {\n        return \"SubscriptionState{\" +\n            \"type=\" + subscriptionType +\n            \", subscribedPattern=\" + subscribedPattern +\n            \", subscription=\" + String.join(\",\", subscription) +\n            \", groupSubscription=\" + String.join(\",\", groupSubscription) +\n            \", defaultResetStrategy=\" + defaultResetStrategy +\n            \", assignment=\" + assignment.partitionStateValues() + \" (id=\" + assignmentId + \")}\";\n    }\n\n    public synchronized String prettyString() {\n        switch (subscriptionType) {\n            case NONE:\n                return \"None\";\n            case AUTO_TOPICS:\n                return \"Subscribe(\" + String.join(\",\", subscription) + \")\";\n            case AUTO_PATTERN:\n                return \"Subscribe(\" + subscribedPattern + \")\";\n            case USER_ASSIGNED:\n                return \"Assign(\" + assignedPartitions() + \" , id=\" + assignmentId + \")\";\n            default:\n                throw new IllegalStateException(\"Unrecognized subscription type: \" + subscriptionType);\n        }\n    }\n\n    public SubscriptionState(LogContext logContext, OffsetResetStrategy defaultResetStrategy) {\n        this.log = logContext.logger(this.getClass());\n        this.defaultResetStrategy = defaultResetStrategy;\n        this.subscription = new TreeSet<>(); // use a sorted set for better logging\n        this.assignment = new PartitionStates<>();\n        this.groupSubscription = new HashSet<>();\n        this.subscribedPattern = null;\n        this.subscriptionType = SubscriptionType.NONE;\n    }\n\n    /**\n     * Monotonically increasing id which is incremented after every assignment change. This can\n     * be used to check when an assignment has changed.\n     *\n     * @return The current assignment Id\n     */\n    synchronized int assignmentId() {\n        return assignmentId;\n    }\n\n    /**\n     * This method sets the subscription type if it is not already set (i.e. when it is NONE),\n     * or verifies that the subscription type is equal to the give type when it is set (i.e.\n     * when it is not NONE)\n     * @param type The given subscription type\n     */\n    private void setSubscriptionType(SubscriptionType type) {\n        if (this.subscriptionType == SubscriptionType.NONE)\n            this.subscriptionType = type;\n        else if (this.subscriptionType != type)\n            throw new IllegalStateException(SUBSCRIPTION_EXCEPTION_MESSAGE);\n    }\n\n    public synchronized boolean subscribe(Set<String> topics, Optional<ConsumerRebalanceListener> listener) {\n        registerRebalanceListener(listener);\n        setSubscriptionType(SubscriptionType.AUTO_TOPICS);\n        return changeSubscription(topics);\n    }\n\n    public synchronized void subscribe(Pattern pattern, Optional<ConsumerRebalanceListener> listener) {\n        registerRebalanceListener(listener);\n        setSubscriptionType(SubscriptionType.AUTO_PATTERN);\n        this.subscribedPattern = pattern;\n    }\n\n    public synchronized boolean subscribeFromPattern(Set<String> topics) {\n        if (subscriptionType != SubscriptionType.AUTO_PATTERN)\n            throw new IllegalArgumentException(\"Attempt to subscribe from pattern while subscription type set to \" +\n                    subscriptionType);\n\n        return changeSubscription(topics);\n    }\n\n    private boolean changeSubscription(Set<String> topicsToSubscribe) {\n        if (subscription.equals(topicsToSubscribe))\n            return false;\n\n        subscription = topicsToSubscribe;\n        return true;\n    }\n\n    /**\n     * Set the current group subscription. This is used by the group leader to ensure\n     * that it receives metadata updates for all topics that the group is interested in.\n     *\n     * @param topics All topics from the group subscription\n     * @return true if the group subscription contains topics which are not part of the local subscription\n     */\n    synchronized boolean groupSubscribe(Collection<String> topics) {\n        if (!hasAutoAssignedPartitions())\n            throw new IllegalStateException(SUBSCRIPTION_EXCEPTION_MESSAGE);\n        groupSubscription = new HashSet<>(topics);\n        return !subscription.containsAll(groupSubscription);\n    }\n\n    /**\n     * Reset the group's subscription to only contain topics subscribed by this consumer.\n     */\n    synchronized void resetGroupSubscription() {\n        groupSubscription = Collections.emptySet();\n    }\n\n    /**\n     * Change the assignment to the specified partitions provided by the user,\n     * note this is different from {@link #assignFromSubscribed(Collection)}\n     * whose input partitions are provided from the subscribed topics.\n     */\n    public synchronized boolean assignFromUser(Set<TopicPartition> partitions) {\n        setSubscriptionType(SubscriptionType.USER_ASSIGNED);\n\n        if (this.assignment.partitionSet().equals(partitions))\n            return false;\n\n        assignmentId++;\n\n        // update the subscribed topics\n        Set<String> manualSubscribedTopics = new HashSet<>();\n        Map<TopicPartition, TopicPartitionState> partitionToState = new HashMap<>();\n        for (TopicPartition partition : partitions) {\n            TopicPartitionState state = assignment.stateValue(partition);\n            if (state == null)\n                state = new TopicPartitionState();\n            partitionToState.put(partition, state);\n\n            manualSubscribedTopics.add(partition.topic());\n        }\n\n        this.assignment.set(partitionToState);\n        return changeSubscription(manualSubscribedTopics);\n    }\n\n    /**\n     * @return true if assignments matches subscription, otherwise false\n     */\n    public synchronized boolean checkAssignmentMatchedSubscription(Collection<TopicPartition> assignments) {\n        for (TopicPartition topicPartition : assignments) {\n            if (this.subscribedPattern != null) {\n                if (!this.subscribedPattern.matcher(topicPartition.topic()).matches()) {\n                    log.info(\"Assigned partition {} for non-subscribed topic regex pattern; subscription pattern is {}\",\n                        topicPartition,\n                        this.subscribedPattern);\n\n                    return false;\n                }\n            } else {\n                if (!this.subscription.contains(topicPartition.topic())) {\n                    log.info(\"Assigned partition {} for non-subscribed topic; subscription is {}\", topicPartition, this.subscription);\n\n                    return false;\n                }\n            }\n        }\n\n        return true;\n    }\n\n    /**\n     * Change the assignment to the specified partitions returned from the coordinator, note this is\n     * different from {@link #assignFromUser(Set)} which directly set the assignment from user inputs.\n     */\n    public synchronized void assignFromSubscribed(Collection<TopicPartition> assignments) {\n        if (!this.hasAutoAssignedPartitions())\n            throw new IllegalArgumentException(\"Attempt to dynamically assign partitions while manual assignment in use\");\n\n        Map<TopicPartition, TopicPartitionState> assignedPartitionStates = new HashMap<>(assignments.size());\n        for (TopicPartition tp : assignments) {\n            TopicPartitionState state = this.assignment.stateValue(tp);\n            if (state == null)\n                state = new TopicPartitionState();\n            assignedPartitionStates.put(tp, state);\n        }\n\n        assignmentId++;\n        this.assignment.set(assignedPartitionStates);\n    }\n\n    private void registerRebalanceListener(Optional<ConsumerRebalanceListener> listener) {\n        this.rebalanceListener = Objects.requireNonNull(listener, \"RebalanceListener cannot be null\");\n    }\n\n    /**\n     * Check whether pattern subscription is in use.\n     *\n     */\n    synchronized boolean hasPatternSubscription() {\n        return this.subscriptionType == SubscriptionType.AUTO_PATTERN;\n    }\n\n    public synchronized boolean hasNoSubscriptionOrUserAssignment() {\n        return this.subscriptionType == SubscriptionType.NONE;\n    }\n\n    public synchronized void unsubscribe() {\n        this.subscription = Collections.emptySet();\n        this.groupSubscription = Collections.emptySet();\n        this.assignment.clear();\n        this.subscribedPattern = null;\n        this.subscriptionType = SubscriptionType.NONE;\n        this.assignmentId++;\n    }\n\n    /**\n     * Check whether a topic matches a subscribed pattern.\n     *\n     * @return true if pattern subscription is in use and the topic matches the subscribed pattern, false otherwise\n     */\n    synchronized boolean matchesSubscribedPattern(String topic) {\n        Pattern pattern = this.subscribedPattern;\n        if (hasPatternSubscription() && pattern != null)\n            return pattern.matcher(topic).matches();\n        return false;\n    }\n\n    public synchronized Set<String> subscription() {\n        if (hasAutoAssignedPartitions())\n            return this.subscription;\n        return Collections.emptySet();\n    }\n\n    public synchronized Set<TopicPartition> pausedPartitions() {\n        return collectPartitions(TopicPartitionState::isPaused);\n    }\n\n    /**\n     * Get the subscription topics for which metadata is required. For the leader, this will include\n     * the union of the subscriptions of all group members. For followers, it is just that member's\n     * subscription. This is used when querying topic metadata to detect the metadata changes which would\n     * require rebalancing. The leader fetches metadata for all topics in the group so that it\n     * can do the partition assignment (which requires at least partition counts for all topics\n     * to be assigned).\n     *\n     * @return The union of all subscribed topics in the group if this member is the leader\n     *   of the current generation; otherwise it returns the same set as {@link #subscription()}\n     */\n    synchronized Set<String> metadataTopics() {\n        if (groupSubscription.isEmpty())\n            return subscription;\n        else if (groupSubscription.containsAll(subscription))\n            return groupSubscription;\n        else {\n            // When subscription changes `groupSubscription` may be outdated, ensure that\n            // new subscription topics are returned.\n            Set<String> topics = new HashSet<>(groupSubscription);\n            topics.addAll(subscription);\n            return topics;\n        }\n    }\n\n    synchronized boolean needsMetadata(String topic) {\n        return subscription.contains(topic) || groupSubscription.contains(topic);\n    }\n\n    private TopicPartitionState assignedState(TopicPartition tp) {\n        TopicPartitionState state = this.assignment.stateValue(tp);\n        if (state == null)\n            throw new IllegalStateException(\"No current assignment for partition \" + tp);\n        return state;\n    }\n\n    private TopicPartitionState assignedStateOrNull(TopicPartition tp) {\n        return this.assignment.stateValue(tp);\n    }\n\n    public synchronized void seekValidated(TopicPartition tp, FetchPosition position) {\n        assignedState(tp).seekValidated(position);\n    }\n\n    public void seek(TopicPartition tp, long offset) {\n        seekValidated(tp, new FetchPosition(offset));\n    }\n\n    public void seekUnvalidated(TopicPartition tp, FetchPosition position) {\n        assignedState(tp).seekUnvalidated(position);\n    }\n\n    synchronized void maybeSeekUnvalidated(TopicPartition tp, FetchPosition position, OffsetResetStrategy requestedResetStrategy) {\n        TopicPartitionState state = assignedStateOrNull(tp);\n        if (state == null) {\n            log.debug(\"Skipping reset of partition {} since it is no longer assigned\", tp);\n        } else if (!state.awaitingReset()) {\n            log.debug(\"Skipping reset of partition {} since reset is no longer needed\", tp);\n        } else if (requestedResetStrategy != state.resetStrategy) {\n            log.debug(\"Skipping reset of partition {} since an alternative reset has been requested\", tp);\n        } else {\n            log.info(\"Resetting offset for partition {} to position {}.\", tp, position);\n            state.seekUnvalidated(position);\n        }\n    }\n\n    /**\n     * @return a modifiable copy of the currently assigned partitions\n     */\n    public synchronized Set<TopicPartition> assignedPartitions() {\n        return new HashSet<>(this.assignment.partitionSet());\n    }\n\n    /**\n     * @return a modifiable copy of the currently assigned partitions as a list\n     */\n    public synchronized List<TopicPartition> assignedPartitionsList() {\n        return new ArrayList<>(this.assignment.partitionSet());\n    }\n\n    /**\n     * Provides the number of assigned partitions in a thread safe manner.\n     * @return the number of assigned partitions.\n     */\n    synchronized int numAssignedPartitions() {\n        return this.assignment.size();\n    }\n\n    // Visible for testing\n    public synchronized List<TopicPartition> fetchablePartitions(Predicate<TopicPartition> isAvailable) {\n        // Since this is in the hot-path for fetching, we do this instead of using java.util.stream API\n        List<TopicPartition> result = new ArrayList<>();\n        assignment.forEach((topicPartition, topicPartitionState) -> {\n            // Cheap check is first to avoid evaluating the predicate if possible\n            if (topicPartitionState.isFetchable() && isAvailable.test(topicPartition)) {\n                result.add(topicPartition);\n            }\n        });\n        return result;\n    }\n\n    public synchronized boolean hasAutoAssignedPartitions() {\n        return this.subscriptionType == SubscriptionType.AUTO_TOPICS || this.subscriptionType == SubscriptionType.AUTO_PATTERN;\n    }\n\n    public synchronized void position(TopicPartition tp, FetchPosition position) {\n        assignedState(tp).position(position);\n    }\n\n    /**\n     * Enter the offset validation state if the leader for this partition is known to support a usable version of the\n     * OffsetsForLeaderEpoch API. If the leader node does not support the API, simply complete the offset validation.\n     *\n     * @param apiVersions supported API versions\n     * @param tp topic partition to validate\n     * @param leaderAndEpoch leader epoch of the topic partition\n     * @return true if we enter the offset validation state\n     */\n    public synchronized boolean maybeValidatePositionForCurrentLeader(ApiVersions apiVersions,\n                                                                      TopicPartition tp,\n                                                                      Metadata.LeaderAndEpoch leaderAndEpoch) {\n        TopicPartitionState state = assignedStateOrNull(tp);\n        if (state == null) {\n            log.debug(\"Skipping validating position for partition {} which is not currently assigned.\", tp);\n            return false;\n        }\n        if (leaderAndEpoch.leader.isPresent()) {\n            NodeApiVersions nodeApiVersions = apiVersions.get(leaderAndEpoch.leader.get().idString());\n            if (nodeApiVersions == null || hasUsableOffsetForLeaderEpochVersion(nodeApiVersions)) {\n                return state.maybeValidatePosition(leaderAndEpoch);\n            } else {\n                // If the broker does not support a newer version of OffsetsForLeaderEpoch, we skip validation\n                state.updatePositionLeaderNoValidation(leaderAndEpoch);\n                return false;\n            }\n        } else {\n            return state.maybeValidatePosition(leaderAndEpoch);\n        }\n    }\n\n    /**\n     * Attempt to complete validation with the end offset returned from the OffsetForLeaderEpoch request.\n     * @return Log truncation details if detected and no reset policy is defined.\n     */\n    public synchronized Optional<LogTruncation> maybeCompleteValidation(TopicPartition tp,\n                                                                        FetchPosition requestPosition,\n                                                                        EpochEndOffset epochEndOffset) {\n        TopicPartitionState state = assignedStateOrNull(tp);\n        if (state == null) {\n            log.debug(\"Skipping completed validation for partition {} which is not currently assigned.\", tp);\n        } else if (!state.awaitingValidation()) {\n            log.debug(\"Skipping completed validation for partition {} which is no longer expecting validation.\", tp);\n        } else {\n            SubscriptionState.FetchPosition currentPosition = state.position;\n            if (!currentPosition.equals(requestPosition)) {\n                log.debug(\"Skipping completed validation for partition {} since the current position {} \" +\n                          \"no longer matches the position {} when the request was sent\",\n                          tp, currentPosition, requestPosition);\n            } else if (epochEndOffset.endOffset() == UNDEFINED_EPOCH_OFFSET ||\n                        epochEndOffset.leaderEpoch() == UNDEFINED_EPOCH) {\n                if (hasDefaultOffsetResetPolicy()) {\n                    log.info(\"Truncation detected for partition {} at offset {}, resetting offset\",\n                             tp, currentPosition);\n                    requestOffsetReset(tp);\n                } else {\n                    log.warn(\"Truncation detected for partition {} at offset {}, but no reset policy is set\",\n                             tp, currentPosition);\n                    return Optional.of(new LogTruncation(tp, requestPosition, Optional.empty()));\n                }\n            } else if (epochEndOffset.endOffset() < currentPosition.offset) {\n                if (hasDefaultOffsetResetPolicy()) {\n                    SubscriptionState.FetchPosition newPosition = new SubscriptionState.FetchPosition(\n                            epochEndOffset.endOffset(), Optional.of(epochEndOffset.leaderEpoch()),\n                            currentPosition.currentLeader);\n                    log.info(\"Truncation detected for partition {} at offset {}, resetting offset to \" +\n                             \"the first offset known to diverge {}\", tp, currentPosition, newPosition);\n                    state.seekValidated(newPosition);\n                } else {\n                    OffsetAndMetadata divergentOffset = new OffsetAndMetadata(epochEndOffset.endOffset(),\n                        Optional.of(epochEndOffset.leaderEpoch()), null);\n                    log.warn(\"Truncation detected for partition {} at offset {} (the end offset from the \" +\n                             \"broker is {}), but no reset policy is set\", tp, currentPosition, divergentOffset);\n                    return Optional.of(new LogTruncation(tp, requestPosition, Optional.of(divergentOffset)));\n                }\n            } else {\n                state.completeValidation();\n            }\n        }\n\n        return Optional.empty();\n    }\n\n    public synchronized boolean awaitingValidation(TopicPartition tp) {\n        return assignedState(tp).awaitingValidation();\n    }\n\n    public synchronized void completeValidation(TopicPartition tp) {\n        assignedState(tp).completeValidation();\n    }\n\n    public synchronized FetchPosition validPosition(TopicPartition tp) {\n        return assignedState(tp).validPosition();\n    }\n\n    public synchronized FetchPosition position(TopicPartition tp) {\n        return assignedState(tp).position;\n    }\n\n    public synchronized FetchPosition positionOrNull(TopicPartition tp) {\n        final TopicPartitionState state = assignedStateOrNull(tp);\n        if (state == null) {\n            return null;\n        }\n        return assignedState(tp).position;\n    }\n\n    public synchronized Long partitionLag(TopicPartition tp, IsolationLevel isolationLevel) {\n        TopicPartitionState topicPartitionState = assignedState(tp);\n        if (topicPartitionState.position == null) {\n            return null;\n        } else if (isolationLevel == IsolationLevel.READ_COMMITTED) {\n            return topicPartitionState.lastStableOffset == null ? null : topicPartitionState.lastStableOffset - topicPartitionState.position.offset;\n        } else {\n            return topicPartitionState.highWatermark == null ? null : topicPartitionState.highWatermark - topicPartitionState.position.offset;\n        }\n    }\n\n    public synchronized Long partitionEndOffset(TopicPartition tp, IsolationLevel isolationLevel) {\n        TopicPartitionState topicPartitionState = assignedState(tp);\n        if (isolationLevel == IsolationLevel.READ_COMMITTED) {\n            return topicPartitionState.lastStableOffset;\n        } else {\n            return topicPartitionState.highWatermark;\n        }\n    }\n\n    public synchronized void requestPartitionEndOffset(TopicPartition tp) {\n        TopicPartitionState topicPartitionState = assignedState(tp);\n        topicPartitionState.requestEndOffset();\n    }\n\n    public synchronized boolean partitionEndOffsetRequested(TopicPartition tp) {\n        TopicPartitionState topicPartitionState = assignedState(tp);\n        return topicPartitionState.endOffsetRequested();\n    }\n\n    synchronized Long partitionLead(TopicPartition tp) {\n        TopicPartitionState topicPartitionState = assignedState(tp);\n        return topicPartitionState.logStartOffset == null ? null : topicPartitionState.position.offset - topicPartitionState.logStartOffset;\n    }\n\n    synchronized void updateHighWatermark(TopicPartition tp, long highWatermark) {\n        assignedState(tp).highWatermark(highWatermark);\n    }\n\n    synchronized boolean tryUpdatingHighWatermark(TopicPartition tp, long highWatermark) {\n        final TopicPartitionState state = assignedStateOrNull(tp);\n        if (state != null) {\n            assignedState(tp).highWatermark(highWatermark);\n            return true;\n        }\n        return false;\n    }\n\n    synchronized boolean tryUpdatingLogStartOffset(TopicPartition tp, long highWatermark) {\n        final TopicPartitionState state = assignedStateOrNull(tp);\n        if (state != null) {\n            assignedState(tp).logStartOffset(highWatermark);\n            return true;\n        }\n        return false;\n    }\n\n    synchronized void updateLastStableOffset(TopicPartition tp, long lastStableOffset) {\n        assignedState(tp).lastStableOffset(lastStableOffset);\n    }\n\n    synchronized boolean tryUpdatingLastStableOffset(TopicPartition tp, long lastStableOffset) {\n        final TopicPartitionState state = assignedStateOrNull(tp);\n        if (state != null) {\n            assignedState(tp).lastStableOffset(lastStableOffset);\n            return true;\n        }\n        return false;\n    }\n\n    /**\n     * Set the preferred read replica with a lease timeout. After this time, the replica will no longer be valid and\n     * {@link #preferredReadReplica(TopicPartition, long)} will return an empty result.\n     *\n     * @param tp The topic partition\n     * @param preferredReadReplicaId The preferred read replica\n     * @param timeMs The time at which this preferred replica is no longer valid\n     */\n    public synchronized void updatePreferredReadReplica(TopicPartition tp, int preferredReadReplicaId, LongSupplier timeMs) {\n        assignedState(tp).updatePreferredReadReplica(preferredReadReplicaId, timeMs);\n    }\n\n    /**\n     * Tries to set the preferred read replica with a lease timeout. After this time, the replica will no longer be valid and\n     * {@link #preferredReadReplica(TopicPartition, long)} will return an empty result. If the preferred replica of\n     * the partition could not be updated (e.g. because the partition is not assigned) this method will return\n     * {@code false}, otherwise it will return {@code true}.\n     *\n     * @param tp The topic partition\n     * @param preferredReadReplicaId The preferred read replica\n     * @param timeMs The time at which this preferred replica is no longer valid\n     * @return {@code true} if the preferred read replica was updated, {@code false} otherwise.\n     */\n    public synchronized boolean tryUpdatingPreferredReadReplica(TopicPartition tp,\n                                                             int preferredReadReplicaId,\n                                                             LongSupplier timeMs) {\n        final TopicPartitionState state = assignedStateOrNull(tp);\n        if (state != null) {\n            assignedState(tp).updatePreferredReadReplica(preferredReadReplicaId, timeMs);\n            return true;\n        }\n        return false;\n    }\n\n    /**\n     * Get the preferred read replica\n     *\n     * @param tp The topic partition\n     * @param timeMs The current time\n     * @return Returns the current preferred read replica, if it has been set and if it has not expired.\n     */\n    public synchronized Optional<Integer> preferredReadReplica(TopicPartition tp, long timeMs) {\n        final TopicPartitionState topicPartitionState = assignedStateOrNull(tp);\n        if (topicPartitionState == null) {\n            return Optional.empty();\n        } else {\n            return topicPartitionState.preferredReadReplica(timeMs);\n        }\n    }\n\n    /**\n     * Unset the preferred read replica. This causes the fetcher to go back to the leader for fetches.\n     *\n     * @param tp The topic partition\n     * @return the removed preferred read replica if set, Empty otherwise.\n     */\n    public synchronized Optional<Integer> clearPreferredReadReplica(TopicPartition tp) {\n        final TopicPartitionState topicPartitionState = assignedStateOrNull(tp);\n        if (topicPartitionState == null) {\n            return Optional.empty();\n        } else {\n            return topicPartitionState.clearPreferredReadReplica();\n        }\n    }\n\n    public synchronized Map<TopicPartition, OffsetAndMetadata> allConsumed() {\n        Map<TopicPartition, OffsetAndMetadata> allConsumed = new HashMap<>();\n        assignment.forEach((topicPartition, partitionState) -> {\n            if (partitionState.hasValidPosition())\n                allConsumed.put(topicPartition, new OffsetAndMetadata(partitionState.position.offset,\n                        partitionState.position.offsetEpoch, \"\"));\n        });\n        return allConsumed;\n    }\n\n    public synchronized void requestOffsetReset(TopicPartition partition, OffsetResetStrategy offsetResetStrategy) {\n        assignedState(partition).reset(offsetResetStrategy);\n    }\n\n    public synchronized void requestOffsetReset(Collection<TopicPartition> partitions, OffsetResetStrategy offsetResetStrategy) {\n        partitions.forEach(tp -> {\n            log.info(\"Seeking to {} offset of partition {}\", offsetResetStrategy, tp);\n            assignedState(tp).reset(offsetResetStrategy);\n        });\n    }\n\n    public void requestOffsetReset(TopicPartition partition) {\n        requestOffsetReset(partition, defaultResetStrategy);\n    }\n\n    public synchronized void requestOffsetResetIfPartitionAssigned(TopicPartition partition) {\n        final TopicPartitionState state = assignedStateOrNull(partition);\n        if (state != null) {\n            state.reset(defaultResetStrategy);\n        }\n    }\n\n\n    synchronized void setNextAllowedRetry(Set<TopicPartition> partitions, long nextAllowResetTimeMs) {\n        for (TopicPartition partition : partitions) {\n            assignedState(partition).setNextAllowedRetry(nextAllowResetTimeMs);\n        }\n    }\n\n    boolean hasDefaultOffsetResetPolicy() {\n        return defaultResetStrategy != OffsetResetStrategy.NONE;\n    }\n\n    public synchronized boolean isOffsetResetNeeded(TopicPartition partition) {\n        return assignedState(partition).awaitingReset();\n    }\n\n    public synchronized OffsetResetStrategy resetStrategy(TopicPartition partition) {\n        return assignedState(partition).resetStrategy();\n    }\n\n    public synchronized boolean hasAllFetchPositions() {\n        // Since this is in the hot-path for fetching, we do this instead of using java.util.stream API\n        Iterator<TopicPartitionState> it = assignment.stateIterator();\n        while (it.hasNext()) {\n            if (!it.next().hasValidPosition()) {\n                return false;\n            }\n        }\n        return true;\n    }\n\n    public synchronized Set<TopicPartition> initializingPartitions() {\n        return collectPartitions(state -> shouldInitialize(state) && !state.pendingOnAssignedCallback);\n    }\n\n    private Set<TopicPartition> collectPartitions(Predicate<TopicPartitionState> filter) {\n        Set<TopicPartition> result = new HashSet<>();\n        assignment.forEach((topicPartition, topicPartitionState) -> {\n            if (filter.test(topicPartitionState)) {\n                result.add(topicPartition);\n            }\n        });\n        return result;\n    }\n\n\n    public synchronized void resetInitializingPositions() {\n        final Set<TopicPartition> partitionsWithNoOffsets = new HashSet<>();\n        assignment.forEach((tp, partitionState) -> {\n            if (shouldInitialize(partitionState)) {\n                if (defaultResetStrategy == OffsetResetStrategy.NONE)\n                    partitionsWithNoOffsets.add(tp);\n                else\n                    requestOffsetReset(tp);\n            }\n        });\n\n        if (!partitionsWithNoOffsets.isEmpty())\n            throw new NoOffsetForPartitionException(partitionsWithNoOffsets);\n    }\n\n    private boolean shouldInitialize(TopicPartitionState partitionState) {\n        return partitionState.fetchState.equals(FetchStates.INITIALIZING);\n    }\n\n    public synchronized Set<TopicPartition> partitionsNeedingReset(long nowMs) {\n        return collectPartitions(state -> state.awaitingReset() && !state.awaitingRetryBackoff(nowMs));\n    }\n\n    public synchronized Set<TopicPartition> partitionsNeedingValidation(long nowMs) {\n        return collectPartitions(state -> state.awaitingValidation() && !state.awaitingRetryBackoff(nowMs));\n    }\n\n    public synchronized boolean isAssigned(TopicPartition tp) {\n        return assignment.contains(tp);\n    }\n\n    public synchronized boolean isPaused(TopicPartition tp) {\n        TopicPartitionState assignedOrNull = assignedStateOrNull(tp);\n        return assignedOrNull != null && assignedOrNull.isPaused();\n    }\n\n    synchronized boolean isFetchable(TopicPartition tp) {\n        TopicPartitionState assignedOrNull = assignedStateOrNull(tp);\n        return assignedOrNull != null && assignedOrNull.isFetchable();\n    }\n\n    public synchronized boolean hasValidPosition(TopicPartition tp) {\n        TopicPartitionState assignedOrNull = assignedStateOrNull(tp);\n        return assignedOrNull != null && assignedOrNull.hasValidPosition();\n    }\n\n    public synchronized void pause(TopicPartition tp) {\n        assignedState(tp).pause();\n    }\n\n    public synchronized void markPendingRevocation(Set<TopicPartition> tps) {\n        tps.forEach(tp -> assignedState(tp).markPendingRevocation());\n    }\n\n    // Visible for testing\n    synchronized void markPendingOnAssignedCallback(Collection<TopicPartition> tps,\n                                                    boolean pendingOnAssignedCallback) {\n        tps.forEach(tp -> assignedState(tp).markPendingOnAssignedCallback(pendingOnAssignedCallback));\n    }\n\n    /**\n     * Change the assignment to the specified partitions returned from the coordinator and mark\n     * them as awaiting onPartitionsAssigned callback. This will ensure that the partitions are\n     * included in the assignment, but are not fetchable or initialize positions while the\n     * callback runs. This is expected to be used by the async consumer.\n     *\n     * @param fullAssignment  Full collection of partitions assigned. Includes previously owned\n     *                        and newly added partitions.\n     * @param addedPartitions Subset of the fullAssignment containing the added partitions. These\n     *                        are not fetchable until the onPartitionsAssigned callback completes.\n     */\n    public synchronized void assignFromSubscribedAwaitingCallback(Collection<TopicPartition> fullAssignment,\n                                                                  Collection<TopicPartition> addedPartitions) {\n        assignFromSubscribed(fullAssignment);\n        markPendingOnAssignedCallback(addedPartitions, true);\n    }\n\n    /**\n     * Enable fetching and updating positions for the given partitions that were added to the\n     * assignment, but waiting for the onPartitionsAssigned callback to complete. This is\n     * expected to be used by the async consumer.\n     */\n    public synchronized void enablePartitionsAwaitingCallback(Collection<TopicPartition> partitions) {\n        markPendingOnAssignedCallback(partitions, false);\n    }\n\n    public synchronized void resume(TopicPartition tp) {\n        assignedState(tp).resume();\n    }\n\n    synchronized void requestFailed(Set<TopicPartition> partitions, long nextRetryTimeMs) {\n        for (TopicPartition partition : partitions) {\n            // by the time the request failed, the assignment may no longer\n            // contain this partition any more, in which case we would just ignore.\n            final TopicPartitionState state = assignedStateOrNull(partition);\n            if (state != null)\n                state.requestFailed(nextRetryTimeMs);\n        }\n    }\n\n    synchronized void movePartitionToEnd(TopicPartition tp) {\n        assignment.moveToEnd(tp);\n    }\n\n    public synchronized Optional<ConsumerRebalanceListener> rebalanceListener() {\n        return rebalanceListener;\n    }\n\n    private static class TopicPartitionState {\n\n        private FetchState fetchState;\n        private FetchPosition position; // last consumed position\n\n        private Long highWatermark; // the high watermark from last fetch\n        private Long logStartOffset; // the log start offset\n        private Long lastStableOffset;\n        private boolean paused;  // whether this partition has been paused by the user\n        private boolean pendingRevocation;\n        private boolean pendingOnAssignedCallback;\n        private OffsetResetStrategy resetStrategy;  // the strategy to use if the offset needs resetting\n        private Long nextRetryTimeMs;\n        private Integer preferredReadReplica;\n        private Long preferredReadReplicaExpireTimeMs;\n        private boolean endOffsetRequested;\n        \n        TopicPartitionState() {\n            this.paused = false;\n            this.pendingRevocation = false;\n            this.pendingOnAssignedCallback = false;\n            this.endOffsetRequested = false;\n            this.fetchState = FetchStates.INITIALIZING;\n            this.position = null;\n            this.highWatermark = null;\n            this.logStartOffset = null;\n            this.lastStableOffset = null;\n            this.resetStrategy = null;\n            this.nextRetryTimeMs = null;\n            this.preferredReadReplica = null;\n        }\n\n        public boolean endOffsetRequested() {\n            return endOffsetRequested;\n        }\n\n        public void requestEndOffset() {\n            endOffsetRequested = true;\n        }\n\n        private void transitionState(FetchState newState, Runnable runIfTransitioned) {\n            FetchState nextState = this.fetchState.transitionTo(newState);\n            if (nextState.equals(newState)) {\n                this.fetchState = nextState;\n                runIfTransitioned.run();\n                if (this.position == null && nextState.requiresPosition()) {\n                    throw new IllegalStateException(\"Transitioned subscription state to \" + nextState + \", but position is null\");\n                } else if (!nextState.requiresPosition()) {\n                    this.position = null;\n                }\n            }\n        }\n\n        private Optional<Integer> preferredReadReplica(long timeMs) {\n            if (preferredReadReplicaExpireTimeMs != null && timeMs > preferredReadReplicaExpireTimeMs) {\n                preferredReadReplica = null;\n                return Optional.empty();\n            } else {\n                return Optional.ofNullable(preferredReadReplica);\n            }\n        }\n\n        private void updatePreferredReadReplica(int preferredReadReplica, LongSupplier timeMs) {\n            if (this.preferredReadReplica == null || preferredReadReplica != this.preferredReadReplica) {\n                this.preferredReadReplica = preferredReadReplica;\n                this.preferredReadReplicaExpireTimeMs = timeMs.getAsLong();\n            }\n        }\n\n        private Optional<Integer> clearPreferredReadReplica() {\n            if (preferredReadReplica != null) {\n                int removedReplicaId = this.preferredReadReplica;\n                this.preferredReadReplica = null;\n                this.preferredReadReplicaExpireTimeMs = null;\n                return Optional.of(removedReplicaId);\n            } else {\n                return Optional.empty();\n            }\n        }\n\n        private void reset(OffsetResetStrategy strategy) {\n            transitionState(FetchStates.AWAIT_RESET, () -> {\n                this.resetStrategy = strategy;\n                this.nextRetryTimeMs = null;\n            });\n        }\n\n        /**\n         * Check if the position exists and needs to be validated. If so, enter the AWAIT_VALIDATION state. This method\n         * also will update the position with the current leader and epoch.\n         *\n         * @param currentLeaderAndEpoch leader and epoch to compare the offset with\n         * @return true if the position is now awaiting validation\n         */\n        private boolean maybeValidatePosition(Metadata.LeaderAndEpoch currentLeaderAndEpoch) {\n            if (this.fetchState.equals(FetchStates.AWAIT_RESET)) {\n                return false;\n            }\n\n            if (!currentLeaderAndEpoch.leader.isPresent()) {\n                return false;\n            }\n\n            if (position != null && !position.currentLeader.equals(currentLeaderAndEpoch)) {\n                FetchPosition newPosition = new FetchPosition(position.offset, position.offsetEpoch, currentLeaderAndEpoch);\n                validatePosition(newPosition);\n                preferredReadReplica = null;\n            }\n            return this.fetchState.equals(FetchStates.AWAIT_VALIDATION);\n        }\n\n        /**\n         * For older versions of the API, we cannot perform offset validation so we simply transition directly to FETCHING\n         */\n        private void updatePositionLeaderNoValidation(Metadata.LeaderAndEpoch currentLeaderAndEpoch) {\n            if (position != null) {\n                transitionState(FetchStates.FETCHING, () -> {\n                    this.position = new FetchPosition(position.offset, position.offsetEpoch, currentLeaderAndEpoch);\n                    this.nextRetryTimeMs = null;\n                });\n            }\n        }\n\n        private void validatePosition(FetchPosition position) {\n            if (position.offsetEpoch.isPresent() && position.currentLeader.epoch.isPresent()) {\n                transitionState(FetchStates.AWAIT_VALIDATION, () -> {\n                    this.position = position;\n                    this.nextRetryTimeMs = null;\n                });\n            } else {\n                // If we have no epoch information for the current position, then we can skip validation\n                transitionState(FetchStates.FETCHING, () -> {\n                    this.position = position;\n                    this.nextRetryTimeMs = null;\n                });\n            }\n        }\n\n        /**\n         * Clear the awaiting validation state and enter fetching.\n         */\n        private void completeValidation() {\n            if (hasPosition()) {\n                transitionState(FetchStates.FETCHING, () -> this.nextRetryTimeMs = null);\n            }\n        }\n\n        private boolean awaitingValidation() {\n            return fetchState.equals(FetchStates.AWAIT_VALIDATION);\n        }\n\n        private boolean awaitingRetryBackoff(long nowMs) {\n            return nextRetryTimeMs != null && nowMs < nextRetryTimeMs;\n        }\n\n        private boolean awaitingReset() {\n            return fetchState.equals(FetchStates.AWAIT_RESET);\n        }\n\n        private void setNextAllowedRetry(long nextAllowedRetryTimeMs) {\n            this.nextRetryTimeMs = nextAllowedRetryTimeMs;\n        }\n\n        private void requestFailed(long nextAllowedRetryTimeMs) {\n            this.nextRetryTimeMs = nextAllowedRetryTimeMs;\n        }\n\n        private boolean hasValidPosition() {\n            return fetchState.hasValidPosition();\n        }\n\n        private boolean hasPosition() {\n            return position != null;\n        }\n\n        private boolean isPaused() {\n            return paused;\n        }\n\n        private void seekValidated(FetchPosition position) {\n            transitionState(FetchStates.FETCHING, () -> {\n                this.position = position;\n                this.resetStrategy = null;\n                this.nextRetryTimeMs = null;\n            });\n        }\n\n        private void seekUnvalidated(FetchPosition fetchPosition) {\n            seekValidated(fetchPosition);\n            validatePosition(fetchPosition);\n        }\n\n        private void position(FetchPosition position) {\n            if (!hasValidPosition())\n                throw new IllegalStateException(\"Cannot set a new position without a valid current position\");\n            this.position = position;\n        }\n\n        private FetchPosition validPosition() {\n            if (hasValidPosition()) {\n                return position;\n            } else {\n                return null;\n            }\n        }\n\n        private void pause() {\n            this.paused = true;\n        }\n\n        private void markPendingRevocation() {\n            this.pendingRevocation = true;\n        }\n\n        private void markPendingOnAssignedCallback(boolean pendingOnAssignedCallback) {\n            this.pendingOnAssignedCallback = pendingOnAssignedCallback;\n        }\n\n        private void resume() {\n            this.paused = false;\n        }\n\n        private boolean isFetchable() {\n            return !paused && !pendingRevocation && !pendingOnAssignedCallback && hasValidPosition();\n        }\n\n        private void highWatermark(Long highWatermark) {\n            this.highWatermark = highWatermark;\n            this.endOffsetRequested = false;\n        }\n\n        private void logStartOffset(Long logStartOffset) {\n            this.logStartOffset = logStartOffset;\n        }\n\n        private void lastStableOffset(Long lastStableOffset) {\n            this.lastStableOffset = lastStableOffset;\n            this.endOffsetRequested = false;\n        }\n\n        private OffsetResetStrategy resetStrategy() {\n            return resetStrategy;\n        }\n    }\n\n    /**\n     * The fetch state of a partition. This class is used to determine valid state transitions and expose the some of\n     * the behavior of the current fetch state. Actual state variables are stored in the {@link TopicPartitionState}.\n     */\n    interface FetchState {\n        default FetchState transitionTo(FetchState newState) {\n            if (validTransitions().contains(newState)) {\n                return newState;\n            } else {\n                return this;\n            }\n        }\n\n        /**\n         * Return the valid states which this state can transition to\n         */\n        Collection<FetchState> validTransitions();\n\n        /**\n         * Test if this state requires a position to be set\n         */\n        boolean requiresPosition();\n\n        /**\n         * Test if this state is considered to have a valid position which can be used for fetching\n         */\n        boolean hasValidPosition();\n    }\n\n    /**\n     * An enumeration of all the possible fetch states. The state transitions are encoded in the values returned by\n     * {@link FetchState#validTransitions}.\n     */\n    enum FetchStates implements FetchState {\n        INITIALIZING() {\n            @Override\n            public Collection<FetchState> validTransitions() {\n                return Arrays.asList(FetchStates.FETCHING, FetchStates.AWAIT_RESET, FetchStates.AWAIT_VALIDATION);\n            }\n\n            @Override\n            public boolean requiresPosition() {\n                return false;\n            }\n\n            @Override\n            public boolean hasValidPosition() {\n                return false;\n            }\n        },\n\n        FETCHING() {\n            @Override\n            public Collection<FetchState> validTransitions() {\n                return Arrays.asList(FetchStates.FETCHING, FetchStates.AWAIT_RESET, FetchStates.AWAIT_VALIDATION);\n            }\n\n            @Override\n            public boolean requiresPosition() {\n                return true;\n            }\n\n            @Override\n            public boolean hasValidPosition() {\n                return true;\n            }\n        },\n\n        AWAIT_RESET() {\n            @Override\n            public Collection<FetchState> validTransitions() {\n                return Arrays.asList(FetchStates.FETCHING, FetchStates.AWAIT_RESET);\n            }\n\n            @Override\n            public boolean requiresPosition() {\n                return false;\n            }\n\n            @Override\n            public boolean hasValidPosition() {\n                return false;\n            }\n        },\n\n        AWAIT_VALIDATION() {\n            @Override\n            public Collection<FetchState> validTransitions() {\n                return Arrays.asList(FetchStates.FETCHING, FetchStates.AWAIT_RESET, FetchStates.AWAIT_VALIDATION);\n            }\n\n            @Override\n            public boolean requiresPosition() {\n                return true;\n            }\n\n            @Override\n            public boolean hasValidPosition() {\n                return false;\n            }\n        }\n    }\n\n    /**\n     * Represents the position of a partition subscription.\n     *\n     * This includes the offset and epoch from the last record in\n     * the batch from a FetchResponse. It also includes the leader epoch at the time the batch was consumed.\n     */\n    public static class FetchPosition {\n        public final long offset;\n        final Optional<Integer> offsetEpoch;\n        final Metadata.LeaderAndEpoch currentLeader;\n\n        FetchPosition(long offset) {\n            this(offset, Optional.empty(), Metadata.LeaderAndEpoch.noLeaderOrEpoch());\n        }\n\n        public FetchPosition(long offset, Optional<Integer> offsetEpoch, Metadata.LeaderAndEpoch currentLeader) {\n            this.offset = offset;\n            this.offsetEpoch = Objects.requireNonNull(offsetEpoch);\n            this.currentLeader = Objects.requireNonNull(currentLeader);\n        }\n\n        @Override\n        public boolean equals(Object o) {\n            if (this == o) return true;\n            if (o == null || getClass() != o.getClass()) return false;\n            FetchPosition that = (FetchPosition) o;\n            return offset == that.offset &&\n                    offsetEpoch.equals(that.offsetEpoch) &&\n                    currentLeader.equals(that.currentLeader);\n        }\n\n        @Override\n        public int hashCode() {\n            return Objects.hash(offset, offsetEpoch, currentLeader);\n        }\n\n        @Override\n        public String toString() {\n            return \"FetchPosition{\" +\n                    \"offset=\" + offset +\n                    \", offsetEpoch=\" + offsetEpoch +\n                    \", currentLeader=\" + currentLeader +\n                    '}';\n        }\n    }\n\n    public static class LogTruncation {\n        public final TopicPartition topicPartition;\n        public final FetchPosition fetchPosition;\n        public final Optional<OffsetAndMetadata> divergentOffsetOpt;\n\n        public LogTruncation(TopicPartition topicPartition,\n                             FetchPosition fetchPosition,\n                             Optional<OffsetAndMetadata> divergentOffsetOpt) {\n            this.topicPartition = topicPartition;\n            this.fetchPosition = fetchPosition;\n            this.divergentOffsetOpt = divergentOffsetOpt;\n        }\n\n        @Override\n        public String toString() {\n            StringBuilder bldr = new StringBuilder()\n                .append(\"(partition=\")\n                .append(topicPartition)\n                .append(\", fetchOffset=\")\n                .append(fetchPosition.offset)\n                .append(\", fetchEpoch=\")\n                .append(fetchPosition.offsetEpoch);\n\n            if (divergentOffsetOpt.isPresent()) {\n                OffsetAndMetadata divergentOffset = divergentOffsetOpt.get();\n                bldr.append(\", divergentOffset=\")\n                    .append(divergentOffset.offset())\n                    .append(\", divergentEpoch=\")\n                    .append(divergentOffset.leaderEpoch());\n            } else {\n                bldr.append(\", divergentOffset=unknown\")\n                    .append(\", divergentEpoch=unknown\");\n            }\n\n            return bldr.append(\")\").toString();\n\n        }\n    }\n}",
                "methodCount": 141
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 6,
                "candidates": [
                    {
                        "lineStart": 382,
                        "lineEnd": 384,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method seekUnvalidated to class FetchPosition",
                        "description": "Move method seekUnvalidated to org.apache.kafka.clients.consumer.internals.SubscriptionState.FetchPosition\nRationale: The method 'seekUnvalidated' directly takes an instance of FetchPosition, and it makes sense to put it within 'FetchPosition' to encapsulate any behavior related to positions within that class. This enhances cohesion, as methods related to FetchPosition will be found within the same class, making the code more maintainable and intuitive. Moreover, 'seekUnvalidated' seems to operate on FetchPosition and alter its state, hence making FetchPosition the appropriate location to house this method.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 706,
                        "lineEnd": 708,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method requestOffsetReset to class OffsetResetStrategy",
                        "description": "Move method requestOffsetReset to org.apache.kafka.clients.consumer.OffsetResetStrategy\nRationale: The method requestOffsetReset relies on the OffsetResetStrategy for its operation. Since the method directly involves manipulating or utilizing the reset strategy, it will be more cohesive to place it within the OffsetResetStrategy class. This enhances encapsulation and keeps related functionality together, making the code easier to maintain and understand. Currently, if the method is isolated from the strategy, it could lead to poor separation of concerns and potentially scattered logic related to offset reset handling.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1092,
                        "lineEnd": 1094,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method isFetchable to class FetchState",
                        "description": "Move method isFetchable to org.apache.kafka.clients.consumer.internals.SubscriptionState.FetchState\nRationale: The method isFetchable() evaluates the fetchability of a resource by combining multiple fetch-related checks and relies on the hasValidPosition() method, which is part of the FetchState interface. Moving isFetchable() to FetchState would not only ensure better encapsulation of fetch logic within FetchState but also leverage the existing interface methods such as hasValidPosition(), making it a natural fit for the state transition and validation logic associated with fetch states. FetchPosition focuses more on the metadata of a position like offsets and leader information and is not directly concerned with the state transitions or fetchability checks of a resource.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1062,
                        "lineEnd": 1066,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method position to class FetchState",
                        "description": "Move method position to org.apache.kafka.clients.consumer.internals.SubscriptionState.FetchState\nRationale: The method `position(FetchPosition position)` involves checking the validity of the current position and setting a new position accordingly. This functionality logically fits within the domain of managing and transitioning between different fetch states, as handled by `FetchState`. Since `FetchState` already has methods related to position validation (`hasValidPosition()`) and state transitions (`transitionTo()`), it makes sense to move the position management method to `FetchState`. Applying this change will consolidate state transition logic and position setting in one cohesive place.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1057,
                        "lineEnd": 1060,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method seekUnvalidated to class FetchState",
                        "description": "Move method seekUnvalidated to org.apache.kafka.clients.consumer.internals.SubscriptionState.FetchState\nRationale: The method seekUnvalidated involves the operations seekValidated and validatePosition, which are operations likely associated with determining and validating fetch states rather than the fetch positions. The FetchState class is designed to manage the states and behaviors related to fetching, making it a logical place for the method to reside. This helps encapsulate the logic of transitioning and validating states within a single place, adhering to the Single Responsibility Principle.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1021,
                        "lineEnd": 1023,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method awaitingRetryBackoff to class FetchState",
                        "description": "Move method awaitingRetryBackoff to org.apache.kafka.clients.consumer.internals.SubscriptionState.FetchState\nRationale: The awaitingRetryBackoff() method appears to be concerned with retry timings and conditions which are often related to the state of the fetch operation. Moreover, FetchState already includes various methods related to state transitions and conditions, such as validTransitions(), requiresPosition(), and hasValidPosition(). This method would naturally align with the behavior and responsibilities encapsulated by FetchState.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "prettyString",
                            "method_signature": "public synchronized prettyString()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "subscribe",
                            "method_signature": "public synchronized subscribe(Set<String> topics, Optional<ConsumerRebalanceListener> listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "subscribe",
                            "method_signature": "public synchronized subscribe(Pattern pattern, Optional<ConsumerRebalanceListener> listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "subscribeFromPattern",
                            "method_signature": "public synchronized subscribeFromPattern(Set<String> topics)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "changeSubscription",
                            "method_signature": "private changeSubscription(Set<String> topicsToSubscribe)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "groupSubscribe",
                            "method_signature": "synchronized groupSubscribe(Collection<String> topics)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assignFromUser",
                            "method_signature": "public synchronized assignFromUser(Set<TopicPartition> partitions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "checkAssignmentMatchedSubscription",
                            "method_signature": "public synchronized checkAssignmentMatchedSubscription(Collection<TopicPartition> assignments)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assignFromSubscribed",
                            "method_signature": "public synchronized assignFromSubscribed(Collection<TopicPartition> assignments)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasPatternSubscription",
                            "method_signature": "synchronized hasPatternSubscription()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasNoSubscriptionOrUserAssignment",
                            "method_signature": "public synchronized hasNoSubscriptionOrUserAssignment()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "unsubscribe",
                            "method_signature": "public synchronized unsubscribe()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "matchesSubscribedPattern",
                            "method_signature": "synchronized matchesSubscribedPattern(String topic)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "subscription",
                            "method_signature": "public synchronized subscription()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pausedPartitions",
                            "method_signature": "public synchronized pausedPartitions()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "metadataTopics",
                            "method_signature": "synchronized metadataTopics()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "needsMetadata",
                            "method_signature": "synchronized needsMetadata(String topic)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assignedState",
                            "method_signature": "private assignedState(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assignedStateOrNull",
                            "method_signature": "private assignedStateOrNull(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "seekValidated",
                            "method_signature": "public synchronized seekValidated(TopicPartition tp, FetchPosition position)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "seek",
                            "method_signature": "public seek(TopicPartition tp, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "seekUnvalidated",
                            "method_signature": "public seekUnvalidated(TopicPartition tp, FetchPosition position)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSeekUnvalidated",
                            "method_signature": "synchronized maybeSeekUnvalidated(TopicPartition tp, FetchPosition position, OffsetResetStrategy requestedResetStrategy)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assignedPartitions",
                            "method_signature": "public synchronized assignedPartitions()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assignedPartitionsList",
                            "method_signature": "public synchronized assignedPartitionsList()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "numAssignedPartitions",
                            "method_signature": "synchronized numAssignedPartitions()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchablePartitions",
                            "method_signature": "public synchronized fetchablePartitions(Predicate<TopicPartition> isAvailable)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasAutoAssignedPartitions",
                            "method_signature": "public synchronized hasAutoAssignedPartitions()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "position",
                            "method_signature": "public synchronized position(TopicPartition tp, FetchPosition position)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeValidatePositionForCurrentLeader",
                            "method_signature": "public synchronized maybeValidatePositionForCurrentLeader(ApiVersions apiVersions,\n                                                                      TopicPartition tp,\n                                                                      Metadata.LeaderAndEpoch leaderAndEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeCompleteValidation",
                            "method_signature": "public synchronized maybeCompleteValidation(TopicPartition tp,\n                                                                        FetchPosition requestPosition,\n                                                                        EpochEndOffset epochEndOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitingValidation",
                            "method_signature": "public synchronized awaitingValidation(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeValidation",
                            "method_signature": "public synchronized completeValidation(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validPosition",
                            "method_signature": "public synchronized validPosition(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "positionOrNull",
                            "method_signature": "public synchronized positionOrNull(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "partitionLag",
                            "method_signature": "public synchronized partitionLag(TopicPartition tp, IsolationLevel isolationLevel)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "partitionEndOffset",
                            "method_signature": "public synchronized partitionEndOffset(TopicPartition tp, IsolationLevel isolationLevel)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "requestPartitionEndOffset",
                            "method_signature": "public synchronized requestPartitionEndOffset(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "partitionEndOffsetRequested",
                            "method_signature": "public synchronized partitionEndOffsetRequested(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "partitionLead",
                            "method_signature": "synchronized partitionLead(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateHighWatermark",
                            "method_signature": "synchronized updateHighWatermark(TopicPartition tp, long highWatermark)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "tryUpdatingHighWatermark",
                            "method_signature": "synchronized tryUpdatingHighWatermark(TopicPartition tp, long highWatermark)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "tryUpdatingLogStartOffset",
                            "method_signature": "synchronized tryUpdatingLogStartOffset(TopicPartition tp, long highWatermark)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateLastStableOffset",
                            "method_signature": "synchronized updateLastStableOffset(TopicPartition tp, long lastStableOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "tryUpdatingLastStableOffset",
                            "method_signature": "synchronized tryUpdatingLastStableOffset(TopicPartition tp, long lastStableOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updatePreferredReadReplica",
                            "method_signature": "public synchronized updatePreferredReadReplica(TopicPartition tp, int preferredReadReplicaId, LongSupplier timeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "tryUpdatingPreferredReadReplica",
                            "method_signature": "public synchronized tryUpdatingPreferredReadReplica(TopicPartition tp,\n                                                             int preferredReadReplicaId,\n                                                             LongSupplier timeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "preferredReadReplica",
                            "method_signature": "public synchronized preferredReadReplica(TopicPartition tp, long timeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "clearPreferredReadReplica",
                            "method_signature": "public synchronized clearPreferredReadReplica(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "allConsumed",
                            "method_signature": "public synchronized allConsumed()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "requestOffsetReset",
                            "method_signature": "public synchronized requestOffsetReset(TopicPartition partition, OffsetResetStrategy offsetResetStrategy)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "requestOffsetReset",
                            "method_signature": "public synchronized requestOffsetReset(Collection<TopicPartition> partitions, OffsetResetStrategy offsetResetStrategy)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "requestOffsetReset",
                            "method_signature": "public requestOffsetReset(TopicPartition partition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "requestOffsetResetIfPartitionAssigned",
                            "method_signature": "public synchronized requestOffsetResetIfPartitionAssigned(TopicPartition partition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "setNextAllowedRetry",
                            "method_signature": "synchronized setNextAllowedRetry(Set<TopicPartition> partitions, long nextAllowResetTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasDefaultOffsetResetPolicy",
                            "method_signature": " hasDefaultOffsetResetPolicy()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isOffsetResetNeeded",
                            "method_signature": "public synchronized isOffsetResetNeeded(TopicPartition partition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resetStrategy",
                            "method_signature": "public synchronized resetStrategy(TopicPartition partition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasAllFetchPositions",
                            "method_signature": "public synchronized hasAllFetchPositions()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "initializingPartitions",
                            "method_signature": "public synchronized initializingPartitions()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "collectPartitions",
                            "method_signature": "private collectPartitions(Predicate<TopicPartitionState> filter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resetInitializingPositions",
                            "method_signature": "public synchronized resetInitializingPositions()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "shouldInitialize",
                            "method_signature": "private shouldInitialize(TopicPartitionState partitionState)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "partitionsNeedingReset",
                            "method_signature": "public synchronized partitionsNeedingReset(long nowMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "partitionsNeedingValidation",
                            "method_signature": "public synchronized partitionsNeedingValidation(long nowMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isAssigned",
                            "method_signature": "public synchronized isAssigned(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isPaused",
                            "method_signature": "public synchronized isPaused(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isFetchable",
                            "method_signature": "synchronized isFetchable(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasValidPosition",
                            "method_signature": "public synchronized hasValidPosition(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pause",
                            "method_signature": "public synchronized pause(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "markPendingRevocation",
                            "method_signature": "public synchronized markPendingRevocation(Set<TopicPartition> tps)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "markPendingOnAssignedCallback",
                            "method_signature": "synchronized markPendingOnAssignedCallback(Collection<TopicPartition> tps,\n                                                    boolean pendingOnAssignedCallback)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assignFromSubscribedAwaitingCallback",
                            "method_signature": "public synchronized assignFromSubscribedAwaitingCallback(Collection<TopicPartition> fullAssignment,\n                                                                  Collection<TopicPartition> addedPartitions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "enablePartitionsAwaitingCallback",
                            "method_signature": "public synchronized enablePartitionsAwaitingCallback(Collection<TopicPartition> partitions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resume",
                            "method_signature": "public synchronized resume(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "requestFailed",
                            "method_signature": "synchronized requestFailed(Set<TopicPartition> partitions, long nextRetryTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "movePartitionToEnd",
                            "method_signature": "synchronized movePartitionToEnd(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionState",
                            "method_signature": "private transitionState(FetchState newState, Runnable runIfTransitioned)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "preferredReadReplica",
                            "method_signature": "private preferredReadReplica(long timeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updatePreferredReadReplica",
                            "method_signature": "private updatePreferredReadReplica(int preferredReadReplica, LongSupplier timeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "clearPreferredReadReplica",
                            "method_signature": "private clearPreferredReadReplica()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "reset",
                            "method_signature": "private reset(OffsetResetStrategy strategy)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeValidatePosition",
                            "method_signature": "private maybeValidatePosition(Metadata.LeaderAndEpoch currentLeaderAndEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updatePositionLeaderNoValidation",
                            "method_signature": "private updatePositionLeaderNoValidation(Metadata.LeaderAndEpoch currentLeaderAndEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validatePosition",
                            "method_signature": "private validatePosition(FetchPosition position)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeValidation",
                            "method_signature": "private completeValidation()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitingValidation",
                            "method_signature": "private awaitingValidation()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitingRetryBackoff",
                            "method_signature": "private awaitingRetryBackoff(long nowMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitingReset",
                            "method_signature": "private awaitingReset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasValidPosition",
                            "method_signature": "private hasValidPosition()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasPosition",
                            "method_signature": "private hasPosition()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "seekValidated",
                            "method_signature": "private seekValidated(FetchPosition position)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "seekUnvalidated",
                            "method_signature": "private seekUnvalidated(FetchPosition fetchPosition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "position",
                            "method_signature": "private position(FetchPosition position)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validPosition",
                            "method_signature": "private validPosition()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isFetchable",
                            "method_signature": "private isFetchable()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "highWatermark",
                            "method_signature": "private highWatermark(Long highWatermark)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "lastStableOffset",
                            "method_signature": "private lastStableOffset(Long lastStableOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionTo",
                            "method_signature": "default transitionTo(FetchState newState)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "prettyString",
                            "method_signature": "public synchronized prettyString()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isFetchable",
                            "method_signature": "private isFetchable()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "markPendingOnAssignedCallback",
                            "method_signature": "synchronized markPendingOnAssignedCallback(Collection<TopicPartition> tps,\n                                                    boolean pendingOnAssignedCallback)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "position",
                            "method_signature": "private position(FetchPosition position)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "partitionLead",
                            "method_signature": "synchronized partitionLead(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "unsubscribe",
                            "method_signature": "public synchronized unsubscribe()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "seek",
                            "method_signature": "public seek(TopicPartition tp, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "seekUnvalidated",
                            "method_signature": "private seekUnvalidated(FetchPosition fetchPosition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitingRetryBackoff",
                            "method_signature": "private awaitingRetryBackoff(long nowMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasAutoAssignedPartitions",
                            "method_signature": "public synchronized hasAutoAssignedPartitions()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateLastStableOffset",
                            "method_signature": "synchronized updateLastStableOffset(TopicPartition tp, long lastStableOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateHighWatermark",
                            "method_signature": "synchronized updateHighWatermark(TopicPartition tp, long highWatermark)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "subscribeFromPattern",
                            "method_signature": "public synchronized subscribeFromPattern(Set<String> topics)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "requestOffsetReset",
                            "method_signature": "public requestOffsetReset(TopicPartition partition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "seekUnvalidated",
                            "method_signature": "public seekUnvalidated(TopicPartition tp, FetchPosition position)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "prettyString",
                            "method_signature": "public synchronized prettyString()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isFetchable",
                            "method_signature": "private isFetchable()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "markPendingOnAssignedCallback",
                            "method_signature": "synchronized markPendingOnAssignedCallback(Collection<TopicPartition> tps,\n                                                    boolean pendingOnAssignedCallback)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "position",
                            "method_signature": "private position(FetchPosition position)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "partitionLead",
                            "method_signature": "synchronized partitionLead(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "unsubscribe",
                            "method_signature": "public synchronized unsubscribe()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "seek",
                            "method_signature": "public seek(TopicPartition tp, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "seekUnvalidated",
                            "method_signature": "private seekUnvalidated(FetchPosition fetchPosition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitingRetryBackoff",
                            "method_signature": "private awaitingRetryBackoff(long nowMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasAutoAssignedPartitions",
                            "method_signature": "public synchronized hasAutoAssignedPartitions()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateLastStableOffset",
                            "method_signature": "synchronized updateLastStableOffset(TopicPartition tp, long lastStableOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateHighWatermark",
                            "method_signature": "synchronized updateHighWatermark(TopicPartition tp, long highWatermark)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "subscribeFromPattern",
                            "method_signature": "public synchronized subscribeFromPattern(Set<String> topics)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "requestOffsetReset",
                            "method_signature": "public requestOffsetReset(TopicPartition partition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "seekUnvalidated",
                            "method_signature": "public seekUnvalidated(TopicPartition tp, FetchPosition position)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "public synchronized prettyString()": {
                    "first": {
                        "method_name": "prettyString",
                        "method_signature": "public synchronized prettyString()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.30774228913126034
                },
                "private isFetchable()": {
                    "first": {
                        "method_name": "isFetchable",
                        "method_signature": "private isFetchable()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.34868739173938423
                },
                "synchronized markPendingOnAssignedCallback(Collection<TopicPartition> tps,\n                                                    boolean pendingOnAssignedCallback)": {
                    "first": {
                        "method_name": "markPendingOnAssignedCallback",
                        "method_signature": "synchronized markPendingOnAssignedCallback(Collection<TopicPartition> tps,\n                                                    boolean pendingOnAssignedCallback)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3734813126122038
                },
                "private position(FetchPosition position)": {
                    "first": {
                        "method_name": "position",
                        "method_signature": "private position(FetchPosition position)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3777531429918431
                },
                "synchronized partitionLead(TopicPartition tp)": {
                    "first": {
                        "method_name": "partitionLead",
                        "method_signature": "synchronized partitionLead(TopicPartition tp)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3942319637628688
                },
                "public synchronized unsubscribe()": {
                    "first": {
                        "method_name": "unsubscribe",
                        "method_signature": "public synchronized unsubscribe()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4069634551794308
                },
                "public seek(TopicPartition tp, long offset)": {
                    "first": {
                        "method_name": "seek",
                        "method_signature": "public seek(TopicPartition tp, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.41021494324481134
                },
                "private seekUnvalidated(FetchPosition fetchPosition)": {
                    "first": {
                        "method_name": "seekUnvalidated",
                        "method_signature": "private seekUnvalidated(FetchPosition fetchPosition)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4106517309844091
                },
                "private awaitingRetryBackoff(long nowMs)": {
                    "first": {
                        "method_name": "awaitingRetryBackoff",
                        "method_signature": "private awaitingRetryBackoff(long nowMs)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.41293061948585136
                },
                "public synchronized hasAutoAssignedPartitions()": {
                    "first": {
                        "method_name": "hasAutoAssignedPartitions",
                        "method_signature": "public synchronized hasAutoAssignedPartitions()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4166028559485715
                },
                "synchronized updateLastStableOffset(TopicPartition tp, long lastStableOffset)": {
                    "first": {
                        "method_name": "updateLastStableOffset",
                        "method_signature": "synchronized updateLastStableOffset(TopicPartition tp, long lastStableOffset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4241713373658485
                },
                "synchronized updateHighWatermark(TopicPartition tp, long highWatermark)": {
                    "first": {
                        "method_name": "updateHighWatermark",
                        "method_signature": "synchronized updateHighWatermark(TopicPartition tp, long highWatermark)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.42486969703922445
                },
                "public synchronized subscribeFromPattern(Set<String> topics)": {
                    "first": {
                        "method_name": "subscribeFromPattern",
                        "method_signature": "public synchronized subscribeFromPattern(Set<String> topics)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4251726381670217
                },
                "public requestOffsetReset(TopicPartition partition)": {
                    "first": {
                        "method_name": "requestOffsetReset",
                        "method_signature": "public requestOffsetReset(TopicPartition partition)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.42990365117364393
                },
                "public seekUnvalidated(TopicPartition tp, FetchPosition position)": {
                    "first": {
                        "method_name": "seekUnvalidated",
                        "method_signature": "public seekUnvalidated(TopicPartition tp, FetchPosition position)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4311552088998399
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "public synchronized prettyString()",
                    "public synchronized subscribeFromPattern(Set<String> topics)",
                    "public synchronized unsubscribe()",
                    "public seekUnvalidated(TopicPartition tp, FetchPosition position)",
                    "public synchronized hasAutoAssignedPartitions()",
                    "synchronized updateLastStableOffset(TopicPartition tp, long lastStableOffset)",
                    "synchronized updateHighWatermark(TopicPartition tp, long highWatermark)",
                    "public requestOffsetReset(TopicPartition partition)",
                    "synchronized partitionLead(TopicPartition tp)",
                    "private isFetchable()",
                    "private position(FetchPosition position)",
                    "private seekUnvalidated(FetchPosition fetchPosition)",
                    "private awaitingRetryBackoff(long nowMs)",
                    "public seek(TopicPartition tp, long offset)"
                ],
                "llm_response_time": 7117
            },
            "targetClassMap": {
                "prettyString": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 4468,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "subscribeFromPattern": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3858,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "unsubscribe": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3497,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "seekUnvalidated": {
                    "target_classes": [
                        {
                            "class_name": "FetchPosition",
                            "similarity_score": 0.20579830217101058
                        },
                        {
                            "class_name": "FetchState",
                            "similarity_score": 0.15936381457791915
                        },
                        {
                            "class_name": "FetchPosition",
                            "similarity_score": 0.20579830217101058
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "FetchState",
                        "FetchPosition",
                        "FetchPosition"
                    ],
                    "llm_response_time": 3002,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "hasAutoAssignedPartitions": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3898,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "updateLastStableOffset": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3967,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "updateHighWatermark": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 1844,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "requestOffsetReset": {
                    "target_classes": [
                        {
                            "class_name": "OffsetResetStrategy",
                            "similarity_score": 0.4522670168666454
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "OffsetResetStrategy"
                    ],
                    "llm_response_time": 2159,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "partitionLead": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 4419,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "isFetchable": {
                    "target_classes": [
                        {
                            "class_name": "FetchState",
                            "similarity_score": 0.1809655713435456
                        },
                        {
                            "class_name": "FetchPosition",
                            "similarity_score": 0.26707914435583546
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "FetchState",
                        "FetchPosition"
                    ],
                    "llm_response_time": 2125,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "position": {
                    "target_classes": [
                        {
                            "class_name": "FetchPosition",
                            "similarity_score": 0.21470961288429483
                        },
                        {
                            "class_name": "FetchState",
                            "similarity_score": 0.2602400194529422
                        },
                        {
                            "class_name": "FetchPosition",
                            "similarity_score": 0.21470961288429483
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "FetchState",
                        "FetchPosition",
                        "FetchPosition"
                    ],
                    "llm_response_time": 3581,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "awaitingRetryBackoff": {
                    "target_classes": [
                        {
                            "class_name": "FetchState",
                            "similarity_score": 0.1957600456294711
                        },
                        {
                            "class_name": "FetchPosition",
                            "similarity_score": 0.2852096383629523
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "FetchState",
                        "FetchPosition"
                    ],
                    "llm_response_time": 2649,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "seek": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3975,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "459da4795a511f6933e940fcf105a824bd9e589c",
        "url": "https://github.com/apache/kafka/commit/459da4795a511f6933e940fcf105a824bd9e589c",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public hasAnyInflightRequest(currentTimeMs long) : boolean extracted from private pollFollowerAsObserver(state FollowerState, currentTimeMs long) : long in class org.apache.kafka.raft.KafkaRaftClient & moved to class org.apache.kafka.raft.RequestManager",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 2157,
                    "endLine": 2178,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private pollFollowerAsObserver(state FollowerState, currentTimeMs long) : long"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 2166,
                    "endLine": 2166,
                    "startColumn": 13,
                    "endColumn": 87,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 63,
                    "endLine": 94,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public hasAnyInflightRequest(currentTimeMs long) : boolean"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 78,
                    "endLine": 78,
                    "startColumn": 13,
                    "endColumn": 58,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 2220,
                    "endLine": 2243,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private pollFollowerAsObserver(state FollowerState, currentTimeMs long) : long"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 2235,
                    "endLine": 2235,
                    "startColumn": 25,
                    "endColumn": 76,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "requestManager.hasAnyInflightRequest(currentTimeMs)"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 74,
                    "endLine": 74,
                    "startColumn": 9,
                    "endColumn": 32,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 76,
                    "endLine": 76,
                    "startColumn": 9,
                    "endColumn": 78,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 81,
                    "endLine": 81,
                    "startColumn": 17,
                    "endColumn": 35,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 84,
                    "endLine": 84,
                    "startColumn": 17,
                    "endColumn": 35,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 88,
                    "endLine": 88,
                    "startColumn": 17,
                    "endColumn": 31,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 89,
                    "endLine": 89,
                    "startColumn": 17,
                    "endColumn": 23,
                    "codeElementType": "BREAK_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 93,
                    "endLine": 93,
                    "startColumn": 9,
                    "endColumn": 23,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 79,
                    "endLine": 82,
                    "startColumn": 63,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 82,
                    "endLine": 85,
                    "startColumn": 69,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 85,
                    "endLine": 90,
                    "startColumn": 70,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 85,
                    "endLine": 90,
                    "startColumn": 20,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 82,
                    "endLine": 90,
                    "startColumn": 20,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 77,
                    "endLine": 91,
                    "startColumn": 36,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 77,
                    "endLine": 91,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "WHILE_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 79,
                    "endLine": 90,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": false
        },
        "ref_id": 581,
        "extraction_results": {
            "success": true,
            "newCommitHash": "f62b73b6006f87b0004fd8ae26fec3980db04ca8",
            "newBranchName": "extract-hasAnyInflightRequest-pollFollowerAsObserver-8a882a7"
        },
        "telemetry": {
            "id": "4783db8c-6bbd-4231-a654-78dc52ea1f05",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 2693,
                "lineStart": 109,
                "lineEnd": 2801,
                "bodyLineStart": 109,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                "sourceCode": "/**\n * This class implements a Kafkaesque version of the Raft protocol. Leader election\n * is more or less pure Raft, but replication is driven by replica fetching and we use Kafka's\n * log reconciliation protocol to truncate the log to a common point following each leader\n * election.\n *\n * Like Zookeeper, this protocol distinguishes between voters and observers. Voters are\n * the only ones who are eligible to handle protocol requests and they are the only ones\n * who take part in elections. The protocol does not yet support dynamic quorum changes.\n *\n * These are the APIs in this protocol:\n *\n * 1) {@link VoteRequestData}: Sent by valid voters when their election timeout expires and they\n *    become a candidate. This request includes the last offset in the log which electors use\n *    to tell whether or not to grant the vote.\n *\n * 2) {@link BeginQuorumEpochRequestData}: Sent by the leader of an epoch only to valid voters to\n *    assert its leadership of the new epoch. This request will be retried indefinitely for\n *    each voter until it acknowledges the request or a new election occurs.\n *\n *    This is not needed in usual Raft because the leader can use an empty data push\n *    to achieve the same purpose. The Kafka Raft implementation, however, is driven by\n *    fetch requests from followers, so there must be a way to find the new leader after\n *    an election has completed.\n *\n * 3) {@link EndQuorumEpochRequestData}: Sent by the leader of an epoch to valid voters in order to\n *    gracefully resign from the current epoch. This causes remaining voters to immediately\n *    begin a new election.\n *\n * 4) {@link FetchRequestData}: This is the same as the usual Fetch API in Kafka, but we add snapshot\n *    check before responding, and we also piggyback some additional metadata on responses (i.e. current\n *    leader and epoch). Unlike partition replication, we also piggyback truncation detection on this API\n *    rather than through a separate truncation state.\n *\n * 5) {@link FetchSnapshotRequestData}: Sent by the follower to the epoch leader in order to fetch a snapshot.\n *    This happens when a FetchResponse includes a snapshot ID due to the follower's log end offset being less\n *    than the leader's log start offset. This API is similar to the Fetch API since the snapshot is stored\n *    as FileRecords, but we use {@link UnalignedRecords} in FetchSnapshotResponse because the records\n *    are not necessarily offset-aligned.\n */\nfinal public class KafkaRaftClient<T> implements RaftClient<T> {\n    private static final int RETRY_BACKOFF_BASE_MS = 100;\n    public static final int MAX_FETCH_WAIT_MS = 500;\n    public static final int MAX_BATCH_SIZE_BYTES = 8 * 1024 * 1024;\n    public static final int MAX_FETCH_SIZE_BYTES = MAX_BATCH_SIZE_BYTES;\n\n    private final OptionalInt nodeId;\n    private final Uuid nodeDirectoryId;\n    private final AtomicReference<GracefulShutdown> shutdown = new AtomicReference<>();\n    private final LogContext logContext;\n    private final Logger logger;\n    private final Time time;\n    private final int fetchMaxWaitMs;\n    private final String clusterId;\n    private final NetworkChannel channel;\n    private final ReplicatedLog log;\n    private final Random random;\n    private final FuturePurgatory<Long> appendPurgatory;\n    private final FuturePurgatory<Long> fetchPurgatory;\n    private final RecordSerde<T> serde;\n    private final MemoryPool memoryPool;\n    private final RaftMessageQueue messageQueue;\n    private final QuorumConfig quorumConfig;\n    private final RaftMetadataLogCleanerManager snapshotCleaner;\n\n    private final Map<Listener<T>, ListenerContext> listenerContexts = new IdentityHashMap<>();\n    private final ConcurrentLinkedQueue<Registration<T>> pendingRegistrations = new ConcurrentLinkedQueue<>();\n\n    // These components need to be initialized by the method initialize() because they depend on\n    // the voter set\n    /*\n     * The key invariant for the kraft control record state machine is that it has always read to\n     * the LEO. This is achieved by:\n     *\n     * 1. reading the entire partition (snapshot and log) at start up,\n     * 2. updating the state when a snapshot is replaced, because of FETCH_SNAPSHOT, on the\n     *    followers\n     * 3. updating the state when the leader (call to append()) or follower (FETCH) appends to the\n     *    log\n     * 4. truncate new entries when a follower truncates their log\n     * 5. truncate old entries when a snapshot gets generated\n     */\n    private volatile KRaftControlRecordStateMachine partitionState;\n    private volatile KafkaRaftMetrics kafkaRaftMetrics;\n    private volatile QuorumState quorum;\n    private volatile RequestManager requestManager;\n\n    /**\n     * Create a new instance.\n     *\n     * Note that if the node ID is empty, then the client will behave as a\n     * non-participating observer.\n     */\n    public KafkaRaftClient(\n        OptionalInt nodeId,\n        Uuid nodeDirectoryId,\n        RecordSerde<T> serde,\n        NetworkChannel channel,\n        ReplicatedLog log,\n        Time time,\n        ExpirationService expirationService,\n        LogContext logContext,\n        String clusterId,\n        QuorumConfig quorumConfig\n    ) {\n        this(\n            nodeId,\n            nodeDirectoryId,\n            serde,\n            channel,\n            new BlockingMessageQueue(),\n            log,\n            new BatchMemoryPool(5, MAX_BATCH_SIZE_BYTES),\n            time,\n            expirationService,\n            MAX_FETCH_WAIT_MS,\n            clusterId,\n            logContext,\n            new Random(),\n            quorumConfig\n        );\n    }\n\n    KafkaRaftClient(\n        OptionalInt nodeId,\n        Uuid nodeDirectoryId,\n        RecordSerde<T> serde,\n        NetworkChannel channel,\n        RaftMessageQueue messageQueue,\n        ReplicatedLog log,\n        MemoryPool memoryPool,\n        Time time,\n        ExpirationService expirationService,\n        int fetchMaxWaitMs,\n        String clusterId,\n        LogContext logContext,\n        Random random,\n        QuorumConfig quorumConfig\n    ) {\n        this.nodeId = nodeId;\n        this.nodeDirectoryId = nodeDirectoryId;\n        this.logContext = logContext;\n        this.serde = serde;\n        this.channel = channel;\n        this.messageQueue = messageQueue;\n        this.log = log;\n        this.memoryPool = memoryPool;\n        this.fetchPurgatory = new ThresholdPurgatory<>(expirationService);\n        this.appendPurgatory = new ThresholdPurgatory<>(expirationService);\n        this.time = time;\n        this.clusterId = clusterId;\n        this.fetchMaxWaitMs = fetchMaxWaitMs;\n        this.logger = logContext.logger(KafkaRaftClient.class);\n        this.random = random;\n        this.quorumConfig = quorumConfig;\n        this.snapshotCleaner = new RaftMetadataLogCleanerManager(logger, time, 60000, log::maybeClean);\n    }\n\n    private void updateFollowerHighWatermark(\n        FollowerState state,\n        OptionalLong highWatermarkOpt\n    ) {\n        highWatermarkOpt.ifPresent(highWatermark -> {\n            long newHighWatermark = Math.min(endOffset().offset(), highWatermark);\n            if (state.updateHighWatermark(OptionalLong.of(newHighWatermark))) {\n                logger.debug(\"Follower high watermark updated to {}\", newHighWatermark);\n                log.updateHighWatermark(new LogOffsetMetadata(newHighWatermark));\n                updateListenersProgress(newHighWatermark);\n            }\n        });\n    }\n\n    private void updateLeaderEndOffsetAndTimestamp(\n        LeaderState<T> state,\n        long currentTimeMs\n    ) {\n        final LogOffsetMetadata endOffsetMetadata = log.endOffset();\n\n        if (state.updateLocalState(endOffsetMetadata)) {\n            onUpdateLeaderHighWatermark(state, currentTimeMs);\n        }\n\n        fetchPurgatory.maybeComplete(endOffsetMetadata.offset, currentTimeMs);\n    }\n\n    private void onUpdateLeaderHighWatermark(\n        LeaderState<T> state,\n        long currentTimeMs\n    ) {\n        state.highWatermark().ifPresent(highWatermark -> {\n            logger.debug(\"Leader high watermark updated to {}\", highWatermark);\n            log.updateHighWatermark(highWatermark);\n\n            // After updating the high watermark, we first clear the append\n            // purgatory so that we have an opportunity to route the pending\n            // records still held in memory directly to the listener\n            appendPurgatory.maybeComplete(highWatermark.offset, currentTimeMs);\n\n            // It is also possible that the high watermark is being updated\n            // for the first time following the leader election, so we need\n            // to give lagging listeners an opportunity to catch up as well\n            updateListenersProgress(highWatermark.offset);\n        });\n    }\n\n    private void updateListenersProgress(long highWatermark) {\n        for (ListenerContext listenerContext : listenerContexts.values()) {\n            listenerContext.nextExpectedOffset().ifPresent(nextExpectedOffset -> {\n                // Send snapshot to the listener, if the listener is at the beginning of the log and there is a snapshot,\n                // or the listener is trying to read an offset for which there isn't a segment in the log.\n                if (nextExpectedOffset < highWatermark &&\n                    ((nextExpectedOffset == 0 && latestSnapshot().isPresent()) ||\n                     nextExpectedOffset < log.startOffset())\n                ) {\n                    SnapshotReader<T> snapshot = latestSnapshot().orElseThrow(() -> new IllegalStateException(\n                        String.format(\n                            \"Snapshot expected since next offset of %s is %d, log start offset is %d and high-watermark is %d\",\n                            listenerContext.listenerName(),\n                            nextExpectedOffset,\n                            log.startOffset(),\n                            highWatermark\n                        )\n                    ));\n                    listenerContext.fireHandleSnapshot(snapshot);\n                }\n            });\n\n            // Re-read the expected offset in case the snapshot had to be reloaded\n            listenerContext.nextExpectedOffset().ifPresent(nextExpectedOffset -> {\n                if (nextExpectedOffset < highWatermark) {\n                    LogFetchInfo readInfo = log.read(nextExpectedOffset, Isolation.COMMITTED);\n                    listenerContext.fireHandleCommit(nextExpectedOffset, readInfo.records);\n                }\n            });\n        }\n    }\n\n    private Optional<SnapshotReader<T>> latestSnapshot() {\n        return log.latestSnapshot().map(reader ->\n            RecordsSnapshotReader.of(reader,\n                serde,\n                BufferSupplier.create(),\n                MAX_BATCH_SIZE_BYTES,\n                true /* Validate batch CRC*/\n            )\n        );\n    }\n\n    private void maybeFireHandleCommit(long baseOffset, int epoch, long appendTimestamp, int sizeInBytes, List<T> records) {\n        for (ListenerContext listenerContext : listenerContexts.values()) {\n            listenerContext.nextExpectedOffset().ifPresent(nextOffset -> {\n                if (nextOffset == baseOffset) {\n                    listenerContext.fireHandleCommit(baseOffset, epoch, appendTimestamp, sizeInBytes, records);\n                }\n            });\n        }\n    }\n\n    private void maybeFireLeaderChange(LeaderState<T> state) {\n        for (ListenerContext listenerContext : listenerContexts.values()) {\n            listenerContext.maybeFireLeaderChange(quorum.leaderAndEpoch(), state.epochStartOffset());\n        }\n    }\n\n    private void maybeFireLeaderChange() {\n        for (ListenerContext listenerContext : listenerContexts.values()) {\n            listenerContext.maybeFireLeaderChange(quorum.leaderAndEpoch());\n        }\n    }\n\n    public void initialize(\n        Map<Integer, InetSocketAddress> voterAddresses,\n        String listenerName,\n        QuorumStateStore quorumStateStore,\n        Metrics metrics\n    ) {\n        partitionState = new KRaftControlRecordStateMachine(\n            Optional.of(VoterSet.fromInetSocketAddresses(listenerName, voterAddresses)),\n            log,\n            serde,\n            BufferSupplier.create(),\n            MAX_BATCH_SIZE_BYTES,\n            logContext\n        );\n        // Read the entire log\n        logger.info(\"Reading KRaft snapshot and log as part of the initialization\");\n        partitionState.updateState();\n\n        VoterSet lastVoterSet = partitionState.lastVoterSet();\n        requestManager = new RequestManager(\n            lastVoterSet.voterIds(),\n            quorumConfig.retryBackoffMs(),\n            quorumConfig.requestTimeoutMs(),\n            random\n        );\n\n        quorum = new QuorumState(\n            nodeId,\n            nodeDirectoryId,\n            partitionState::lastVoterSet,\n            partitionState::lastKraftVersion,\n            quorumConfig.electionTimeoutMs(),\n            quorumConfig.fetchTimeoutMs(),\n            quorumStateStore,\n            time,\n            logContext,\n            random\n        );\n\n        kafkaRaftMetrics = new KafkaRaftMetrics(metrics, \"raft\", quorum);\n        // All Raft voters are statically configured and known at startup\n        // so there are no unknown voter connections. Report this metric as 0.\n        kafkaRaftMetrics.updateNumUnknownVoterConnections(0);\n\n        for (Integer voterId : lastVoterSet.voterIds()) {\n            channel.updateEndpoint(voterId, lastVoterSet.voterAddress(voterId, listenerName).get());\n        }\n\n        quorum.initialize(new OffsetAndEpoch(log.endOffset().offset, log.lastFetchedEpoch()));\n\n        long currentTimeMs = time.milliseconds();\n        if (quorum.isLeader()) {\n            throw new IllegalStateException(\"Voter cannot initialize as a Leader\");\n        } else if (quorum.isCandidate()) {\n            onBecomeCandidate(currentTimeMs);\n        } else if (quorum.isFollower()) {\n            onBecomeFollower(currentTimeMs);\n        }\n\n        // When there is only a single voter, become candidate immediately\n        if (quorum.isOnlyVoter() && !quorum.isCandidate()) {\n            transitionToCandidate(currentTimeMs);\n        }\n    }\n\n    @Override\n    public void register(Listener<T> listener) {\n        pendingRegistrations.add(Registration.register(listener));\n        wakeup();\n    }\n\n    @Override\n    public void unregister(Listener<T> listener) {\n        pendingRegistrations.add(Registration.unregister(listener));\n        // No need to wake up the polling thread. It is a removal so the updates can be\n        // delayed until the polling thread wakes up for other reasons.\n    }\n\n    @Override\n    public LeaderAndEpoch leaderAndEpoch() {\n        if (isInitialized()) {\n            return quorum.leaderAndEpoch();\n        } else {\n            return LeaderAndEpoch.UNKNOWN;\n        }\n    }\n\n    @Override\n    public OptionalInt nodeId() {\n        return nodeId;\n    }\n\n    private OffsetAndEpoch endOffset() {\n        return new OffsetAndEpoch(log.endOffset().offset, log.lastFetchedEpoch());\n    }\n\n    private void resetConnections() {\n        requestManager.resetAll();\n    }\n\n    private void onBecomeLeader(long currentTimeMs) {\n        long endOffset = log.endOffset().offset;\n\n        BatchAccumulator<T> accumulator = new BatchAccumulator<>(\n            quorum.epoch(),\n            endOffset,\n            quorumConfig.appendLingerMs(),\n            MAX_BATCH_SIZE_BYTES,\n            memoryPool,\n            time,\n            Compression.NONE,\n            serde\n        );\n\n        LeaderState<T> state = quorum.transitionToLeader(endOffset, accumulator);\n\n        log.initializeLeaderEpoch(quorum.epoch());\n\n        // The high watermark can only be advanced once we have written a record\n        // from the new leader's epoch. Hence we write a control message immediately\n        // to ensure there is no delay committing pending data.\n        state.appendLeaderChangeMessage(currentTimeMs);\n\n        resetConnections();\n        kafkaRaftMetrics.maybeUpdateElectionLatency(currentTimeMs);\n    }\n\n    private void flushLeaderLog(LeaderState<T> state, long currentTimeMs) {\n        // We update the end offset before flushing so that parked fetches can return sooner.\n        updateLeaderEndOffsetAndTimestamp(state, currentTimeMs);\n        log.flush(false);\n    }\n\n    private boolean maybeTransitionToLeader(CandidateState state, long currentTimeMs) {\n        if (state.isVoteGranted()) {\n            onBecomeLeader(currentTimeMs);\n            return true;\n        } else {\n            return false;\n        }\n    }\n\n    private void onBecomeCandidate(long currentTimeMs) {\n        CandidateState state = quorum.candidateStateOrThrow();\n        if (!maybeTransitionToLeader(state, currentTimeMs)) {\n            resetConnections();\n            kafkaRaftMetrics.updateElectionStartMs(currentTimeMs);\n        }\n    }\n\n    private void transitionToCandidate(long currentTimeMs) {\n        quorum.transitionToCandidate();\n        maybeFireLeaderChange();\n        onBecomeCandidate(currentTimeMs);\n    }\n\n    private void transitionToUnattached(int epoch) {\n        quorum.transitionToUnattached(epoch);\n        maybeFireLeaderChange();\n        resetConnections();\n    }\n\n    private void transitionToResigned(List<Integer> preferredSuccessors) {\n        fetchPurgatory.completeAllExceptionally(\n            Errors.NOT_LEADER_OR_FOLLOWER.exception(\"Not handling request since this node is resigning\"));\n        quorum.transitionToResigned(preferredSuccessors);\n        maybeFireLeaderChange();\n        resetConnections();\n    }\n\n    private void transitionToVoted(ReplicaKey candidateKey, int epoch) {\n        quorum.transitionToVoted(epoch, candidateKey);\n        maybeFireLeaderChange();\n        resetConnections();\n    }\n\n    private void onBecomeFollower(long currentTimeMs) {\n        kafkaRaftMetrics.maybeUpdateElectionLatency(currentTimeMs);\n\n        resetConnections();\n\n        // After becoming a follower, we need to complete all pending fetches so that\n        // they can be re-sent to the leader without waiting for their expirations\n        fetchPurgatory.completeAllExceptionally(new NotLeaderOrFollowerException(\n            \"Cannot process the fetch request because the node is no longer the leader.\"));\n\n        // Clearing the append purgatory should complete all futures exceptionally since this node is no longer the leader\n        appendPurgatory.completeAllExceptionally(new NotLeaderOrFollowerException(\n            \"Failed to receive sufficient acknowledgments for this append before leader change.\"));\n    }\n\n    private void transitionToFollower(\n        int epoch,\n        int leaderId,\n        long currentTimeMs\n    ) {\n        quorum.transitionToFollower(epoch, leaderId);\n        maybeFireLeaderChange();\n        onBecomeFollower(currentTimeMs);\n    }\n\n    private VoteResponseData buildVoteResponse(Errors partitionLevelError, boolean voteGranted) {\n        return VoteResponse.singletonResponse(\n            Errors.NONE,\n            log.topicPartition(),\n            partitionLevelError,\n            quorum.epoch(),\n            quorum.leaderIdOrSentinel(),\n            voteGranted);\n    }\n\n    /**\n     * Handle a Vote request. This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *      but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     * - {@link Errors#INCONSISTENT_VOTER_SET} if the request suggests inconsistent voter membership (e.g.\n     *      if this node or the sender is not one of the current known voters)\n     * - {@link Errors#INVALID_REQUEST} if the last epoch or offset are invalid\n     */\n    private VoteResponseData handleVoteRequest(\n        RaftRequest.Inbound requestMetadata\n    ) {\n        VoteRequestData request = (VoteRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(request.clusterId())) {\n            return new VoteResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code());\n        }\n\n        if (!hasValidTopicPartition(request, log.topicPartition())) {\n            // Until we support multi-raft, we treat individual topic partition mismatches as invalid requests\n            return new VoteResponseData().setErrorCode(Errors.INVALID_REQUEST.code());\n        }\n\n        VoteRequestData.PartitionData partitionRequest =\n            request.topics().get(0).partitions().get(0);\n\n        int candidateId = partitionRequest.candidateId();\n        int candidateEpoch = partitionRequest.candidateEpoch();\n\n        int lastEpoch = partitionRequest.lastOffsetEpoch();\n        long lastEpochEndOffset = partitionRequest.lastOffset();\n        if (lastEpochEndOffset < 0 || lastEpoch < 0 || lastEpoch >= candidateEpoch) {\n            return buildVoteResponse(Errors.INVALID_REQUEST, false);\n        }\n\n        Optional<Errors> errorOpt = validateVoterOnlyRequest(candidateId, candidateEpoch);\n        if (errorOpt.isPresent()) {\n            return buildVoteResponse(errorOpt.get(), false);\n        }\n\n        if (candidateEpoch > quorum.epoch()) {\n            transitionToUnattached(candidateEpoch);\n        }\n\n        OffsetAndEpoch lastEpochEndOffsetAndEpoch = new OffsetAndEpoch(lastEpochEndOffset, lastEpoch);\n        ReplicaKey candidateKey = ReplicaKey.of(candidateId, Optional.empty());\n        boolean voteGranted = quorum.canGrantVote(\n            candidateKey,\n            lastEpochEndOffsetAndEpoch.compareTo(endOffset()) >= 0\n        );\n\n        if (voteGranted && quorum.isUnattached()) {\n            transitionToVoted(candidateKey, candidateEpoch);\n        }\n\n        logger.info(\"Vote request {} with epoch {} is {}\", request, candidateEpoch, voteGranted ? \"granted\" : \"rejected\");\n        return buildVoteResponse(Errors.NONE, voteGranted);\n    }\n\n    private boolean handleVoteResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        int remoteNodeId = responseMetadata.sourceId();\n        VoteResponseData response = (VoteResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(response.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (!hasValidTopicPartition(response, log.topicPartition())) {\n            return false;\n        }\n\n        VoteResponseData.PartitionData partitionResponse =\n            response.topics().get(0).partitions().get(0);\n\n        Errors error = Errors.forCode(partitionResponse.errorCode());\n        OptionalInt responseLeaderId = optionalLeaderId(partitionResponse.leaderId());\n        int responseEpoch = partitionResponse.leaderEpoch();\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            error, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        } else if (error == Errors.NONE) {\n            if (quorum.isLeader()) {\n                logger.debug(\"Ignoring vote response {} since we already became leader for epoch {}\",\n                    partitionResponse, quorum.epoch());\n            } else if (quorum.isCandidate()) {\n                CandidateState state = quorum.candidateStateOrThrow();\n                if (partitionResponse.voteGranted()) {\n                    state.recordGrantedVote(remoteNodeId);\n                    maybeTransitionToLeader(state, currentTimeMs);\n                } else {\n                    state.recordRejectedVote(remoteNodeId);\n\n                    // If our vote is rejected, we go immediately to the random backoff. This\n                    // ensures that we are not stuck waiting for the election timeout when the\n                    // vote has become gridlocked.\n                    if (state.isVoteRejected() && !state.isBackingOff()) {\n                        logger.info(\"Insufficient remaining votes to become leader (rejected by {}). \" +\n                            \"We will backoff before retrying election again\", state.rejectingVoters());\n\n                        state.startBackingOff(\n                            currentTimeMs,\n                            binaryExponentialElectionBackoffMs(state.retries())\n                        );\n                    }\n                }\n            } else {\n                logger.debug(\"Ignoring vote response {} since we are no longer a candidate in epoch {}\",\n                    partitionResponse, quorum.epoch());\n            }\n            return true;\n        } else {\n            return handleUnexpectedError(error, responseMetadata);\n        }\n    }\n\n    private int binaryExponentialElectionBackoffMs(int retries) {\n        if (retries <= 0) {\n            throw new IllegalArgumentException(\"Retries \" + retries + \" should be larger than zero\");\n        }\n        // upper limit exponential co-efficients at 20 to avoid overflow\n        return Math.min(RETRY_BACKOFF_BASE_MS * random.nextInt(2 << Math.min(20, retries - 1)),\n                quorumConfig.electionBackoffMaxMs());\n    }\n\n    private int strictExponentialElectionBackoffMs(int positionInSuccessors, int totalNumSuccessors) {\n        if (positionInSuccessors <= 0 || positionInSuccessors >= totalNumSuccessors) {\n            throw new IllegalArgumentException(\"Position \" + positionInSuccessors + \" should be larger than zero\" +\n                    \" and smaller than total number of successors \" + totalNumSuccessors);\n        }\n\n        int retryBackOffBaseMs = quorumConfig.electionBackoffMaxMs() >> (totalNumSuccessors - 1);\n        return Math.min(quorumConfig.electionBackoffMaxMs(), retryBackOffBaseMs << (positionInSuccessors - 1));\n    }\n\n    private BeginQuorumEpochResponseData buildBeginQuorumEpochResponse(Errors partitionLevelError) {\n        return BeginQuorumEpochResponse.singletonResponse(\n            Errors.NONE,\n            log.topicPartition(),\n            partitionLevelError,\n            quorum.epoch(),\n            quorum.leaderIdOrSentinel());\n    }\n\n    /**\n     * Handle a BeginEpoch request. This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *      but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#INCONSISTENT_VOTER_SET} if the request suggests inconsistent voter membership (e.g.\n     *      if this node or the sender is not one of the current known voters)\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     */\n    private BeginQuorumEpochResponseData handleBeginQuorumEpochRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        BeginQuorumEpochRequestData request = (BeginQuorumEpochRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(request.clusterId())) {\n            return new BeginQuorumEpochResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code());\n        }\n\n        if (!hasValidTopicPartition(request, log.topicPartition())) {\n            // Until we support multi-raft, we treat topic partition mismatches as invalid requests\n            return new BeginQuorumEpochResponseData().setErrorCode(Errors.INVALID_REQUEST.code());\n        }\n\n        BeginQuorumEpochRequestData.PartitionData partitionRequest =\n            request.topics().get(0).partitions().get(0);\n\n        int requestLeaderId = partitionRequest.leaderId();\n        int requestEpoch = partitionRequest.leaderEpoch();\n\n        Optional<Errors> errorOpt = validateVoterOnlyRequest(requestLeaderId, requestEpoch);\n        if (errorOpt.isPresent()) {\n            return buildBeginQuorumEpochResponse(errorOpt.get());\n        }\n\n        maybeTransition(OptionalInt.of(requestLeaderId), requestEpoch, currentTimeMs);\n        return buildBeginQuorumEpochResponse(Errors.NONE);\n    }\n\n    private boolean handleBeginQuorumEpochResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        int remoteNodeId = responseMetadata.sourceId();\n        BeginQuorumEpochResponseData response = (BeginQuorumEpochResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(response.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (!hasValidTopicPartition(response, log.topicPartition())) {\n            return false;\n        }\n\n        BeginQuorumEpochResponseData.PartitionData partitionResponse =\n            response.topics().get(0).partitions().get(0);\n\n        Errors partitionError = Errors.forCode(partitionResponse.errorCode());\n        OptionalInt responseLeaderId = optionalLeaderId(partitionResponse.leaderId());\n        int responseEpoch = partitionResponse.leaderEpoch();\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            partitionError, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        } else if (partitionError == Errors.NONE) {\n            if (quorum.isLeader()) {\n                LeaderState<T> state = quorum.leaderStateOrThrow();\n                state.addAcknowledgementFrom(remoteNodeId);\n            } else {\n                logger.debug(\"Ignoring BeginQuorumEpoch response {} since \" +\n                    \"this node is not the leader anymore\", response);\n            }\n            return true;\n        } else {\n            return handleUnexpectedError(partitionError, responseMetadata);\n        }\n    }\n\n    private EndQuorumEpochResponseData buildEndQuorumEpochResponse(Errors partitionLevelError) {\n        return EndQuorumEpochResponse.singletonResponse(\n            Errors.NONE,\n            log.topicPartition(),\n            partitionLevelError,\n            quorum.epoch(),\n            quorum.leaderIdOrSentinel());\n    }\n\n    /**\n     * Handle an EndEpoch request. This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *      but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#INCONSISTENT_VOTER_SET} if the request suggests inconsistent voter membership (e.g.\n     *      if this node or the sender is not one of the current known voters)\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     */\n    private EndQuorumEpochResponseData handleEndQuorumEpochRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        EndQuorumEpochRequestData request = (EndQuorumEpochRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(request.clusterId())) {\n            return new EndQuorumEpochResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code());\n        }\n\n        if (!hasValidTopicPartition(request, log.topicPartition())) {\n            // Until we support multi-raft, we treat topic partition mismatches as invalid requests\n            return new EndQuorumEpochResponseData().setErrorCode(Errors.INVALID_REQUEST.code());\n        }\n\n        EndQuorumEpochRequestData.PartitionData partitionRequest =\n            request.topics().get(0).partitions().get(0);\n\n        int requestEpoch = partitionRequest.leaderEpoch();\n        int requestLeaderId = partitionRequest.leaderId();\n\n        Optional<Errors> errorOpt = validateVoterOnlyRequest(requestLeaderId, requestEpoch);\n        if (errorOpt.isPresent()) {\n            return buildEndQuorumEpochResponse(errorOpt.get());\n        }\n        maybeTransition(OptionalInt.of(requestLeaderId), requestEpoch, currentTimeMs);\n\n        if (quorum.isFollower()) {\n            FollowerState state = quorum.followerStateOrThrow();\n            if (state.leaderId() == requestLeaderId) {\n                List<Integer> preferredSuccessors = partitionRequest.preferredSuccessors();\n                long electionBackoffMs = endEpochElectionBackoff(preferredSuccessors);\n                logger.debug(\"Overriding follower fetch timeout to {} after receiving \" +\n                    \"EndQuorumEpoch request from leader {} in epoch {}\", electionBackoffMs,\n                    requestLeaderId, requestEpoch);\n                state.overrideFetchTimeout(currentTimeMs, electionBackoffMs);\n            }\n        }\n        return buildEndQuorumEpochResponse(Errors.NONE);\n    }\n\n    private long endEpochElectionBackoff(List<Integer> preferredSuccessors) {\n        // Based on the priority inside the preferred successors, choose the corresponding delayed\n        // election backoff time based on strict exponential mechanism so that the most up-to-date\n        // voter has a higher chance to be elected. If the node's priority is highest, become\n        // candidate immediately instead of waiting for next poll.\n        int position = preferredSuccessors.indexOf(quorum.localIdOrThrow());\n        if (position <= 0) {\n            return 0;\n        } else {\n            return strictExponentialElectionBackoffMs(position, preferredSuccessors.size());\n        }\n    }\n\n    private boolean handleEndQuorumEpochResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        EndQuorumEpochResponseData response = (EndQuorumEpochResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(response.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (!hasValidTopicPartition(response, log.topicPartition())) {\n            return false;\n        }\n\n        EndQuorumEpochResponseData.PartitionData partitionResponse =\n            response.topics().get(0).partitions().get(0);\n\n        Errors partitionError = Errors.forCode(partitionResponse.errorCode());\n        OptionalInt responseLeaderId = optionalLeaderId(partitionResponse.leaderId());\n        int responseEpoch = partitionResponse.leaderEpoch();\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            partitionError, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        } else if (partitionError == Errors.NONE) {\n            ResignedState resignedState = quorum.resignedStateOrThrow();\n            resignedState.acknowledgeResignation(responseMetadata.sourceId());\n            return true;\n        } else {\n            return handleUnexpectedError(partitionError, responseMetadata);\n        }\n    }\n\n    private FetchResponseData buildFetchResponse(\n        Errors error,\n        Records records,\n        ValidOffsetAndEpoch validOffsetAndEpoch,\n        Optional<LogOffsetMetadata> highWatermark\n    ) {\n        return RaftUtil.singletonFetchResponse(log.topicPartition(), log.topicId(), Errors.NONE, partitionData -> {\n            partitionData\n                .setRecords(records)\n                .setErrorCode(error.code())\n                .setLogStartOffset(log.startOffset())\n                .setHighWatermark(\n                    highWatermark.map(offsetMetadata -> offsetMetadata.offset).orElse(-1L)\n                );\n\n            partitionData.currentLeader()\n                .setLeaderEpoch(quorum.epoch())\n                .setLeaderId(quorum.leaderIdOrSentinel());\n\n            switch (validOffsetAndEpoch.kind()) {\n                case DIVERGING:\n                    partitionData.divergingEpoch()\n                        .setEpoch(validOffsetAndEpoch.offsetAndEpoch().epoch())\n                        .setEndOffset(validOffsetAndEpoch.offsetAndEpoch().offset());\n                    break;\n                case SNAPSHOT:\n                    partitionData.snapshotId()\n                        .setEpoch(validOffsetAndEpoch.offsetAndEpoch().epoch())\n                        .setEndOffset(validOffsetAndEpoch.offsetAndEpoch().offset());\n                    break;\n                default:\n            }\n        });\n    }\n\n    private FetchResponseData buildEmptyFetchResponse(\n        Errors error,\n        Optional<LogOffsetMetadata> highWatermark\n    ) {\n        return buildFetchResponse(\n            error,\n            MemoryRecords.EMPTY,\n            ValidOffsetAndEpoch.valid(),\n            highWatermark\n        );\n    }\n\n    private boolean hasValidClusterId(String requestClusterId) {\n        // We don't enforce the cluster id if it is not provided.\n        if (requestClusterId == null) {\n            return true;\n        }\n        return clusterId.equals(requestClusterId);\n    }\n\n    /**\n     * Handle a Fetch request. The fetch offset and last fetched epoch are always\n     * validated against the current log. In the case that they do not match, the response will\n     * indicate the diverging offset/epoch. A follower is expected to truncate its log in this\n     * case and resend the fetch.\n     *\n     * This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *     but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     * - {@link Errors#INVALID_REQUEST} if the request epoch is larger than the leader's current epoch\n     *     or if either the fetch offset or the last fetched epoch is invalid\n     */\n    private CompletableFuture<FetchResponseData> handleFetchRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        FetchRequestData request = (FetchRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(request.clusterId())) {\n            return completedFuture(new FetchResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code()));\n        }\n\n        if (!hasValidTopicPartition(request, log.topicPartition(), log.topicId())) {\n            // Until we support multi-raft, we treat topic partition mismatches as invalid requests\n            return completedFuture(new FetchResponseData().setErrorCode(Errors.INVALID_REQUEST.code()));\n        }\n        // If the ID is valid, we can set the topic name.\n        request.topics().get(0).setTopic(log.topicPartition().topic());\n\n        FetchRequestData.FetchPartition fetchPartition = request.topics().get(0).partitions().get(0);\n        if (request.maxWaitMs() < 0\n            || fetchPartition.fetchOffset() < 0\n            || fetchPartition.lastFetchedEpoch() < 0\n            || fetchPartition.lastFetchedEpoch() > fetchPartition.currentLeaderEpoch()) {\n            return completedFuture(\n                buildEmptyFetchResponse(Errors.INVALID_REQUEST, Optional.empty())\n            );\n        }\n\n        int replicaId = FetchRequest.replicaId(request);\n        FetchResponseData response = tryCompleteFetchRequest(replicaId, fetchPartition, currentTimeMs);\n        FetchResponseData.PartitionData partitionResponse =\n            response.responses().get(0).partitions().get(0);\n\n        if (partitionResponse.errorCode() != Errors.NONE.code()\n            || FetchResponse.recordsSize(partitionResponse) > 0\n            || request.maxWaitMs() == 0\n            || isPartitionDiverged(partitionResponse)\n            || isPartitionSnapshotted(partitionResponse)) {\n            // Reply immediately if any of the following is true\n            // 1. The response contains an error\n            // 2. There are records in the response\n            // 3. The fetching replica doesn't want to wait for the partition to contain new data\n            // 4. The fetching replica needs to truncate because the log diverged\n            // 5. The fetching replica needs to fetch a snapshot\n            return completedFuture(response);\n        }\n\n        CompletableFuture<Long> future = fetchPurgatory.await(\n            fetchPartition.fetchOffset(),\n            request.maxWaitMs());\n\n        return future.handle((completionTimeMs, exception) -> {\n            if (exception != null) {\n                Throwable cause = exception instanceof ExecutionException ?\n                    exception.getCause() : exception;\n\n                Errors error = Errors.forException(cause);\n                if (error == Errors.REQUEST_TIMED_OUT) {\n                    // Note that for this case the calling thread is the expiration service thread and not the\n                    // polling thread.\n                    //\n                    // If the fetch request timed out in purgatory, it means no new data is available,\n                    // just return the original fetch response.\n                    return response;\n                } else {\n                    // If there was any error other than REQUEST_TIMED_OUT, return it.\n                    logger.info(\"Failed to handle fetch from {} at {} due to {}\",\n                        replicaId, fetchPartition.fetchOffset(), error);\n                    return buildEmptyFetchResponse(error, Optional.empty());\n                }\n            }\n\n            // FIXME: `completionTimeMs`, which can be null\n            logger.trace(\"Completing delayed fetch from {} starting at offset {} at {}\",\n                replicaId, fetchPartition.fetchOffset(), completionTimeMs);\n\n            // It is safe to call tryCompleteFetchRequest because only the polling thread completes this\n            // future successfully. This is true because only the polling thread appends record batches to\n            // the log from maybeAppendBatches.\n            return tryCompleteFetchRequest(replicaId, fetchPartition, time.milliseconds());\n        });\n    }\n\n    private FetchResponseData tryCompleteFetchRequest(\n        int replicaId,\n        FetchRequestData.FetchPartition request,\n        long currentTimeMs\n    ) {\n        try {\n            Optional<Errors> errorOpt = validateLeaderOnlyRequest(request.currentLeaderEpoch());\n            if (errorOpt.isPresent()) {\n                return buildEmptyFetchResponse(errorOpt.get(), Optional.empty());\n            }\n\n            long fetchOffset = request.fetchOffset();\n            int lastFetchedEpoch = request.lastFetchedEpoch();\n            LeaderState<T> state = quorum.leaderStateOrThrow();\n\n            Optional<OffsetAndEpoch> latestSnapshotId = log.latestSnapshotId();\n            final ValidOffsetAndEpoch validOffsetAndEpoch;\n            if (fetchOffset == 0 && latestSnapshotId.isPresent()) {\n                // If the follower has an empty log and a snapshot exist, it is always more efficient\n                // to reply with a snapshot id (FETCH_SNAPSHOT) instead of fetching from the log segments.\n                validOffsetAndEpoch = ValidOffsetAndEpoch.snapshot(latestSnapshotId.get());\n            } else {\n                validOffsetAndEpoch = log.validateOffsetAndEpoch(fetchOffset, lastFetchedEpoch);\n            }\n\n            final Records records;\n            if (validOffsetAndEpoch.kind() == ValidOffsetAndEpoch.Kind.VALID) {\n                LogFetchInfo info = log.read(fetchOffset, Isolation.UNCOMMITTED);\n\n                if (state.updateReplicaState(replicaId, currentTimeMs, info.startOffsetMetadata)) {\n                    onUpdateLeaderHighWatermark(state, currentTimeMs);\n                }\n\n                records = info.records;\n            } else {\n                records = MemoryRecords.EMPTY;\n            }\n\n            return buildFetchResponse(Errors.NONE, records, validOffsetAndEpoch, state.highWatermark());\n        } catch (Exception e) {\n            logger.error(\"Caught unexpected error in fetch completion of request {}\", request, e);\n            return buildEmptyFetchResponse(Errors.UNKNOWN_SERVER_ERROR, Optional.empty());\n        }\n    }\n\n    private static boolean isPartitionDiverged(FetchResponseData.PartitionData partitionResponseData) {\n        FetchResponseData.EpochEndOffset divergingEpoch = partitionResponseData.divergingEpoch();\n\n        return divergingEpoch.epoch() != -1 || divergingEpoch.endOffset() != -1;\n    }\n\n    private static boolean isPartitionSnapshotted(FetchResponseData.PartitionData partitionResponseData) {\n        FetchResponseData.SnapshotId snapshotId = partitionResponseData.snapshotId();\n\n        return snapshotId.epoch() != -1 || snapshotId.endOffset() != -1;\n    }\n\n    private static OptionalInt optionalLeaderId(int leaderIdOrNil) {\n        if (leaderIdOrNil < 0)\n            return OptionalInt.empty();\n        return OptionalInt.of(leaderIdOrNil);\n    }\n\n    private static String listenerName(Listener<?> listener) {\n        return String.format(\"%s@%d\", listener.getClass().getTypeName(), System.identityHashCode(listener));\n    }\n\n    private boolean handleFetchResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        FetchResponseData response = (FetchResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(response.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (!RaftUtil.hasValidTopicPartition(response, log.topicPartition(), log.topicId())) {\n            return false;\n        }\n        // If the ID is valid, we can set the topic name.\n        response.responses().get(0).setTopic(log.topicPartition().topic());\n\n        FetchResponseData.PartitionData partitionResponse =\n            response.responses().get(0).partitions().get(0);\n\n        FetchResponseData.LeaderIdAndEpoch currentLeaderIdAndEpoch = partitionResponse.currentLeader();\n        OptionalInt responseLeaderId = optionalLeaderId(currentLeaderIdAndEpoch.leaderId());\n        int responseEpoch = currentLeaderIdAndEpoch.leaderEpoch();\n        Errors error = Errors.forCode(partitionResponse.errorCode());\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            error, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        }\n\n        FollowerState state = quorum.followerStateOrThrow();\n        if (error == Errors.NONE) {\n            FetchResponseData.EpochEndOffset divergingEpoch = partitionResponse.divergingEpoch();\n            if (divergingEpoch.epoch() >= 0) {\n                // The leader is asking us to truncate before continuing\n                final OffsetAndEpoch divergingOffsetAndEpoch = new OffsetAndEpoch(\n                    divergingEpoch.endOffset(), divergingEpoch.epoch());\n\n                state.highWatermark().ifPresent(highWatermark -> {\n                    if (divergingOffsetAndEpoch.offset() < highWatermark.offset) {\n                        throw new KafkaException(\"The leader requested truncation to offset \" +\n                            divergingOffsetAndEpoch.offset() + \", which is below the current high watermark\" +\n                            \" \" + highWatermark);\n                    }\n                });\n\n                long truncationOffset = log.truncateToEndOffset(divergingOffsetAndEpoch);\n                logger.info(\n                    \"Truncated to offset {} from Fetch response from leader {}\",\n                    truncationOffset,\n                    quorum.leaderIdOrSentinel()\n                );\n\n                // Update the internal listener to the new end offset\n                partitionState.truncateNewEntries(truncationOffset);\n            } else if (partitionResponse.snapshotId().epoch() >= 0 ||\n                       partitionResponse.snapshotId().endOffset() >= 0) {\n                // The leader is asking us to fetch a snapshot\n\n                if (partitionResponse.snapshotId().epoch() < 0) {\n                    logger.error(\n                        \"The leader sent a snapshot id with a valid end offset {} but with an invalid epoch {}\",\n                        partitionResponse.snapshotId().endOffset(),\n                        partitionResponse.snapshotId().epoch()\n                    );\n                    return false;\n                } else if (partitionResponse.snapshotId().endOffset() < 0) {\n                    logger.error(\n                        \"The leader sent a snapshot id with a valid epoch {} but with an invalid end offset {}\",\n                        partitionResponse.snapshotId().epoch(),\n                        partitionResponse.snapshotId().endOffset()\n                    );\n                    return false;\n                } else {\n                    final OffsetAndEpoch snapshotId = new OffsetAndEpoch(\n                        partitionResponse.snapshotId().endOffset(),\n                        partitionResponse.snapshotId().epoch()\n                    );\n\n                    // Do not validate the snapshot id against the local replicated log since this\n                    // snapshot is expected to reference offsets and epochs greater than the log\n                    // end offset and high-watermark.\n                    state.setFetchingSnapshot(log.createNewSnapshotUnchecked(snapshotId));\n                    logger.info(\n                        \"Fetching snapshot {} from Fetch response from leader {}\",\n                        snapshotId,\n                        quorum.leaderIdOrSentinel()\n                    );\n                }\n            } else {\n                Records records = FetchResponse.recordsOrFail(partitionResponse);\n                if (records.sizeInBytes() > 0) {\n                    appendAsFollower(records);\n                }\n\n                OptionalLong highWatermark = partitionResponse.highWatermark() < 0 ?\n                    OptionalLong.empty() : OptionalLong.of(partitionResponse.highWatermark());\n                updateFollowerHighWatermark(state, highWatermark);\n            }\n\n            state.resetFetchTimeout(currentTimeMs);\n            return true;\n        } else {\n            return handleUnexpectedError(error, responseMetadata);\n        }\n    }\n\n    private void appendAsFollower(\n        Records records\n    ) {\n        LogAppendInfo info = log.appendAsFollower(records);\n        if (quorum.isVoter()) {\n            // the leader only requires that voters have flushed their log before sending\n            // a Fetch request\n            log.flush(false);\n        }\n\n        partitionState.updateState();\n\n        OffsetAndEpoch endOffset = endOffset();\n        kafkaRaftMetrics.updateFetchedRecords(info.lastOffset - info.firstOffset + 1);\n        kafkaRaftMetrics.updateLogEnd(endOffset);\n        logger.trace(\"Follower end offset updated to {} after append\", endOffset);\n    }\n\n    private LogAppendInfo appendAsLeader(\n        Records records\n    ) {\n        LogAppendInfo info = log.appendAsLeader(records, quorum.epoch());\n\n        partitionState.updateState();\n\n        OffsetAndEpoch endOffset = endOffset();\n        kafkaRaftMetrics.updateAppendRecords(info.lastOffset - info.firstOffset + 1);\n        kafkaRaftMetrics.updateLogEnd(endOffset);\n        logger.trace(\"Leader appended records at base offset {}, new end offset is {}\", info.firstOffset, endOffset);\n        return info;\n    }\n\n    private DescribeQuorumResponseData handleDescribeQuorumRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        DescribeQuorumRequestData describeQuorumRequestData = (DescribeQuorumRequestData) requestMetadata.data;\n        if (!hasValidTopicPartition(describeQuorumRequestData, log.topicPartition())) {\n            return DescribeQuorumRequest.getPartitionLevelErrorResponse(\n                describeQuorumRequestData, Errors.UNKNOWN_TOPIC_OR_PARTITION);\n        }\n\n        if (!quorum.isLeader()) {\n            return DescribeQuorumResponse.singletonErrorResponse(\n                log.topicPartition(),\n                Errors.NOT_LEADER_OR_FOLLOWER\n            );\n        }\n\n        LeaderState<T> leaderState = quorum.leaderStateOrThrow();\n        return DescribeQuorumResponse.singletonResponse(\n            log.topicPartition(),\n            leaderState.describeQuorum(currentTimeMs)\n        );\n    }\n\n    /**\n     * Handle a FetchSnapshot request, similar to the Fetch request but we use {@link UnalignedRecords}\n     * in response because the records are not necessarily offset-aligned.\n     *\n     * This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *     but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     * - {@link Errors#INVALID_REQUEST} if the request epoch is larger than the leader's current epoch\n     *     or if either the fetch offset or the last fetched epoch is invalid\n     * - {@link Errors#SNAPSHOT_NOT_FOUND} if the request snapshot id does not exists\n     * - {@link Errors#POSITION_OUT_OF_RANGE} if the request snapshot offset out of range\n     */\n    private FetchSnapshotResponseData handleFetchSnapshotRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        FetchSnapshotRequestData data = (FetchSnapshotRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(data.clusterId())) {\n            return new FetchSnapshotResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code());\n        }\n\n        if (data.topics().size() != 1 && data.topics().get(0).partitions().size() != 1) {\n            return FetchSnapshotResponse.withTopLevelError(Errors.INVALID_REQUEST);\n        }\n\n        Optional<FetchSnapshotRequestData.PartitionSnapshot> partitionSnapshotOpt = FetchSnapshotRequest\n            .forTopicPartition(data, log.topicPartition());\n        if (!partitionSnapshotOpt.isPresent()) {\n            // The Raft client assumes that there is only one topic partition.\n            TopicPartition unknownTopicPartition = new TopicPartition(\n                data.topics().get(0).name(),\n                data.topics().get(0).partitions().get(0).partition()\n            );\n\n            return FetchSnapshotResponse.singleton(\n                unknownTopicPartition,\n                responsePartitionSnapshot -> responsePartitionSnapshot\n                    .setErrorCode(Errors.UNKNOWN_TOPIC_OR_PARTITION.code())\n            );\n        }\n\n        FetchSnapshotRequestData.PartitionSnapshot partitionSnapshot = partitionSnapshotOpt.get();\n        Optional<Errors> leaderValidation = validateLeaderOnlyRequest(\n                partitionSnapshot.currentLeaderEpoch()\n        );\n        if (leaderValidation.isPresent()) {\n            return FetchSnapshotResponse.singleton(\n                log.topicPartition(),\n                responsePartitionSnapshot -> addQuorumLeader(responsePartitionSnapshot)\n                    .setErrorCode(leaderValidation.get().code())\n            );\n        }\n\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(\n            partitionSnapshot.snapshotId().endOffset(),\n            partitionSnapshot.snapshotId().epoch()\n        );\n        Optional<RawSnapshotReader> snapshotOpt = log.readSnapshot(snapshotId);\n        if (!snapshotOpt.isPresent()) {\n            return FetchSnapshotResponse.singleton(\n                log.topicPartition(),\n                responsePartitionSnapshot -> addQuorumLeader(responsePartitionSnapshot)\n                    .setErrorCode(Errors.SNAPSHOT_NOT_FOUND.code())\n            );\n        }\n\n        RawSnapshotReader snapshot = snapshotOpt.get();\n        long snapshotSize = snapshot.sizeInBytes();\n        if (partitionSnapshot.position() < 0 || partitionSnapshot.position() >= snapshotSize) {\n            return FetchSnapshotResponse.singleton(\n                log.topicPartition(),\n                responsePartitionSnapshot -> addQuorumLeader(responsePartitionSnapshot)\n                    .setErrorCode(Errors.POSITION_OUT_OF_RANGE.code())\n            );\n        }\n\n        if (partitionSnapshot.position() > Integer.MAX_VALUE) {\n            throw new IllegalStateException(\n                String.format(\n                    \"Trying to fetch a snapshot with size (%d) and a position (%d) larger than %d\",\n                    snapshotSize,\n                    partitionSnapshot.position(),\n                    Integer.MAX_VALUE\n                )\n            );\n        }\n\n        int maxSnapshotSize;\n        try {\n            maxSnapshotSize = Math.toIntExact(snapshotSize);\n        } catch (ArithmeticException e) {\n            maxSnapshotSize = Integer.MAX_VALUE;\n        }\n\n        UnalignedRecords records = snapshot.slice(partitionSnapshot.position(), Math.min(data.maxBytes(), maxSnapshotSize));\n\n        LeaderState<T> state = quorum.leaderStateOrThrow();\n        state.updateCheckQuorumForFollowingVoter(data.replicaId(), currentTimeMs);\n\n        return FetchSnapshotResponse.singleton(\n            log.topicPartition(),\n            responsePartitionSnapshot -> {\n                addQuorumLeader(responsePartitionSnapshot)\n                    .snapshotId()\n                    .setEndOffset(snapshotId.offset())\n                    .setEpoch(snapshotId.epoch());\n\n                return responsePartitionSnapshot\n                    .setSize(snapshotSize)\n                    .setPosition(partitionSnapshot.position())\n                    .setUnalignedRecords(records);\n            }\n        );\n    }\n\n    private boolean handleFetchSnapshotResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        FetchSnapshotResponseData data = (FetchSnapshotResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(data.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (data.topics().size() != 1 && data.topics().get(0).partitions().size() != 1) {\n            return false;\n        }\n\n        Optional<FetchSnapshotResponseData.PartitionSnapshot> partitionSnapshotOpt = FetchSnapshotResponse\n            .forTopicPartition(data, log.topicPartition());\n        if (!partitionSnapshotOpt.isPresent()) {\n            return false;\n        }\n\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot = partitionSnapshotOpt.get();\n\n        FetchSnapshotResponseData.LeaderIdAndEpoch currentLeaderIdAndEpoch = partitionSnapshot.currentLeader();\n        OptionalInt responseLeaderId = optionalLeaderId(currentLeaderIdAndEpoch.leaderId());\n        int responseEpoch = currentLeaderIdAndEpoch.leaderEpoch();\n        Errors error = Errors.forCode(partitionSnapshot.errorCode());\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            error, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        }\n\n        FollowerState state = quorum.followerStateOrThrow();\n\n        if (Errors.forCode(partitionSnapshot.errorCode()) == Errors.SNAPSHOT_NOT_FOUND ||\n            partitionSnapshot.snapshotId().endOffset() < 0 ||\n            partitionSnapshot.snapshotId().epoch() < 0) {\n\n            /* The leader deleted the snapshot before the follower could download it. Start over by\n             * resetting the fetching snapshot state and sending another fetch request.\n             */\n            logger.info(\n                \"Leader doesn't know about snapshot id {}, returned error {} and snapshot id {}\",\n                state.fetchingSnapshot(),\n                partitionSnapshot.errorCode(),\n                partitionSnapshot.snapshotId()\n            );\n            state.setFetchingSnapshot(Optional.empty());\n            state.resetFetchTimeout(currentTimeMs);\n            return true;\n        }\n\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(\n            partitionSnapshot.snapshotId().endOffset(),\n            partitionSnapshot.snapshotId().epoch()\n        );\n\n        RawSnapshotWriter snapshot;\n        if (state.fetchingSnapshot().isPresent()) {\n            snapshot = state.fetchingSnapshot().get();\n        } else {\n            throw new IllegalStateException(\n                String.format(\"Received unexpected fetch snapshot response: %s\", partitionSnapshot)\n            );\n        }\n\n        if (!snapshot.snapshotId().equals(snapshotId)) {\n            throw new IllegalStateException(\n                String.format(\n                    \"Received fetch snapshot response with an invalid id. Expected %s; Received %s\",\n                    snapshot.snapshotId(),\n                    snapshotId\n                )\n            );\n        }\n        if (snapshot.sizeInBytes() != partitionSnapshot.position()) {\n            throw new IllegalStateException(\n                String.format(\n                    \"Received fetch snapshot response with an invalid position. Expected %d; Received %d\",\n                    snapshot.sizeInBytes(),\n                    partitionSnapshot.position()\n                )\n            );\n        }\n\n        final UnalignedMemoryRecords records;\n        if (partitionSnapshot.unalignedRecords() instanceof MemoryRecords) {\n            records = new UnalignedMemoryRecords(((MemoryRecords) partitionSnapshot.unalignedRecords()).buffer());\n        } else if (partitionSnapshot.unalignedRecords() instanceof UnalignedMemoryRecords) {\n            records = (UnalignedMemoryRecords) partitionSnapshot.unalignedRecords();\n        } else {\n            throw new IllegalStateException(String.format(\"Received unexpected fetch snapshot response: %s\", partitionSnapshot));\n        }\n        snapshot.append(records);\n\n        if (snapshot.sizeInBytes() == partitionSnapshot.size()) {\n            // Finished fetching the snapshot.\n            snapshot.freeze();\n            state.setFetchingSnapshot(Optional.empty());\n\n            if (log.truncateToLatestSnapshot()) {\n                logger.info(\n                    \"Fully truncated the log at ({}, {}) after downloading snapshot {} from leader {}\",\n                    log.endOffset(),\n                    log.lastFetchedEpoch(),\n                    snapshot.snapshotId(),\n                    quorum.leaderIdOrSentinel()\n                );\n\n                // This will always reload the snapshot because the internal next offset\n                // is always less than the snapshot id just downloaded.\n                partitionState.updateState();\n\n                updateFollowerHighWatermark(state, OptionalLong.of(log.highWatermark().offset));\n            } else {\n                throw new IllegalStateException(\n                    String.format(\n                        \"Full log truncation expected but didn't happen. Snapshot of %s, log end offset %s, last fetched %d\",\n                        snapshot.snapshotId(),\n                        log.endOffset(),\n                        log.lastFetchedEpoch()\n                    )\n                );\n            }\n        }\n\n        state.resetFetchTimeout(currentTimeMs);\n        return true;\n    }\n\n    private boolean hasConsistentLeader(int epoch, OptionalInt leaderId) {\n        // Only elected leaders are sent in the request/response header, so if we have an elected\n        // leaderId, it should be consistent with what is in the message.\n        if (leaderId.isPresent() && leaderId.getAsInt() == quorum.localIdOrSentinel()) {\n            // The response indicates that we should be the leader, so we verify that is the case\n            return quorum.isLeader();\n        } else {\n            return epoch != quorum.epoch()\n                || !leaderId.isPresent()\n                || !quorum.leaderId().isPresent()\n                || leaderId.equals(quorum.leaderId());\n        }\n    }\n\n    /**\n     * Handle response errors that are common across request types.\n     *\n     * @param error Error from the received response\n     * @param leaderId Optional leaderId from the response\n     * @param epoch Epoch received from the response\n     * @param currentTimeMs Current epoch time in milliseconds\n     * @return Optional value indicating whether the error was handled here and the outcome of\n     *    that handling. Specifically:\n     *\n     *    - Optional.empty means that the response was not handled here and the custom\n     *        API handler should be applied\n     *    - Optional.of(true) indicates that the response was successfully handled here and\n     *        the request does not need to be retried\n     *    - Optional.of(false) indicates that the response was handled here, but that the request\n     *        will need to be retried\n     */\n    private Optional<Boolean> maybeHandleCommonResponse(\n        Errors error,\n        OptionalInt leaderId,\n        int epoch,\n        long currentTimeMs\n    ) {\n        if (epoch < quorum.epoch() || error == Errors.UNKNOWN_LEADER_EPOCH) {\n            // We have a larger epoch, so the response is no longer relevant\n            return Optional.of(true);\n        } else if (epoch > quorum.epoch()\n            || error == Errors.FENCED_LEADER_EPOCH\n            || error == Errors.NOT_LEADER_OR_FOLLOWER) {\n\n            // The response indicates that the request had a stale epoch, but we need\n            // to validate the epoch from the response against our current state.\n            maybeTransition(leaderId, epoch, currentTimeMs);\n            return Optional.of(true);\n        } else if (epoch == quorum.epoch()\n            && leaderId.isPresent()\n            && !quorum.hasLeader()) {\n\n            // Since we are transitioning to Follower, we will only forward the\n            // request to the handler if there is no error. Otherwise, we will let\n            // the request be retried immediately (if needed) after the transition.\n            // This handling allows an observer to discover the leader and append\n            // to the log in the same Fetch request.\n            transitionToFollower(epoch, leaderId.getAsInt(), currentTimeMs);\n            if (error == Errors.NONE) {\n                return Optional.empty();\n            } else {\n                return Optional.of(true);\n            }\n        } else if (error == Errors.BROKER_NOT_AVAILABLE) {\n            return Optional.of(false);\n        } else if (error == Errors.INCONSISTENT_GROUP_PROTOCOL) {\n            // For now we treat this as a fatal error. Once we have support for quorum\n            // reassignment, this error could suggest that either we or the recipient of\n            // the request just has stale voter information, which means we can retry\n            // after backing off.\n            throw new IllegalStateException(\"Received error indicating inconsistent voter sets\");\n        } else if (error == Errors.INVALID_REQUEST) {\n            throw new IllegalStateException(\"Received unexpected invalid request error\");\n        }\n\n        return Optional.empty();\n    }\n\n    private void maybeTransition(\n        OptionalInt leaderId,\n        int epoch,\n        long currentTimeMs\n    ) {\n        if (!hasConsistentLeader(epoch, leaderId)) {\n            throw new IllegalStateException(\"Received request or response with leader \" + leaderId +\n                \" and epoch \" + epoch + \" which is inconsistent with current leader \" +\n                quorum.leaderId() + \" and epoch \" + quorum.epoch());\n        } else if (epoch > quorum.epoch()) {\n            if (leaderId.isPresent()) {\n                transitionToFollower(epoch, leaderId.getAsInt(), currentTimeMs);\n            } else {\n                transitionToUnattached(epoch);\n            }\n        } else if (leaderId.isPresent() && !quorum.hasLeader()) {\n            // The request or response indicates the leader of the current epoch,\n            // which is currently unknown\n            transitionToFollower(epoch, leaderId.getAsInt(), currentTimeMs);\n        }\n    }\n\n    private boolean handleTopLevelError(Errors error, RaftResponse.Inbound response) {\n        if (error == Errors.BROKER_NOT_AVAILABLE) {\n            return false;\n        } else if (error == Errors.CLUSTER_AUTHORIZATION_FAILED) {\n            throw new ClusterAuthorizationException(\"Received cluster authorization error in response \" + response);\n        } else {\n            return handleUnexpectedError(error, response);\n        }\n    }\n\n    private boolean handleUnexpectedError(Errors error, RaftResponse.Inbound response) {\n        logger.error(\"Unexpected error {} in {} response: {}\",\n            error, ApiKeys.forId(response.data.apiKey()), response);\n        return false;\n    }\n\n    private void handleResponse(RaftResponse.Inbound response, long currentTimeMs) {\n        // The response epoch matches the local epoch, so we can handle the response\n        ApiKeys apiKey = ApiKeys.forId(response.data.apiKey());\n        final boolean handledSuccessfully;\n\n        switch (apiKey) {\n            case FETCH:\n                handledSuccessfully = handleFetchResponse(response, currentTimeMs);\n                break;\n\n            case VOTE:\n                handledSuccessfully = handleVoteResponse(response, currentTimeMs);\n                break;\n\n            case BEGIN_QUORUM_EPOCH:\n                handledSuccessfully = handleBeginQuorumEpochResponse(response, currentTimeMs);\n                break;\n\n            case END_QUORUM_EPOCH:\n                handledSuccessfully = handleEndQuorumEpochResponse(response, currentTimeMs);\n                break;\n\n            case FETCH_SNAPSHOT:\n                handledSuccessfully = handleFetchSnapshotResponse(response, currentTimeMs);\n                break;\n\n            default:\n                throw new IllegalArgumentException(\"Received unexpected response type: \" + apiKey);\n        }\n\n        ConnectionState connection = requestManager.getOrCreate(response.sourceId());\n        if (handledSuccessfully) {\n            connection.onResponseReceived(response.correlationId);\n        } else {\n            connection.onResponseError(response.correlationId, currentTimeMs);\n        }\n    }\n\n    /**\n     * Validate common state for requests to establish leadership.\n     *\n     * These include the Vote, BeginQuorumEpoch and EndQuorumEpoch RPCs. If an error is present in\n     * the returned value, it should be returned in the response.\n     */\n    private Optional<Errors> validateVoterOnlyRequest(int remoteNodeId, int requestEpoch) {\n        if (requestEpoch < quorum.epoch()) {\n            return Optional.of(Errors.FENCED_LEADER_EPOCH);\n        } else if (remoteNodeId < 0) {\n            return Optional.of(Errors.INVALID_REQUEST);\n        } else {\n            return Optional.empty();\n        }\n    }\n\n    /**\n     * Validate a request which is intended for the current quorum leader.\n     * If an error is present in the returned value, it should be returned\n     * in the response.\n     */\n    private Optional<Errors> validateLeaderOnlyRequest(int requestEpoch) {\n        if (requestEpoch < quorum.epoch()) {\n            return Optional.of(Errors.FENCED_LEADER_EPOCH);\n        } else if (requestEpoch > quorum.epoch()) {\n            return Optional.of(Errors.UNKNOWN_LEADER_EPOCH);\n        } else if (!quorum.isLeader()) {\n            // In general, non-leaders do not expect to receive requests\n            // matching their own epoch, but it is possible when observers\n            // are using the Fetch API to find the result of an election.\n            return Optional.of(Errors.NOT_LEADER_OR_FOLLOWER);\n        } else if (shutdown.get() != null) {\n            return Optional.of(Errors.BROKER_NOT_AVAILABLE);\n        } else {\n            return Optional.empty();\n        }\n    }\n\n    private void handleRequest(RaftRequest.Inbound request, long currentTimeMs) {\n        ApiKeys apiKey = ApiKeys.forId(request.data.apiKey());\n        final CompletableFuture<? extends ApiMessage> responseFuture;\n\n        switch (apiKey) {\n            case FETCH:\n                responseFuture = handleFetchRequest(request, currentTimeMs);\n                break;\n\n            case VOTE:\n                responseFuture = completedFuture(handleVoteRequest(request));\n                break;\n\n            case BEGIN_QUORUM_EPOCH:\n                responseFuture = completedFuture(handleBeginQuorumEpochRequest(request, currentTimeMs));\n                break;\n\n            case END_QUORUM_EPOCH:\n                responseFuture = completedFuture(handleEndQuorumEpochRequest(request, currentTimeMs));\n                break;\n\n            case DESCRIBE_QUORUM:\n                responseFuture = completedFuture(handleDescribeQuorumRequest(request, currentTimeMs));\n                break;\n\n            case FETCH_SNAPSHOT:\n                responseFuture = completedFuture(handleFetchSnapshotRequest(request, currentTimeMs));\n                break;\n\n            default:\n                throw new IllegalArgumentException(\"Unexpected request type \" + apiKey);\n        }\n\n        responseFuture.whenComplete((response, exception) -> {\n            final ApiMessage message;\n            if (response != null) {\n                message = response;\n            } else {\n                message = RaftUtil.errorResponse(apiKey, Errors.forException(exception));\n            }\n\n            RaftResponse.Outbound responseMessage = new RaftResponse.Outbound(request.correlationId(), message);\n            request.completion.complete(responseMessage);\n            logger.trace(\"Sent response {} to inbound request {}\", responseMessage, request);\n        });\n    }\n\n    private void handleInboundMessage(RaftMessage message, long currentTimeMs) {\n        logger.trace(\"Received inbound message {}\", message);\n\n        if (message instanceof RaftRequest.Inbound) {\n            RaftRequest.Inbound request = (RaftRequest.Inbound) message;\n            handleRequest(request, currentTimeMs);\n        } else if (message instanceof RaftResponse.Inbound) {\n            RaftResponse.Inbound response = (RaftResponse.Inbound) message;\n            ConnectionState connection = requestManager.getOrCreate(response.sourceId());\n            if (connection.isResponseExpected(response.correlationId)) {\n                handleResponse(response, currentTimeMs);\n            } else {\n                logger.debug(\"Ignoring response {} since it is no longer needed\", response);\n            }\n        } else {\n            throw new IllegalArgumentException(\"Unexpected message \" + message);\n        }\n    }\n\n    /**\n     * Attempt to send a request. Return the time to wait before the request can be retried.\n     */\n    private long maybeSendRequest(\n        long currentTimeMs,\n        int destinationId,\n        Supplier<ApiMessage> requestSupplier\n    )  {\n        ConnectionState connection = requestManager.getOrCreate(destinationId);\n\n        if (connection.isBackingOff(currentTimeMs)) {\n            long remainingBackoffMs = connection.remainingBackoffMs(currentTimeMs);\n            logger.debug(\"Connection for {} is backing off for {} ms\", destinationId, remainingBackoffMs);\n            return remainingBackoffMs;\n        }\n\n        if (connection.isReady(currentTimeMs)) {\n            int correlationId = channel.newCorrelationId();\n            ApiMessage request = requestSupplier.get();\n\n            RaftRequest.Outbound requestMessage = new RaftRequest.Outbound(\n                correlationId,\n                request,\n                destinationId,\n                currentTimeMs\n            );\n\n            requestMessage.completion.whenComplete((response, exception) -> {\n                if (exception != null) {\n                    ApiKeys api = ApiKeys.forId(request.apiKey());\n                    Errors error = Errors.forException(exception);\n                    ApiMessage errorResponse = RaftUtil.errorResponse(api, error);\n\n                    response = new RaftResponse.Inbound(\n                        correlationId,\n                        errorResponse,\n                        destinationId\n                    );\n                }\n\n                messageQueue.add(response);\n            });\n\n            channel.send(requestMessage);\n            logger.trace(\"Sent outbound request: {}\", requestMessage);\n            connection.onRequestSent(correlationId, currentTimeMs);\n            return Long.MAX_VALUE;\n        }\n\n        return connection.remainingRequestTimeMs(currentTimeMs);\n    }\n\n    private EndQuorumEpochRequestData buildEndQuorumEpochRequest(\n        ResignedState state\n    ) {\n        return EndQuorumEpochRequest.singletonRequest(\n            log.topicPartition(),\n            clusterId,\n            quorum.epoch(),\n            quorum.localIdOrThrow(),\n            state.preferredSuccessors()\n        );\n    }\n\n    private long maybeSendRequests(\n        long currentTimeMs,\n        Set<Integer> destinationIds,\n        Supplier<ApiMessage> requestSupplier\n    ) {\n        long minBackoffMs = Long.MAX_VALUE;\n        for (Integer destinationId : destinationIds) {\n            long backoffMs = maybeSendRequest(currentTimeMs, destinationId, requestSupplier);\n            if (backoffMs < minBackoffMs) {\n                minBackoffMs = backoffMs;\n            }\n        }\n        return minBackoffMs;\n    }\n\n    private BeginQuorumEpochRequestData buildBeginQuorumEpochRequest() {\n        return BeginQuorumEpochRequest.singletonRequest(\n            log.topicPartition(),\n            clusterId,\n            quorum.epoch(),\n            quorum.localIdOrThrow()\n        );\n    }\n\n    private VoteRequestData buildVoteRequest() {\n        OffsetAndEpoch endOffset = endOffset();\n        return VoteRequest.singletonRequest(\n            log.topicPartition(),\n            clusterId,\n            quorum.epoch(),\n            quorum.localIdOrThrow(),\n            endOffset.epoch(),\n            endOffset.offset()\n        );\n    }\n\n    private FetchRequestData buildFetchRequest() {\n        FetchRequestData request = RaftUtil.singletonFetchRequest(log.topicPartition(), log.topicId(), fetchPartition -> {\n            fetchPartition\n                .setCurrentLeaderEpoch(quorum.epoch())\n                .setLastFetchedEpoch(log.lastFetchedEpoch())\n                .setFetchOffset(log.endOffset().offset);\n        });\n        return request\n            .setMaxBytes(MAX_FETCH_SIZE_BYTES)\n            .setMaxWaitMs(fetchMaxWaitMs)\n            .setClusterId(clusterId)\n            .setReplicaState(new FetchRequestData.ReplicaState().setReplicaId(quorum.localIdOrSentinel()));\n    }\n\n    private long maybeSendAnyVoterFetch(long currentTimeMs) {\n        OptionalInt readyVoterIdOpt = requestManager.findReadyVoter(currentTimeMs);\n        if (readyVoterIdOpt.isPresent()) {\n            return maybeSendRequest(\n                currentTimeMs,\n                readyVoterIdOpt.getAsInt(),\n                this::buildFetchRequest\n            );\n        } else {\n            return requestManager.backoffBeforeAvailableVoter(currentTimeMs);\n        }\n    }\n\n    private FetchSnapshotRequestData buildFetchSnapshotRequest(OffsetAndEpoch snapshotId, long snapshotSize) {\n        FetchSnapshotRequestData.SnapshotId requestSnapshotId = new FetchSnapshotRequestData.SnapshotId()\n            .setEpoch(snapshotId.epoch())\n            .setEndOffset(snapshotId.offset());\n\n        FetchSnapshotRequestData request = FetchSnapshotRequest.singleton(\n            clusterId,\n            quorum().localIdOrSentinel(),\n            log.topicPartition(),\n            snapshotPartition -> snapshotPartition\n                .setCurrentLeaderEpoch(quorum.epoch())\n                .setSnapshotId(requestSnapshotId)\n                .setPosition(snapshotSize)\n        );\n\n        return request.setReplicaId(quorum.localIdOrSentinel());\n    }\n\n    private FetchSnapshotResponseData.PartitionSnapshot addQuorumLeader(\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot\n    ) {\n        partitionSnapshot.currentLeader()\n            .setLeaderEpoch(quorum.epoch())\n            .setLeaderId(quorum.leaderIdOrSentinel());\n\n        return partitionSnapshot;\n    }\n\n    public boolean isRunning() {\n        GracefulShutdown gracefulShutdown = shutdown.get();\n        return gracefulShutdown == null || !gracefulShutdown.isFinished();\n    }\n\n    public boolean isShuttingDown() {\n        GracefulShutdown gracefulShutdown = shutdown.get();\n        return gracefulShutdown != null && !gracefulShutdown.isFinished();\n    }\n\n    private void appendBatch(\n        LeaderState<T> state,\n        BatchAccumulator.CompletedBatch<T> batch,\n        long appendTimeMs\n    ) {\n        try {\n            int epoch = state.epoch();\n            LogAppendInfo info = appendAsLeader(batch.data);\n            OffsetAndEpoch offsetAndEpoch = new OffsetAndEpoch(info.lastOffset, epoch);\n            CompletableFuture<Long> future = appendPurgatory.await(\n                offsetAndEpoch.offset() + 1, Integer.MAX_VALUE);\n\n            future.whenComplete((commitTimeMs, exception) -> {\n                if (exception != null) {\n                    logger.debug(\"Failed to commit {} records up to last offset {}\", batch.numRecords, offsetAndEpoch, exception);\n                } else {\n                    long elapsedTime = Math.max(0, commitTimeMs - appendTimeMs);\n                    double elapsedTimePerRecord = (double) elapsedTime / batch.numRecords;\n                    kafkaRaftMetrics.updateCommitLatency(elapsedTimePerRecord, appendTimeMs);\n                    logger.debug(\"Completed commit of {} records up to last offset {}\", batch.numRecords, offsetAndEpoch);\n                    batch.records.ifPresent(records -> {\n                        maybeFireHandleCommit(batch.baseOffset, epoch, batch.appendTimestamp(), batch.sizeInBytes(), records);\n                    });\n                }\n            });\n        } finally {\n            batch.release();\n        }\n    }\n\n    private long maybeAppendBatches(\n        LeaderState<T> state,\n        long currentTimeMs\n    ) {\n        long timeUntilDrain = state.accumulator().timeUntilDrain(currentTimeMs);\n        if (timeUntilDrain <= 0) {\n            List<BatchAccumulator.CompletedBatch<T>> batches = state.accumulator().drain();\n            Iterator<BatchAccumulator.CompletedBatch<T>> iterator = batches.iterator();\n\n            try {\n                while (iterator.hasNext()) {\n                    BatchAccumulator.CompletedBatch<T> batch = iterator.next();\n                    appendBatch(state, batch, currentTimeMs);\n                }\n                flushLeaderLog(state, currentTimeMs);\n            } finally {\n                // Release and discard any batches which failed to be appended\n                while (iterator.hasNext()) {\n                    iterator.next().release();\n                }\n            }\n        }\n        return timeUntilDrain;\n    }\n\n    private long pollResigned(long currentTimeMs) {\n        ResignedState state = quorum.resignedStateOrThrow();\n        long endQuorumBackoffMs = maybeSendRequests(\n            currentTimeMs,\n            state.unackedVoters(),\n            () -> buildEndQuorumEpochRequest(state)\n        );\n\n        GracefulShutdown shutdown = this.shutdown.get();\n        final long stateTimeoutMs;\n        if (shutdown != null) {\n            // If we are shutting down, then we will remain in the resigned state\n            // until either the shutdown expires or an election bumps the epoch\n            stateTimeoutMs = shutdown.remainingTimeMs();\n        } else if (state.hasElectionTimeoutExpired(currentTimeMs)) {\n            transitionToCandidate(currentTimeMs);\n            stateTimeoutMs = 0L;\n        } else {\n            stateTimeoutMs = state.remainingElectionTimeMs(currentTimeMs);\n        }\n\n        return Math.min(stateTimeoutMs, endQuorumBackoffMs);\n    }\n\n    private long pollLeader(long currentTimeMs) {\n        LeaderState<T> state = quorum.leaderStateOrThrow();\n        maybeFireLeaderChange(state);\n\n        long timeUntilCheckQuorumExpires = state.timeUntilCheckQuorumExpires(currentTimeMs);\n        if (shutdown.get() != null || state.isResignRequested() || timeUntilCheckQuorumExpires == 0) {\n            transitionToResigned(state.nonLeaderVotersByDescendingFetchOffset());\n            return 0L;\n        }\n\n        long timeUntilFlush = maybeAppendBatches(\n            state,\n            currentTimeMs\n        );\n\n        long timeUntilSend = maybeSendRequests(\n            currentTimeMs,\n            state.nonAcknowledgingVoters(),\n            this::buildBeginQuorumEpochRequest\n        );\n\n        return Math.min(timeUntilFlush, Math.min(timeUntilSend, timeUntilCheckQuorumExpires));\n    }\n\n    private long maybeSendVoteRequests(\n        CandidateState state,\n        long currentTimeMs\n    ) {\n        // Continue sending Vote requests as long as we still have a chance to win the election\n        if (!state.isVoteRejected()) {\n            return maybeSendRequests(\n                currentTimeMs,\n                state.unrecordedVoters(),\n                this::buildVoteRequest\n            );\n        }\n        return Long.MAX_VALUE;\n    }\n\n    private long pollCandidate(long currentTimeMs) {\n        CandidateState state = quorum.candidateStateOrThrow();\n        GracefulShutdown shutdown = this.shutdown.get();\n\n        if (shutdown != null) {\n            // If we happen to shutdown while we are a candidate, we will continue\n            // with the current election until one of the following conditions is met:\n            //  1) we are elected as leader (which allows us to resign)\n            //  2) another leader is elected\n            //  3) the shutdown timer expires\n            long minRequestBackoffMs = maybeSendVoteRequests(state, currentTimeMs);\n            return Math.min(shutdown.remainingTimeMs(), minRequestBackoffMs);\n        } else if (state.isBackingOff()) {\n            if (state.isBackoffComplete(currentTimeMs)) {\n                logger.info(\"Re-elect as candidate after election backoff has completed\");\n                transitionToCandidate(currentTimeMs);\n                return 0L;\n            }\n            return state.remainingBackoffMs(currentTimeMs);\n        } else if (state.hasElectionTimeoutExpired(currentTimeMs)) {\n            long backoffDurationMs = binaryExponentialElectionBackoffMs(state.retries());\n            logger.info(\"Election has timed out, backing off for {}ms before becoming a candidate again\",\n                backoffDurationMs);\n            state.startBackingOff(currentTimeMs, backoffDurationMs);\n            return backoffDurationMs;\n        } else {\n            long minRequestBackoffMs = maybeSendVoteRequests(state, currentTimeMs);\n            return Math.min(minRequestBackoffMs, state.remainingElectionTimeMs(currentTimeMs));\n        }\n    }\n\n    private long pollFollower(long currentTimeMs) {\n        FollowerState state = quorum.followerStateOrThrow();\n        if (quorum.isVoter()) {\n            return pollFollowerAsVoter(state, currentTimeMs);\n        } else {\n            return pollFollowerAsObserver(state, currentTimeMs);\n        }\n    }\n\n    private long pollFollowerAsVoter(FollowerState state, long currentTimeMs) {\n        GracefulShutdown shutdown = this.shutdown.get();\n        if (shutdown != null) {\n            // If we are a follower, then we can shutdown immediately. We want to\n            // skip the transition to candidate in any case.\n            return 0;\n        } else if (state.hasFetchTimeoutExpired(currentTimeMs)) {\n            logger.info(\"Become candidate due to fetch timeout\");\n            transitionToCandidate(currentTimeMs);\n            return 0L;\n        } else {\n            long backoffMs = maybeSendFetchOrFetchSnapshot(state, currentTimeMs);\n\n            return Math.min(backoffMs, state.remainingFetchTimeMs(currentTimeMs));\n        }\n    }\n\n    private long pollFollowerAsObserver(FollowerState state, long currentTimeMs) {\n        if (state.hasFetchTimeoutExpired(currentTimeMs)) {\n            return maybeSendAnyVoterFetch(currentTimeMs);\n        } else {\n            final long backoffMs;\n\n            // If the current leader is backing off due to some failure or if the\n            // request has timed out, then we attempt to send the Fetch to another\n            // voter in order to discover if there has been a leader change.\n            ConnectionState connection = hasAnyInflightRequest(state);\n            if (connection.hasRequestTimedOut(currentTimeMs)) {\n                backoffMs = maybeSendAnyVoterFetch(currentTimeMs);\n                connection.reset();\n            } else if (connection.isBackingOff(currentTimeMs)) {\n                backoffMs = maybeSendAnyVoterFetch(currentTimeMs);\n            } else {\n                backoffMs = maybeSendFetchOrFetchSnapshot(state, currentTimeMs);\n            }\n\n            return Math.min(backoffMs, state.remainingFetchTimeMs(currentTimeMs));\n        }\n    }\n\n    private ConnectionState hasAnyInflightRequest(FollowerState state) {\n        ConnectionState connection = requestManager.getOrCreate(state.leaderId());\n        return connection;\n    }\n\n    private long maybeSendFetchOrFetchSnapshot(FollowerState state, long currentTimeMs) {\n        final Supplier<ApiMessage> requestSupplier;\n\n        if (state.fetchingSnapshot().isPresent()) {\n            RawSnapshotWriter snapshot = state.fetchingSnapshot().get();\n            long snapshotSize = snapshot.sizeInBytes();\n\n            requestSupplier = () -> buildFetchSnapshotRequest(snapshot.snapshotId(), snapshotSize);\n        } else {\n            requestSupplier = this::buildFetchRequest;\n        }\n\n        return maybeSendRequest(currentTimeMs, state.leaderId(), requestSupplier);\n    }\n\n    private long pollVoted(long currentTimeMs) {\n        VotedState state = quorum.votedStateOrThrow();\n        GracefulShutdown shutdown = this.shutdown.get();\n\n        if (shutdown != null) {\n            // If shutting down, then remain in this state until either the\n            // shutdown completes or an epoch bump forces another state transition\n            return shutdown.remainingTimeMs();\n        } else if (state.hasElectionTimeoutExpired(currentTimeMs)) {\n            transitionToCandidate(currentTimeMs);\n            return 0L;\n        } else {\n            return state.remainingElectionTimeMs(currentTimeMs);\n        }\n    }\n\n    private long pollUnattached(long currentTimeMs) {\n        UnattachedState state = quorum.unattachedStateOrThrow();\n        if (quorum.isVoter()) {\n            return pollUnattachedAsVoter(state, currentTimeMs);\n        } else {\n            return pollUnattachedAsObserver(state, currentTimeMs);\n        }\n    }\n\n    private long pollUnattachedAsVoter(UnattachedState state, long currentTimeMs) {\n        GracefulShutdown shutdown = this.shutdown.get();\n        if (shutdown != null) {\n            // If shutting down, then remain in this state until either the\n            // shutdown completes or an epoch bump forces another state transition\n            return shutdown.remainingTimeMs();\n        } else if (state.hasElectionTimeoutExpired(currentTimeMs)) {\n            transitionToCandidate(currentTimeMs);\n            return 0L;\n        } else {\n            return state.remainingElectionTimeMs(currentTimeMs);\n        }\n    }\n\n    private long pollUnattachedAsObserver(UnattachedState state, long currentTimeMs) {\n        long fetchBackoffMs = maybeSendAnyVoterFetch(currentTimeMs);\n        return Math.min(fetchBackoffMs, state.remainingElectionTimeMs(currentTimeMs));\n    }\n\n    private long pollCurrentState(long currentTimeMs) {\n        if (quorum.isLeader()) {\n            return pollLeader(currentTimeMs);\n        } else if (quorum.isCandidate()) {\n            return pollCandidate(currentTimeMs);\n        } else if (quorum.isFollower()) {\n            return pollFollower(currentTimeMs);\n        } else if (quorum.isVoted()) {\n            return pollVoted(currentTimeMs);\n        } else if (quorum.isUnattached()) {\n            return pollUnattached(currentTimeMs);\n        } else if (quorum.isResigned()) {\n            return pollResigned(currentTimeMs);\n        } else {\n            throw new IllegalStateException(\"Unexpected quorum state \" + quorum);\n        }\n    }\n\n    private void pollListeners() {\n        // Apply all of the pending registration\n        while (true) {\n            Registration<T> registration = pendingRegistrations.poll();\n            if (registration == null) {\n                break;\n            }\n\n            processRegistration(registration);\n        }\n\n        // Check listener progress to see if reads are expected\n        quorum.highWatermark().ifPresent(highWatermarkMetadata -> {\n            updateListenersProgress(highWatermarkMetadata.offset);\n        });\n\n        // Notify the new listeners of the latest leader and epoch\n        Optional<LeaderState<T>> leaderState = quorum.maybeLeaderState();\n        if (leaderState.isPresent()) {\n            maybeFireLeaderChange(leaderState.get());\n        } else {\n            maybeFireLeaderChange();\n        }\n    }\n\n    private void processRegistration(Registration<T> registration) {\n        Listener<T> listener = registration.listener();\n        Registration.Ops ops = registration.ops();\n\n        if (ops == Registration.Ops.REGISTER) {\n            if (listenerContexts.putIfAbsent(listener, new ListenerContext(listener)) != null) {\n                logger.error(\"Attempting to add a listener that already exists: {}\", listenerName(listener));\n            } else {\n                logger.info(\"Registered the listener {}\", listenerName(listener));\n            }\n        } else {\n            if (listenerContexts.remove(listener) == null) {\n                logger.error(\"Attempting to remove a listener that doesn't exists: {}\", listenerName(listener));\n            } else {\n                logger.info(\"Unregistered the listener {}\", listenerName(listener));\n            }\n        }\n    }\n\n    private boolean maybeCompleteShutdown(long currentTimeMs) {\n        GracefulShutdown shutdown = this.shutdown.get();\n        if (shutdown == null) {\n            return false;\n        }\n\n        shutdown.update(currentTimeMs);\n        if (shutdown.hasTimedOut()) {\n            shutdown.failWithTimeout();\n            return true;\n        }\n\n        if (quorum.isObserver()\n            || quorum.isOnlyVoter()\n            || quorum.hasRemoteLeader()\n        ) {\n            shutdown.complete();\n            return true;\n        }\n\n        return false;\n    }\n\n    /**\n     * A simple timer based log cleaner\n     */\n    private static class RaftMetadataLogCleanerManager {\n        private final Logger logger;\n        private final Timer timer;\n        private final long delayMs;\n        private final Runnable cleaner;\n\n        RaftMetadataLogCleanerManager(Logger logger, Time time, long delayMs, Runnable cleaner) {\n            this.logger = logger;\n            this.timer = time.timer(delayMs);\n            this.delayMs = delayMs;\n            this.cleaner = cleaner;\n        }\n\n        public long maybeClean(long currentTimeMs) {\n            timer.update(currentTimeMs);\n            if (timer.isExpired()) {\n                try {\n                    cleaner.run();\n                } catch (Throwable t) {\n                    logger.error(\"Had an error during log cleaning\", t);\n                }\n                timer.reset(delayMs);\n            }\n            return timer.remainingMs();\n        }\n    }\n\n    private void wakeup() {\n        messageQueue.wakeup();\n    }\n\n    /**\n     * Handle an inbound request. The response will be returned through\n     * {@link RaftRequest.Inbound#completion}.\n     *\n     * @param request The inbound request\n     */\n    public void handle(RaftRequest.Inbound request) {\n        messageQueue.add(Objects.requireNonNull(request));\n    }\n\n    /**\n     * Poll for new events. This allows the client to handle inbound\n     * requests and send any needed outbound requests.\n     */\n    public void poll() {\n        if (!isInitialized()) {\n            throw new IllegalStateException(\"Replica needs to be initialized before polling\");\n        }\n\n        long startPollTimeMs = time.milliseconds();\n        if (maybeCompleteShutdown(startPollTimeMs)) {\n            return;\n        }\n\n        long pollStateTimeoutMs = pollCurrentState(startPollTimeMs);\n        long cleaningTimeoutMs = snapshotCleaner.maybeClean(startPollTimeMs);\n        long pollTimeoutMs = Math.min(pollStateTimeoutMs, cleaningTimeoutMs);\n\n        long startWaitTimeMs = time.milliseconds();\n        kafkaRaftMetrics.updatePollStart(startWaitTimeMs);\n\n        RaftMessage message = messageQueue.poll(pollTimeoutMs);\n\n        long endWaitTimeMs = time.milliseconds();\n        kafkaRaftMetrics.updatePollEnd(endWaitTimeMs);\n\n        if (message != null) {\n            handleInboundMessage(message, endWaitTimeMs);\n        }\n\n        pollListeners();\n    }\n\n    @Override\n    public long scheduleAppend(int epoch, List<T> records) {\n        return append(epoch, records, OptionalLong.empty(), false);\n    }\n\n    @Override\n    public long scheduleAtomicAppend(int epoch, OptionalLong requiredBaseOffset, List<T> records) {\n        return append(epoch, records, requiredBaseOffset, true);\n    }\n\n    private long append(int epoch, List<T> records, OptionalLong requiredBaseOffset, boolean isAtomic) {\n        if (!isInitialized()) {\n            throw new NotLeaderException(\"Append failed because the replica is not the current leader\");\n        }\n\n        LeaderState<T> leaderState = quorum.<T>maybeLeaderState().orElseThrow(\n            () -> new NotLeaderException(\"Append failed because the replica is not the current leader\")\n        );\n\n        BatchAccumulator<T> accumulator = leaderState.accumulator();\n        boolean isFirstAppend = accumulator.isEmpty();\n        final long offset = accumulator.append(epoch, records, requiredBaseOffset, isAtomic);\n\n        // Wakeup the network channel if either this is the first append\n        // or the accumulator is ready to drain now. Checking for the first\n        // append ensures that we give the IO thread a chance to observe\n        // the linger timeout so that it can schedule its own wakeup in case\n        // there are no additional appends.\n        if (isFirstAppend || accumulator.needsDrain(time.milliseconds())) {\n            wakeup();\n        }\n        return offset;\n    }\n\n    @Override\n    public CompletableFuture<Void> shutdown(int timeoutMs) {\n        logger.info(\"Beginning graceful shutdown\");\n        CompletableFuture<Void> shutdownComplete = new CompletableFuture<>();\n        shutdown.set(new GracefulShutdown(timeoutMs, shutdownComplete));\n        wakeup();\n        return shutdownComplete;\n    }\n\n    @Override\n    public void resign(int epoch) {\n        if (epoch < 0) {\n            throw new IllegalArgumentException(\"Attempt to resign from an invalid negative epoch \" + epoch);\n        } else if (!isInitialized()) {\n            throw new IllegalStateException(\"Replica needs to be initialized before resigning\");\n        } else if (!quorum.isVoter()) {\n            throw new IllegalStateException(\"Attempt to resign by a non-voter\");\n        }\n\n        LeaderAndEpoch leaderAndEpoch = leaderAndEpoch();\n        int currentEpoch = leaderAndEpoch.epoch();\n\n        if (epoch > currentEpoch) {\n            throw new IllegalArgumentException(\"Attempt to resign from epoch \" + epoch +\n                \" which is larger than the current epoch \" + currentEpoch);\n        } else if (epoch < currentEpoch) {\n            // If the passed epoch is smaller than the current epoch, then it might mean\n            // that the listener has not been notified about a leader change that already\n            // took place. In this case, we consider the call as already fulfilled and\n            // take no further action.\n            logger.debug(\"Ignoring call to resign from epoch {} since it is smaller than the \" +\n                \"current epoch {}\", epoch, currentEpoch);\n        } else if (!leaderAndEpoch.isLeader(quorum.localIdOrThrow())) {\n            throw new IllegalArgumentException(\"Cannot resign from epoch \" + epoch +\n                \" since we are not the leader\");\n        } else {\n            // Note that if we transition to another state before we have a chance to\n            // request resignation, then we consider the call fulfilled.\n            Optional<LeaderState<Object>> leaderStateOpt = quorum.maybeLeaderState();\n            if (!leaderStateOpt.isPresent()) {\n                logger.debug(\"Ignoring call to resign from epoch {} since this node is \" +\n                    \"no longer the leader\", epoch);\n                return;\n            }\n\n            LeaderState<Object> leaderState = leaderStateOpt.get();\n            if (leaderState.epoch() != epoch) {\n                logger.debug(\"Ignoring call to resign from epoch {} since it is smaller than the \" +\n                    \"current epoch {}\", epoch, leaderState.epoch());\n            } else {\n                logger.info(\"Received user request to resign from the current epoch {}\", currentEpoch);\n                leaderState.requestResign();\n                wakeup();\n            }\n        }\n    }\n\n    @Override\n    public Optional<SnapshotWriter<T>> createSnapshot(\n        OffsetAndEpoch snapshotId,\n        long lastContainedLogTimestamp\n    ) {\n        if (!isInitialized()) {\n            throw new IllegalStateException(\"Cannot create snapshot before the replica has been initialized\");\n        }\n\n        return log.createNewSnapshot(snapshotId).map(writer -> {\n            long lastContainedLogOffset = snapshotId.offset() - 1;\n\n            RawSnapshotWriter wrappedWriter = new NotifyingRawSnapshotWriter(writer, offsetAndEpoch -> {\n                // Trim the state in the internal listener up to the new snapshot\n                partitionState.truncateOldEntries(offsetAndEpoch.offset());\n            });\n\n            return new RecordsSnapshotWriter.Builder()\n                .setLastContainedLogTimestamp(lastContainedLogTimestamp)\n                .setTime(time)\n                .setMaxBatchSize(MAX_BATCH_SIZE_BYTES)\n                .setMemoryPool(memoryPool)\n                .setRawSnapshotWriter(wrappedWriter)\n                .setKraftVersion(partitionState.kraftVersionAtOffset(lastContainedLogOffset))\n                .setVoterSet(partitionState.voterSetAtOffset(lastContainedLogOffset))\n                .build(serde);\n        });\n    }\n\n    @Override\n    public Optional<OffsetAndEpoch> latestSnapshotId() {\n        return log.latestSnapshotId();\n    }\n\n    @Override\n    public long logEndOffset() {\n        return log.endOffset().offset;\n    }\n\n    @Override\n    public void close() {\n        log.flush(true);\n        if (kafkaRaftMetrics != null) {\n            kafkaRaftMetrics.close();\n        }\n        if (memoryPool instanceof BatchMemoryPool) {\n            BatchMemoryPool batchMemoryPool = (BatchMemoryPool) memoryPool;\n            batchMemoryPool.releaseRetained();\n        }\n    }\n\n    @Override\n    public OptionalLong highWatermark() {\n        if (isInitialized() && quorum.highWatermark().isPresent()) {\n            return OptionalLong.of(quorum.highWatermark().get().offset);\n        } else {\n            return OptionalLong.empty();\n        }\n    }\n\n    public Optional<Node> voterNode(int id, String listener) {\n        return partitionState.lastVoterSet().voterNode(id, listener);\n    }\n\n    // Visible only for test\n    QuorumState quorum() {\n        // It's okay to return null since this method is only called by tests\n        return quorum;\n    }\n\n    private boolean isInitialized() {\n        return partitionState != null && quorum != null && requestManager != null && kafkaRaftMetrics != null;\n    }\n\n    private class GracefulShutdown {\n        final Timer finishTimer;\n        final CompletableFuture<Void> completeFuture;\n\n        public GracefulShutdown(long shutdownTimeoutMs,\n                                CompletableFuture<Void> completeFuture) {\n            this.finishTimer = time.timer(shutdownTimeoutMs);\n            this.completeFuture = completeFuture;\n        }\n\n        public void update(long currentTimeMs) {\n            finishTimer.update(currentTimeMs);\n        }\n\n        public boolean hasTimedOut() {\n            return finishTimer.isExpired();\n        }\n\n        public boolean isFinished() {\n            return completeFuture.isDone();\n        }\n\n        public long remainingTimeMs() {\n            return finishTimer.remainingMs();\n        }\n\n        public void failWithTimeout() {\n            logger.warn(\"Graceful shutdown timed out after {}ms\", finishTimer.timeoutMs());\n            completeFuture.completeExceptionally(\n                new TimeoutException(\"Timeout expired before graceful shutdown completed\"));\n        }\n\n        public void complete() {\n            logger.info(\"Graceful shutdown completed\");\n            completeFuture.complete(null);\n        }\n    }\n\n    private static final class Registration<T> {\n        private final Ops ops;\n        private final Listener<T> listener;\n\n        private Registration(Ops ops, Listener<T> listener) {\n            this.ops = ops;\n            this.listener = listener;\n        }\n\n        private Ops ops() {\n            return ops;\n        }\n\n        private Listener<T> listener() {\n            return listener;\n        }\n\n        private enum Ops {\n            REGISTER, UNREGISTER\n        }\n\n        private static <T> Registration<T> register(Listener<T> listener) {\n            return new Registration<>(Ops.REGISTER, listener);\n        }\n\n        private static <T> Registration<T> unregister(Listener<T> listener) {\n            return new Registration<>(Ops.UNREGISTER, listener);\n        }\n    }\n\n    private final class ListenerContext implements CloseListener<BatchReader<T>> {\n        private final RaftClient.Listener<T> listener;\n        // This field is used only by the Raft IO thread\n        private LeaderAndEpoch lastFiredLeaderChange = LeaderAndEpoch.UNKNOWN;\n\n        // These fields are visible to both the Raft IO thread and the listener\n        // and are protected through synchronization on this ListenerContext instance\n        private BatchReader<T> lastSent = null;\n        private long nextOffset = 0;\n\n        private ListenerContext(Listener<T> listener) {\n            this.listener = listener;\n        }\n\n        /**\n         * Get the last acked offset, which is one greater than the offset of the\n         * last record which was acked by the state machine.\n         */\n        private synchronized long nextOffset() {\n            return nextOffset;\n        }\n\n        /**\n         * Get the next expected offset, which might be larger than the last acked\n         * offset if there are inflight batches which have not been acked yet.\n         * Note that when fetching from disk, we may not know the last offset of\n         * inflight data until it has been processed by the state machine. In this case,\n         * we delay sending additional data until the state machine has read to the\n         * end and the last offset is determined.\n         */\n        private synchronized OptionalLong nextExpectedOffset() {\n            if (lastSent != null) {\n                OptionalLong lastSentOffset = lastSent.lastOffset();\n                if (lastSentOffset.isPresent()) {\n                    return OptionalLong.of(lastSentOffset.getAsLong() + 1);\n                } else {\n                    return OptionalLong.empty();\n                }\n            } else {\n                return OptionalLong.of(nextOffset);\n            }\n        }\n\n        /**\n         * This API is used when the Listener needs to be notified of a new snapshot. This happens\n         * when the context's next offset is less than the log start offset.\n         */\n        private void fireHandleSnapshot(SnapshotReader<T> reader) {\n            synchronized (this) {\n                nextOffset = reader.snapshotId().offset();\n                lastSent = null;\n            }\n\n            logger.debug(\"Notifying listener {} of snapshot {}\", listenerName(), reader.snapshotId());\n            listener.handleLoadSnapshot(reader);\n        }\n\n        /**\n         * This API is used for committed records that have been received through\n         * replication. In general, followers will write new data to disk before they\n         * know whether it has been committed. Rather than retaining the uncommitted\n         * data in memory, we let the state machine read the records from disk.\n         */\n        private void fireHandleCommit(long baseOffset, Records records) {\n            fireHandleCommit(\n                RecordsBatchReader.of(\n                    baseOffset,\n                    records,\n                    serde,\n                    BufferSupplier.create(),\n                    MAX_BATCH_SIZE_BYTES,\n                    this,\n                    true /* Validate batch CRC*/\n                )\n            );\n        }\n\n        /**\n         * This API is used for committed records originating from {@link #scheduleAppend(int, List)}\n         * or {@link #scheduleAtomicAppend(int, OptionalLong, List)} on this instance. In this case,\n         * we are able to save the original record objects, which saves the need to read them back\n         * from disk. This is a nice optimization for the leader which is typically doing more work\n         * than all of the * followers.\n         */\n        private void fireHandleCommit(\n            long baseOffset,\n            int epoch,\n            long appendTimestamp,\n            int sizeInBytes,\n            List<T> records\n        ) {\n            Batch<T> batch = Batch.data(baseOffset, epoch, appendTimestamp, sizeInBytes, records);\n            MemoryBatchReader<T> reader = MemoryBatchReader.of(Collections.singletonList(batch), this);\n            fireHandleCommit(reader);\n        }\n\n        private String listenerName() {\n            return KafkaRaftClient.listenerName(listener);\n        }\n\n        private void fireHandleCommit(BatchReader<T> reader) {\n            synchronized (this) {\n                this.lastSent = reader;\n            }\n            logger.debug(\n                \"Notifying listener {} of batch for baseOffset {} and lastOffset {}\",\n                listenerName(),\n                reader.baseOffset(),\n                reader.lastOffset()\n            );\n            listener.handleCommit(reader);\n        }\n\n        private void maybeFireLeaderChange(LeaderAndEpoch leaderAndEpoch) {\n            if (shouldFireLeaderChange(leaderAndEpoch)) {\n                lastFiredLeaderChange = leaderAndEpoch;\n                logger.debug(\"Notifying listener {} of leader change {}\", listenerName(), leaderAndEpoch);\n                listener.handleLeaderChange(leaderAndEpoch);\n            }\n        }\n\n        private boolean shouldFireLeaderChange(LeaderAndEpoch leaderAndEpoch) {\n            if (leaderAndEpoch.equals(lastFiredLeaderChange)) {\n                return false;\n            } else if (leaderAndEpoch.epoch() > lastFiredLeaderChange.epoch()) {\n                return true;\n            } else {\n                return leaderAndEpoch.leaderId().isPresent() &&\n                    !lastFiredLeaderChange.leaderId().isPresent();\n            }\n        }\n\n        private void maybeFireLeaderChange(LeaderAndEpoch leaderAndEpoch, long epochStartOffset) {\n            // If this node is becoming the leader, then we can fire `handleLeaderChange` as soon\n            // as the listener has caught up to the start of the leader epoch. This guarantees\n            // that the state machine has seen the full committed state before it becomes\n            // leader and begins writing to the log.\n            //\n            // Note that the raft client doesn't need to compare nextOffset against the high-watermark\n            // to guarantee that the listener has caught up to the high-watermark. This is true because\n            // the only way nextOffset can be greater than epochStartOffset is for the leader to have\n            // established the new high-watermark (of at least epochStartOffset + 1) and for the listener\n            // to have consumed up to that new high-watermark.\n            if (shouldFireLeaderChange(leaderAndEpoch) && nextOffset() > epochStartOffset) {\n                lastFiredLeaderChange = leaderAndEpoch;\n                listener.handleLeaderChange(leaderAndEpoch);\n            }\n        }\n\n        public synchronized void onClose(BatchReader<T> reader) {\n            OptionalLong lastOffset = reader.lastOffset();\n\n            if (lastOffset.isPresent()) {\n                nextOffset = lastOffset.getAsLong() + 1;\n            }\n\n            if (lastSent == reader) {\n                lastSent = null;\n                wakeup();\n            }\n        }\n    }\n}",
                "methodCount": 136
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 12,
                "candidates": [
                    {
                        "lineStart": 1668,
                        "lineEnd": 1672,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method handleUnexpectedError to class NetworkChannel",
                        "description": "Move method handleUnexpectedError to org.apache.kafka.raft.NetworkChannel\nRationale: The method `handleUnexpectedError` is handling errors related to `RaftResponse.Inbound` which seems closely tied to network operations, thus aligning it more with the responsibilities of `NetworkChannel`. `NetworkChannel` also deals directly with communication and error handling, making it a more suitable place for this method compared to MemoryPool or Time.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 540,
                        "lineEnd": 546,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method transitionToResigned to class NetworkChannel",
                        "description": "Move method transitionToResigned to org.apache.kafka.raft.NetworkChannel\nRationale: The method transitionToResigned involves handling network state transitions such as completing pending requests with an exception and reseating connections. These operations are related to network operations and state management, which are more closely aligned with the responsibilities of the NetworkChannel class. This class specifically deals with network communication and endpoint management, making it a logical place for handling resignations that require resetting network connections and notifying related components of state changes. Moving this method will help in maintaining cohesive functionality related to network transitions within the same class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 569,
                        "lineEnd": 577,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method transitionToFollower to class RaftMetadataLogCleanerManager",
                        "description": "Move method transitionToFollower to org.apache.kafka.raft.KafkaRaftClient.RaftMetadataLogCleanerManager\nRationale: The method 'transitionToFollower' deals with transitions in state (specifically transitioning to a follower in a quorum process). The RaftMetadataLogCleanerManager is a class that deals directly with the Raft protocol, a consensus algorithm that likely involves transitions like these. Therefore, logically, the 'transitionToFollower' method fits better here as it is related to the quorum processes.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 548,
                        "lineEnd": 552,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method transitionToVoted to class NetworkChannel",
                        "description": "Move method transitionToVoted to org.apache.kafka.raft.NetworkChannel\nRationale: The method transitionToVoted involves managing state transitions, possibly within a networked protocol context (e.g., quorum transition). This aligns well with the responsibilities of NetworkChannel, which deals with sending requests and managing connections. It suggests that state changes and leader elections (likely part of a Raft consensus protocol) are relevant to network interactions and connection management, making NetworkChannel a suitable target class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1961,
                        "lineEnd": 1969,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method addQuorumLeader to class QuorumState",
                        "description": "Move method addQuorumLeader to org.apache.kafka.raft.QuorumState\nRationale: The method addQuorumLeader should be moved to the QuorumState class. The rationale is that the method is performing operations directly related to the 'quorum' object, including setting the leader epoch and leader ID from the QuorumState. This aligns closely with QuorumState's responsibility of managing and maintaining the quorum's state, including leader and epoch information. Keeping these operations within QuorumState ensures cohesion and maintains organized, maintainable code.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1270,
                        "lineEnd": 1282,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method appendAsLeader to class ReplicatedLog",
                        "description": "Move method appendAsLeader to org.apache.kafka.raft.ReplicatedLog\nRationale: The method appendAsLeader is specifically about appending records as a leader, which is a responsibility of the ReplicatedLog class. This method uses LogAppendInfo, Records, and OffsetAndEpoch, all of which are directly relevant to ReplicatedLog. Hence, the ReplicatedLog class is the most suitable place for this method as it aligns with its existing functionalities like appending as a leader or follower, and managing log offsets and epochs.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 926,
                        "lineEnd": 959,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method buildFetchResponse to class RaftMetadataLogCleanerManager",
                        "description": "Move method buildFetchResponse to org.apache.kafka.raft.KafkaRaftClient.RaftMetadataLogCleanerManager\nRationale: The method buildFetchResponse() is heavily reliant on Raft-specific constructs such as Errors, RaftUtil, and quorum. The RaftMetadataLogCleanerManager class, being a timer-based log cleaner manager specific to Raft metadata, implies a close relationship with the same systems and abstractions. Additionally, RaftMetadataLogCleanerManager already deals with Raft-specific logging and metadata, making it a more natural home for buildFetchResponse(). The other candidate classes, MemoryPool and Time, do not inherently deal with Raft-specific logic or metadata and therefore are less appropriate choices.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 2566,
                        "lineEnd": 2568,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method isInitialized to class RaftMetadataLogCleanerManager",
                        "description": "Move method isInitialized to org.apache.kafka.raft.KafkaRaftClient.RaftMetadataLogCleanerManager\nRationale: The isInitialized() method checks the state of various components like partitionState, quorum, requestManager, and kafkaRaftMetrics. The RaftMetadataLogCleanerManager class, with its management of log cleaning and dependency on timers, is more closely related to the components being checked. It is more likely that isInitialized() method is ensuring the readiness of components that RaftMetadataLogCleanerManager may be dependent on or interact with. Moving this method to RaftMetadataLogCleanerManager would place it in a context where such initialization checking is relevant and useful.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1971,
                        "lineEnd": 1974,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method isRunning to class ReplicatedLog",
                        "description": "Move method isRunning to org.apache.kafka.raft.ReplicatedLog\nRationale: The method `isRunning()` appears to be checking the status of a shutdown operation involving `GracefulShutdown`. The `ReplicatedLog` class already deals with operations such as starting and stopping, truncating logs, and managing the state of the log (e.g., `truncateTo`, `flush`, `close`). As such, it would be a more appropriate place for methods related to determining the running state, given that the `ReplicatedLog` would likely need to know if it is in a shutdown state or not.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1976,
                        "lineEnd": 1979,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method isShuttingDown to class Time",
                        "description": "Move method isShuttingDown to org.apache.kafka.common.utils.Time\nRationale: The `isShuttingDown()` method is concerning the state of a potential shutdown process, which typically involves timing mechanisms to monitor and ensure proper graceful shutdown. Additionally, the Time class conceptually fits a method that checks states related to processes and timing, making it a more natural fit for managing and integrating with system time and states.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 961,
                        "lineEnd": 971,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method buildEmptyFetchResponse to class RaftMetadataLogCleanerManager",
                        "description": "Move method buildEmptyFetchResponse to org.apache.kafka.raft.KafkaRaftClient.RaftMetadataLogCleanerManager\nRationale: The method buildEmptyFetchResponse() constructs a FetchResponseData object, which seems to be related to log management, the core responsibility of RaftMetadataLogCleanerManager. Moving the method here aligns with single responsibility principles, as RaftMetadataLogCleanerManager is tasked with handling the log cleaning, making it a logical place to handle responses related to log metadata.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 579,
                        "lineEnd": 587,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method buildVoteResponse to class RaftMetadataLogCleanerManager",
                        "description": "Move method buildVoteResponse to org.apache.kafka.raft.KafkaRaftClient.RaftMetadataLogCleanerManager\nRationale: The buildVoteResponse() method refers to logical structures (e.g., vote responses, partition errors, and quorum details) that are common in the management of metadata in distributed systems. The RaftMetadataLogCleanerManager class, which manages log cleaning using timers and logs, seems more aligned with metadata management and would benefit from having methods that handle vote responses. The other classes, MemoryPool and Time, do not deal directly with such logic and are primarily concerned with memory management and time functionality respectively.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "updateFollowerHighWatermark",
                            "method_signature": "private updateFollowerHighWatermark(\n        FollowerState state,\n        OptionalLong highWatermarkOpt\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateLeaderEndOffsetAndTimestamp",
                            "method_signature": "private updateLeaderEndOffsetAndTimestamp(\n        LeaderState<T> state,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onUpdateLeaderHighWatermark",
                            "method_signature": "private onUpdateLeaderHighWatermark(\n        LeaderState<T> state,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateListenersProgress",
                            "method_signature": "private updateListenersProgress(long highWatermark)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "latestSnapshot",
                            "method_signature": "private latestSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireHandleCommit",
                            "method_signature": "private maybeFireHandleCommit(long baseOffset, int epoch, long appendTimestamp, int sizeInBytes, List<T> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireLeaderChange",
                            "method_signature": "private maybeFireLeaderChange(LeaderState<T> state)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireLeaderChange",
                            "method_signature": "private maybeFireLeaderChange()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "initialize",
                            "method_signature": "public initialize(\n        Map<Integer, InetSocketAddress> voterAddresses,\n        String listenerName,\n        QuorumStateStore quorumStateStore,\n        Metrics metrics\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endOffset",
                            "method_signature": "private endOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resetConnections",
                            "method_signature": "private resetConnections()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onBecomeLeader",
                            "method_signature": "private onBecomeLeader(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "flushLeaderLog",
                            "method_signature": "private flushLeaderLog(LeaderState<T> state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeTransitionToLeader",
                            "method_signature": "private maybeTransitionToLeader(CandidateState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onBecomeCandidate",
                            "method_signature": "private onBecomeCandidate(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToCandidate",
                            "method_signature": "private transitionToCandidate(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToUnattached",
                            "method_signature": "private transitionToUnattached(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToResigned",
                            "method_signature": "private transitionToResigned(List<Integer> preferredSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToVoted",
                            "method_signature": "private transitionToVoted(ReplicaKey candidateKey, int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onBecomeFollower",
                            "method_signature": "private onBecomeFollower(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToFollower",
                            "method_signature": "private transitionToFollower(\n        int epoch,\n        int leaderId,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildVoteResponse",
                            "method_signature": "private buildVoteResponse(Errors partitionLevelError, boolean voteGranted)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleVoteRequest",
                            "method_signature": "private handleVoteRequest(\n        RaftRequest.Inbound requestMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleVoteResponse",
                            "method_signature": "private handleVoteResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "binaryExponentialElectionBackoffMs",
                            "method_signature": "private binaryExponentialElectionBackoffMs(int retries)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "strictExponentialElectionBackoffMs",
                            "method_signature": "private strictExponentialElectionBackoffMs(int positionInSuccessors, int totalNumSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildBeginQuorumEpochResponse",
                            "method_signature": "private buildBeginQuorumEpochResponse(Errors partitionLevelError)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleBeginQuorumEpochRequest",
                            "method_signature": "private handleBeginQuorumEpochRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleBeginQuorumEpochResponse",
                            "method_signature": "private handleBeginQuorumEpochResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildEndQuorumEpochResponse",
                            "method_signature": "private buildEndQuorumEpochResponse(Errors partitionLevelError)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleEndQuorumEpochRequest",
                            "method_signature": "private handleEndQuorumEpochRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endEpochElectionBackoff",
                            "method_signature": "private endEpochElectionBackoff(List<Integer> preferredSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleEndQuorumEpochResponse",
                            "method_signature": "private handleEndQuorumEpochResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildFetchResponse",
                            "method_signature": "private buildFetchResponse(\n        Errors error,\n        Records records,\n        ValidOffsetAndEpoch validOffsetAndEpoch,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildEmptyFetchResponse",
                            "method_signature": "private buildEmptyFetchResponse(\n        Errors error,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasValidClusterId",
                            "method_signature": "private hasValidClusterId(String requestClusterId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleFetchRequest",
                            "method_signature": "private handleFetchRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "tryCompleteFetchRequest",
                            "method_signature": "private tryCompleteFetchRequest(\n        int replicaId,\n        FetchRequestData.FetchPartition request,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isPartitionDiverged",
                            "method_signature": "private static isPartitionDiverged(FetchResponseData.PartitionData partitionResponseData)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isPartitionSnapshotted",
                            "method_signature": "private static isPartitionSnapshotted(FetchResponseData.PartitionData partitionResponseData)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "optionalLeaderId",
                            "method_signature": "private static optionalLeaderId(int leaderIdOrNil)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "listenerName",
                            "method_signature": "private static listenerName(Listener<?> listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleFetchResponse",
                            "method_signature": "private handleFetchResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "appendAsFollower",
                            "method_signature": "private appendAsFollower(\n        Records records\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "appendAsLeader",
                            "method_signature": "private appendAsLeader(\n        Records records\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleDescribeQuorumRequest",
                            "method_signature": "private handleDescribeQuorumRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleFetchSnapshotRequest",
                            "method_signature": "private handleFetchSnapshotRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleFetchSnapshotResponse",
                            "method_signature": "private handleFetchSnapshotResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasConsistentLeader",
                            "method_signature": "private hasConsistentLeader(int epoch, OptionalInt leaderId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeHandleCommonResponse",
                            "method_signature": "private maybeHandleCommonResponse(\n        Errors error,\n        OptionalInt leaderId,\n        int epoch,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeTransition",
                            "method_signature": "private maybeTransition(\n        OptionalInt leaderId,\n        int epoch,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleTopLevelError",
                            "method_signature": "private handleTopLevelError(Errors error, RaftResponse.Inbound response)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleUnexpectedError",
                            "method_signature": "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleResponse",
                            "method_signature": "private handleResponse(RaftResponse.Inbound response, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validateVoterOnlyRequest",
                            "method_signature": "private validateVoterOnlyRequest(int remoteNodeId, int requestEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validateLeaderOnlyRequest",
                            "method_signature": "private validateLeaderOnlyRequest(int requestEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleRequest",
                            "method_signature": "private handleRequest(RaftRequest.Inbound request, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleInboundMessage",
                            "method_signature": "private handleInboundMessage(RaftMessage message, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendRequest",
                            "method_signature": "private maybeSendRequest(\n        long currentTimeMs,\n        int destinationId,\n        Supplier<ApiMessage> requestSupplier\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildEndQuorumEpochRequest",
                            "method_signature": "private buildEndQuorumEpochRequest(\n        ResignedState state\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendRequests",
                            "method_signature": "private maybeSendRequests(\n        long currentTimeMs,\n        Set<Integer> destinationIds,\n        Supplier<ApiMessage> requestSupplier\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildBeginQuorumEpochRequest",
                            "method_signature": "private buildBeginQuorumEpochRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildVoteRequest",
                            "method_signature": "private buildVoteRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildFetchRequest",
                            "method_signature": "private buildFetchRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendAnyVoterFetch",
                            "method_signature": "private maybeSendAnyVoterFetch(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildFetchSnapshotRequest",
                            "method_signature": "private buildFetchSnapshotRequest(OffsetAndEpoch snapshotId, long snapshotSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addQuorumLeader",
                            "method_signature": "private addQuorumLeader(\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isRunning",
                            "method_signature": "public isRunning()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isShuttingDown",
                            "method_signature": "public isShuttingDown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "appendBatch",
                            "method_signature": "private appendBatch(\n        LeaderState<T> state,\n        BatchAccumulator.CompletedBatch<T> batch,\n        long appendTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAppendBatches",
                            "method_signature": "private maybeAppendBatches(\n        LeaderState<T> state,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollResigned",
                            "method_signature": "private pollResigned(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollLeader",
                            "method_signature": "private pollLeader(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendVoteRequests",
                            "method_signature": "private maybeSendVoteRequests(\n        CandidateState state,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollCandidate",
                            "method_signature": "private pollCandidate(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollFollower",
                            "method_signature": "private pollFollower(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollFollowerAsVoter",
                            "method_signature": "private pollFollowerAsVoter(FollowerState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollFollowerAsObserver",
                            "method_signature": "private pollFollowerAsObserver(FollowerState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasAnyInflightRequest",
                            "method_signature": "private hasAnyInflightRequest(FollowerState state)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendFetchOrFetchSnapshot",
                            "method_signature": "private maybeSendFetchOrFetchSnapshot(FollowerState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollVoted",
                            "method_signature": "private pollVoted(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUnattached",
                            "method_signature": "private pollUnattached(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUnattachedAsVoter",
                            "method_signature": "private pollUnattachedAsVoter(UnattachedState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUnattachedAsObserver",
                            "method_signature": "private pollUnattachedAsObserver(UnattachedState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollCurrentState",
                            "method_signature": "private pollCurrentState(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollListeners",
                            "method_signature": "private pollListeners()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "processRegistration",
                            "method_signature": "private processRegistration(Registration<T> registration)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeCompleteShutdown",
                            "method_signature": "private maybeCompleteShutdown(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeClean",
                            "method_signature": "public maybeClean(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "wakeup",
                            "method_signature": "private wakeup()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handle",
                            "method_signature": "public handle(RaftRequest.Inbound request)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "poll",
                            "method_signature": "public poll()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "append",
                            "method_signature": "private append(int epoch, List<T> records, OptionalLong requiredBaseOffset, boolean isAtomic)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voterNode",
                            "method_signature": "public voterNode(int id, String listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isInitialized",
                            "method_signature": "private isInitialized()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "update",
                            "method_signature": "public update(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasTimedOut",
                            "method_signature": "public hasTimedOut()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isFinished",
                            "method_signature": "public isFinished()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "remainingTimeMs",
                            "method_signature": "public remainingTimeMs()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "failWithTimeout",
                            "method_signature": "public failWithTimeout()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "complete",
                            "method_signature": "public complete()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "register",
                            "method_signature": "private static register(Listener<T> listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "unregister",
                            "method_signature": "private static unregister(Listener<T> listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "nextExpectedOffset",
                            "method_signature": "private synchronized nextExpectedOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fireHandleSnapshot",
                            "method_signature": "private fireHandleSnapshot(SnapshotReader<T> reader)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fireHandleCommit",
                            "method_signature": "private fireHandleCommit(long baseOffset, Records records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fireHandleCommit",
                            "method_signature": "private fireHandleCommit(\n            long baseOffset,\n            int epoch,\n            long appendTimestamp,\n            int sizeInBytes,\n            List<T> records\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "listenerName",
                            "method_signature": "private listenerName()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fireHandleCommit",
                            "method_signature": "private fireHandleCommit(BatchReader<T> reader)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireLeaderChange",
                            "method_signature": "private maybeFireLeaderChange(LeaderAndEpoch leaderAndEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "shouldFireLeaderChange",
                            "method_signature": "private shouldFireLeaderChange(LeaderAndEpoch leaderAndEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireLeaderChange",
                            "method_signature": "private maybeFireLeaderChange(LeaderAndEpoch leaderAndEpoch, long epochStartOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onClose",
                            "method_signature": "public synchronized onClose(BatchReader<T> reader)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "isInitialized",
                            "method_signature": "private isInitialized()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "failWithTimeout",
                            "method_signature": "public failWithTimeout()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildEmptyFetchResponse",
                            "method_signature": "private buildEmptyFetchResponse(\n        Errors error,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleUnexpectedError",
                            "method_signature": "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildVoteResponse",
                            "method_signature": "private buildVoteResponse(Errors partitionLevelError, boolean voteGranted)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "latestSnapshot",
                            "method_signature": "private latestSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToResigned",
                            "method_signature": "private transitionToResigned(List<Integer> preferredSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToFollower",
                            "method_signature": "private transitionToFollower(\n        int epoch,\n        int leaderId,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "complete",
                            "method_signature": "public complete()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToVoted",
                            "method_signature": "private transitionToVoted(ReplicaKey candidateKey, int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addQuorumLeader",
                            "method_signature": "private addQuorumLeader(\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "appendAsLeader",
                            "method_signature": "private appendAsLeader(\n        Records records\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isShuttingDown",
                            "method_signature": "public isShuttingDown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isRunning",
                            "method_signature": "public isRunning()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildFetchResponse",
                            "method_signature": "private buildFetchResponse(\n        Errors error,\n        Records records,\n        ValidOffsetAndEpoch validOffsetAndEpoch,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "isInitialized",
                            "method_signature": "private isInitialized()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "failWithTimeout",
                            "method_signature": "public failWithTimeout()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildEmptyFetchResponse",
                            "method_signature": "private buildEmptyFetchResponse(\n        Errors error,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleUnexpectedError",
                            "method_signature": "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildVoteResponse",
                            "method_signature": "private buildVoteResponse(Errors partitionLevelError, boolean voteGranted)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "latestSnapshot",
                            "method_signature": "private latestSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToResigned",
                            "method_signature": "private transitionToResigned(List<Integer> preferredSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToFollower",
                            "method_signature": "private transitionToFollower(\n        int epoch,\n        int leaderId,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "complete",
                            "method_signature": "public complete()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToVoted",
                            "method_signature": "private transitionToVoted(ReplicaKey candidateKey, int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addQuorumLeader",
                            "method_signature": "private addQuorumLeader(\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "appendAsLeader",
                            "method_signature": "private appendAsLeader(\n        Records records\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isShuttingDown",
                            "method_signature": "public isShuttingDown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isRunning",
                            "method_signature": "public isRunning()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildFetchResponse",
                            "method_signature": "private buildFetchResponse(\n        Errors error,\n        Records records,\n        ValidOffsetAndEpoch validOffsetAndEpoch,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private isInitialized()": {
                    "first": {
                        "method_name": "isInitialized",
                        "method_signature": "private isInitialized()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.23077478558967446
                },
                "public failWithTimeout()": {
                    "first": {
                        "method_name": "failWithTimeout",
                        "method_signature": "public failWithTimeout()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.24569711685267134
                },
                "private buildEmptyFetchResponse(\n        Errors error,\n        Optional<LogOffsetMetadata> highWatermark\n    )": {
                    "first": {
                        "method_name": "buildEmptyFetchResponse",
                        "method_signature": "private buildEmptyFetchResponse(\n        Errors error,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.31408301298905417
                },
                "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)": {
                    "first": {
                        "method_name": "handleUnexpectedError",
                        "method_signature": "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3148528256741625
                },
                "private buildVoteResponse(Errors partitionLevelError, boolean voteGranted)": {
                    "first": {
                        "method_name": "buildVoteResponse",
                        "method_signature": "private buildVoteResponse(Errors partitionLevelError, boolean voteGranted)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.31784767941305514
                },
                "private latestSnapshot()": {
                    "first": {
                        "method_name": "latestSnapshot",
                        "method_signature": "private latestSnapshot()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.33251439200312055
                },
                "private transitionToResigned(List<Integer> preferredSuccessors)": {
                    "first": {
                        "method_name": "transitionToResigned",
                        "method_signature": "private transitionToResigned(List<Integer> preferredSuccessors)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3334349844567339
                },
                "private transitionToFollower(\n        int epoch,\n        int leaderId,\n        long currentTimeMs\n    )": {
                    "first": {
                        "method_name": "transitionToFollower",
                        "method_signature": "private transitionToFollower(\n        int epoch,\n        int leaderId,\n        long currentTimeMs\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3347560484856552
                },
                "public complete()": {
                    "first": {
                        "method_name": "complete",
                        "method_signature": "public complete()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3363460625287738
                },
                "private transitionToVoted(ReplicaKey candidateKey, int epoch)": {
                    "first": {
                        "method_name": "transitionToVoted",
                        "method_signature": "private transitionToVoted(ReplicaKey candidateKey, int epoch)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.34011180824655607
                },
                "private addQuorumLeader(\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot\n    )": {
                    "first": {
                        "method_name": "addQuorumLeader",
                        "method_signature": "private addQuorumLeader(\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3467371153947286
                },
                "private appendAsLeader(\n        Records records\n    )": {
                    "first": {
                        "method_name": "appendAsLeader",
                        "method_signature": "private appendAsLeader(\n        Records records\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.34715179557885995
                },
                "public isShuttingDown()": {
                    "first": {
                        "method_name": "isShuttingDown",
                        "method_signature": "public isShuttingDown()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3520074047662494
                },
                "public isRunning()": {
                    "first": {
                        "method_name": "isRunning",
                        "method_signature": "public isRunning()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.35323101772797033
                },
                "private buildFetchResponse(\n        Errors error,\n        Records records,\n        ValidOffsetAndEpoch validOffsetAndEpoch,\n        Optional<LogOffsetMetadata> highWatermark\n    )": {
                    "first": {
                        "method_name": "buildFetchResponse",
                        "method_signature": "private buildFetchResponse(\n        Errors error,\n        Records records,\n        ValidOffsetAndEpoch validOffsetAndEpoch,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3543391755961774
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)",
                    "private transitionToResigned(List<Integer> preferredSuccessors)",
                    "private transitionToFollower(\n        int epoch,\n        int leaderId,\n        long currentTimeMs\n    )",
                    "private transitionToVoted(ReplicaKey candidateKey, int epoch)",
                    "private addQuorumLeader(\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot\n    )",
                    "private appendAsLeader(\n        Records records\n    )",
                    "private buildFetchResponse(\n        Errors error,\n        Records records,\n        ValidOffsetAndEpoch validOffsetAndEpoch,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                    "private isInitialized()",
                    "public isRunning()",
                    "public isShuttingDown()",
                    "private buildEmptyFetchResponse(\n        Errors error,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                    "private buildVoteResponse(Errors partitionLevelError, boolean voteGranted)",
                    "public complete()",
                    "private latestSnapshot()",
                    "public failWithTimeout()"
                ],
                "llm_response_time": 6323
            },
            "targetClassMap": {
                "handleUnexpectedError": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.10577027387372219
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.05594542388644595
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.04352434006443845
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14159846508095775
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.042691922456284
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "NetworkChannel",
                        "Time",
                        "MemoryPool"
                    ],
                    "llm_response_time": 4291,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "transitionToResigned": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.13841739114470894
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.1615005277038364
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.08233688458905682
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14306584587982538
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.057512450152169124
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.2798567190507171
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "NetworkChannel",
                        "RaftMetadataLogCleanerManager",
                        "MemoryPool"
                    ],
                    "llm_response_time": 3702,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "transitionToFollower": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.11399079270740736
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.11305036939268549
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.045980338147135626
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14306584587982538
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.03286425722981093
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.3292431988831966
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "RaftMetadataLogCleanerManager",
                        "Time",
                        "MemoryPool"
                    ],
                    "llm_response_time": 3801,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "transitionToVoted": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.09972117053875637
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.11867816581938534
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.04583712596673114
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14184408272449836
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.04025033049430489
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.3427530813818426
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "NetworkChannel",
                        "RaftMetadataLogCleanerManager",
                        "MemoryPool"
                    ],
                    "llm_response_time": 2977,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "addQuorumLeader": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.10257232541543393
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.03538299282895168
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.04216934244541642
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14925788761321498
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.02700074253062953
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.3246010684802756
                        },
                        {
                            "class_name": "QuorumState",
                            "similarity_score": 0.275797464034139
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "QuorumState",
                        "RaftMetadataLogCleanerManager",
                        "MemoryPool"
                    ],
                    "llm_response_time": 3953,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                },
                "appendAsLeader": {
                    "target_classes": [
                        {
                            "class_name": "Records",
                            "similarity_score": 0.11837617651007314
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.1077851268021598
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.03800743117926434
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.1434409434297192
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.10020543130080872
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.03383735833851262
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "ReplicatedLog",
                        "Records",
                        "Time"
                    ],
                    "llm_response_time": 3863,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "buildFetchResponse": {
                    "target_classes": [
                        {
                            "class_name": "Records",
                            "similarity_score": 0.026094068112009733
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.10249173027591259
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.04065846966124125
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.04361097561378978
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14750006307045013
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.024821164819372063
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.2983983352366476
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "RaftMetadataLogCleanerManager",
                        "Time",
                        "MemoryPool"
                    ],
                    "llm_response_time": 4056,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                },
                "isInitialized": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.05922015969000667
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.020428380434529853
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.0371960129634052
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.10771760199020573
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.05196295322792911
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.18740851426632726
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "RaftMetadataLogCleanerManager",
                        "Time",
                        "MemoryPool"
                    ],
                    "llm_response_time": 3411,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "isRunning": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.10930407304457855
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.04485613040162566
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.053459398780271276
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.1955262050905541
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.05324618233804511
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "ReplicatedLog",
                        "Time",
                        "MemoryPool"
                    ],
                    "llm_response_time": 4495,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "isShuttingDown": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.10553496707752412
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.04485613040162566
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.05048943218136732
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.1955262050905541
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.05324618233804511
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time",
                        "RaftMessageQueue",
                        "MemoryPool"
                    ],
                    "llm_response_time": 3799,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "buildEmptyFetchResponse": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.08668943724225196
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.029904086934417106
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.03563959918684752
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.12614593876809943
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.022819792430590763
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.2743379741155275
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "RaftMetadataLogCleanerManager",
                        "Time",
                        "MemoryPool"
                    ],
                    "llm_response_time": 4925,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "buildVoteResponse": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.08956419427010578
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.03230010554076729
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.039564477010326005
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14987850520743612
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.03286425722981093
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.2963188789948769
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "RaftMetadataLogCleanerManager",
                        "Time",
                        "MemoryPool"
                    ],
                    "llm_response_time": 4139,
                    "similarity_computation_time": 4,
                    "similarity_metric": "cosine"
                },
                "complete": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3419,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "latestSnapshot": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3281,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "failWithTimeout": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3770,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "459da4795a511f6933e940fcf105a824bd9e589c",
        "url": "https://github.com/apache/kafka/commit/459da4795a511f6933e940fcf105a824bd9e589c",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public onResponseResult(node Node, correlationId long, success boolean, timeMs long) : void extracted from private handleResponse(response RaftResponse.Inbound, currentTimeMs long) : void in class org.apache.kafka.raft.KafkaRaftClient & moved to class org.apache.kafka.raft.RequestManager",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 1675,
                    "endLine": 1711,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private handleResponse(response RaftResponse.Inbound, currentTimeMs long) : void"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 1709,
                    "endLine": 1709,
                    "startColumn": 13,
                    "endColumn": 79,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 1706,
                    "endLine": 1710,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 1706,
                    "endLine": 1708,
                    "startColumn": 34,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 1708,
                    "endLine": 1710,
                    "startColumn": 16,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 234,
                    "endLine": 252,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public onResponseResult(node Node, correlationId long, success boolean, timeMs long) : void"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 249,
                    "endLine": 249,
                    "startColumn": 17,
                    "endColumn": 89,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 244,
                    "endLine": 250,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 244,
                    "endLine": 247,
                    "startColumn": 26,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 247,
                    "endLine": 250,
                    "startColumn": 20,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 1736,
                    "endLine": 1772,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private handleResponse(response RaftResponse.Inbound, currentTimeMs long) : void"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 1766,
                    "endLine": 1771,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "requestManager.onResponseResult(response.source(),response.correlationId(),handledSuccessfully,currentTimeMs)"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 246,
                    "endLine": 246,
                    "startColumn": 17,
                    "endColumn": 29,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 243,
                    "endLine": 251,
                    "startColumn": 54,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 243,
                    "endLine": 251,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": false
        },
        "ref_id": 582,
        "extraction_results": {
            "success": true,
            "newCommitHash": "c1a446eb827fb6a57b6199f41beca07a2b229185",
            "newBranchName": "extract-onResponseResult-handleResponse-8a882a7"
        },
        "telemetry": {
            "id": "4027836b-37bd-4587-a720-a300546d9973",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 2692,
                "lineStart": 109,
                "lineEnd": 2800,
                "bodyLineStart": 109,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                "sourceCode": "/**\n * This class implements a Kafkaesque version of the Raft protocol. Leader election\n * is more or less pure Raft, but replication is driven by replica fetching and we use Kafka's\n * log reconciliation protocol to truncate the log to a common point following each leader\n * election.\n *\n * Like Zookeeper, this protocol distinguishes between voters and observers. Voters are\n * the only ones who are eligible to handle protocol requests and they are the only ones\n * who take part in elections. The protocol does not yet support dynamic quorum changes.\n *\n * These are the APIs in this protocol:\n *\n * 1) {@link VoteRequestData}: Sent by valid voters when their election timeout expires and they\n *    become a candidate. This request includes the last offset in the log which electors use\n *    to tell whether or not to grant the vote.\n *\n * 2) {@link BeginQuorumEpochRequestData}: Sent by the leader of an epoch only to valid voters to\n *    assert its leadership of the new epoch. This request will be retried indefinitely for\n *    each voter until it acknowledges the request or a new election occurs.\n *\n *    This is not needed in usual Raft because the leader can use an empty data push\n *    to achieve the same purpose. The Kafka Raft implementation, however, is driven by\n *    fetch requests from followers, so there must be a way to find the new leader after\n *    an election has completed.\n *\n * 3) {@link EndQuorumEpochRequestData}: Sent by the leader of an epoch to valid voters in order to\n *    gracefully resign from the current epoch. This causes remaining voters to immediately\n *    begin a new election.\n *\n * 4) {@link FetchRequestData}: This is the same as the usual Fetch API in Kafka, but we add snapshot\n *    check before responding, and we also piggyback some additional metadata on responses (i.e. current\n *    leader and epoch). Unlike partition replication, we also piggyback truncation detection on this API\n *    rather than through a separate truncation state.\n *\n * 5) {@link FetchSnapshotRequestData}: Sent by the follower to the epoch leader in order to fetch a snapshot.\n *    This happens when a FetchResponse includes a snapshot ID due to the follower's log end offset being less\n *    than the leader's log start offset. This API is similar to the Fetch API since the snapshot is stored\n *    as FileRecords, but we use {@link UnalignedRecords} in FetchSnapshotResponse because the records\n *    are not necessarily offset-aligned.\n */\nfinal public class KafkaRaftClient<T> implements RaftClient<T> {\n    private static final int RETRY_BACKOFF_BASE_MS = 100;\n    public static final int MAX_FETCH_WAIT_MS = 500;\n    public static final int MAX_BATCH_SIZE_BYTES = 8 * 1024 * 1024;\n    public static final int MAX_FETCH_SIZE_BYTES = MAX_BATCH_SIZE_BYTES;\n\n    private final OptionalInt nodeId;\n    private final Uuid nodeDirectoryId;\n    private final AtomicReference<GracefulShutdown> shutdown = new AtomicReference<>();\n    private final LogContext logContext;\n    private final Logger logger;\n    private final Time time;\n    private final int fetchMaxWaitMs;\n    private final String clusterId;\n    private final NetworkChannel channel;\n    private final ReplicatedLog log;\n    private final Random random;\n    private final FuturePurgatory<Long> appendPurgatory;\n    private final FuturePurgatory<Long> fetchPurgatory;\n    private final RecordSerde<T> serde;\n    private final MemoryPool memoryPool;\n    private final RaftMessageQueue messageQueue;\n    private final QuorumConfig quorumConfig;\n    private final RaftMetadataLogCleanerManager snapshotCleaner;\n\n    private final Map<Listener<T>, ListenerContext> listenerContexts = new IdentityHashMap<>();\n    private final ConcurrentLinkedQueue<Registration<T>> pendingRegistrations = new ConcurrentLinkedQueue<>();\n\n    // These components need to be initialized by the method initialize() because they depend on\n    // the voter set\n    /*\n     * The key invariant for the kraft control record state machine is that it has always read to\n     * the LEO. This is achieved by:\n     *\n     * 1. reading the entire partition (snapshot and log) at start up,\n     * 2. updating the state when a snapshot is replaced, because of FETCH_SNAPSHOT, on the\n     *    followers\n     * 3. updating the state when the leader (call to append()) or follower (FETCH) appends to the\n     *    log\n     * 4. truncate new entries when a follower truncates their log\n     * 5. truncate old entries when a snapshot gets generated\n     */\n    private volatile KRaftControlRecordStateMachine partitionState;\n    private volatile KafkaRaftMetrics kafkaRaftMetrics;\n    private volatile QuorumState quorum;\n    private volatile RequestManager requestManager;\n\n    /**\n     * Create a new instance.\n     *\n     * Note that if the node ID is empty, then the client will behave as a\n     * non-participating observer.\n     */\n    public KafkaRaftClient(\n        OptionalInt nodeId,\n        Uuid nodeDirectoryId,\n        RecordSerde<T> serde,\n        NetworkChannel channel,\n        ReplicatedLog log,\n        Time time,\n        ExpirationService expirationService,\n        LogContext logContext,\n        String clusterId,\n        QuorumConfig quorumConfig\n    ) {\n        this(\n            nodeId,\n            nodeDirectoryId,\n            serde,\n            channel,\n            new BlockingMessageQueue(),\n            log,\n            new BatchMemoryPool(5, MAX_BATCH_SIZE_BYTES),\n            time,\n            expirationService,\n            MAX_FETCH_WAIT_MS,\n            clusterId,\n            logContext,\n            new Random(),\n            quorumConfig\n        );\n    }\n\n    KafkaRaftClient(\n        OptionalInt nodeId,\n        Uuid nodeDirectoryId,\n        RecordSerde<T> serde,\n        NetworkChannel channel,\n        RaftMessageQueue messageQueue,\n        ReplicatedLog log,\n        MemoryPool memoryPool,\n        Time time,\n        ExpirationService expirationService,\n        int fetchMaxWaitMs,\n        String clusterId,\n        LogContext logContext,\n        Random random,\n        QuorumConfig quorumConfig\n    ) {\n        this.nodeId = nodeId;\n        this.nodeDirectoryId = nodeDirectoryId;\n        this.logContext = logContext;\n        this.serde = serde;\n        this.channel = channel;\n        this.messageQueue = messageQueue;\n        this.log = log;\n        this.memoryPool = memoryPool;\n        this.fetchPurgatory = new ThresholdPurgatory<>(expirationService);\n        this.appendPurgatory = new ThresholdPurgatory<>(expirationService);\n        this.time = time;\n        this.clusterId = clusterId;\n        this.fetchMaxWaitMs = fetchMaxWaitMs;\n        this.logger = logContext.logger(KafkaRaftClient.class);\n        this.random = random;\n        this.quorumConfig = quorumConfig;\n        this.snapshotCleaner = new RaftMetadataLogCleanerManager(logger, time, 60000, log::maybeClean);\n    }\n\n    private void updateFollowerHighWatermark(\n        FollowerState state,\n        OptionalLong highWatermarkOpt\n    ) {\n        highWatermarkOpt.ifPresent(highWatermark -> {\n            long newHighWatermark = Math.min(endOffset().offset(), highWatermark);\n            if (state.updateHighWatermark(OptionalLong.of(newHighWatermark))) {\n                logger.debug(\"Follower high watermark updated to {}\", newHighWatermark);\n                log.updateHighWatermark(new LogOffsetMetadata(newHighWatermark));\n                updateListenersProgress(newHighWatermark);\n            }\n        });\n    }\n\n    private void updateLeaderEndOffsetAndTimestamp(\n        LeaderState<T> state,\n        long currentTimeMs\n    ) {\n        final LogOffsetMetadata endOffsetMetadata = log.endOffset();\n\n        if (state.updateLocalState(endOffsetMetadata)) {\n            onUpdateLeaderHighWatermark(state, currentTimeMs);\n        }\n\n        fetchPurgatory.maybeComplete(endOffsetMetadata.offset, currentTimeMs);\n    }\n\n    private void onUpdateLeaderHighWatermark(\n        LeaderState<T> state,\n        long currentTimeMs\n    ) {\n        state.highWatermark().ifPresent(highWatermark -> {\n            logger.debug(\"Leader high watermark updated to {}\", highWatermark);\n            log.updateHighWatermark(highWatermark);\n\n            // After updating the high watermark, we first clear the append\n            // purgatory so that we have an opportunity to route the pending\n            // records still held in memory directly to the listener\n            appendPurgatory.maybeComplete(highWatermark.offset, currentTimeMs);\n\n            // It is also possible that the high watermark is being updated\n            // for the first time following the leader election, so we need\n            // to give lagging listeners an opportunity to catch up as well\n            updateListenersProgress(highWatermark.offset);\n        });\n    }\n\n    private void updateListenersProgress(long highWatermark) {\n        for (ListenerContext listenerContext : listenerContexts.values()) {\n            listenerContext.nextExpectedOffset().ifPresent(nextExpectedOffset -> {\n                // Send snapshot to the listener, if the listener is at the beginning of the log and there is a snapshot,\n                // or the listener is trying to read an offset for which there isn't a segment in the log.\n                if (nextExpectedOffset < highWatermark &&\n                    ((nextExpectedOffset == 0 && latestSnapshot().isPresent()) ||\n                     nextExpectedOffset < log.startOffset())\n                ) {\n                    SnapshotReader<T> snapshot = latestSnapshot().orElseThrow(() -> new IllegalStateException(\n                        String.format(\n                            \"Snapshot expected since next offset of %s is %d, log start offset is %d and high-watermark is %d\",\n                            listenerContext.listenerName(),\n                            nextExpectedOffset,\n                            log.startOffset(),\n                            highWatermark\n                        )\n                    ));\n                    listenerContext.fireHandleSnapshot(snapshot);\n                }\n            });\n\n            // Re-read the expected offset in case the snapshot had to be reloaded\n            listenerContext.nextExpectedOffset().ifPresent(nextExpectedOffset -> {\n                if (nextExpectedOffset < highWatermark) {\n                    LogFetchInfo readInfo = log.read(nextExpectedOffset, Isolation.COMMITTED);\n                    listenerContext.fireHandleCommit(nextExpectedOffset, readInfo.records);\n                }\n            });\n        }\n    }\n\n    private Optional<SnapshotReader<T>> latestSnapshot() {\n        return log.latestSnapshot().map(reader ->\n            RecordsSnapshotReader.of(reader,\n                serde,\n                BufferSupplier.create(),\n                MAX_BATCH_SIZE_BYTES,\n                true /* Validate batch CRC*/\n            )\n        );\n    }\n\n    private void maybeFireHandleCommit(long baseOffset, int epoch, long appendTimestamp, int sizeInBytes, List<T> records) {\n        for (ListenerContext listenerContext : listenerContexts.values()) {\n            listenerContext.nextExpectedOffset().ifPresent(nextOffset -> {\n                if (nextOffset == baseOffset) {\n                    listenerContext.fireHandleCommit(baseOffset, epoch, appendTimestamp, sizeInBytes, records);\n                }\n            });\n        }\n    }\n\n    private void maybeFireLeaderChange(LeaderState<T> state) {\n        for (ListenerContext listenerContext : listenerContexts.values()) {\n            listenerContext.maybeFireLeaderChange(quorum.leaderAndEpoch(), state.epochStartOffset());\n        }\n    }\n\n    private void maybeFireLeaderChange() {\n        for (ListenerContext listenerContext : listenerContexts.values()) {\n            listenerContext.maybeFireLeaderChange(quorum.leaderAndEpoch());\n        }\n    }\n\n    public void initialize(\n        Map<Integer, InetSocketAddress> voterAddresses,\n        String listenerName,\n        QuorumStateStore quorumStateStore,\n        Metrics metrics\n    ) {\n        partitionState = new KRaftControlRecordStateMachine(\n            Optional.of(VoterSet.fromInetSocketAddresses(listenerName, voterAddresses)),\n            log,\n            serde,\n            BufferSupplier.create(),\n            MAX_BATCH_SIZE_BYTES,\n            logContext\n        );\n        // Read the entire log\n        logger.info(\"Reading KRaft snapshot and log as part of the initialization\");\n        partitionState.updateState();\n\n        VoterSet lastVoterSet = partitionState.lastVoterSet();\n        requestManager = new RequestManager(\n            lastVoterSet.voterIds(),\n            quorumConfig.retryBackoffMs(),\n            quorumConfig.requestTimeoutMs(),\n            random\n        );\n\n        quorum = new QuorumState(\n            nodeId,\n            nodeDirectoryId,\n            partitionState::lastVoterSet,\n            partitionState::lastKraftVersion,\n            quorumConfig.electionTimeoutMs(),\n            quorumConfig.fetchTimeoutMs(),\n            quorumStateStore,\n            time,\n            logContext,\n            random\n        );\n\n        kafkaRaftMetrics = new KafkaRaftMetrics(metrics, \"raft\", quorum);\n        // All Raft voters are statically configured and known at startup\n        // so there are no unknown voter connections. Report this metric as 0.\n        kafkaRaftMetrics.updateNumUnknownVoterConnections(0);\n\n        for (Integer voterId : lastVoterSet.voterIds()) {\n            channel.updateEndpoint(voterId, lastVoterSet.voterAddress(voterId, listenerName).get());\n        }\n\n        quorum.initialize(new OffsetAndEpoch(log.endOffset().offset, log.lastFetchedEpoch()));\n\n        long currentTimeMs = time.milliseconds();\n        if (quorum.isLeader()) {\n            throw new IllegalStateException(\"Voter cannot initialize as a Leader\");\n        } else if (quorum.isCandidate()) {\n            onBecomeCandidate(currentTimeMs);\n        } else if (quorum.isFollower()) {\n            onBecomeFollower(currentTimeMs);\n        }\n\n        // When there is only a single voter, become candidate immediately\n        if (quorum.isOnlyVoter() && !quorum.isCandidate()) {\n            transitionToCandidate(currentTimeMs);\n        }\n    }\n\n    @Override\n    public void register(Listener<T> listener) {\n        pendingRegistrations.add(Registration.register(listener));\n        wakeup();\n    }\n\n    @Override\n    public void unregister(Listener<T> listener) {\n        pendingRegistrations.add(Registration.unregister(listener));\n        // No need to wake up the polling thread. It is a removal so the updates can be\n        // delayed until the polling thread wakes up for other reasons.\n    }\n\n    @Override\n    public LeaderAndEpoch leaderAndEpoch() {\n        if (isInitialized()) {\n            return quorum.leaderAndEpoch();\n        } else {\n            return LeaderAndEpoch.UNKNOWN;\n        }\n    }\n\n    @Override\n    public OptionalInt nodeId() {\n        return nodeId;\n    }\n\n    private OffsetAndEpoch endOffset() {\n        return new OffsetAndEpoch(log.endOffset().offset, log.lastFetchedEpoch());\n    }\n\n    private void resetConnections() {\n        requestManager.resetAll();\n    }\n\n    private void onBecomeLeader(long currentTimeMs) {\n        long endOffset = log.endOffset().offset;\n\n        BatchAccumulator<T> accumulator = new BatchAccumulator<>(\n            quorum.epoch(),\n            endOffset,\n            quorumConfig.appendLingerMs(),\n            MAX_BATCH_SIZE_BYTES,\n            memoryPool,\n            time,\n            Compression.NONE,\n            serde\n        );\n\n        LeaderState<T> state = quorum.transitionToLeader(endOffset, accumulator);\n\n        log.initializeLeaderEpoch(quorum.epoch());\n\n        // The high watermark can only be advanced once we have written a record\n        // from the new leader's epoch. Hence we write a control message immediately\n        // to ensure there is no delay committing pending data.\n        state.appendLeaderChangeMessage(currentTimeMs);\n\n        resetConnections();\n        kafkaRaftMetrics.maybeUpdateElectionLatency(currentTimeMs);\n    }\n\n    private void flushLeaderLog(LeaderState<T> state, long currentTimeMs) {\n        // We update the end offset before flushing so that parked fetches can return sooner.\n        updateLeaderEndOffsetAndTimestamp(state, currentTimeMs);\n        log.flush(false);\n    }\n\n    private boolean maybeTransitionToLeader(CandidateState state, long currentTimeMs) {\n        if (state.isVoteGranted()) {\n            onBecomeLeader(currentTimeMs);\n            return true;\n        } else {\n            return false;\n        }\n    }\n\n    private void onBecomeCandidate(long currentTimeMs) {\n        CandidateState state = quorum.candidateStateOrThrow();\n        if (!maybeTransitionToLeader(state, currentTimeMs)) {\n            resetConnections();\n            kafkaRaftMetrics.updateElectionStartMs(currentTimeMs);\n        }\n    }\n\n    private void transitionToCandidate(long currentTimeMs) {\n        quorum.transitionToCandidate();\n        maybeFireLeaderChange();\n        onBecomeCandidate(currentTimeMs);\n    }\n\n    private void transitionToUnattached(int epoch) {\n        quorum.transitionToUnattached(epoch);\n        maybeFireLeaderChange();\n        resetConnections();\n    }\n\n    private void transitionToResigned(List<Integer> preferredSuccessors) {\n        fetchPurgatory.completeAllExceptionally(\n            Errors.NOT_LEADER_OR_FOLLOWER.exception(\"Not handling request since this node is resigning\"));\n        quorum.transitionToResigned(preferredSuccessors);\n        maybeFireLeaderChange();\n        resetConnections();\n    }\n\n    private void transitionToVoted(ReplicaKey candidateKey, int epoch) {\n        quorum.transitionToVoted(epoch, candidateKey);\n        maybeFireLeaderChange();\n        resetConnections();\n    }\n\n    private void onBecomeFollower(long currentTimeMs) {\n        kafkaRaftMetrics.maybeUpdateElectionLatency(currentTimeMs);\n\n        resetConnections();\n\n        // After becoming a follower, we need to complete all pending fetches so that\n        // they can be re-sent to the leader without waiting for their expirations\n        fetchPurgatory.completeAllExceptionally(new NotLeaderOrFollowerException(\n            \"Cannot process the fetch request because the node is no longer the leader.\"));\n\n        // Clearing the append purgatory should complete all futures exceptionally since this node is no longer the leader\n        appendPurgatory.completeAllExceptionally(new NotLeaderOrFollowerException(\n            \"Failed to receive sufficient acknowledgments for this append before leader change.\"));\n    }\n\n    private void transitionToFollower(\n        int epoch,\n        int leaderId,\n        long currentTimeMs\n    ) {\n        quorum.transitionToFollower(epoch, leaderId);\n        maybeFireLeaderChange();\n        onBecomeFollower(currentTimeMs);\n    }\n\n    private VoteResponseData buildVoteResponse(Errors partitionLevelError, boolean voteGranted) {\n        return VoteResponse.singletonResponse(\n            Errors.NONE,\n            log.topicPartition(),\n            partitionLevelError,\n            quorum.epoch(),\n            quorum.leaderIdOrSentinel(),\n            voteGranted);\n    }\n\n    /**\n     * Handle a Vote request. This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *      but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     * - {@link Errors#INCONSISTENT_VOTER_SET} if the request suggests inconsistent voter membership (e.g.\n     *      if this node or the sender is not one of the current known voters)\n     * - {@link Errors#INVALID_REQUEST} if the last epoch or offset are invalid\n     */\n    private VoteResponseData handleVoteRequest(\n        RaftRequest.Inbound requestMetadata\n    ) {\n        VoteRequestData request = (VoteRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(request.clusterId())) {\n            return new VoteResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code());\n        }\n\n        if (!hasValidTopicPartition(request, log.topicPartition())) {\n            // Until we support multi-raft, we treat individual topic partition mismatches as invalid requests\n            return new VoteResponseData().setErrorCode(Errors.INVALID_REQUEST.code());\n        }\n\n        VoteRequestData.PartitionData partitionRequest =\n            request.topics().get(0).partitions().get(0);\n\n        int candidateId = partitionRequest.candidateId();\n        int candidateEpoch = partitionRequest.candidateEpoch();\n\n        int lastEpoch = partitionRequest.lastOffsetEpoch();\n        long lastEpochEndOffset = partitionRequest.lastOffset();\n        if (lastEpochEndOffset < 0 || lastEpoch < 0 || lastEpoch >= candidateEpoch) {\n            return buildVoteResponse(Errors.INVALID_REQUEST, false);\n        }\n\n        Optional<Errors> errorOpt = validateVoterOnlyRequest(candidateId, candidateEpoch);\n        if (errorOpt.isPresent()) {\n            return buildVoteResponse(errorOpt.get(), false);\n        }\n\n        if (candidateEpoch > quorum.epoch()) {\n            transitionToUnattached(candidateEpoch);\n        }\n\n        OffsetAndEpoch lastEpochEndOffsetAndEpoch = new OffsetAndEpoch(lastEpochEndOffset, lastEpoch);\n        ReplicaKey candidateKey = ReplicaKey.of(candidateId, Optional.empty());\n        boolean voteGranted = quorum.canGrantVote(\n            candidateKey,\n            lastEpochEndOffsetAndEpoch.compareTo(endOffset()) >= 0\n        );\n\n        if (voteGranted && quorum.isUnattached()) {\n            transitionToVoted(candidateKey, candidateEpoch);\n        }\n\n        logger.info(\"Vote request {} with epoch {} is {}\", request, candidateEpoch, voteGranted ? \"granted\" : \"rejected\");\n        return buildVoteResponse(Errors.NONE, voteGranted);\n    }\n\n    private boolean handleVoteResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        int remoteNodeId = responseMetadata.sourceId();\n        VoteResponseData response = (VoteResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(response.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (!hasValidTopicPartition(response, log.topicPartition())) {\n            return false;\n        }\n\n        VoteResponseData.PartitionData partitionResponse =\n            response.topics().get(0).partitions().get(0);\n\n        Errors error = Errors.forCode(partitionResponse.errorCode());\n        OptionalInt responseLeaderId = optionalLeaderId(partitionResponse.leaderId());\n        int responseEpoch = partitionResponse.leaderEpoch();\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            error, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        } else if (error == Errors.NONE) {\n            if (quorum.isLeader()) {\n                logger.debug(\"Ignoring vote response {} since we already became leader for epoch {}\",\n                    partitionResponse, quorum.epoch());\n            } else if (quorum.isCandidate()) {\n                CandidateState state = quorum.candidateStateOrThrow();\n                if (partitionResponse.voteGranted()) {\n                    state.recordGrantedVote(remoteNodeId);\n                    maybeTransitionToLeader(state, currentTimeMs);\n                } else {\n                    state.recordRejectedVote(remoteNodeId);\n\n                    // If our vote is rejected, we go immediately to the random backoff. This\n                    // ensures that we are not stuck waiting for the election timeout when the\n                    // vote has become gridlocked.\n                    if (state.isVoteRejected() && !state.isBackingOff()) {\n                        logger.info(\"Insufficient remaining votes to become leader (rejected by {}). \" +\n                            \"We will backoff before retrying election again\", state.rejectingVoters());\n\n                        state.startBackingOff(\n                            currentTimeMs,\n                            binaryExponentialElectionBackoffMs(state.retries())\n                        );\n                    }\n                }\n            } else {\n                logger.debug(\"Ignoring vote response {} since we are no longer a candidate in epoch {}\",\n                    partitionResponse, quorum.epoch());\n            }\n            return true;\n        } else {\n            return handleUnexpectedError(error, responseMetadata);\n        }\n    }\n\n    private int binaryExponentialElectionBackoffMs(int retries) {\n        if (retries <= 0) {\n            throw new IllegalArgumentException(\"Retries \" + retries + \" should be larger than zero\");\n        }\n        // upper limit exponential co-efficients at 20 to avoid overflow\n        return Math.min(RETRY_BACKOFF_BASE_MS * random.nextInt(2 << Math.min(20, retries - 1)),\n                quorumConfig.electionBackoffMaxMs());\n    }\n\n    private int strictExponentialElectionBackoffMs(int positionInSuccessors, int totalNumSuccessors) {\n        if (positionInSuccessors <= 0 || positionInSuccessors >= totalNumSuccessors) {\n            throw new IllegalArgumentException(\"Position \" + positionInSuccessors + \" should be larger than zero\" +\n                    \" and smaller than total number of successors \" + totalNumSuccessors);\n        }\n\n        int retryBackOffBaseMs = quorumConfig.electionBackoffMaxMs() >> (totalNumSuccessors - 1);\n        return Math.min(quorumConfig.electionBackoffMaxMs(), retryBackOffBaseMs << (positionInSuccessors - 1));\n    }\n\n    private BeginQuorumEpochResponseData buildBeginQuorumEpochResponse(Errors partitionLevelError) {\n        return BeginQuorumEpochResponse.singletonResponse(\n            Errors.NONE,\n            log.topicPartition(),\n            partitionLevelError,\n            quorum.epoch(),\n            quorum.leaderIdOrSentinel());\n    }\n\n    /**\n     * Handle a BeginEpoch request. This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *      but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#INCONSISTENT_VOTER_SET} if the request suggests inconsistent voter membership (e.g.\n     *      if this node or the sender is not one of the current known voters)\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     */\n    private BeginQuorumEpochResponseData handleBeginQuorumEpochRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        BeginQuorumEpochRequestData request = (BeginQuorumEpochRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(request.clusterId())) {\n            return new BeginQuorumEpochResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code());\n        }\n\n        if (!hasValidTopicPartition(request, log.topicPartition())) {\n            // Until we support multi-raft, we treat topic partition mismatches as invalid requests\n            return new BeginQuorumEpochResponseData().setErrorCode(Errors.INVALID_REQUEST.code());\n        }\n\n        BeginQuorumEpochRequestData.PartitionData partitionRequest =\n            request.topics().get(0).partitions().get(0);\n\n        int requestLeaderId = partitionRequest.leaderId();\n        int requestEpoch = partitionRequest.leaderEpoch();\n\n        Optional<Errors> errorOpt = validateVoterOnlyRequest(requestLeaderId, requestEpoch);\n        if (errorOpt.isPresent()) {\n            return buildBeginQuorumEpochResponse(errorOpt.get());\n        }\n\n        maybeTransition(OptionalInt.of(requestLeaderId), requestEpoch, currentTimeMs);\n        return buildBeginQuorumEpochResponse(Errors.NONE);\n    }\n\n    private boolean handleBeginQuorumEpochResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        int remoteNodeId = responseMetadata.sourceId();\n        BeginQuorumEpochResponseData response = (BeginQuorumEpochResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(response.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (!hasValidTopicPartition(response, log.topicPartition())) {\n            return false;\n        }\n\n        BeginQuorumEpochResponseData.PartitionData partitionResponse =\n            response.topics().get(0).partitions().get(0);\n\n        Errors partitionError = Errors.forCode(partitionResponse.errorCode());\n        OptionalInt responseLeaderId = optionalLeaderId(partitionResponse.leaderId());\n        int responseEpoch = partitionResponse.leaderEpoch();\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            partitionError, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        } else if (partitionError == Errors.NONE) {\n            if (quorum.isLeader()) {\n                LeaderState<T> state = quorum.leaderStateOrThrow();\n                state.addAcknowledgementFrom(remoteNodeId);\n            } else {\n                logger.debug(\"Ignoring BeginQuorumEpoch response {} since \" +\n                    \"this node is not the leader anymore\", response);\n            }\n            return true;\n        } else {\n            return handleUnexpectedError(partitionError, responseMetadata);\n        }\n    }\n\n    private EndQuorumEpochResponseData buildEndQuorumEpochResponse(Errors partitionLevelError) {\n        return EndQuorumEpochResponse.singletonResponse(\n            Errors.NONE,\n            log.topicPartition(),\n            partitionLevelError,\n            quorum.epoch(),\n            quorum.leaderIdOrSentinel());\n    }\n\n    /**\n     * Handle an EndEpoch request. This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *      but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#INCONSISTENT_VOTER_SET} if the request suggests inconsistent voter membership (e.g.\n     *      if this node or the sender is not one of the current known voters)\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     */\n    private EndQuorumEpochResponseData handleEndQuorumEpochRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        EndQuorumEpochRequestData request = (EndQuorumEpochRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(request.clusterId())) {\n            return new EndQuorumEpochResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code());\n        }\n\n        if (!hasValidTopicPartition(request, log.topicPartition())) {\n            // Until we support multi-raft, we treat topic partition mismatches as invalid requests\n            return new EndQuorumEpochResponseData().setErrorCode(Errors.INVALID_REQUEST.code());\n        }\n\n        EndQuorumEpochRequestData.PartitionData partitionRequest =\n            request.topics().get(0).partitions().get(0);\n\n        int requestEpoch = partitionRequest.leaderEpoch();\n        int requestLeaderId = partitionRequest.leaderId();\n\n        Optional<Errors> errorOpt = validateVoterOnlyRequest(requestLeaderId, requestEpoch);\n        if (errorOpt.isPresent()) {\n            return buildEndQuorumEpochResponse(errorOpt.get());\n        }\n        maybeTransition(OptionalInt.of(requestLeaderId), requestEpoch, currentTimeMs);\n\n        if (quorum.isFollower()) {\n            FollowerState state = quorum.followerStateOrThrow();\n            if (state.leaderId() == requestLeaderId) {\n                List<Integer> preferredSuccessors = partitionRequest.preferredSuccessors();\n                long electionBackoffMs = endEpochElectionBackoff(preferredSuccessors);\n                logger.debug(\"Overriding follower fetch timeout to {} after receiving \" +\n                    \"EndQuorumEpoch request from leader {} in epoch {}\", electionBackoffMs,\n                    requestLeaderId, requestEpoch);\n                state.overrideFetchTimeout(currentTimeMs, electionBackoffMs);\n            }\n        }\n        return buildEndQuorumEpochResponse(Errors.NONE);\n    }\n\n    private long endEpochElectionBackoff(List<Integer> preferredSuccessors) {\n        // Based on the priority inside the preferred successors, choose the corresponding delayed\n        // election backoff time based on strict exponential mechanism so that the most up-to-date\n        // voter has a higher chance to be elected. If the node's priority is highest, become\n        // candidate immediately instead of waiting for next poll.\n        int position = preferredSuccessors.indexOf(quorum.localIdOrThrow());\n        if (position <= 0) {\n            return 0;\n        } else {\n            return strictExponentialElectionBackoffMs(position, preferredSuccessors.size());\n        }\n    }\n\n    private boolean handleEndQuorumEpochResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        EndQuorumEpochResponseData response = (EndQuorumEpochResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(response.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (!hasValidTopicPartition(response, log.topicPartition())) {\n            return false;\n        }\n\n        EndQuorumEpochResponseData.PartitionData partitionResponse =\n            response.topics().get(0).partitions().get(0);\n\n        Errors partitionError = Errors.forCode(partitionResponse.errorCode());\n        OptionalInt responseLeaderId = optionalLeaderId(partitionResponse.leaderId());\n        int responseEpoch = partitionResponse.leaderEpoch();\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            partitionError, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        } else if (partitionError == Errors.NONE) {\n            ResignedState resignedState = quorum.resignedStateOrThrow();\n            resignedState.acknowledgeResignation(responseMetadata.sourceId());\n            return true;\n        } else {\n            return handleUnexpectedError(partitionError, responseMetadata);\n        }\n    }\n\n    private FetchResponseData buildFetchResponse(\n        Errors error,\n        Records records,\n        ValidOffsetAndEpoch validOffsetAndEpoch,\n        Optional<LogOffsetMetadata> highWatermark\n    ) {\n        return RaftUtil.singletonFetchResponse(log.topicPartition(), log.topicId(), Errors.NONE, partitionData -> {\n            partitionData\n                .setRecords(records)\n                .setErrorCode(error.code())\n                .setLogStartOffset(log.startOffset())\n                .setHighWatermark(\n                    highWatermark.map(offsetMetadata -> offsetMetadata.offset).orElse(-1L)\n                );\n\n            partitionData.currentLeader()\n                .setLeaderEpoch(quorum.epoch())\n                .setLeaderId(quorum.leaderIdOrSentinel());\n\n            switch (validOffsetAndEpoch.kind()) {\n                case DIVERGING:\n                    partitionData.divergingEpoch()\n                        .setEpoch(validOffsetAndEpoch.offsetAndEpoch().epoch())\n                        .setEndOffset(validOffsetAndEpoch.offsetAndEpoch().offset());\n                    break;\n                case SNAPSHOT:\n                    partitionData.snapshotId()\n                        .setEpoch(validOffsetAndEpoch.offsetAndEpoch().epoch())\n                        .setEndOffset(validOffsetAndEpoch.offsetAndEpoch().offset());\n                    break;\n                default:\n            }\n        });\n    }\n\n    private FetchResponseData buildEmptyFetchResponse(\n        Errors error,\n        Optional<LogOffsetMetadata> highWatermark\n    ) {\n        return buildFetchResponse(\n            error,\n            MemoryRecords.EMPTY,\n            ValidOffsetAndEpoch.valid(),\n            highWatermark\n        );\n    }\n\n    private boolean hasValidClusterId(String requestClusterId) {\n        // We don't enforce the cluster id if it is not provided.\n        if (requestClusterId == null) {\n            return true;\n        }\n        return clusterId.equals(requestClusterId);\n    }\n\n    /**\n     * Handle a Fetch request. The fetch offset and last fetched epoch are always\n     * validated against the current log. In the case that they do not match, the response will\n     * indicate the diverging offset/epoch. A follower is expected to truncate its log in this\n     * case and resend the fetch.\n     *\n     * This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *     but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     * - {@link Errors#INVALID_REQUEST} if the request epoch is larger than the leader's current epoch\n     *     or if either the fetch offset or the last fetched epoch is invalid\n     */\n    private CompletableFuture<FetchResponseData> handleFetchRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        FetchRequestData request = (FetchRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(request.clusterId())) {\n            return completedFuture(new FetchResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code()));\n        }\n\n        if (!hasValidTopicPartition(request, log.topicPartition(), log.topicId())) {\n            // Until we support multi-raft, we treat topic partition mismatches as invalid requests\n            return completedFuture(new FetchResponseData().setErrorCode(Errors.INVALID_REQUEST.code()));\n        }\n        // If the ID is valid, we can set the topic name.\n        request.topics().get(0).setTopic(log.topicPartition().topic());\n\n        FetchRequestData.FetchPartition fetchPartition = request.topics().get(0).partitions().get(0);\n        if (request.maxWaitMs() < 0\n            || fetchPartition.fetchOffset() < 0\n            || fetchPartition.lastFetchedEpoch() < 0\n            || fetchPartition.lastFetchedEpoch() > fetchPartition.currentLeaderEpoch()) {\n            return completedFuture(\n                buildEmptyFetchResponse(Errors.INVALID_REQUEST, Optional.empty())\n            );\n        }\n\n        int replicaId = FetchRequest.replicaId(request);\n        FetchResponseData response = tryCompleteFetchRequest(replicaId, fetchPartition, currentTimeMs);\n        FetchResponseData.PartitionData partitionResponse =\n            response.responses().get(0).partitions().get(0);\n\n        if (partitionResponse.errorCode() != Errors.NONE.code()\n            || FetchResponse.recordsSize(partitionResponse) > 0\n            || request.maxWaitMs() == 0\n            || isPartitionDiverged(partitionResponse)\n            || isPartitionSnapshotted(partitionResponse)) {\n            // Reply immediately if any of the following is true\n            // 1. The response contains an error\n            // 2. There are records in the response\n            // 3. The fetching replica doesn't want to wait for the partition to contain new data\n            // 4. The fetching replica needs to truncate because the log diverged\n            // 5. The fetching replica needs to fetch a snapshot\n            return completedFuture(response);\n        }\n\n        CompletableFuture<Long> future = fetchPurgatory.await(\n            fetchPartition.fetchOffset(),\n            request.maxWaitMs());\n\n        return future.handle((completionTimeMs, exception) -> {\n            if (exception != null) {\n                Throwable cause = exception instanceof ExecutionException ?\n                    exception.getCause() : exception;\n\n                Errors error = Errors.forException(cause);\n                if (error == Errors.REQUEST_TIMED_OUT) {\n                    // Note that for this case the calling thread is the expiration service thread and not the\n                    // polling thread.\n                    //\n                    // If the fetch request timed out in purgatory, it means no new data is available,\n                    // just return the original fetch response.\n                    return response;\n                } else {\n                    // If there was any error other than REQUEST_TIMED_OUT, return it.\n                    logger.info(\"Failed to handle fetch from {} at {} due to {}\",\n                        replicaId, fetchPartition.fetchOffset(), error);\n                    return buildEmptyFetchResponse(error, Optional.empty());\n                }\n            }\n\n            // FIXME: `completionTimeMs`, which can be null\n            logger.trace(\"Completing delayed fetch from {} starting at offset {} at {}\",\n                replicaId, fetchPartition.fetchOffset(), completionTimeMs);\n\n            // It is safe to call tryCompleteFetchRequest because only the polling thread completes this\n            // future successfully. This is true because only the polling thread appends record batches to\n            // the log from maybeAppendBatches.\n            return tryCompleteFetchRequest(replicaId, fetchPartition, time.milliseconds());\n        });\n    }\n\n    private FetchResponseData tryCompleteFetchRequest(\n        int replicaId,\n        FetchRequestData.FetchPartition request,\n        long currentTimeMs\n    ) {\n        try {\n            Optional<Errors> errorOpt = validateLeaderOnlyRequest(request.currentLeaderEpoch());\n            if (errorOpt.isPresent()) {\n                return buildEmptyFetchResponse(errorOpt.get(), Optional.empty());\n            }\n\n            long fetchOffset = request.fetchOffset();\n            int lastFetchedEpoch = request.lastFetchedEpoch();\n            LeaderState<T> state = quorum.leaderStateOrThrow();\n\n            Optional<OffsetAndEpoch> latestSnapshotId = log.latestSnapshotId();\n            final ValidOffsetAndEpoch validOffsetAndEpoch;\n            if (fetchOffset == 0 && latestSnapshotId.isPresent()) {\n                // If the follower has an empty log and a snapshot exist, it is always more efficient\n                // to reply with a snapshot id (FETCH_SNAPSHOT) instead of fetching from the log segments.\n                validOffsetAndEpoch = ValidOffsetAndEpoch.snapshot(latestSnapshotId.get());\n            } else {\n                validOffsetAndEpoch = log.validateOffsetAndEpoch(fetchOffset, lastFetchedEpoch);\n            }\n\n            final Records records;\n            if (validOffsetAndEpoch.kind() == ValidOffsetAndEpoch.Kind.VALID) {\n                LogFetchInfo info = log.read(fetchOffset, Isolation.UNCOMMITTED);\n\n                if (state.updateReplicaState(replicaId, currentTimeMs, info.startOffsetMetadata)) {\n                    onUpdateLeaderHighWatermark(state, currentTimeMs);\n                }\n\n                records = info.records;\n            } else {\n                records = MemoryRecords.EMPTY;\n            }\n\n            return buildFetchResponse(Errors.NONE, records, validOffsetAndEpoch, state.highWatermark());\n        } catch (Exception e) {\n            logger.error(\"Caught unexpected error in fetch completion of request {}\", request, e);\n            return buildEmptyFetchResponse(Errors.UNKNOWN_SERVER_ERROR, Optional.empty());\n        }\n    }\n\n    private static boolean isPartitionDiverged(FetchResponseData.PartitionData partitionResponseData) {\n        FetchResponseData.EpochEndOffset divergingEpoch = partitionResponseData.divergingEpoch();\n\n        return divergingEpoch.epoch() != -1 || divergingEpoch.endOffset() != -1;\n    }\n\n    private static boolean isPartitionSnapshotted(FetchResponseData.PartitionData partitionResponseData) {\n        FetchResponseData.SnapshotId snapshotId = partitionResponseData.snapshotId();\n\n        return snapshotId.epoch() != -1 || snapshotId.endOffset() != -1;\n    }\n\n    private static OptionalInt optionalLeaderId(int leaderIdOrNil) {\n        if (leaderIdOrNil < 0)\n            return OptionalInt.empty();\n        return OptionalInt.of(leaderIdOrNil);\n    }\n\n    private static String listenerName(Listener<?> listener) {\n        return String.format(\"%s@%d\", listener.getClass().getTypeName(), System.identityHashCode(listener));\n    }\n\n    private boolean handleFetchResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        FetchResponseData response = (FetchResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(response.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (!RaftUtil.hasValidTopicPartition(response, log.topicPartition(), log.topicId())) {\n            return false;\n        }\n        // If the ID is valid, we can set the topic name.\n        response.responses().get(0).setTopic(log.topicPartition().topic());\n\n        FetchResponseData.PartitionData partitionResponse =\n            response.responses().get(0).partitions().get(0);\n\n        FetchResponseData.LeaderIdAndEpoch currentLeaderIdAndEpoch = partitionResponse.currentLeader();\n        OptionalInt responseLeaderId = optionalLeaderId(currentLeaderIdAndEpoch.leaderId());\n        int responseEpoch = currentLeaderIdAndEpoch.leaderEpoch();\n        Errors error = Errors.forCode(partitionResponse.errorCode());\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            error, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        }\n\n        FollowerState state = quorum.followerStateOrThrow();\n        if (error == Errors.NONE) {\n            FetchResponseData.EpochEndOffset divergingEpoch = partitionResponse.divergingEpoch();\n            if (divergingEpoch.epoch() >= 0) {\n                // The leader is asking us to truncate before continuing\n                final OffsetAndEpoch divergingOffsetAndEpoch = new OffsetAndEpoch(\n                    divergingEpoch.endOffset(), divergingEpoch.epoch());\n\n                state.highWatermark().ifPresent(highWatermark -> {\n                    if (divergingOffsetAndEpoch.offset() < highWatermark.offset) {\n                        throw new KafkaException(\"The leader requested truncation to offset \" +\n                            divergingOffsetAndEpoch.offset() + \", which is below the current high watermark\" +\n                            \" \" + highWatermark);\n                    }\n                });\n\n                long truncationOffset = log.truncateToEndOffset(divergingOffsetAndEpoch);\n                logger.info(\n                    \"Truncated to offset {} from Fetch response from leader {}\",\n                    truncationOffset,\n                    quorum.leaderIdOrSentinel()\n                );\n\n                // Update the internal listener to the new end offset\n                partitionState.truncateNewEntries(truncationOffset);\n            } else if (partitionResponse.snapshotId().epoch() >= 0 ||\n                       partitionResponse.snapshotId().endOffset() >= 0) {\n                // The leader is asking us to fetch a snapshot\n\n                if (partitionResponse.snapshotId().epoch() < 0) {\n                    logger.error(\n                        \"The leader sent a snapshot id with a valid end offset {} but with an invalid epoch {}\",\n                        partitionResponse.snapshotId().endOffset(),\n                        partitionResponse.snapshotId().epoch()\n                    );\n                    return false;\n                } else if (partitionResponse.snapshotId().endOffset() < 0) {\n                    logger.error(\n                        \"The leader sent a snapshot id with a valid epoch {} but with an invalid end offset {}\",\n                        partitionResponse.snapshotId().epoch(),\n                        partitionResponse.snapshotId().endOffset()\n                    );\n                    return false;\n                } else {\n                    final OffsetAndEpoch snapshotId = new OffsetAndEpoch(\n                        partitionResponse.snapshotId().endOffset(),\n                        partitionResponse.snapshotId().epoch()\n                    );\n\n                    // Do not validate the snapshot id against the local replicated log since this\n                    // snapshot is expected to reference offsets and epochs greater than the log\n                    // end offset and high-watermark.\n                    state.setFetchingSnapshot(log.createNewSnapshotUnchecked(snapshotId));\n                    logger.info(\n                        \"Fetching snapshot {} from Fetch response from leader {}\",\n                        snapshotId,\n                        quorum.leaderIdOrSentinel()\n                    );\n                }\n            } else {\n                Records records = FetchResponse.recordsOrFail(partitionResponse);\n                if (records.sizeInBytes() > 0) {\n                    appendAsFollower(records);\n                }\n\n                OptionalLong highWatermark = partitionResponse.highWatermark() < 0 ?\n                    OptionalLong.empty() : OptionalLong.of(partitionResponse.highWatermark());\n                updateFollowerHighWatermark(state, highWatermark);\n            }\n\n            state.resetFetchTimeout(currentTimeMs);\n            return true;\n        } else {\n            return handleUnexpectedError(error, responseMetadata);\n        }\n    }\n\n    private void appendAsFollower(\n        Records records\n    ) {\n        LogAppendInfo info = log.appendAsFollower(records);\n        if (quorum.isVoter()) {\n            // the leader only requires that voters have flushed their log before sending\n            // a Fetch request\n            log.flush(false);\n        }\n\n        partitionState.updateState();\n\n        OffsetAndEpoch endOffset = endOffset();\n        kafkaRaftMetrics.updateFetchedRecords(info.lastOffset - info.firstOffset + 1);\n        kafkaRaftMetrics.updateLogEnd(endOffset);\n        logger.trace(\"Follower end offset updated to {} after append\", endOffset);\n    }\n\n    private LogAppendInfo appendAsLeader(\n        Records records\n    ) {\n        LogAppendInfo info = log.appendAsLeader(records, quorum.epoch());\n\n        partitionState.updateState();\n\n        OffsetAndEpoch endOffset = endOffset();\n        kafkaRaftMetrics.updateAppendRecords(info.lastOffset - info.firstOffset + 1);\n        kafkaRaftMetrics.updateLogEnd(endOffset);\n        logger.trace(\"Leader appended records at base offset {}, new end offset is {}\", info.firstOffset, endOffset);\n        return info;\n    }\n\n    private DescribeQuorumResponseData handleDescribeQuorumRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        DescribeQuorumRequestData describeQuorumRequestData = (DescribeQuorumRequestData) requestMetadata.data;\n        if (!hasValidTopicPartition(describeQuorumRequestData, log.topicPartition())) {\n            return DescribeQuorumRequest.getPartitionLevelErrorResponse(\n                describeQuorumRequestData, Errors.UNKNOWN_TOPIC_OR_PARTITION);\n        }\n\n        if (!quorum.isLeader()) {\n            return DescribeQuorumResponse.singletonErrorResponse(\n                log.topicPartition(),\n                Errors.NOT_LEADER_OR_FOLLOWER\n            );\n        }\n\n        LeaderState<T> leaderState = quorum.leaderStateOrThrow();\n        return DescribeQuorumResponse.singletonResponse(\n            log.topicPartition(),\n            leaderState.describeQuorum(currentTimeMs)\n        );\n    }\n\n    /**\n     * Handle a FetchSnapshot request, similar to the Fetch request but we use {@link UnalignedRecords}\n     * in response because the records are not necessarily offset-aligned.\n     *\n     * This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *     but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     * - {@link Errors#INVALID_REQUEST} if the request epoch is larger than the leader's current epoch\n     *     or if either the fetch offset or the last fetched epoch is invalid\n     * - {@link Errors#SNAPSHOT_NOT_FOUND} if the request snapshot id does not exists\n     * - {@link Errors#POSITION_OUT_OF_RANGE} if the request snapshot offset out of range\n     */\n    private FetchSnapshotResponseData handleFetchSnapshotRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        FetchSnapshotRequestData data = (FetchSnapshotRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(data.clusterId())) {\n            return new FetchSnapshotResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code());\n        }\n\n        if (data.topics().size() != 1 && data.topics().get(0).partitions().size() != 1) {\n            return FetchSnapshotResponse.withTopLevelError(Errors.INVALID_REQUEST);\n        }\n\n        Optional<FetchSnapshotRequestData.PartitionSnapshot> partitionSnapshotOpt = FetchSnapshotRequest\n            .forTopicPartition(data, log.topicPartition());\n        if (!partitionSnapshotOpt.isPresent()) {\n            // The Raft client assumes that there is only one topic partition.\n            TopicPartition unknownTopicPartition = new TopicPartition(\n                data.topics().get(0).name(),\n                data.topics().get(0).partitions().get(0).partition()\n            );\n\n            return FetchSnapshotResponse.singleton(\n                unknownTopicPartition,\n                responsePartitionSnapshot -> responsePartitionSnapshot\n                    .setErrorCode(Errors.UNKNOWN_TOPIC_OR_PARTITION.code())\n            );\n        }\n\n        FetchSnapshotRequestData.PartitionSnapshot partitionSnapshot = partitionSnapshotOpt.get();\n        Optional<Errors> leaderValidation = validateLeaderOnlyRequest(\n                partitionSnapshot.currentLeaderEpoch()\n        );\n        if (leaderValidation.isPresent()) {\n            return FetchSnapshotResponse.singleton(\n                log.topicPartition(),\n                responsePartitionSnapshot -> addQuorumLeader(responsePartitionSnapshot)\n                    .setErrorCode(leaderValidation.get().code())\n            );\n        }\n\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(\n            partitionSnapshot.snapshotId().endOffset(),\n            partitionSnapshot.snapshotId().epoch()\n        );\n        Optional<RawSnapshotReader> snapshotOpt = log.readSnapshot(snapshotId);\n        if (!snapshotOpt.isPresent()) {\n            return FetchSnapshotResponse.singleton(\n                log.topicPartition(),\n                responsePartitionSnapshot -> addQuorumLeader(responsePartitionSnapshot)\n                    .setErrorCode(Errors.SNAPSHOT_NOT_FOUND.code())\n            );\n        }\n\n        RawSnapshotReader snapshot = snapshotOpt.get();\n        long snapshotSize = snapshot.sizeInBytes();\n        if (partitionSnapshot.position() < 0 || partitionSnapshot.position() >= snapshotSize) {\n            return FetchSnapshotResponse.singleton(\n                log.topicPartition(),\n                responsePartitionSnapshot -> addQuorumLeader(responsePartitionSnapshot)\n                    .setErrorCode(Errors.POSITION_OUT_OF_RANGE.code())\n            );\n        }\n\n        if (partitionSnapshot.position() > Integer.MAX_VALUE) {\n            throw new IllegalStateException(\n                String.format(\n                    \"Trying to fetch a snapshot with size (%d) and a position (%d) larger than %d\",\n                    snapshotSize,\n                    partitionSnapshot.position(),\n                    Integer.MAX_VALUE\n                )\n            );\n        }\n\n        int maxSnapshotSize;\n        try {\n            maxSnapshotSize = Math.toIntExact(snapshotSize);\n        } catch (ArithmeticException e) {\n            maxSnapshotSize = Integer.MAX_VALUE;\n        }\n\n        UnalignedRecords records = snapshot.slice(partitionSnapshot.position(), Math.min(data.maxBytes(), maxSnapshotSize));\n\n        LeaderState<T> state = quorum.leaderStateOrThrow();\n        state.updateCheckQuorumForFollowingVoter(data.replicaId(), currentTimeMs);\n\n        return FetchSnapshotResponse.singleton(\n            log.topicPartition(),\n            responsePartitionSnapshot -> {\n                addQuorumLeader(responsePartitionSnapshot)\n                    .snapshotId()\n                    .setEndOffset(snapshotId.offset())\n                    .setEpoch(snapshotId.epoch());\n\n                return responsePartitionSnapshot\n                    .setSize(snapshotSize)\n                    .setPosition(partitionSnapshot.position())\n                    .setUnalignedRecords(records);\n            }\n        );\n    }\n\n    private boolean handleFetchSnapshotResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        FetchSnapshotResponseData data = (FetchSnapshotResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(data.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (data.topics().size() != 1 && data.topics().get(0).partitions().size() != 1) {\n            return false;\n        }\n\n        Optional<FetchSnapshotResponseData.PartitionSnapshot> partitionSnapshotOpt = FetchSnapshotResponse\n            .forTopicPartition(data, log.topicPartition());\n        if (!partitionSnapshotOpt.isPresent()) {\n            return false;\n        }\n\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot = partitionSnapshotOpt.get();\n\n        FetchSnapshotResponseData.LeaderIdAndEpoch currentLeaderIdAndEpoch = partitionSnapshot.currentLeader();\n        OptionalInt responseLeaderId = optionalLeaderId(currentLeaderIdAndEpoch.leaderId());\n        int responseEpoch = currentLeaderIdAndEpoch.leaderEpoch();\n        Errors error = Errors.forCode(partitionSnapshot.errorCode());\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            error, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        }\n\n        FollowerState state = quorum.followerStateOrThrow();\n\n        if (Errors.forCode(partitionSnapshot.errorCode()) == Errors.SNAPSHOT_NOT_FOUND ||\n            partitionSnapshot.snapshotId().endOffset() < 0 ||\n            partitionSnapshot.snapshotId().epoch() < 0) {\n\n            /* The leader deleted the snapshot before the follower could download it. Start over by\n             * resetting the fetching snapshot state and sending another fetch request.\n             */\n            logger.info(\n                \"Leader doesn't know about snapshot id {}, returned error {} and snapshot id {}\",\n                state.fetchingSnapshot(),\n                partitionSnapshot.errorCode(),\n                partitionSnapshot.snapshotId()\n            );\n            state.setFetchingSnapshot(Optional.empty());\n            state.resetFetchTimeout(currentTimeMs);\n            return true;\n        }\n\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(\n            partitionSnapshot.snapshotId().endOffset(),\n            partitionSnapshot.snapshotId().epoch()\n        );\n\n        RawSnapshotWriter snapshot;\n        if (state.fetchingSnapshot().isPresent()) {\n            snapshot = state.fetchingSnapshot().get();\n        } else {\n            throw new IllegalStateException(\n                String.format(\"Received unexpected fetch snapshot response: %s\", partitionSnapshot)\n            );\n        }\n\n        if (!snapshot.snapshotId().equals(snapshotId)) {\n            throw new IllegalStateException(\n                String.format(\n                    \"Received fetch snapshot response with an invalid id. Expected %s; Received %s\",\n                    snapshot.snapshotId(),\n                    snapshotId\n                )\n            );\n        }\n        if (snapshot.sizeInBytes() != partitionSnapshot.position()) {\n            throw new IllegalStateException(\n                String.format(\n                    \"Received fetch snapshot response with an invalid position. Expected %d; Received %d\",\n                    snapshot.sizeInBytes(),\n                    partitionSnapshot.position()\n                )\n            );\n        }\n\n        final UnalignedMemoryRecords records;\n        if (partitionSnapshot.unalignedRecords() instanceof MemoryRecords) {\n            records = new UnalignedMemoryRecords(((MemoryRecords) partitionSnapshot.unalignedRecords()).buffer());\n        } else if (partitionSnapshot.unalignedRecords() instanceof UnalignedMemoryRecords) {\n            records = (UnalignedMemoryRecords) partitionSnapshot.unalignedRecords();\n        } else {\n            throw new IllegalStateException(String.format(\"Received unexpected fetch snapshot response: %s\", partitionSnapshot));\n        }\n        snapshot.append(records);\n\n        if (snapshot.sizeInBytes() == partitionSnapshot.size()) {\n            // Finished fetching the snapshot.\n            snapshot.freeze();\n            state.setFetchingSnapshot(Optional.empty());\n\n            if (log.truncateToLatestSnapshot()) {\n                logger.info(\n                    \"Fully truncated the log at ({}, {}) after downloading snapshot {} from leader {}\",\n                    log.endOffset(),\n                    log.lastFetchedEpoch(),\n                    snapshot.snapshotId(),\n                    quorum.leaderIdOrSentinel()\n                );\n\n                // This will always reload the snapshot because the internal next offset\n                // is always less than the snapshot id just downloaded.\n                partitionState.updateState();\n\n                updateFollowerHighWatermark(state, OptionalLong.of(log.highWatermark().offset));\n            } else {\n                throw new IllegalStateException(\n                    String.format(\n                        \"Full log truncation expected but didn't happen. Snapshot of %s, log end offset %s, last fetched %d\",\n                        snapshot.snapshotId(),\n                        log.endOffset(),\n                        log.lastFetchedEpoch()\n                    )\n                );\n            }\n        }\n\n        state.resetFetchTimeout(currentTimeMs);\n        return true;\n    }\n\n    private boolean hasConsistentLeader(int epoch, OptionalInt leaderId) {\n        // Only elected leaders are sent in the request/response header, so if we have an elected\n        // leaderId, it should be consistent with what is in the message.\n        if (leaderId.isPresent() && leaderId.getAsInt() == quorum.localIdOrSentinel()) {\n            // The response indicates that we should be the leader, so we verify that is the case\n            return quorum.isLeader();\n        } else {\n            return epoch != quorum.epoch()\n                || !leaderId.isPresent()\n                || !quorum.leaderId().isPresent()\n                || leaderId.equals(quorum.leaderId());\n        }\n    }\n\n    /**\n     * Handle response errors that are common across request types.\n     *\n     * @param error Error from the received response\n     * @param leaderId Optional leaderId from the response\n     * @param epoch Epoch received from the response\n     * @param currentTimeMs Current epoch time in milliseconds\n     * @return Optional value indicating whether the error was handled here and the outcome of\n     *    that handling. Specifically:\n     *\n     *    - Optional.empty means that the response was not handled here and the custom\n     *        API handler should be applied\n     *    - Optional.of(true) indicates that the response was successfully handled here and\n     *        the request does not need to be retried\n     *    - Optional.of(false) indicates that the response was handled here, but that the request\n     *        will need to be retried\n     */\n    private Optional<Boolean> maybeHandleCommonResponse(\n        Errors error,\n        OptionalInt leaderId,\n        int epoch,\n        long currentTimeMs\n    ) {\n        if (epoch < quorum.epoch() || error == Errors.UNKNOWN_LEADER_EPOCH) {\n            // We have a larger epoch, so the response is no longer relevant\n            return Optional.of(true);\n        } else if (epoch > quorum.epoch()\n            || error == Errors.FENCED_LEADER_EPOCH\n            || error == Errors.NOT_LEADER_OR_FOLLOWER) {\n\n            // The response indicates that the request had a stale epoch, but we need\n            // to validate the epoch from the response against our current state.\n            maybeTransition(leaderId, epoch, currentTimeMs);\n            return Optional.of(true);\n        } else if (epoch == quorum.epoch()\n            && leaderId.isPresent()\n            && !quorum.hasLeader()) {\n\n            // Since we are transitioning to Follower, we will only forward the\n            // request to the handler if there is no error. Otherwise, we will let\n            // the request be retried immediately (if needed) after the transition.\n            // This handling allows an observer to discover the leader and append\n            // to the log in the same Fetch request.\n            transitionToFollower(epoch, leaderId.getAsInt(), currentTimeMs);\n            if (error == Errors.NONE) {\n                return Optional.empty();\n            } else {\n                return Optional.of(true);\n            }\n        } else if (error == Errors.BROKER_NOT_AVAILABLE) {\n            return Optional.of(false);\n        } else if (error == Errors.INCONSISTENT_GROUP_PROTOCOL) {\n            // For now we treat this as a fatal error. Once we have support for quorum\n            // reassignment, this error could suggest that either we or the recipient of\n            // the request just has stale voter information, which means we can retry\n            // after backing off.\n            throw new IllegalStateException(\"Received error indicating inconsistent voter sets\");\n        } else if (error == Errors.INVALID_REQUEST) {\n            throw new IllegalStateException(\"Received unexpected invalid request error\");\n        }\n\n        return Optional.empty();\n    }\n\n    private void maybeTransition(\n        OptionalInt leaderId,\n        int epoch,\n        long currentTimeMs\n    ) {\n        if (!hasConsistentLeader(epoch, leaderId)) {\n            throw new IllegalStateException(\"Received request or response with leader \" + leaderId +\n                \" and epoch \" + epoch + \" which is inconsistent with current leader \" +\n                quorum.leaderId() + \" and epoch \" + quorum.epoch());\n        } else if (epoch > quorum.epoch()) {\n            if (leaderId.isPresent()) {\n                transitionToFollower(epoch, leaderId.getAsInt(), currentTimeMs);\n            } else {\n                transitionToUnattached(epoch);\n            }\n        } else if (leaderId.isPresent() && !quorum.hasLeader()) {\n            // The request or response indicates the leader of the current epoch,\n            // which is currently unknown\n            transitionToFollower(epoch, leaderId.getAsInt(), currentTimeMs);\n        }\n    }\n\n    private boolean handleTopLevelError(Errors error, RaftResponse.Inbound response) {\n        if (error == Errors.BROKER_NOT_AVAILABLE) {\n            return false;\n        } else if (error == Errors.CLUSTER_AUTHORIZATION_FAILED) {\n            throw new ClusterAuthorizationException(\"Received cluster authorization error in response \" + response);\n        } else {\n            return handleUnexpectedError(error, response);\n        }\n    }\n\n    private boolean handleUnexpectedError(Errors error, RaftResponse.Inbound response) {\n        logger.error(\"Unexpected error {} in {} response: {}\",\n            error, ApiKeys.forId(response.data.apiKey()), response);\n        return false;\n    }\n\n    private void handleResponse(RaftResponse.Inbound response, long currentTimeMs) {\n        // The response epoch matches the local epoch, so we can handle the response\n        ApiKeys apiKey = ApiKeys.forId(response.data.apiKey());\n        final boolean handledSuccessfully;\n\n        switch (apiKey) {\n            case FETCH:\n                handledSuccessfully = handleFetchResponse(response, currentTimeMs);\n                break;\n\n            case VOTE:\n                handledSuccessfully = handleVoteResponse(response, currentTimeMs);\n                break;\n\n            case BEGIN_QUORUM_EPOCH:\n                handledSuccessfully = handleBeginQuorumEpochResponse(response, currentTimeMs);\n                break;\n\n            case END_QUORUM_EPOCH:\n                handledSuccessfully = handleEndQuorumEpochResponse(response, currentTimeMs);\n                break;\n\n            case FETCH_SNAPSHOT:\n                handledSuccessfully = handleFetchSnapshotResponse(response, currentTimeMs);\n                break;\n\n            default:\n                throw new IllegalArgumentException(\"Received unexpected response type: \" + apiKey);\n        }\n\n        ConnectionState connection = requestManager.getOrCreate(response.sourceId());\n        onResponseResult(response, currentTimeMs, handledSuccessfully, connection);\n    }\n\n    private void onResponseResult(RaftResponse.Inbound response, long currentTimeMs, boolean handledSuccessfully, ConnectionState connection) {\n        if (handledSuccessfully) {\n            connection.onResponseReceived(response.correlationId);\n        } else {\n            connection.onResponseError(response.correlationId, currentTimeMs);\n        }\n    }\n\n    /**\n     * Validate common state for requests to establish leadership.\n     *\n     * These include the Vote, BeginQuorumEpoch and EndQuorumEpoch RPCs. If an error is present in\n     * the returned value, it should be returned in the response.\n     */\n    private Optional<Errors> validateVoterOnlyRequest(int remoteNodeId, int requestEpoch) {\n        if (requestEpoch < quorum.epoch()) {\n            return Optional.of(Errors.FENCED_LEADER_EPOCH);\n        } else if (remoteNodeId < 0) {\n            return Optional.of(Errors.INVALID_REQUEST);\n        } else {\n            return Optional.empty();\n        }\n    }\n\n    /**\n     * Validate a request which is intended for the current quorum leader.\n     * If an error is present in the returned value, it should be returned\n     * in the response.\n     */\n    private Optional<Errors> validateLeaderOnlyRequest(int requestEpoch) {\n        if (requestEpoch < quorum.epoch()) {\n            return Optional.of(Errors.FENCED_LEADER_EPOCH);\n        } else if (requestEpoch > quorum.epoch()) {\n            return Optional.of(Errors.UNKNOWN_LEADER_EPOCH);\n        } else if (!quorum.isLeader()) {\n            // In general, non-leaders do not expect to receive requests\n            // matching their own epoch, but it is possible when observers\n            // are using the Fetch API to find the result of an election.\n            return Optional.of(Errors.NOT_LEADER_OR_FOLLOWER);\n        } else if (shutdown.get() != null) {\n            return Optional.of(Errors.BROKER_NOT_AVAILABLE);\n        } else {\n            return Optional.empty();\n        }\n    }\n\n    private void handleRequest(RaftRequest.Inbound request, long currentTimeMs) {\n        ApiKeys apiKey = ApiKeys.forId(request.data.apiKey());\n        final CompletableFuture<? extends ApiMessage> responseFuture;\n\n        switch (apiKey) {\n            case FETCH:\n                responseFuture = handleFetchRequest(request, currentTimeMs);\n                break;\n\n            case VOTE:\n                responseFuture = completedFuture(handleVoteRequest(request));\n                break;\n\n            case BEGIN_QUORUM_EPOCH:\n                responseFuture = completedFuture(handleBeginQuorumEpochRequest(request, currentTimeMs));\n                break;\n\n            case END_QUORUM_EPOCH:\n                responseFuture = completedFuture(handleEndQuorumEpochRequest(request, currentTimeMs));\n                break;\n\n            case DESCRIBE_QUORUM:\n                responseFuture = completedFuture(handleDescribeQuorumRequest(request, currentTimeMs));\n                break;\n\n            case FETCH_SNAPSHOT:\n                responseFuture = completedFuture(handleFetchSnapshotRequest(request, currentTimeMs));\n                break;\n\n            default:\n                throw new IllegalArgumentException(\"Unexpected request type \" + apiKey);\n        }\n\n        responseFuture.whenComplete((response, exception) -> {\n            final ApiMessage message;\n            if (response != null) {\n                message = response;\n            } else {\n                message = RaftUtil.errorResponse(apiKey, Errors.forException(exception));\n            }\n\n            RaftResponse.Outbound responseMessage = new RaftResponse.Outbound(request.correlationId(), message);\n            request.completion.complete(responseMessage);\n            logger.trace(\"Sent response {} to inbound request {}\", responseMessage, request);\n        });\n    }\n\n    private void handleInboundMessage(RaftMessage message, long currentTimeMs) {\n        logger.trace(\"Received inbound message {}\", message);\n\n        if (message instanceof RaftRequest.Inbound) {\n            RaftRequest.Inbound request = (RaftRequest.Inbound) message;\n            handleRequest(request, currentTimeMs);\n        } else if (message instanceof RaftResponse.Inbound) {\n            RaftResponse.Inbound response = (RaftResponse.Inbound) message;\n            ConnectionState connection = requestManager.getOrCreate(response.sourceId());\n            if (connection.isResponseExpected(response.correlationId)) {\n                handleResponse(response, currentTimeMs);\n            } else {\n                logger.debug(\"Ignoring response {} since it is no longer needed\", response);\n            }\n        } else {\n            throw new IllegalArgumentException(\"Unexpected message \" + message);\n        }\n    }\n\n    /**\n     * Attempt to send a request. Return the time to wait before the request can be retried.\n     */\n    private long maybeSendRequest(\n        long currentTimeMs,\n        int destinationId,\n        Supplier<ApiMessage> requestSupplier\n    )  {\n        ConnectionState connection = requestManager.getOrCreate(destinationId);\n\n        if (connection.isBackingOff(currentTimeMs)) {\n            long remainingBackoffMs = connection.remainingBackoffMs(currentTimeMs);\n            logger.debug(\"Connection for {} is backing off for {} ms\", destinationId, remainingBackoffMs);\n            return remainingBackoffMs;\n        }\n\n        if (connection.isReady(currentTimeMs)) {\n            int correlationId = channel.newCorrelationId();\n            ApiMessage request = requestSupplier.get();\n\n            RaftRequest.Outbound requestMessage = new RaftRequest.Outbound(\n                correlationId,\n                request,\n                destinationId,\n                currentTimeMs\n            );\n\n            requestMessage.completion.whenComplete((response, exception) -> {\n                if (exception != null) {\n                    ApiKeys api = ApiKeys.forId(request.apiKey());\n                    Errors error = Errors.forException(exception);\n                    ApiMessage errorResponse = RaftUtil.errorResponse(api, error);\n\n                    response = new RaftResponse.Inbound(\n                        correlationId,\n                        errorResponse,\n                        destinationId\n                    );\n                }\n\n                messageQueue.add(response);\n            });\n\n            channel.send(requestMessage);\n            logger.trace(\"Sent outbound request: {}\", requestMessage);\n            connection.onRequestSent(correlationId, currentTimeMs);\n            return Long.MAX_VALUE;\n        }\n\n        return connection.remainingRequestTimeMs(currentTimeMs);\n    }\n\n    private EndQuorumEpochRequestData buildEndQuorumEpochRequest(\n        ResignedState state\n    ) {\n        return EndQuorumEpochRequest.singletonRequest(\n            log.topicPartition(),\n            clusterId,\n            quorum.epoch(),\n            quorum.localIdOrThrow(),\n            state.preferredSuccessors()\n        );\n    }\n\n    private long maybeSendRequests(\n        long currentTimeMs,\n        Set<Integer> destinationIds,\n        Supplier<ApiMessage> requestSupplier\n    ) {\n        long minBackoffMs = Long.MAX_VALUE;\n        for (Integer destinationId : destinationIds) {\n            long backoffMs = maybeSendRequest(currentTimeMs, destinationId, requestSupplier);\n            if (backoffMs < minBackoffMs) {\n                minBackoffMs = backoffMs;\n            }\n        }\n        return minBackoffMs;\n    }\n\n    private BeginQuorumEpochRequestData buildBeginQuorumEpochRequest() {\n        return BeginQuorumEpochRequest.singletonRequest(\n            log.topicPartition(),\n            clusterId,\n            quorum.epoch(),\n            quorum.localIdOrThrow()\n        );\n    }\n\n    private VoteRequestData buildVoteRequest() {\n        OffsetAndEpoch endOffset = endOffset();\n        return VoteRequest.singletonRequest(\n            log.topicPartition(),\n            clusterId,\n            quorum.epoch(),\n            quorum.localIdOrThrow(),\n            endOffset.epoch(),\n            endOffset.offset()\n        );\n    }\n\n    private FetchRequestData buildFetchRequest() {\n        FetchRequestData request = RaftUtil.singletonFetchRequest(log.topicPartition(), log.topicId(), fetchPartition -> {\n            fetchPartition\n                .setCurrentLeaderEpoch(quorum.epoch())\n                .setLastFetchedEpoch(log.lastFetchedEpoch())\n                .setFetchOffset(log.endOffset().offset);\n        });\n        return request\n            .setMaxBytes(MAX_FETCH_SIZE_BYTES)\n            .setMaxWaitMs(fetchMaxWaitMs)\n            .setClusterId(clusterId)\n            .setReplicaState(new FetchRequestData.ReplicaState().setReplicaId(quorum.localIdOrSentinel()));\n    }\n\n    private long maybeSendAnyVoterFetch(long currentTimeMs) {\n        OptionalInt readyVoterIdOpt = requestManager.findReadyVoter(currentTimeMs);\n        if (readyVoterIdOpt.isPresent()) {\n            return maybeSendRequest(\n                currentTimeMs,\n                readyVoterIdOpt.getAsInt(),\n                this::buildFetchRequest\n            );\n        } else {\n            return requestManager.backoffBeforeAvailableVoter(currentTimeMs);\n        }\n    }\n\n    private FetchSnapshotRequestData buildFetchSnapshotRequest(OffsetAndEpoch snapshotId, long snapshotSize) {\n        FetchSnapshotRequestData.SnapshotId requestSnapshotId = new FetchSnapshotRequestData.SnapshotId()\n            .setEpoch(snapshotId.epoch())\n            .setEndOffset(snapshotId.offset());\n\n        FetchSnapshotRequestData request = FetchSnapshotRequest.singleton(\n            clusterId,\n            quorum().localIdOrSentinel(),\n            log.topicPartition(),\n            snapshotPartition -> snapshotPartition\n                .setCurrentLeaderEpoch(quorum.epoch())\n                .setSnapshotId(requestSnapshotId)\n                .setPosition(snapshotSize)\n        );\n\n        return request.setReplicaId(quorum.localIdOrSentinel());\n    }\n\n    private FetchSnapshotResponseData.PartitionSnapshot addQuorumLeader(\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot\n    ) {\n        partitionSnapshot.currentLeader()\n            .setLeaderEpoch(quorum.epoch())\n            .setLeaderId(quorum.leaderIdOrSentinel());\n\n        return partitionSnapshot;\n    }\n\n    public boolean isRunning() {\n        GracefulShutdown gracefulShutdown = shutdown.get();\n        return gracefulShutdown == null || !gracefulShutdown.isFinished();\n    }\n\n    public boolean isShuttingDown() {\n        GracefulShutdown gracefulShutdown = shutdown.get();\n        return gracefulShutdown != null && !gracefulShutdown.isFinished();\n    }\n\n    private void appendBatch(\n        LeaderState<T> state,\n        BatchAccumulator.CompletedBatch<T> batch,\n        long appendTimeMs\n    ) {\n        try {\n            int epoch = state.epoch();\n            LogAppendInfo info = appendAsLeader(batch.data);\n            OffsetAndEpoch offsetAndEpoch = new OffsetAndEpoch(info.lastOffset, epoch);\n            CompletableFuture<Long> future = appendPurgatory.await(\n                offsetAndEpoch.offset() + 1, Integer.MAX_VALUE);\n\n            future.whenComplete((commitTimeMs, exception) -> {\n                if (exception != null) {\n                    logger.debug(\"Failed to commit {} records up to last offset {}\", batch.numRecords, offsetAndEpoch, exception);\n                } else {\n                    long elapsedTime = Math.max(0, commitTimeMs - appendTimeMs);\n                    double elapsedTimePerRecord = (double) elapsedTime / batch.numRecords;\n                    kafkaRaftMetrics.updateCommitLatency(elapsedTimePerRecord, appendTimeMs);\n                    logger.debug(\"Completed commit of {} records up to last offset {}\", batch.numRecords, offsetAndEpoch);\n                    batch.records.ifPresent(records -> {\n                        maybeFireHandleCommit(batch.baseOffset, epoch, batch.appendTimestamp(), batch.sizeInBytes(), records);\n                    });\n                }\n            });\n        } finally {\n            batch.release();\n        }\n    }\n\n    private long maybeAppendBatches(\n        LeaderState<T> state,\n        long currentTimeMs\n    ) {\n        long timeUntilDrain = state.accumulator().timeUntilDrain(currentTimeMs);\n        if (timeUntilDrain <= 0) {\n            List<BatchAccumulator.CompletedBatch<T>> batches = state.accumulator().drain();\n            Iterator<BatchAccumulator.CompletedBatch<T>> iterator = batches.iterator();\n\n            try {\n                while (iterator.hasNext()) {\n                    BatchAccumulator.CompletedBatch<T> batch = iterator.next();\n                    appendBatch(state, batch, currentTimeMs);\n                }\n                flushLeaderLog(state, currentTimeMs);\n            } finally {\n                // Release and discard any batches which failed to be appended\n                while (iterator.hasNext()) {\n                    iterator.next().release();\n                }\n            }\n        }\n        return timeUntilDrain;\n    }\n\n    private long pollResigned(long currentTimeMs) {\n        ResignedState state = quorum.resignedStateOrThrow();\n        long endQuorumBackoffMs = maybeSendRequests(\n            currentTimeMs,\n            state.unackedVoters(),\n            () -> buildEndQuorumEpochRequest(state)\n        );\n\n        GracefulShutdown shutdown = this.shutdown.get();\n        final long stateTimeoutMs;\n        if (shutdown != null) {\n            // If we are shutting down, then we will remain in the resigned state\n            // until either the shutdown expires or an election bumps the epoch\n            stateTimeoutMs = shutdown.remainingTimeMs();\n        } else if (state.hasElectionTimeoutExpired(currentTimeMs)) {\n            transitionToCandidate(currentTimeMs);\n            stateTimeoutMs = 0L;\n        } else {\n            stateTimeoutMs = state.remainingElectionTimeMs(currentTimeMs);\n        }\n\n        return Math.min(stateTimeoutMs, endQuorumBackoffMs);\n    }\n\n    private long pollLeader(long currentTimeMs) {\n        LeaderState<T> state = quorum.leaderStateOrThrow();\n        maybeFireLeaderChange(state);\n\n        long timeUntilCheckQuorumExpires = state.timeUntilCheckQuorumExpires(currentTimeMs);\n        if (shutdown.get() != null || state.isResignRequested() || timeUntilCheckQuorumExpires == 0) {\n            transitionToResigned(state.nonLeaderVotersByDescendingFetchOffset());\n            return 0L;\n        }\n\n        long timeUntilFlush = maybeAppendBatches(\n            state,\n            currentTimeMs\n        );\n\n        long timeUntilSend = maybeSendRequests(\n            currentTimeMs,\n            state.nonAcknowledgingVoters(),\n            this::buildBeginQuorumEpochRequest\n        );\n\n        return Math.min(timeUntilFlush, Math.min(timeUntilSend, timeUntilCheckQuorumExpires));\n    }\n\n    private long maybeSendVoteRequests(\n        CandidateState state,\n        long currentTimeMs\n    ) {\n        // Continue sending Vote requests as long as we still have a chance to win the election\n        if (!state.isVoteRejected()) {\n            return maybeSendRequests(\n                currentTimeMs,\n                state.unrecordedVoters(),\n                this::buildVoteRequest\n            );\n        }\n        return Long.MAX_VALUE;\n    }\n\n    private long pollCandidate(long currentTimeMs) {\n        CandidateState state = quorum.candidateStateOrThrow();\n        GracefulShutdown shutdown = this.shutdown.get();\n\n        if (shutdown != null) {\n            // If we happen to shutdown while we are a candidate, we will continue\n            // with the current election until one of the following conditions is met:\n            //  1) we are elected as leader (which allows us to resign)\n            //  2) another leader is elected\n            //  3) the shutdown timer expires\n            long minRequestBackoffMs = maybeSendVoteRequests(state, currentTimeMs);\n            return Math.min(shutdown.remainingTimeMs(), minRequestBackoffMs);\n        } else if (state.isBackingOff()) {\n            if (state.isBackoffComplete(currentTimeMs)) {\n                logger.info(\"Re-elect as candidate after election backoff has completed\");\n                transitionToCandidate(currentTimeMs);\n                return 0L;\n            }\n            return state.remainingBackoffMs(currentTimeMs);\n        } else if (state.hasElectionTimeoutExpired(currentTimeMs)) {\n            long backoffDurationMs = binaryExponentialElectionBackoffMs(state.retries());\n            logger.info(\"Election has timed out, backing off for {}ms before becoming a candidate again\",\n                backoffDurationMs);\n            state.startBackingOff(currentTimeMs, backoffDurationMs);\n            return backoffDurationMs;\n        } else {\n            long minRequestBackoffMs = maybeSendVoteRequests(state, currentTimeMs);\n            return Math.min(minRequestBackoffMs, state.remainingElectionTimeMs(currentTimeMs));\n        }\n    }\n\n    private long pollFollower(long currentTimeMs) {\n        FollowerState state = quorum.followerStateOrThrow();\n        if (quorum.isVoter()) {\n            return pollFollowerAsVoter(state, currentTimeMs);\n        } else {\n            return pollFollowerAsObserver(state, currentTimeMs);\n        }\n    }\n\n    private long pollFollowerAsVoter(FollowerState state, long currentTimeMs) {\n        GracefulShutdown shutdown = this.shutdown.get();\n        if (shutdown != null) {\n            // If we are a follower, then we can shutdown immediately. We want to\n            // skip the transition to candidate in any case.\n            return 0;\n        } else if (state.hasFetchTimeoutExpired(currentTimeMs)) {\n            logger.info(\"Become candidate due to fetch timeout\");\n            transitionToCandidate(currentTimeMs);\n            return 0L;\n        } else {\n            long backoffMs = maybeSendFetchOrFetchSnapshot(state, currentTimeMs);\n\n            return Math.min(backoffMs, state.remainingFetchTimeMs(currentTimeMs));\n        }\n    }\n\n    private long pollFollowerAsObserver(FollowerState state, long currentTimeMs) {\n        if (state.hasFetchTimeoutExpired(currentTimeMs)) {\n            return maybeSendAnyVoterFetch(currentTimeMs);\n        } else {\n            final long backoffMs;\n\n            // If the current leader is backing off due to some failure or if the\n            // request has timed out, then we attempt to send the Fetch to another\n            // voter in order to discover if there has been a leader change.\n            ConnectionState connection = requestManager.getOrCreate(state.leaderId());\n            if (connection.hasRequestTimedOut(currentTimeMs)) {\n                backoffMs = maybeSendAnyVoterFetch(currentTimeMs);\n                connection.reset();\n            } else if (connection.isBackingOff(currentTimeMs)) {\n                backoffMs = maybeSendAnyVoterFetch(currentTimeMs);\n            } else {\n                backoffMs = maybeSendFetchOrFetchSnapshot(state, currentTimeMs);\n            }\n\n            return Math.min(backoffMs, state.remainingFetchTimeMs(currentTimeMs));\n        }\n    }\n\n    private long maybeSendFetchOrFetchSnapshot(FollowerState state, long currentTimeMs) {\n        final Supplier<ApiMessage> requestSupplier;\n\n        if (state.fetchingSnapshot().isPresent()) {\n            RawSnapshotWriter snapshot = state.fetchingSnapshot().get();\n            long snapshotSize = snapshot.sizeInBytes();\n\n            requestSupplier = () -> buildFetchSnapshotRequest(snapshot.snapshotId(), snapshotSize);\n        } else {\n            requestSupplier = this::buildFetchRequest;\n        }\n\n        return maybeSendRequest(currentTimeMs, state.leaderId(), requestSupplier);\n    }\n\n    private long pollVoted(long currentTimeMs) {\n        VotedState state = quorum.votedStateOrThrow();\n        GracefulShutdown shutdown = this.shutdown.get();\n\n        if (shutdown != null) {\n            // If shutting down, then remain in this state until either the\n            // shutdown completes or an epoch bump forces another state transition\n            return shutdown.remainingTimeMs();\n        } else if (state.hasElectionTimeoutExpired(currentTimeMs)) {\n            transitionToCandidate(currentTimeMs);\n            return 0L;\n        } else {\n            return state.remainingElectionTimeMs(currentTimeMs);\n        }\n    }\n\n    private long pollUnattached(long currentTimeMs) {\n        UnattachedState state = quorum.unattachedStateOrThrow();\n        if (quorum.isVoter()) {\n            return pollUnattachedAsVoter(state, currentTimeMs);\n        } else {\n            return pollUnattachedAsObserver(state, currentTimeMs);\n        }\n    }\n\n    private long pollUnattachedAsVoter(UnattachedState state, long currentTimeMs) {\n        GracefulShutdown shutdown = this.shutdown.get();\n        if (shutdown != null) {\n            // If shutting down, then remain in this state until either the\n            // shutdown completes or an epoch bump forces another state transition\n            return shutdown.remainingTimeMs();\n        } else if (state.hasElectionTimeoutExpired(currentTimeMs)) {\n            transitionToCandidate(currentTimeMs);\n            return 0L;\n        } else {\n            return state.remainingElectionTimeMs(currentTimeMs);\n        }\n    }\n\n    private long pollUnattachedAsObserver(UnattachedState state, long currentTimeMs) {\n        long fetchBackoffMs = maybeSendAnyVoterFetch(currentTimeMs);\n        return Math.min(fetchBackoffMs, state.remainingElectionTimeMs(currentTimeMs));\n    }\n\n    private long pollCurrentState(long currentTimeMs) {\n        if (quorum.isLeader()) {\n            return pollLeader(currentTimeMs);\n        } else if (quorum.isCandidate()) {\n            return pollCandidate(currentTimeMs);\n        } else if (quorum.isFollower()) {\n            return pollFollower(currentTimeMs);\n        } else if (quorum.isVoted()) {\n            return pollVoted(currentTimeMs);\n        } else if (quorum.isUnattached()) {\n            return pollUnattached(currentTimeMs);\n        } else if (quorum.isResigned()) {\n            return pollResigned(currentTimeMs);\n        } else {\n            throw new IllegalStateException(\"Unexpected quorum state \" + quorum);\n        }\n    }\n\n    private void pollListeners() {\n        // Apply all of the pending registration\n        while (true) {\n            Registration<T> registration = pendingRegistrations.poll();\n            if (registration == null) {\n                break;\n            }\n\n            processRegistration(registration);\n        }\n\n        // Check listener progress to see if reads are expected\n        quorum.highWatermark().ifPresent(highWatermarkMetadata -> {\n            updateListenersProgress(highWatermarkMetadata.offset);\n        });\n\n        // Notify the new listeners of the latest leader and epoch\n        Optional<LeaderState<T>> leaderState = quorum.maybeLeaderState();\n        if (leaderState.isPresent()) {\n            maybeFireLeaderChange(leaderState.get());\n        } else {\n            maybeFireLeaderChange();\n        }\n    }\n\n    private void processRegistration(Registration<T> registration) {\n        Listener<T> listener = registration.listener();\n        Registration.Ops ops = registration.ops();\n\n        if (ops == Registration.Ops.REGISTER) {\n            if (listenerContexts.putIfAbsent(listener, new ListenerContext(listener)) != null) {\n                logger.error(\"Attempting to add a listener that already exists: {}\", listenerName(listener));\n            } else {\n                logger.info(\"Registered the listener {}\", listenerName(listener));\n            }\n        } else {\n            if (listenerContexts.remove(listener) == null) {\n                logger.error(\"Attempting to remove a listener that doesn't exists: {}\", listenerName(listener));\n            } else {\n                logger.info(\"Unregistered the listener {}\", listenerName(listener));\n            }\n        }\n    }\n\n    private boolean maybeCompleteShutdown(long currentTimeMs) {\n        GracefulShutdown shutdown = this.shutdown.get();\n        if (shutdown == null) {\n            return false;\n        }\n\n        shutdown.update(currentTimeMs);\n        if (shutdown.hasTimedOut()) {\n            shutdown.failWithTimeout();\n            return true;\n        }\n\n        if (quorum.isObserver()\n            || quorum.isOnlyVoter()\n            || quorum.hasRemoteLeader()\n        ) {\n            shutdown.complete();\n            return true;\n        }\n\n        return false;\n    }\n\n    /**\n     * A simple timer based log cleaner\n     */\n    private static class RaftMetadataLogCleanerManager {\n        private final Logger logger;\n        private final Timer timer;\n        private final long delayMs;\n        private final Runnable cleaner;\n\n        RaftMetadataLogCleanerManager(Logger logger, Time time, long delayMs, Runnable cleaner) {\n            this.logger = logger;\n            this.timer = time.timer(delayMs);\n            this.delayMs = delayMs;\n            this.cleaner = cleaner;\n        }\n\n        public long maybeClean(long currentTimeMs) {\n            timer.update(currentTimeMs);\n            if (timer.isExpired()) {\n                try {\n                    cleaner.run();\n                } catch (Throwable t) {\n                    logger.error(\"Had an error during log cleaning\", t);\n                }\n                timer.reset(delayMs);\n            }\n            return timer.remainingMs();\n        }\n    }\n\n    private void wakeup() {\n        messageQueue.wakeup();\n    }\n\n    /**\n     * Handle an inbound request. The response will be returned through\n     * {@link RaftRequest.Inbound#completion}.\n     *\n     * @param request The inbound request\n     */\n    public void handle(RaftRequest.Inbound request) {\n        messageQueue.add(Objects.requireNonNull(request));\n    }\n\n    /**\n     * Poll for new events. This allows the client to handle inbound\n     * requests and send any needed outbound requests.\n     */\n    public void poll() {\n        if (!isInitialized()) {\n            throw new IllegalStateException(\"Replica needs to be initialized before polling\");\n        }\n\n        long startPollTimeMs = time.milliseconds();\n        if (maybeCompleteShutdown(startPollTimeMs)) {\n            return;\n        }\n\n        long pollStateTimeoutMs = pollCurrentState(startPollTimeMs);\n        long cleaningTimeoutMs = snapshotCleaner.maybeClean(startPollTimeMs);\n        long pollTimeoutMs = Math.min(pollStateTimeoutMs, cleaningTimeoutMs);\n\n        long startWaitTimeMs = time.milliseconds();\n        kafkaRaftMetrics.updatePollStart(startWaitTimeMs);\n\n        RaftMessage message = messageQueue.poll(pollTimeoutMs);\n\n        long endWaitTimeMs = time.milliseconds();\n        kafkaRaftMetrics.updatePollEnd(endWaitTimeMs);\n\n        if (message != null) {\n            handleInboundMessage(message, endWaitTimeMs);\n        }\n\n        pollListeners();\n    }\n\n    @Override\n    public long scheduleAppend(int epoch, List<T> records) {\n        return append(epoch, records, OptionalLong.empty(), false);\n    }\n\n    @Override\n    public long scheduleAtomicAppend(int epoch, OptionalLong requiredBaseOffset, List<T> records) {\n        return append(epoch, records, requiredBaseOffset, true);\n    }\n\n    private long append(int epoch, List<T> records, OptionalLong requiredBaseOffset, boolean isAtomic) {\n        if (!isInitialized()) {\n            throw new NotLeaderException(\"Append failed because the replica is not the current leader\");\n        }\n\n        LeaderState<T> leaderState = quorum.<T>maybeLeaderState().orElseThrow(\n            () -> new NotLeaderException(\"Append failed because the replica is not the current leader\")\n        );\n\n        BatchAccumulator<T> accumulator = leaderState.accumulator();\n        boolean isFirstAppend = accumulator.isEmpty();\n        final long offset = accumulator.append(epoch, records, requiredBaseOffset, isAtomic);\n\n        // Wakeup the network channel if either this is the first append\n        // or the accumulator is ready to drain now. Checking for the first\n        // append ensures that we give the IO thread a chance to observe\n        // the linger timeout so that it can schedule its own wakeup in case\n        // there are no additional appends.\n        if (isFirstAppend || accumulator.needsDrain(time.milliseconds())) {\n            wakeup();\n        }\n        return offset;\n    }\n\n    @Override\n    public CompletableFuture<Void> shutdown(int timeoutMs) {\n        logger.info(\"Beginning graceful shutdown\");\n        CompletableFuture<Void> shutdownComplete = new CompletableFuture<>();\n        shutdown.set(new GracefulShutdown(timeoutMs, shutdownComplete));\n        wakeup();\n        return shutdownComplete;\n    }\n\n    @Override\n    public void resign(int epoch) {\n        if (epoch < 0) {\n            throw new IllegalArgumentException(\"Attempt to resign from an invalid negative epoch \" + epoch);\n        } else if (!isInitialized()) {\n            throw new IllegalStateException(\"Replica needs to be initialized before resigning\");\n        } else if (!quorum.isVoter()) {\n            throw new IllegalStateException(\"Attempt to resign by a non-voter\");\n        }\n\n        LeaderAndEpoch leaderAndEpoch = leaderAndEpoch();\n        int currentEpoch = leaderAndEpoch.epoch();\n\n        if (epoch > currentEpoch) {\n            throw new IllegalArgumentException(\"Attempt to resign from epoch \" + epoch +\n                \" which is larger than the current epoch \" + currentEpoch);\n        } else if (epoch < currentEpoch) {\n            // If the passed epoch is smaller than the current epoch, then it might mean\n            // that the listener has not been notified about a leader change that already\n            // took place. In this case, we consider the call as already fulfilled and\n            // take no further action.\n            logger.debug(\"Ignoring call to resign from epoch {} since it is smaller than the \" +\n                \"current epoch {}\", epoch, currentEpoch);\n        } else if (!leaderAndEpoch.isLeader(quorum.localIdOrThrow())) {\n            throw new IllegalArgumentException(\"Cannot resign from epoch \" + epoch +\n                \" since we are not the leader\");\n        } else {\n            // Note that if we transition to another state before we have a chance to\n            // request resignation, then we consider the call fulfilled.\n            Optional<LeaderState<Object>> leaderStateOpt = quorum.maybeLeaderState();\n            if (!leaderStateOpt.isPresent()) {\n                logger.debug(\"Ignoring call to resign from epoch {} since this node is \" +\n                    \"no longer the leader\", epoch);\n                return;\n            }\n\n            LeaderState<Object> leaderState = leaderStateOpt.get();\n            if (leaderState.epoch() != epoch) {\n                logger.debug(\"Ignoring call to resign from epoch {} since it is smaller than the \" +\n                    \"current epoch {}\", epoch, leaderState.epoch());\n            } else {\n                logger.info(\"Received user request to resign from the current epoch {}\", currentEpoch);\n                leaderState.requestResign();\n                wakeup();\n            }\n        }\n    }\n\n    @Override\n    public Optional<SnapshotWriter<T>> createSnapshot(\n        OffsetAndEpoch snapshotId,\n        long lastContainedLogTimestamp\n    ) {\n        if (!isInitialized()) {\n            throw new IllegalStateException(\"Cannot create snapshot before the replica has been initialized\");\n        }\n\n        return log.createNewSnapshot(snapshotId).map(writer -> {\n            long lastContainedLogOffset = snapshotId.offset() - 1;\n\n            RawSnapshotWriter wrappedWriter = new NotifyingRawSnapshotWriter(writer, offsetAndEpoch -> {\n                // Trim the state in the internal listener up to the new snapshot\n                partitionState.truncateOldEntries(offsetAndEpoch.offset());\n            });\n\n            return new RecordsSnapshotWriter.Builder()\n                .setLastContainedLogTimestamp(lastContainedLogTimestamp)\n                .setTime(time)\n                .setMaxBatchSize(MAX_BATCH_SIZE_BYTES)\n                .setMemoryPool(memoryPool)\n                .setRawSnapshotWriter(wrappedWriter)\n                .setKraftVersion(partitionState.kraftVersionAtOffset(lastContainedLogOffset))\n                .setVoterSet(partitionState.voterSetAtOffset(lastContainedLogOffset))\n                .build(serde);\n        });\n    }\n\n    @Override\n    public Optional<OffsetAndEpoch> latestSnapshotId() {\n        return log.latestSnapshotId();\n    }\n\n    @Override\n    public long logEndOffset() {\n        return log.endOffset().offset;\n    }\n\n    @Override\n    public void close() {\n        log.flush(true);\n        if (kafkaRaftMetrics != null) {\n            kafkaRaftMetrics.close();\n        }\n        if (memoryPool instanceof BatchMemoryPool) {\n            BatchMemoryPool batchMemoryPool = (BatchMemoryPool) memoryPool;\n            batchMemoryPool.releaseRetained();\n        }\n    }\n\n    @Override\n    public OptionalLong highWatermark() {\n        if (isInitialized() && quorum.highWatermark().isPresent()) {\n            return OptionalLong.of(quorum.highWatermark().get().offset);\n        } else {\n            return OptionalLong.empty();\n        }\n    }\n\n    public Optional<Node> voterNode(int id, String listener) {\n        return partitionState.lastVoterSet().voterNode(id, listener);\n    }\n\n    // Visible only for test\n    QuorumState quorum() {\n        // It's okay to return null since this method is only called by tests\n        return quorum;\n    }\n\n    private boolean isInitialized() {\n        return partitionState != null && quorum != null && requestManager != null && kafkaRaftMetrics != null;\n    }\n\n    private class GracefulShutdown {\n        final Timer finishTimer;\n        final CompletableFuture<Void> completeFuture;\n\n        public GracefulShutdown(long shutdownTimeoutMs,\n                                CompletableFuture<Void> completeFuture) {\n            this.finishTimer = time.timer(shutdownTimeoutMs);\n            this.completeFuture = completeFuture;\n        }\n\n        public void update(long currentTimeMs) {\n            finishTimer.update(currentTimeMs);\n        }\n\n        public boolean hasTimedOut() {\n            return finishTimer.isExpired();\n        }\n\n        public boolean isFinished() {\n            return completeFuture.isDone();\n        }\n\n        public long remainingTimeMs() {\n            return finishTimer.remainingMs();\n        }\n\n        public void failWithTimeout() {\n            logger.warn(\"Graceful shutdown timed out after {}ms\", finishTimer.timeoutMs());\n            completeFuture.completeExceptionally(\n                new TimeoutException(\"Timeout expired before graceful shutdown completed\"));\n        }\n\n        public void complete() {\n            logger.info(\"Graceful shutdown completed\");\n            completeFuture.complete(null);\n        }\n    }\n\n    private static final class Registration<T> {\n        private final Ops ops;\n        private final Listener<T> listener;\n\n        private Registration(Ops ops, Listener<T> listener) {\n            this.ops = ops;\n            this.listener = listener;\n        }\n\n        private Ops ops() {\n            return ops;\n        }\n\n        private Listener<T> listener() {\n            return listener;\n        }\n\n        private enum Ops {\n            REGISTER, UNREGISTER\n        }\n\n        private static <T> Registration<T> register(Listener<T> listener) {\n            return new Registration<>(Ops.REGISTER, listener);\n        }\n\n        private static <T> Registration<T> unregister(Listener<T> listener) {\n            return new Registration<>(Ops.UNREGISTER, listener);\n        }\n    }\n\n    private final class ListenerContext implements CloseListener<BatchReader<T>> {\n        private final RaftClient.Listener<T> listener;\n        // This field is used only by the Raft IO thread\n        private LeaderAndEpoch lastFiredLeaderChange = LeaderAndEpoch.UNKNOWN;\n\n        // These fields are visible to both the Raft IO thread and the listener\n        // and are protected through synchronization on this ListenerContext instance\n        private BatchReader<T> lastSent = null;\n        private long nextOffset = 0;\n\n        private ListenerContext(Listener<T> listener) {\n            this.listener = listener;\n        }\n\n        /**\n         * Get the last acked offset, which is one greater than the offset of the\n         * last record which was acked by the state machine.\n         */\n        private synchronized long nextOffset() {\n            return nextOffset;\n        }\n\n        /**\n         * Get the next expected offset, which might be larger than the last acked\n         * offset if there are inflight batches which have not been acked yet.\n         * Note that when fetching from disk, we may not know the last offset of\n         * inflight data until it has been processed by the state machine. In this case,\n         * we delay sending additional data until the state machine has read to the\n         * end and the last offset is determined.\n         */\n        private synchronized OptionalLong nextExpectedOffset() {\n            if (lastSent != null) {\n                OptionalLong lastSentOffset = lastSent.lastOffset();\n                if (lastSentOffset.isPresent()) {\n                    return OptionalLong.of(lastSentOffset.getAsLong() + 1);\n                } else {\n                    return OptionalLong.empty();\n                }\n            } else {\n                return OptionalLong.of(nextOffset);\n            }\n        }\n\n        /**\n         * This API is used when the Listener needs to be notified of a new snapshot. This happens\n         * when the context's next offset is less than the log start offset.\n         */\n        private void fireHandleSnapshot(SnapshotReader<T> reader) {\n            synchronized (this) {\n                nextOffset = reader.snapshotId().offset();\n                lastSent = null;\n            }\n\n            logger.debug(\"Notifying listener {} of snapshot {}\", listenerName(), reader.snapshotId());\n            listener.handleLoadSnapshot(reader);\n        }\n\n        /**\n         * This API is used for committed records that have been received through\n         * replication. In general, followers will write new data to disk before they\n         * know whether it has been committed. Rather than retaining the uncommitted\n         * data in memory, we let the state machine read the records from disk.\n         */\n        private void fireHandleCommit(long baseOffset, Records records) {\n            fireHandleCommit(\n                RecordsBatchReader.of(\n                    baseOffset,\n                    records,\n                    serde,\n                    BufferSupplier.create(),\n                    MAX_BATCH_SIZE_BYTES,\n                    this,\n                    true /* Validate batch CRC*/\n                )\n            );\n        }\n\n        /**\n         * This API is used for committed records originating from {@link #scheduleAppend(int, List)}\n         * or {@link #scheduleAtomicAppend(int, OptionalLong, List)} on this instance. In this case,\n         * we are able to save the original record objects, which saves the need to read them back\n         * from disk. This is a nice optimization for the leader which is typically doing more work\n         * than all of the * followers.\n         */\n        private void fireHandleCommit(\n            long baseOffset,\n            int epoch,\n            long appendTimestamp,\n            int sizeInBytes,\n            List<T> records\n        ) {\n            Batch<T> batch = Batch.data(baseOffset, epoch, appendTimestamp, sizeInBytes, records);\n            MemoryBatchReader<T> reader = MemoryBatchReader.of(Collections.singletonList(batch), this);\n            fireHandleCommit(reader);\n        }\n\n        private String listenerName() {\n            return KafkaRaftClient.listenerName(listener);\n        }\n\n        private void fireHandleCommit(BatchReader<T> reader) {\n            synchronized (this) {\n                this.lastSent = reader;\n            }\n            logger.debug(\n                \"Notifying listener {} of batch for baseOffset {} and lastOffset {}\",\n                listenerName(),\n                reader.baseOffset(),\n                reader.lastOffset()\n            );\n            listener.handleCommit(reader);\n        }\n\n        private void maybeFireLeaderChange(LeaderAndEpoch leaderAndEpoch) {\n            if (shouldFireLeaderChange(leaderAndEpoch)) {\n                lastFiredLeaderChange = leaderAndEpoch;\n                logger.debug(\"Notifying listener {} of leader change {}\", listenerName(), leaderAndEpoch);\n                listener.handleLeaderChange(leaderAndEpoch);\n            }\n        }\n\n        private boolean shouldFireLeaderChange(LeaderAndEpoch leaderAndEpoch) {\n            if (leaderAndEpoch.equals(lastFiredLeaderChange)) {\n                return false;\n            } else if (leaderAndEpoch.epoch() > lastFiredLeaderChange.epoch()) {\n                return true;\n            } else {\n                return leaderAndEpoch.leaderId().isPresent() &&\n                    !lastFiredLeaderChange.leaderId().isPresent();\n            }\n        }\n\n        private void maybeFireLeaderChange(LeaderAndEpoch leaderAndEpoch, long epochStartOffset) {\n            // If this node is becoming the leader, then we can fire `handleLeaderChange` as soon\n            // as the listener has caught up to the start of the leader epoch. This guarantees\n            // that the state machine has seen the full committed state before it becomes\n            // leader and begins writing to the log.\n            //\n            // Note that the raft client doesn't need to compare nextOffset against the high-watermark\n            // to guarantee that the listener has caught up to the high-watermark. This is true because\n            // the only way nextOffset can be greater than epochStartOffset is for the leader to have\n            // established the new high-watermark (of at least epochStartOffset + 1) and for the listener\n            // to have consumed up to that new high-watermark.\n            if (shouldFireLeaderChange(leaderAndEpoch) && nextOffset() > epochStartOffset) {\n                lastFiredLeaderChange = leaderAndEpoch;\n                listener.handleLeaderChange(leaderAndEpoch);\n            }\n        }\n\n        public synchronized void onClose(BatchReader<T> reader) {\n            OptionalLong lastOffset = reader.lastOffset();\n\n            if (lastOffset.isPresent()) {\n                nextOffset = lastOffset.getAsLong() + 1;\n            }\n\n            if (lastSent == reader) {\n                lastSent = null;\n                wakeup();\n            }\n        }\n    }\n}",
                "methodCount": 136
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 12,
                "candidates": [
                    {
                        "lineStart": 569,
                        "lineEnd": 577,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method transitionToFollower to class RaftMetadataLogCleanerManager",
                        "description": "Move method transitionToFollower to org.apache.kafka.raft.KafkaRaftClient.RaftMetadataLogCleanerManager\nRationale: The method 'transitionToFollower' deals with transitions in state (specifically transitioning to a follower in a quorum process). The RaftMetadataLogCleanerManager is a class that deals directly with the Raft protocol, a consensus algorithm that likely involves transitions like these. Therefore, logically, the 'transitionToFollower' method fits better here as it is related to the quorum processes.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 540,
                        "lineEnd": 546,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method transitionToResigned to class NetworkChannel",
                        "description": "Move method transitionToResigned to org.apache.kafka.raft.NetworkChannel\nRationale: The method transitionToResigned involves handling network state transitions such as completing pending requests with an exception and reseating connections. These operations are related to network operations and state management, which are more closely aligned with the responsibilities of the NetworkChannel class. This class specifically deals with network communication and endpoint management, making it a logical place for handling resignations that require resetting network connections and notifying related components of state changes. Moving this method will help in maintaining cohesive functionality related to network transitions within the same class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 548,
                        "lineEnd": 552,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method transitionToVoted to class NetworkChannel",
                        "description": "Move method transitionToVoted to org.apache.kafka.raft.NetworkChannel\nRationale: The method transitionToVoted involves managing state transitions, possibly within a networked protocol context (e.g., quorum transition). This aligns well with the responsibilities of NetworkChannel, which deals with sending requests and managing connections. It suggests that state changes and leader elections (likely part of a Raft consensus protocol) are relevant to network interactions and connection management, making NetworkChannel a suitable target class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1270,
                        "lineEnd": 1282,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method appendAsLeader to class ReplicatedLog",
                        "description": "Move method appendAsLeader to org.apache.kafka.raft.ReplicatedLog\nRationale: The method appendAsLeader is specifically about appending records as a leader, which is a responsibility of the ReplicatedLog class. This method uses LogAppendInfo, Records, and OffsetAndEpoch, all of which are directly relevant to ReplicatedLog. Hence, the ReplicatedLog class is the most suitable place for this method as it aligns with its existing functionalities like appending as a leader or follower, and managing log offsets and epochs.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 926,
                        "lineEnd": 959,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method buildFetchResponse to class RaftMetadataLogCleanerManager",
                        "description": "Move method buildFetchResponse to org.apache.kafka.raft.KafkaRaftClient.RaftMetadataLogCleanerManager\nRationale: The method buildFetchResponse() is heavily reliant on Raft-specific constructs such as Errors, RaftUtil, and quorum. The RaftMetadataLogCleanerManager class, being a timer-based log cleaner manager specific to Raft metadata, implies a close relationship with the same systems and abstractions. Additionally, RaftMetadataLogCleanerManager already deals with Raft-specific logging and metadata, making it a more natural home for buildFetchResponse(). The other candidate classes, MemoryPool and Time, do not inherently deal with Raft-specific logic or metadata and therefore are less appropriate choices.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 579,
                        "lineEnd": 587,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method buildVoteResponse to class RaftMetadataLogCleanerManager",
                        "description": "Move method buildVoteResponse to org.apache.kafka.raft.KafkaRaftClient.RaftMetadataLogCleanerManager\nRationale: The buildVoteResponse() method refers to logical structures (e.g., vote responses, partition errors, and quorum details) that are common in the management of metadata in distributed systems. The RaftMetadataLogCleanerManager class, which manages log cleaning using timers and logs, seems more aligned with metadata management and would benefit from having methods that handle vote responses. The other classes, MemoryPool and Time, do not deal directly with such logic and are primarily concerned with memory management and time functionality respectively.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 2565,
                        "lineEnd": 2567,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method isInitialized to class RaftMetadataLogCleanerManager",
                        "description": "Move method isInitialized to org.apache.kafka.raft.KafkaRaftClient.RaftMetadataLogCleanerManager\nRationale: The isInitialized() method checks the state of various components like partitionState, quorum, requestManager, and kafkaRaftMetrics. The RaftMetadataLogCleanerManager class, with its management of log cleaning and dependency on timers, is more closely related to the components being checked. It is more likely that isInitialized() method is ensuring the readiness of components that RaftMetadataLogCleanerManager may be dependent on or interact with. Moving this method to RaftMetadataLogCleanerManager would place it in a context where such initialization checking is relevant and useful.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1965,
                        "lineEnd": 1973,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method addQuorumLeader to class QuorumState",
                        "description": "Move method addQuorumLeader to org.apache.kafka.raft.QuorumState\nRationale: The method addQuorumLeader should be moved to the QuorumState class. The rationale is that the method is performing operations directly related to the 'quorum' object, including setting the leader epoch and leader ID from the QuorumState. This aligns closely with QuorumState's responsibility of managing and maintaining the quorum's state, including leader and epoch information. Keeping these operations within QuorumState ensures cohesion and maintains organized, maintainable code.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 961,
                        "lineEnd": 971,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method buildEmptyFetchResponse to class RaftMetadataLogCleanerManager",
                        "description": "Move method buildEmptyFetchResponse to org.apache.kafka.raft.KafkaRaftClient.RaftMetadataLogCleanerManager\nRationale: The method buildEmptyFetchResponse() constructs a FetchResponseData object, which seems to be related to log management, the core responsibility of RaftMetadataLogCleanerManager. Moving the method here aligns with single responsibility principles, as RaftMetadataLogCleanerManager is tasked with handling the log cleaning, making it a logical place to handle responses related to log metadata.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1668,
                        "lineEnd": 1672,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method handleUnexpectedError to class NetworkChannel",
                        "description": "Move method handleUnexpectedError to org.apache.kafka.raft.NetworkChannel\nRationale: The method `handleUnexpectedError` is handling errors related to `RaftResponse.Inbound` which seems closely tied to network operations, thus aligning it more with the responsibilities of `NetworkChannel`. `NetworkChannel` also deals directly with communication and error handling, making it a more suitable place for this method compared to MemoryPool or Time.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1980,
                        "lineEnd": 1983,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method isShuttingDown to class Time",
                        "description": "Move method isShuttingDown to org.apache.kafka.common.utils.Time\nRationale: The `isShuttingDown()` method is concerning the state of a potential shutdown process, which typically involves timing mechanisms to monitor and ensure proper graceful shutdown. Additionally, the Time class conceptually fits a method that checks states related to processes and timing, making it a more natural fit for managing and integrating with system time and states.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1975,
                        "lineEnd": 1978,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method isRunning to class ReplicatedLog",
                        "description": "Move method isRunning to org.apache.kafka.raft.ReplicatedLog\nRationale: The method `isRunning()` appears to be checking the status of a shutdown operation involving `GracefulShutdown`. The `ReplicatedLog` class already deals with operations such as starting and stopping, truncating logs, and managing the state of the log (e.g., `truncateTo`, `flush`, `close`). As such, it would be a more appropriate place for methods related to determining the running state, given that the `ReplicatedLog` would likely need to know if it is in a shutdown state or not.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "updateFollowerHighWatermark",
                            "method_signature": "private updateFollowerHighWatermark(\n        FollowerState state,\n        OptionalLong highWatermarkOpt\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateLeaderEndOffsetAndTimestamp",
                            "method_signature": "private updateLeaderEndOffsetAndTimestamp(\n        LeaderState<T> state,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onUpdateLeaderHighWatermark",
                            "method_signature": "private onUpdateLeaderHighWatermark(\n        LeaderState<T> state,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateListenersProgress",
                            "method_signature": "private updateListenersProgress(long highWatermark)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "latestSnapshot",
                            "method_signature": "private latestSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireHandleCommit",
                            "method_signature": "private maybeFireHandleCommit(long baseOffset, int epoch, long appendTimestamp, int sizeInBytes, List<T> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireLeaderChange",
                            "method_signature": "private maybeFireLeaderChange(LeaderState<T> state)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireLeaderChange",
                            "method_signature": "private maybeFireLeaderChange()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "initialize",
                            "method_signature": "public initialize(\n        Map<Integer, InetSocketAddress> voterAddresses,\n        String listenerName,\n        QuorumStateStore quorumStateStore,\n        Metrics metrics\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endOffset",
                            "method_signature": "private endOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resetConnections",
                            "method_signature": "private resetConnections()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onBecomeLeader",
                            "method_signature": "private onBecomeLeader(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "flushLeaderLog",
                            "method_signature": "private flushLeaderLog(LeaderState<T> state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeTransitionToLeader",
                            "method_signature": "private maybeTransitionToLeader(CandidateState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onBecomeCandidate",
                            "method_signature": "private onBecomeCandidate(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToCandidate",
                            "method_signature": "private transitionToCandidate(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToUnattached",
                            "method_signature": "private transitionToUnattached(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToResigned",
                            "method_signature": "private transitionToResigned(List<Integer> preferredSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToVoted",
                            "method_signature": "private transitionToVoted(ReplicaKey candidateKey, int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onBecomeFollower",
                            "method_signature": "private onBecomeFollower(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToFollower",
                            "method_signature": "private transitionToFollower(\n        int epoch,\n        int leaderId,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildVoteResponse",
                            "method_signature": "private buildVoteResponse(Errors partitionLevelError, boolean voteGranted)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleVoteRequest",
                            "method_signature": "private handleVoteRequest(\n        RaftRequest.Inbound requestMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleVoteResponse",
                            "method_signature": "private handleVoteResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "binaryExponentialElectionBackoffMs",
                            "method_signature": "private binaryExponentialElectionBackoffMs(int retries)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "strictExponentialElectionBackoffMs",
                            "method_signature": "private strictExponentialElectionBackoffMs(int positionInSuccessors, int totalNumSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildBeginQuorumEpochResponse",
                            "method_signature": "private buildBeginQuorumEpochResponse(Errors partitionLevelError)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleBeginQuorumEpochRequest",
                            "method_signature": "private handleBeginQuorumEpochRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleBeginQuorumEpochResponse",
                            "method_signature": "private handleBeginQuorumEpochResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildEndQuorumEpochResponse",
                            "method_signature": "private buildEndQuorumEpochResponse(Errors partitionLevelError)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleEndQuorumEpochRequest",
                            "method_signature": "private handleEndQuorumEpochRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endEpochElectionBackoff",
                            "method_signature": "private endEpochElectionBackoff(List<Integer> preferredSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleEndQuorumEpochResponse",
                            "method_signature": "private handleEndQuorumEpochResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildFetchResponse",
                            "method_signature": "private buildFetchResponse(\n        Errors error,\n        Records records,\n        ValidOffsetAndEpoch validOffsetAndEpoch,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildEmptyFetchResponse",
                            "method_signature": "private buildEmptyFetchResponse(\n        Errors error,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasValidClusterId",
                            "method_signature": "private hasValidClusterId(String requestClusterId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleFetchRequest",
                            "method_signature": "private handleFetchRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "tryCompleteFetchRequest",
                            "method_signature": "private tryCompleteFetchRequest(\n        int replicaId,\n        FetchRequestData.FetchPartition request,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isPartitionDiverged",
                            "method_signature": "private static isPartitionDiverged(FetchResponseData.PartitionData partitionResponseData)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isPartitionSnapshotted",
                            "method_signature": "private static isPartitionSnapshotted(FetchResponseData.PartitionData partitionResponseData)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "optionalLeaderId",
                            "method_signature": "private static optionalLeaderId(int leaderIdOrNil)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "listenerName",
                            "method_signature": "private static listenerName(Listener<?> listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleFetchResponse",
                            "method_signature": "private handleFetchResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "appendAsFollower",
                            "method_signature": "private appendAsFollower(\n        Records records\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "appendAsLeader",
                            "method_signature": "private appendAsLeader(\n        Records records\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleDescribeQuorumRequest",
                            "method_signature": "private handleDescribeQuorumRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleFetchSnapshotRequest",
                            "method_signature": "private handleFetchSnapshotRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleFetchSnapshotResponse",
                            "method_signature": "private handleFetchSnapshotResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasConsistentLeader",
                            "method_signature": "private hasConsistentLeader(int epoch, OptionalInt leaderId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeHandleCommonResponse",
                            "method_signature": "private maybeHandleCommonResponse(\n        Errors error,\n        OptionalInt leaderId,\n        int epoch,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeTransition",
                            "method_signature": "private maybeTransition(\n        OptionalInt leaderId,\n        int epoch,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleTopLevelError",
                            "method_signature": "private handleTopLevelError(Errors error, RaftResponse.Inbound response)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleUnexpectedError",
                            "method_signature": "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleResponse",
                            "method_signature": "private handleResponse(RaftResponse.Inbound response, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onResponseResult",
                            "method_signature": "private onResponseResult(RaftResponse.Inbound response, long currentTimeMs, boolean handledSuccessfully, ConnectionState connection)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validateVoterOnlyRequest",
                            "method_signature": "private validateVoterOnlyRequest(int remoteNodeId, int requestEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validateLeaderOnlyRequest",
                            "method_signature": "private validateLeaderOnlyRequest(int requestEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleRequest",
                            "method_signature": "private handleRequest(RaftRequest.Inbound request, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleInboundMessage",
                            "method_signature": "private handleInboundMessage(RaftMessage message, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendRequest",
                            "method_signature": "private maybeSendRequest(\n        long currentTimeMs,\n        int destinationId,\n        Supplier<ApiMessage> requestSupplier\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildEndQuorumEpochRequest",
                            "method_signature": "private buildEndQuorumEpochRequest(\n        ResignedState state\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendRequests",
                            "method_signature": "private maybeSendRequests(\n        long currentTimeMs,\n        Set<Integer> destinationIds,\n        Supplier<ApiMessage> requestSupplier\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildBeginQuorumEpochRequest",
                            "method_signature": "private buildBeginQuorumEpochRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildVoteRequest",
                            "method_signature": "private buildVoteRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildFetchRequest",
                            "method_signature": "private buildFetchRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendAnyVoterFetch",
                            "method_signature": "private maybeSendAnyVoterFetch(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildFetchSnapshotRequest",
                            "method_signature": "private buildFetchSnapshotRequest(OffsetAndEpoch snapshotId, long snapshotSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addQuorumLeader",
                            "method_signature": "private addQuorumLeader(\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isRunning",
                            "method_signature": "public isRunning()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isShuttingDown",
                            "method_signature": "public isShuttingDown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "appendBatch",
                            "method_signature": "private appendBatch(\n        LeaderState<T> state,\n        BatchAccumulator.CompletedBatch<T> batch,\n        long appendTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAppendBatches",
                            "method_signature": "private maybeAppendBatches(\n        LeaderState<T> state,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollResigned",
                            "method_signature": "private pollResigned(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollLeader",
                            "method_signature": "private pollLeader(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendVoteRequests",
                            "method_signature": "private maybeSendVoteRequests(\n        CandidateState state,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollCandidate",
                            "method_signature": "private pollCandidate(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollFollower",
                            "method_signature": "private pollFollower(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollFollowerAsVoter",
                            "method_signature": "private pollFollowerAsVoter(FollowerState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollFollowerAsObserver",
                            "method_signature": "private pollFollowerAsObserver(FollowerState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendFetchOrFetchSnapshot",
                            "method_signature": "private maybeSendFetchOrFetchSnapshot(FollowerState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollVoted",
                            "method_signature": "private pollVoted(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUnattached",
                            "method_signature": "private pollUnattached(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUnattachedAsVoter",
                            "method_signature": "private pollUnattachedAsVoter(UnattachedState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUnattachedAsObserver",
                            "method_signature": "private pollUnattachedAsObserver(UnattachedState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollCurrentState",
                            "method_signature": "private pollCurrentState(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollListeners",
                            "method_signature": "private pollListeners()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "processRegistration",
                            "method_signature": "private processRegistration(Registration<T> registration)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeCompleteShutdown",
                            "method_signature": "private maybeCompleteShutdown(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeClean",
                            "method_signature": "public maybeClean(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "wakeup",
                            "method_signature": "private wakeup()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handle",
                            "method_signature": "public handle(RaftRequest.Inbound request)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "poll",
                            "method_signature": "public poll()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "append",
                            "method_signature": "private append(int epoch, List<T> records, OptionalLong requiredBaseOffset, boolean isAtomic)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voterNode",
                            "method_signature": "public voterNode(int id, String listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isInitialized",
                            "method_signature": "private isInitialized()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "update",
                            "method_signature": "public update(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasTimedOut",
                            "method_signature": "public hasTimedOut()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isFinished",
                            "method_signature": "public isFinished()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "remainingTimeMs",
                            "method_signature": "public remainingTimeMs()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "failWithTimeout",
                            "method_signature": "public failWithTimeout()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "complete",
                            "method_signature": "public complete()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "register",
                            "method_signature": "private static register(Listener<T> listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "unregister",
                            "method_signature": "private static unregister(Listener<T> listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "nextExpectedOffset",
                            "method_signature": "private synchronized nextExpectedOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fireHandleSnapshot",
                            "method_signature": "private fireHandleSnapshot(SnapshotReader<T> reader)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fireHandleCommit",
                            "method_signature": "private fireHandleCommit(long baseOffset, Records records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fireHandleCommit",
                            "method_signature": "private fireHandleCommit(\n            long baseOffset,\n            int epoch,\n            long appendTimestamp,\n            int sizeInBytes,\n            List<T> records\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "listenerName",
                            "method_signature": "private listenerName()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fireHandleCommit",
                            "method_signature": "private fireHandleCommit(BatchReader<T> reader)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireLeaderChange",
                            "method_signature": "private maybeFireLeaderChange(LeaderAndEpoch leaderAndEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "shouldFireLeaderChange",
                            "method_signature": "private shouldFireLeaderChange(LeaderAndEpoch leaderAndEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireLeaderChange",
                            "method_signature": "private maybeFireLeaderChange(LeaderAndEpoch leaderAndEpoch, long epochStartOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onClose",
                            "method_signature": "public synchronized onClose(BatchReader<T> reader)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "isInitialized",
                            "method_signature": "private isInitialized()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "failWithTimeout",
                            "method_signature": "public failWithTimeout()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildEmptyFetchResponse",
                            "method_signature": "private buildEmptyFetchResponse(\n        Errors error,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleUnexpectedError",
                            "method_signature": "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildVoteResponse",
                            "method_signature": "private buildVoteResponse(Errors partitionLevelError, boolean voteGranted)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "latestSnapshot",
                            "method_signature": "private latestSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToResigned",
                            "method_signature": "private transitionToResigned(List<Integer> preferredSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToFollower",
                            "method_signature": "private transitionToFollower(\n        int epoch,\n        int leaderId,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "complete",
                            "method_signature": "public complete()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToVoted",
                            "method_signature": "private transitionToVoted(ReplicaKey candidateKey, int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addQuorumLeader",
                            "method_signature": "private addQuorumLeader(\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "appendAsLeader",
                            "method_signature": "private appendAsLeader(\n        Records records\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isShuttingDown",
                            "method_signature": "public isShuttingDown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isRunning",
                            "method_signature": "public isRunning()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildFetchResponse",
                            "method_signature": "private buildFetchResponse(\n        Errors error,\n        Records records,\n        ValidOffsetAndEpoch validOffsetAndEpoch,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "isInitialized",
                            "method_signature": "private isInitialized()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "failWithTimeout",
                            "method_signature": "public failWithTimeout()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildEmptyFetchResponse",
                            "method_signature": "private buildEmptyFetchResponse(\n        Errors error,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleUnexpectedError",
                            "method_signature": "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildVoteResponse",
                            "method_signature": "private buildVoteResponse(Errors partitionLevelError, boolean voteGranted)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "latestSnapshot",
                            "method_signature": "private latestSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToResigned",
                            "method_signature": "private transitionToResigned(List<Integer> preferredSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToFollower",
                            "method_signature": "private transitionToFollower(\n        int epoch,\n        int leaderId,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "complete",
                            "method_signature": "public complete()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToVoted",
                            "method_signature": "private transitionToVoted(ReplicaKey candidateKey, int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addQuorumLeader",
                            "method_signature": "private addQuorumLeader(\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "appendAsLeader",
                            "method_signature": "private appendAsLeader(\n        Records records\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isShuttingDown",
                            "method_signature": "public isShuttingDown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isRunning",
                            "method_signature": "public isRunning()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildFetchResponse",
                            "method_signature": "private buildFetchResponse(\n        Errors error,\n        Records records,\n        ValidOffsetAndEpoch validOffsetAndEpoch,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private isInitialized()": {
                    "first": {
                        "method_name": "isInitialized",
                        "method_signature": "private isInitialized()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2308627394753452
                },
                "public failWithTimeout()": {
                    "first": {
                        "method_name": "failWithTimeout",
                        "method_signature": "public failWithTimeout()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.24602961244206256
                },
                "private buildEmptyFetchResponse(\n        Errors error,\n        Optional<LogOffsetMetadata> highWatermark\n    )": {
                    "first": {
                        "method_name": "buildEmptyFetchResponse",
                        "method_signature": "private buildEmptyFetchResponse(\n        Errors error,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3139579658127248
                },
                "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)": {
                    "first": {
                        "method_name": "handleUnexpectedError",
                        "method_signature": "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3149728190830739
                },
                "private buildVoteResponse(Errors partitionLevelError, boolean voteGranted)": {
                    "first": {
                        "method_name": "buildVoteResponse",
                        "method_signature": "private buildVoteResponse(Errors partitionLevelError, boolean voteGranted)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3179687607578866
                },
                "private latestSnapshot()": {
                    "first": {
                        "method_name": "latestSnapshot",
                        "method_signature": "private latestSnapshot()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3323838068538221
                },
                "private transitionToResigned(List<Integer> preferredSuccessors)": {
                    "first": {
                        "method_name": "transitionToResigned",
                        "method_signature": "private transitionToResigned(List<Integer> preferredSuccessors)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3338263348997506
                },
                "private transitionToFollower(\n        int epoch,\n        int leaderId,\n        long currentTimeMs\n    )": {
                    "first": {
                        "method_name": "transitionToFollower",
                        "method_signature": "private transitionToFollower(\n        int epoch,\n        int leaderId,\n        long currentTimeMs\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3354122236081435
                },
                "public complete()": {
                    "first": {
                        "method_name": "complete",
                        "method_signature": "public complete()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.33684754500954456
                },
                "private transitionToVoted(ReplicaKey candidateKey, int epoch)": {
                    "first": {
                        "method_name": "transitionToVoted",
                        "method_signature": "private transitionToVoted(ReplicaKey candidateKey, int epoch)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.34056486345018444
                },
                "private addQuorumLeader(\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot\n    )": {
                    "first": {
                        "method_name": "addQuorumLeader",
                        "method_signature": "private addQuorumLeader(\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.34657970773074437
                },
                "private appendAsLeader(\n        Records records\n    )": {
                    "first": {
                        "method_name": "appendAsLeader",
                        "method_signature": "private appendAsLeader(\n        Records records\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3468167180000903
                },
                "public isShuttingDown()": {
                    "first": {
                        "method_name": "isShuttingDown",
                        "method_signature": "public isShuttingDown()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3518967644617226
                },
                "public isRunning()": {
                    "first": {
                        "method_name": "isRunning",
                        "method_signature": "public isRunning()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3531208424411301
                },
                "private buildFetchResponse(\n        Errors error,\n        Records records,\n        ValidOffsetAndEpoch validOffsetAndEpoch,\n        Optional<LogOffsetMetadata> highWatermark\n    )": {
                    "first": {
                        "method_name": "buildFetchResponse",
                        "method_signature": "private buildFetchResponse(\n        Errors error,\n        Records records,\n        ValidOffsetAndEpoch validOffsetAndEpoch,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3543411053406282
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "private transitionToFollower(\n        int epoch,\n        int leaderId,\n        long currentTimeMs\n    )",
                    "private transitionToResigned(List<Integer> preferredSuccessors)",
                    "private transitionToVoted(ReplicaKey candidateKey, int epoch)",
                    "private appendAsLeader(\n        Records records\n    )",
                    "private buildFetchResponse(\n        Errors error,\n        Records records,\n        ValidOffsetAndEpoch validOffsetAndEpoch,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                    "private buildVoteResponse(Errors partitionLevelError, boolean voteGranted)",
                    "private isInitialized()",
                    "private addQuorumLeader(\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot\n    )",
                    "private buildEmptyFetchResponse(\n        Errors error,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                    "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)",
                    "private latestSnapshot()",
                    "public isShuttingDown()",
                    "public isRunning()",
                    "public complete()",
                    "public failWithTimeout()"
                ],
                "llm_response_time": 6070
            },
            "targetClassMap": {
                "transitionToFollower": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.11399079270740736
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.11305036939268549
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.045980338147135626
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14306584587982538
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.03286425722981093
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.3292431988831966
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "RaftMetadataLogCleanerManager",
                        "Time",
                        "MemoryPool"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "transitionToResigned": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.13841739114470894
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.1615005277038364
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.08233688458905682
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14306584587982538
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.057512450152169124
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.2798567190507171
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "NetworkChannel",
                        "RaftMetadataLogCleanerManager",
                        "MemoryPool"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "transitionToVoted": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.09972117053875637
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.11867816581938534
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.04583712596673114
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14184408272449836
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.04025033049430489
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.3427530813818426
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "NetworkChannel",
                        "RaftMetadataLogCleanerManager",
                        "MemoryPool"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "appendAsLeader": {
                    "target_classes": [
                        {
                            "class_name": "Records",
                            "similarity_score": 0.11837617651007314
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.1077851268021598
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.03800743117926434
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.1434409434297192
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.10020543130080872
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.03383735833851262
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "ReplicatedLog",
                        "Records",
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "buildFetchResponse": {
                    "target_classes": [
                        {
                            "class_name": "Records",
                            "similarity_score": 0.026094068112009733
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.10249173027591259
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.04065846966124125
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.04361097561378978
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14750006307045013
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.024821164819372063
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.2983983352366476
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "RaftMetadataLogCleanerManager",
                        "Time",
                        "MemoryPool"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "buildVoteResponse": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.08956419427010578
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.03230010554076729
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.039564477010326005
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14987850520743612
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.03286425722981093
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.2963188789948769
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "RaftMetadataLogCleanerManager",
                        "Time",
                        "MemoryPool"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "isInitialized": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.05922015969000667
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.020428380434529853
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.0371960129634052
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.10771760199020573
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.05196295322792911
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.18740851426632726
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "RaftMetadataLogCleanerManager",
                        "Time",
                        "MemoryPool"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "addQuorumLeader": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.10257232541543393
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.03538299282895168
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.04216934244541642
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14925788761321498
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.02700074253062953
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.3246010684802756
                        },
                        {
                            "class_name": "QuorumState",
                            "similarity_score": 0.275797464034139
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "QuorumState",
                        "RaftMetadataLogCleanerManager",
                        "MemoryPool"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                },
                "buildEmptyFetchResponse": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.08668943724225196
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.029904086934417106
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.03563959918684752
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.12614593876809943
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.022819792430590763
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.2743379741155275
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "RaftMetadataLogCleanerManager",
                        "Time",
                        "MemoryPool"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "handleUnexpectedError": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.10577027387372219
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.05594542388644595
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.04352434006443845
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14159846508095775
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.042691922456284
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "NetworkChannel",
                        "Time",
                        "MemoryPool"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "latestSnapshot": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "isShuttingDown": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.10553496707752412
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.04485613040162566
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.05048943218136732
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.1955262050905541
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.05324618233804511
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time",
                        "RaftMessageQueue",
                        "MemoryPool"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "isRunning": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.10930407304457855
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.04485613040162566
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.053459398780271276
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.1955262050905541
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.05324618233804511
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "ReplicatedLog",
                        "Time",
                        "MemoryPool"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "complete": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "failWithTimeout": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "638844f833b165d6f9ca52c173858d26b7254fac",
        "url": "https://github.com/apache/kafka/commit/638844f833b165d6f9ca52c173858d26b7254fac",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method private append(producerId long, producerEpoch short, verificationGuard VerificationGuard, records List<U>, replay boolean, event DeferredEvent) : void extracted from public run() : void in class org.apache.kafka.coordinator.group.runtime.CoordinatorRuntime.CoordinatorWriteEvent & moved to class org.apache.kafka.coordinator.group.runtime.CoordinatorRuntime.CoordinatorContext",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 773,
                    "endLine": 883,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public run() : void"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 788,
                    "endLine": 788,
                    "startColumn": 25,
                    "endColumn": 104,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 790,
                    "endLine": 790,
                    "startColumn": 29,
                    "endColumn": 93,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 792,
                    "endLine": 792,
                    "startColumn": 29,
                    "endColumn": 44,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 802,
                    "endLine": 802,
                    "startColumn": 25,
                    "endColumn": 66,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 849,
                    "endLine": 851,
                    "startColumn": 37,
                    "endColumn": 85,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 830,
                    "endLine": 835,
                    "startColumn": 37,
                    "endColumn": 39,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 789,
                    "endLine": 793,
                    "startColumn": 25,
                    "endColumn": 26,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 826,
                    "endLine": 836,
                    "startColumn": 33,
                    "endColumn": 34,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 823,
                    "endLine": 853,
                    "startColumn": 29,
                    "endColumn": 30,
                    "codeElementType": "FOR_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 805,
                    "endLine": 877,
                    "startColumn": 25,
                    "endColumn": 26,
                    "codeElementType": "TRY_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 872,
                    "endLine": 875,
                    "startColumn": 27,
                    "endColumn": 26,
                    "codeElementType": "CATCH_CLAUSE",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 784,
                    "endLine": 878,
                    "startColumn": 21,
                    "endColumn": 22,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 789,
                    "endLine": 791,
                    "startColumn": 56,
                    "endColumn": 26,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 791,
                    "endLine": 793,
                    "startColumn": 32,
                    "endColumn": 26,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 826,
                    "endLine": 836,
                    "startColumn": 61,
                    "endColumn": 34,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 823,
                    "endLine": 853,
                    "startColumn": 79,
                    "endColumn": 30,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 784,
                    "endLine": 794,
                    "startColumn": 53,
                    "endColumn": 22,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 794,
                    "endLine": 878,
                    "startColumn": 28,
                    "endColumn": 22,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 848,
                    "endLine": 852,
                    "startColumn": 40,
                    "endColumn": 34,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 869,
                    "endLine": 871,
                    "startColumn": 36,
                    "endColumn": 30,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 871,
                    "endLine": 1008,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "private append(producerId long, producerEpoch short, verificationGuard VerificationGuard, records List<U>, replay boolean, event DeferredEvent) : void"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 902,
                    "endLine": 902,
                    "startColumn": 21,
                    "endColumn": 92,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 904,
                    "endLine": 904,
                    "startColumn": 25,
                    "endColumn": 82,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 906,
                    "endLine": 906,
                    "startColumn": 25,
                    "endColumn": 46,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 912,
                    "endLine": 912,
                    "startColumn": 17,
                    "endColumn": 58,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 951,
                    "endLine": 953,
                    "startColumn": 21,
                    "endColumn": 82,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 980,
                    "endLine": 985,
                    "startColumn": 29,
                    "endColumn": 31,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 903,
                    "endLine": 907,
                    "startColumn": 21,
                    "endColumn": 22,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 975,
                    "endLine": 987,
                    "startColumn": 21,
                    "endColumn": 22,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 976,
                    "endLine": 986,
                    "startColumn": 25,
                    "endColumn": 26,
                    "codeElementType": "FOR_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 973,
                    "endLine": 1000,
                    "startColumn": 17,
                    "endColumn": 18,
                    "codeElementType": "TRY_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 994,
                    "endLine": 1000,
                    "startColumn": 19,
                    "endColumn": 18,
                    "codeElementType": "CATCH_CLAUSE",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 895,
                    "endLine": 1007,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 903,
                    "endLine": 905,
                    "startColumn": 52,
                    "endColumn": 22,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 905,
                    "endLine": 907,
                    "startColumn": 28,
                    "endColumn": 22,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 975,
                    "endLine": 987,
                    "startColumn": 33,
                    "endColumn": 22,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 976,
                    "endLine": 986,
                    "startColumn": 66,
                    "endColumn": 26,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 895,
                    "endLine": 909,
                    "startColumn": 36,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 909,
                    "endLine": 1007,
                    "startColumn": 20,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 950,
                    "endLine": 954,
                    "startColumn": 77,
                    "endColumn": 18,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 956,
                    "endLine": 968,
                    "startColumn": 70,
                    "endColumn": 18,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1246,
                    "endLine": 1276,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public run() : void"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1258,
                    "endLine": 1265,
                    "startColumn": 21,
                    "endColumn": 22,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "context.append(producerId,producerEpoch,verificationGuard,result.records(),result.replayRecords(),this)"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 892,
                    "endLine": 892,
                    "startColumn": 17,
                    "endColumn": 97,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 919,
                    "endLine": 919,
                    "startColumn": 21,
                    "endColumn": 41,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 923,
                    "endLine": 928,
                    "startColumn": 17,
                    "endColumn": 19,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 931,
                    "endLine": 931,
                    "startColumn": 17,
                    "endColumn": 86,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 933,
                    "endLine": 937,
                    "startColumn": 21,
                    "endColumn": 24,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 941,
                    "endLine": 945,
                    "startColumn": 17,
                    "endColumn": 19,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 961,
                    "endLine": 961,
                    "startColumn": 21,
                    "endColumn": 41,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 962,
                    "endLine": 967,
                    "startColumn": 21,
                    "endColumn": 23,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 971,
                    "endLine": 971,
                    "startColumn": 17,
                    "endColumn": 56,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 991,
                    "endLine": 991,
                    "startColumn": 25,
                    "endColumn": 61,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 992,
                    "endLine": 992,
                    "startColumn": 25,
                    "endColumn": 51,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 995,
                    "endLine": 995,
                    "startColumn": 21,
                    "endColumn": 97,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 999,
                    "endLine": 999,
                    "startColumn": 21,
                    "endColumn": 41,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1006,
                    "endLine": 1006,
                    "startColumn": 17,
                    "endColumn": 55,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 900,
                    "endLine": 900,
                    "startColumn": 21,
                    "endColumn": 60,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 891,
                    "endLine": 893,
                    "startColumn": 51,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 891,
                    "endLine": 893,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 899,
                    "endLine": 908,
                    "startColumn": 17,
                    "endColumn": 18,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 916,
                    "endLine": 920,
                    "startColumn": 63,
                    "endColumn": 18,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 916,
                    "endLine": 920,
                    "startColumn": 17,
                    "endColumn": 18,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 932,
                    "endLine": 938,
                    "startColumn": 42,
                    "endColumn": 18,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 932,
                    "endLine": 938,
                    "startColumn": 17,
                    "endColumn": 18,
                    "codeElementType": "ENHANCED_FOR_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 950,
                    "endLine": 954,
                    "startColumn": 17,
                    "endColumn": 18,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 990,
                    "endLine": 993,
                    "startColumn": 65,
                    "endColumn": 22,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 990,
                    "endLine": 993,
                    "startColumn": 21,
                    "endColumn": 22,
                    "codeElementType": "ENHANCED_FOR_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 956,
                    "endLine": 968,
                    "startColumn": 17,
                    "endColumn": 18,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 899,
                    "endLine": 901,
                    "startColumn": 43,
                    "endColumn": 18,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 901,
                    "endLine": 908,
                    "startColumn": 24,
                    "endColumn": 18,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": false
        },
        "ref_id": 583,
        "extraction_results": {
            "success": true,
            "newCommitHash": "009fda06cd29dc3f6fa3a6a31a7b34eccf3695a2",
            "newBranchName": "extract-append-run-39ffdea"
        },
        "telemetry": {
            "id": "ce71f3f5-fcc2-4808-9632-1932cd049d32",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1939,
                "lineStart": 71,
                "lineEnd": 2009,
                "bodyLineStart": 71,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                "sourceCode": "/**\n * The CoordinatorRuntime provides a framework to implement coordinators such as the group coordinator\n * or the transaction coordinator.\n *\n * The runtime framework maps each underlying partitions (e.g. __consumer_offsets) that that broker is a\n * leader of to a coordinator replicated state machine. A replicated state machine holds the hard and soft\n * state of all the objects (e.g. groups or offsets) assigned to the partition. The hard state is stored in\n * timeline datastructures backed by a SnapshotRegistry. The runtime supports two type of operations\n * on state machines: (1) Writes and (2) Reads.\n *\n * (1) A write operation, aka a request, can read the full and potentially **uncommitted** state from state\n * machine to handle the operation. A write operation typically generates a response and a list of\n * records. The records are applied to the state machine and persisted to the partition. The response\n * is parked until the records are committed and delivered when they are.\n *\n * (2) A read operation, aka a request, can only read the committed state from the state machine to handle\n * the operation. A read operation typically generates a response that is immediately completed.\n *\n * The runtime framework exposes an asynchronous, future based, API to the world. All the operations\n * are executed by an CoordinatorEventProcessor. The processor guarantees that operations for a\n * single partition or state machine are not processed concurrently.\n *\n * @param <S> The type of the state machine.\n * @param <U> The type of the record.\n */\npublic class CoordinatorRuntime<S extends CoordinatorShard<U>, U> implements AutoCloseable {\n\n    /**\n     * Builder to create a CoordinatorRuntime.\n     *\n     * @param <S> The type of the state machine.\n     * @param <U> The type of the record.\n     */\n    public static class Builder<S extends CoordinatorShard<U>, U> {\n        private String logPrefix;\n        private LogContext logContext;\n        private CoordinatorEventProcessor eventProcessor;\n        private PartitionWriter partitionWriter;\n        private CoordinatorLoader<U> loader;\n        private CoordinatorShardBuilderSupplier<S, U> coordinatorShardBuilderSupplier;\n        private Time time = Time.SYSTEM;\n        private Timer timer;\n        private Duration defaultWriteTimeout;\n        private CoordinatorRuntimeMetrics runtimeMetrics;\n        private CoordinatorMetrics coordinatorMetrics;\n        private Serializer<U> serializer;\n        private Compression compression;\n\n        public Builder<S, U> withLogPrefix(String logPrefix) {\n            this.logPrefix = logPrefix;\n            return this;\n        }\n\n        public Builder<S, U> withLogContext(LogContext logContext) {\n            this.logContext = logContext;\n            return this;\n        }\n\n        public Builder<S, U> withEventProcessor(CoordinatorEventProcessor eventProcessor) {\n            this.eventProcessor = eventProcessor;\n            return this;\n        }\n\n        public Builder<S, U> withPartitionWriter(PartitionWriter partitionWriter) {\n            this.partitionWriter = partitionWriter;\n            return this;\n        }\n\n        public Builder<S, U> withLoader(CoordinatorLoader<U> loader) {\n            this.loader = loader;\n            return this;\n        }\n\n        public Builder<S, U> withCoordinatorShardBuilderSupplier(CoordinatorShardBuilderSupplier<S, U> coordinatorShardBuilderSupplier) {\n            this.coordinatorShardBuilderSupplier = coordinatorShardBuilderSupplier;\n            return this;\n        }\n\n        public Builder<S, U> withTime(Time time) {\n            this.time = time;\n            return this;\n        }\n\n        public Builder<S, U> withTimer(Timer timer) {\n            this.timer = timer;\n            return this;\n        }\n\n        public Builder<S, U> withDefaultWriteTimeOut(Duration defaultWriteTimeout) {\n            this.defaultWriteTimeout = defaultWriteTimeout;\n            return this;\n        }\n\n        public Builder<S, U> withCoordinatorRuntimeMetrics(CoordinatorRuntimeMetrics runtimeMetrics) {\n            this.runtimeMetrics = runtimeMetrics;\n            return this;\n        }\n\n        public Builder<S, U> withCoordinatorMetrics(CoordinatorMetrics coordinatorMetrics) {\n            this.coordinatorMetrics = coordinatorMetrics;\n            return this;\n        }\n\n        public Builder<S, U> withSerializer(Serializer<U> serializer) {\n            this.serializer = serializer;\n            return this;\n        }\n\n        public Builder<S, U> withCompression(Compression compression) {\n            this.compression = compression;\n            return this;\n        }\n\n        public CoordinatorRuntime<S, U> build() {\n            if (logPrefix == null)\n                logPrefix = \"\";\n            if (logContext == null)\n                logContext = new LogContext(logPrefix);\n            if (eventProcessor == null)\n                throw new IllegalArgumentException(\"Event processor must be set.\");\n            if (partitionWriter == null)\n                throw new IllegalArgumentException(\"Partition write must be set.\");\n            if (loader == null)\n                throw new IllegalArgumentException(\"Loader must be set.\");\n            if (coordinatorShardBuilderSupplier == null)\n                throw new IllegalArgumentException(\"State machine supplier must be set.\");\n            if (time == null)\n                throw new IllegalArgumentException(\"Time must be set.\");\n            if (timer == null)\n                throw new IllegalArgumentException(\"Timer must be set.\");\n            if (runtimeMetrics == null)\n                throw new IllegalArgumentException(\"CoordinatorRuntimeMetrics must be set.\");\n            if (coordinatorMetrics == null)\n                throw new IllegalArgumentException(\"CoordinatorMetrics must be set.\");\n            if (serializer == null)\n                throw new IllegalArgumentException(\"Serializer must be set.\");\n            if (compression == null)\n                compression = Compression.NONE;\n\n            return new CoordinatorRuntime<>(\n                logPrefix,\n                logContext,\n                eventProcessor,\n                partitionWriter,\n                loader,\n                coordinatorShardBuilderSupplier,\n                time,\n                timer,\n                defaultWriteTimeout,\n                runtimeMetrics,\n                coordinatorMetrics,\n                serializer,\n                compression\n            );\n        }\n    }\n\n    /**\n     * The various state that a coordinator for a partition can be in.\n     */\n    public enum CoordinatorState {\n        /**\n         * Initial state when a coordinator is created.\n         */\n        INITIAL {\n            @Override\n            boolean canTransitionFrom(CoordinatorState state) {\n                return false;\n            }\n        },\n\n        /**\n         * The coordinator is being loaded.\n         */\n        LOADING {\n            @Override\n            boolean canTransitionFrom(CoordinatorState state) {\n                return state == INITIAL || state == FAILED;\n            }\n        },\n\n        /**\n         * The coordinator is active and can service requests.\n         */\n        ACTIVE {\n            @Override\n            boolean canTransitionFrom(CoordinatorState state) {\n                return state == ACTIVE || state == LOADING;\n            }\n        },\n\n        /**\n         * The coordinator is closed.\n         */\n        CLOSED {\n            @Override\n            boolean canTransitionFrom(CoordinatorState state) {\n                return true;\n            }\n        },\n\n        /**\n         * The coordinator loading has failed.\n         */\n        FAILED {\n            @Override\n            boolean canTransitionFrom(CoordinatorState state) {\n                return state == LOADING;\n            }\n        };\n\n        abstract boolean canTransitionFrom(CoordinatorState state);\n    }\n\n    /**\n     * The EventBasedCoordinatorTimer implements the CoordinatorTimer interface and provides an event based\n     * timer which turns timeouts of a regular {@link Timer} into {@link CoordinatorWriteEvent} events which\n     * are executed by the {@link CoordinatorEventProcessor} used by this coordinator runtime. This is done\n     * to ensure that the timer respects the threading model of the coordinator runtime.\n     *\n     * The {@link CoordinatorWriteEvent} events pushed by the coordinator timer wraps the\n     * {@link TimeoutOperation} operations scheduled by the coordinators.\n     *\n     * It also keeps track of all the scheduled {@link TimerTask}. This allows timeout operations to be\n     * cancelled or rescheduled. When a timer is cancelled or overridden, the previous timer is guaranteed to\n     * not be executed even if it already expired and got pushed to the event processor.\n     *\n     * When a timer fails with an unexpected exception, the timer is rescheduled with a backoff.\n     */\n    class EventBasedCoordinatorTimer implements CoordinatorTimer<Void, U> {\n        /**\n         * The logger.\n         */\n        final Logger log;\n\n        /**\n         * The topic partition.\n         */\n        final TopicPartition tp;\n\n        /**\n         * The scheduled timers keyed by their key.\n         */\n        final Map<String, TimerTask> tasks = new HashMap<>();\n\n        EventBasedCoordinatorTimer(TopicPartition tp, LogContext logContext) {\n            this.tp = tp;\n            this.log = logContext.logger(EventBasedCoordinatorTimer.class);\n        }\n\n        @Override\n        public void schedule(\n            String key,\n            long delay,\n            TimeUnit unit,\n            boolean retry,\n            TimeoutOperation<Void, U> operation\n        ) {\n            schedule(key, delay, unit, retry, 500, operation);\n        }\n\n        @Override\n        public void schedule(\n            String key,\n            long delay,\n            TimeUnit unit,\n            boolean retry,\n            long retryBackoff,\n            TimeoutOperation<Void, U> operation\n        ) {\n            // The TimerTask wraps the TimeoutOperation into a CoordinatorWriteEvent. When the TimerTask\n            // expires, the event is pushed to the queue of the coordinator runtime to be executed. This\n            // ensures that the threading model of the runtime is respected.\n            TimerTask task = new TimerTask(unit.toMillis(delay)) {\n                @Override\n                public void run() {\n                    String eventName = \"Timeout(tp=\" + tp + \", key=\" + key + \")\";\n                    CoordinatorWriteEvent<Void> event = new CoordinatorWriteEvent<>(eventName, tp, defaultWriteTimeout, coordinator -> {\n                        log.debug(\"Executing write event {} for timer {}.\", eventName, key);\n\n                        // If the task is different, it means that the timer has been\n                        // cancelled while the event was waiting to be processed.\n                        if (!tasks.remove(key, this)) {\n                            throw new RejectedExecutionException(\"Timer \" + key + \" was overridden or cancelled\");\n                        }\n\n                        // Execute the timeout operation.\n                        return operation.generateRecords();\n                    });\n\n                    // If the write event fails, it is rescheduled with a small backoff except if retry\n                    // is disabled or if the error is fatal.\n                    event.future.exceptionally(ex -> {\n                        if (ex instanceof RejectedExecutionException) {\n                            log.debug(\"The write event {} for the timer {} was not executed because it was \" +\n                                \"cancelled or overridden.\", event.name, key);\n                            return null;\n                        }\n\n                        if (ex instanceof NotCoordinatorException || ex instanceof CoordinatorLoadInProgressException) {\n                            log.debug(\"The write event {} for the timer {} failed due to {}. Ignoring it because \" +\n                                \"the coordinator is not active.\", event.name, key, ex.getMessage());\n                            return null;\n                        }\n\n                        if (retry) {\n                            log.info(\"The write event {} for the timer {} failed due to {}. Rescheduling it. \",\n                                event.name, key, ex.getMessage());\n                            schedule(key, retryBackoff, TimeUnit.MILLISECONDS, true, retryBackoff, operation);\n                        } else {\n                            log.error(\"The write event {} for the timer {} failed due to {}. Ignoring it. \",\n                                event.name, key, ex.getMessage());\n                        }\n\n                        return null;\n                    });\n\n                    log.debug(\"Scheduling write event {} for timer {}.\", event.name, key);\n                    try {\n                        enqueueLast(event);\n                    } catch (NotCoordinatorException ex) {\n                        log.info(\"Failed to enqueue write event {} for timer {} because the runtime is closed. Ignoring it.\",\n                            event.name, key);\n                    }\n                }\n            };\n\n            log.debug(\"Registering timer {} with delay of {}ms.\", key, unit.toMillis(delay));\n            TimerTask prevTask = tasks.put(key, task);\n            if (prevTask != null) prevTask.cancel();\n\n            timer.add(task);\n        }\n\n        @Override\n        public void scheduleIfAbsent(\n            String key,\n            long delay,\n            TimeUnit unit,\n            boolean retry,\n            TimeoutOperation<Void, U> operation\n        ) {\n            if (!tasks.containsKey(key)) {\n                schedule(key, delay, unit, retry, 500, operation);\n            }\n        }\n\n        @Override\n        public void cancel(String key) {\n            TimerTask prevTask = tasks.remove(key);\n            if (prevTask != null) prevTask.cancel();\n        }\n\n        public void cancelAll() {\n            Iterator<Map.Entry<String, TimerTask>> iterator = tasks.entrySet().iterator();\n            while (iterator.hasNext()) {\n                iterator.next().getValue().cancel();\n                iterator.remove();\n            }\n        }\n\n        public int size() {\n            return tasks.size();\n        }\n    }\n\n    /**\n     * CoordinatorContext holds all the metadata around a coordinator state machine.\n     */\n    class CoordinatorContext {\n        /**\n         * The lock which protects all data in the context. Note that the context\n         * is never accessed concurrently, but it is accessed by multiple threads.\n         */\n        final ReentrantLock lock;\n\n        /**\n         * The topic partition backing the coordinator.\n         */\n        final TopicPartition tp;\n\n        /**\n         * The log context.\n         */\n        final LogContext logContext;\n\n        /**\n         * The deferred event queue used to park events waiting\n         * on records to be committed.\n         */\n        final DeferredEventQueue deferredEventQueue;\n\n        /**\n         * The coordinator timer.\n         */\n        final EventBasedCoordinatorTimer timer;\n\n        /**\n         * The current state.\n         */\n        CoordinatorState state;\n\n        /**\n         * The current epoch of the coordinator. This represents\n         * the epoch of the partition leader.\n         */\n        int epoch;\n\n        /**\n         * The state machine and the metadata that can be accessed by\n         * other threads.\n         */\n        SnapshottableCoordinator<S, U> coordinator;\n\n        /**\n         * The high watermark listener registered to all the partitions\n         * backing the coordinators.\n         */\n        HighWatermarkListener highWatermarklistener;\n\n        /**\n         * The buffer supplier used to write records to the log.\n         */\n        BufferSupplier bufferSupplier;\n\n        /**\n         * Constructor.\n         *\n         * @param tp The topic partition of the coordinator.\n         */\n        private CoordinatorContext(\n            TopicPartition tp\n        ) {\n            this.lock = new ReentrantLock();\n            this.tp = tp;\n            this.logContext = new LogContext(String.format(\"[%s topic=%s partition=%d] \",\n                logPrefix,\n                tp.topic(),\n                tp.partition()\n            ));\n            this.state = CoordinatorState.INITIAL;\n            this.epoch = -1;\n            this.deferredEventQueue = new DeferredEventQueue(logContext);\n            this.timer = new EventBasedCoordinatorTimer(tp, logContext);\n            this.bufferSupplier = new BufferSupplier.GrowableBufferSupplier();\n        }\n\n        /**\n         * Transitions to the new state.\n         *\n         * @param newState The new state.\n         */\n        private void transitionTo(\n            CoordinatorState newState\n        ) {\n            if (!newState.canTransitionFrom(state)) {\n                throw new IllegalStateException(\"Cannot transition from \" + state + \" to \" + newState);\n            }\n            CoordinatorState oldState = state;\n\n            log.debug(\"Transition from {} to {}.\", state, newState);\n            switch (newState) {\n                case LOADING:\n                    state = CoordinatorState.LOADING;\n                    SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n                    coordinator = new SnapshottableCoordinator<>(\n                        logContext,\n                        snapshotRegistry,\n                        coordinatorShardBuilderSupplier\n                            .get()\n                            .withLogContext(logContext)\n                            .withSnapshotRegistry(snapshotRegistry)\n                            .withTime(time)\n                            .withTimer(timer)\n                            .withCoordinatorMetrics(coordinatorMetrics)\n                            .withTopicPartition(tp)\n                            .build(),\n                        tp\n                    );\n                    break;\n\n                case ACTIVE:\n                    state = CoordinatorState.ACTIVE;\n                    highWatermarklistener = new HighWatermarkListener();\n                    partitionWriter.registerListener(tp, highWatermarklistener);\n                    coordinator.onLoaded(metadataImage);\n                    break;\n\n                case FAILED:\n                    state = CoordinatorState.FAILED;\n                    unload();\n                    break;\n\n                case CLOSED:\n                    state = CoordinatorState.CLOSED;\n                    unload();\n                    break;\n\n                default:\n                    throw new IllegalArgumentException(\"Transitioning to \" + newState + \" is not supported.\");\n            }\n\n            runtimeMetrics.recordPartitionStateChange(oldState, state);\n        }\n\n        /**\n         * Unloads the coordinator.\n         */\n        private void unload() {\n            if (highWatermarklistener != null) {\n                partitionWriter.deregisterListener(tp, highWatermarklistener);\n                highWatermarklistener = null;\n            }\n            timer.cancelAll();\n            deferredEventQueue.failAll(Errors.NOT_COORDINATOR.exception());\n            if (coordinator != null) {\n                coordinator.onUnloaded();\n            }\n            coordinator = null;\n        }\n    }\n\n    class OperationTimeout extends TimerTask {\n        private final TopicPartition tp;\n        private final DeferredEvent event;\n\n        public OperationTimeout(\n            TopicPartition tp,\n            DeferredEvent event,\n            long delayMs\n        ) {\n            super(delayMs);\n            this.event = event;\n            this.tp = tp;\n        }\n\n        @Override\n        public void run() {\n            String name = event.toString();\n            scheduleInternalOperation(\"OperationTimeout(name=\" + name + \", tp=\" + tp + \")\", tp,\n                () -> event.complete(new TimeoutException(name + \" timed out after \" + delayMs + \"ms\")));\n        }\n    }\n\n    /**\n     * A coordinator write operation.\n     *\n     * @param <S> The type of the coordinator state machine.\n     * @param <T> The type of the response.\n     * @param <U> The type of the records.\n     */\n    public interface CoordinatorWriteOperation<S, T, U> {\n        /**\n         * Generates the records needed to implement this coordinator write operation. In general,\n         * this operation should not modify the hard state of the coordinator. That modifications\n         * will happen later on, when the records generated by this function are applied to the\n         * coordinator.\n         *\n         * @param coordinator The coordinator state machine.\n         * @return A result containing a list of records and the RPC result.\n         * @throws KafkaException\n         */\n        CoordinatorResult<T, U> generateRecordsAndResult(S coordinator) throws KafkaException;\n    }\n\n    /**\n     * A coordinator event that modifies the coordinator state.\n     *\n     * @param <T> The type of the response.\n     */\n    class CoordinatorWriteEvent<T> implements CoordinatorEvent, DeferredEvent {\n        /**\n         * The topic partition that this write event is applied to.\n         */\n        final TopicPartition tp;\n\n        /**\n         * The operation name.\n         */\n        final String name;\n\n        /**\n         * The transactional id.\n         */\n        final String transactionalId;\n\n        /**\n         * The producer id.\n         */\n        final long producerId;\n\n        /**\n         * The producer epoch.\n         */\n        final short producerEpoch;\n\n        /**\n         * The verification guard.\n         */\n        final VerificationGuard verificationGuard;\n\n        /**\n         * The write operation to execute.\n         */\n        final CoordinatorWriteOperation<S, T, U> op;\n\n        /**\n         * The future that will be completed with the response\n         * generated by the write operation or an error.\n         */\n        final CompletableFuture<T> future;\n\n        /**\n         * Timeout value for the write operation.\n         */\n        final Duration writeTimeout;\n\n        /**\n         * The operation timeout.\n         */\n        private OperationTimeout operationTimeout = null;\n\n        /**\n         * The result of the write operation. It could be null\n         * if an exception is thrown before it is assigned.\n         */\n        CoordinatorResult<T, U> result;\n\n        /**\n         * The time this event was created.\n         */\n        private final long createdTimeMs;\n\n        /**\n         * Constructor.\n         *\n         * @param name                  The operation name.\n         * @param tp                    The topic partition that the operation is applied to.\n         * @param writeTimeout          The write operation timeout\n         * @param op                    The write operation.\n         */\n        CoordinatorWriteEvent(\n            String name,\n            TopicPartition tp,\n            Duration writeTimeout,\n            CoordinatorWriteOperation<S, T, U> op\n        ) {\n            this(\n                name,\n                tp,\n                null,\n                RecordBatch.NO_PRODUCER_ID,\n                RecordBatch.NO_PRODUCER_EPOCH,\n                VerificationGuard.SENTINEL,\n                writeTimeout,\n                op\n            );\n        }\n\n        /**\n         * Constructor.\n         *\n         * @param name                      The operation name.\n         * @param tp                        The topic partition that the operation is applied to.\n         * @param transactionalId           The transactional id.\n         * @param producerId                The producer id.\n         * @param producerEpoch             The producer epoch.\n         * @param verificationGuard         The verification guard.\n         * @param writeTimeout              The write operation timeout\n         * @param op                        The write operation.\n         */\n        CoordinatorWriteEvent(\n            String name,\n            TopicPartition tp,\n            String transactionalId,\n            long producerId,\n            short producerEpoch,\n            VerificationGuard verificationGuard,\n            Duration writeTimeout,\n            CoordinatorWriteOperation<S, T, U> op\n        ) {\n            this.tp = tp;\n            this.name = name;\n            this.op = op;\n            this.transactionalId = transactionalId;\n            this.producerId = producerId;\n            this.producerEpoch = producerEpoch;\n            this.verificationGuard = verificationGuard;\n            this.future = new CompletableFuture<>();\n            this.createdTimeMs = time.milliseconds();\n            this.writeTimeout = writeTimeout;\n        }\n\n        /**\n         * @return The key used by the CoordinatorEventProcessor to ensure\n         * that events with the same key are not processed concurrently.\n         */\n        @Override\n        public TopicPartition key() {\n            return tp;\n        }\n\n        /**\n         * Called by the CoordinatorEventProcessor when the event is executed.\n         */\n        @Override\n        public void run() {\n            try {\n                // Get the context of the coordinator or fail if the coordinator is not in active state.\n                withActiveContextOrThrow(tp, context -> {\n                    // Execute the operation.\n                    result = op.generateRecordsAndResult(context.coordinator.coordinator());\n\n                    append(context);\n                });\n            } catch (Throwable t) {\n                complete(t);\n            }\n        }\n\n        private void append(CoordinatorContext context) {\n            if (result.records().isEmpty()) {\n                // If the records are empty, it was a read operation after all. In this case,\n                // the response can be returned directly iff there are no pending write operations;\n                // otherwise, the read needs to wait on the last write operation to be completed.\n                OptionalLong pendingOffset = context.deferredEventQueue.highestPendingOffset();\n                if (pendingOffset.isPresent()) {\n                    context.deferredEventQueue.add(pendingOffset.getAsLong(), this);\n                } else {\n                    complete(null);\n                }\n            } else {\n                // If the records are not empty, first, they are applied to the state machine,\n                // second, then are written to the partition/log, and finally, the response\n                // is put into the deferred event queue.\n                long prevLastWrittenOffset = context.coordinator.lastWrittenOffset();\n                LogConfig logConfig = partitionWriter.config(tp);\n                byte magic = logConfig.recordVersion().value;\n                int maxBatchSize = logConfig.maxMessageSize();\n                long currentTimeMs = time.milliseconds();\n                ByteBuffer buffer = context.bufferSupplier.get(Math.min(MIN_BUFFER_SIZE, maxBatchSize));\n\n                try {\n                    MemoryRecordsBuilder builder = new MemoryRecordsBuilder(\n                        buffer,\n                        magic,\n                        compression,\n                        TimestampType.CREATE_TIME,\n                        0L,\n                        currentTimeMs,\n                        producerId,\n                        producerEpoch,\n                        0,\n                        producerId != RecordBatch.NO_PRODUCER_ID,\n                        false,\n                        RecordBatch.NO_PARTITION_LEADER_EPOCH,\n                        maxBatchSize\n                    );\n\n                    // Apply the records to the state machine and add them to the batch.\n                    for (int i = 0; i < result.records().size(); i++) {\n                        U record = result.records().get(i);\n\n                        if (result.replayRecords()) {\n                            // We compute the offset of the record based on the last written offset. The\n                            // coordinator is the single writer to the underlying partition so we can\n                            // deduce it like this.\n                            context.coordinator.replay(\n                                prevLastWrittenOffset + i,\n                                producerId,\n                                producerEpoch,\n                                record\n                            );\n                        }\n\n                        byte[] keyBytes = serializer.serializeKey(record);\n                        byte[] valBytes = serializer.serializeValue(record);\n\n                        if (builder.hasRoomFor(currentTimeMs, keyBytes, valBytes, EMPTY_HEADERS)) {\n                            builder.append(\n                                currentTimeMs,\n                                keyBytes,\n                                valBytes,\n                                EMPTY_HEADERS\n                            );\n                        } else {\n                            throw new RecordTooLargeException(\"Message batch size is \" + builder.estimatedSizeInBytes() +\n                                \" bytes in append to partition \" + tp + \" which exceeds the maximum \" +\n                                \"configured size of \" + maxBatchSize + \".\");\n                        }\n                    }\n\n                    // Write the records to the log and update the last written\n                    // offset.\n                    long offset = partitionWriter.append(\n                        tp,\n                        verificationGuard,\n                        builder.build()\n                    );\n                    context.coordinator.updateLastWrittenOffset(offset);\n\n                    // Add the response to the deferred queue.\n                    if (!future.isDone()) {\n                        context.deferredEventQueue.add(offset, this);\n                        operationTimeout = new OperationTimeout(tp, this, writeTimeout.toMillis());\n                        timer.add(operationTimeout);\n                    } else {\n                        complete(null);\n                    }\n                } catch (Throwable t) {\n                    context.coordinator.revertLastWrittenOffset(prevLastWrittenOffset);\n                    complete(t);\n                } finally {\n                    context.bufferSupplier.release(buffer);\n                }\n            }\n        }\n\n        /**\n         * Completes the future with either the result of the write operation\n         * or the provided exception.\n         *\n         * @param exception The exception to complete the future with.\n         */\n        @Override\n        public void complete(Throwable exception) {\n            CompletableFuture<Void> appendFuture = result != null ? result.appendFuture() : null;\n\n            if (exception == null) {\n                if (appendFuture != null) result.appendFuture().complete(null);\n                future.complete(result.response());\n            } else {\n                if (appendFuture != null) result.appendFuture().completeExceptionally(exception);\n                future.completeExceptionally(exception);\n            }\n\n            if (operationTimeout != null) {\n                operationTimeout.cancel();\n                operationTimeout = null;\n            }\n        }\n\n        @Override\n        public long createdTimeMs() {\n            return this.createdTimeMs;\n        }\n\n        @Override\n        public String toString() {\n            return \"CoordinatorWriteEvent(name=\" + name + \")\";\n        }\n    }\n\n    /**\n     * A coordinator read operation.\n     *\n     * @param <S> The type of the coordinator state machine.\n     * @param <T> The type of the response.\n     */\n    public interface CoordinatorReadOperation<S, T> {\n        /**\n         * Generates the response to implement this coordinator read operation. A read\n         * operation received the last committed offset. It must use it to ensure that\n         * it does not read uncommitted data from the timeline data structures.\n         *\n         * @param state     The coordinator state machine.\n         * @param offset    The last committed offset.\n         * @return A response.\n         * @throws KafkaException\n         */\n        T generateResponse(S state, long offset) throws KafkaException;\n    }\n\n    /**\n     * A coordinator that reads the committed coordinator state.\n     *\n     * @param <T> The type of the response.\n     */\n    class CoordinatorReadEvent<T> implements CoordinatorEvent {\n        /**\n         * The topic partition that this read event is applied to.\n         */\n        final TopicPartition tp;\n\n        /**\n         * The operation name.\n         */\n        final String name;\n\n        /**\n         * The read operation to execute.\n         */\n        final CoordinatorReadOperation<S, T> op;\n\n        /**\n         * The future that will be completed with the response\n         * generated by the read operation or an error.\n         */\n        final CompletableFuture<T> future;\n\n        /**\n         * The result of the read operation. It could be null\n         * if an exception is thrown before it is assigned.\n         */\n        T response;\n\n        /**\n         * The time this event was created.\n         */\n        private final long createdTimeMs;\n\n        /**\n         * Constructor.\n         *\n         * @param name  The operation name.\n         * @param tp    The topic partition that the operation is applied to.\n         * @param op    The read operation.\n         */\n        CoordinatorReadEvent(\n            String name,\n            TopicPartition tp,\n            CoordinatorReadOperation<S, T> op\n        ) {\n            this.tp = tp;\n            this.name = name;\n            this.op = op;\n            this.future = new CompletableFuture<>();\n            this.createdTimeMs = time.milliseconds();\n        }\n\n        /**\n         * @return The key used by the CoordinatorEventProcessor to ensure\n         * that events with the same key are not processed concurrently.\n         */\n        @Override\n        public TopicPartition key() {\n            return tp;\n        }\n\n        /**\n         * Called by the CoordinatorEventProcessor when the event is executed.\n         */\n        @Override\n        public void run() {\n            try {\n                // Get the context of the coordinator or fail if the coordinator is not in active state.\n                withActiveContextOrThrow(tp, context -> {\n                    // Execute the read operation.\n                    response = op.generateResponse(\n                        context.coordinator.coordinator(),\n                        context.coordinator.lastCommittedOffset()\n                    );\n\n                    // The response can be completed immediately.\n                    complete(null);\n                });\n            } catch (Throwable t) {\n                complete(t);\n            }\n        }\n\n        /**\n         * Completes the future with either the result of the read operation\n         * or the provided exception.\n         *\n         * @param exception The exception to complete the future with.\n         */\n        @Override\n        public void complete(Throwable exception) {\n            if (exception == null) {\n                future.complete(response);\n            } else {\n                future.completeExceptionally(exception);\n            }\n        }\n\n        @Override\n        public long createdTimeMs() {\n            return this.createdTimeMs;\n        }\n\n        @Override\n        public String toString() {\n            return \"CoordinatorReadEvent(name=\" + name + \")\";\n        }\n    }\n\n    /**\n     * A coordinator event that applies and writes a transaction end marker.\n     */\n    class CoordinatorCompleteTransactionEvent implements CoordinatorEvent, DeferredEvent {\n        /**\n         * The topic partition that this write event is applied to.\n         */\n        final TopicPartition tp;\n\n        /**\n         * The operation name.\n         */\n        final String name;\n\n        /**\n         * The producer id.\n         */\n        final long producerId;\n\n        /**\n         * The producer epoch.\n         */\n        final short producerEpoch;\n\n        /**\n         * The coordinator epoch of the transaction coordinator.\n         */\n        final int coordinatorEpoch;\n\n        /**\n         * The transaction result.\n         */\n        final TransactionResult result;\n\n        /**\n         * Timeout value for the write operation.\n         */\n        final Duration writeTimeout;\n\n        /**\n         * The operation timeout.\n         */\n        private OperationTimeout operationTimeout = null;\n\n        /**\n         * The future that will be completed with the response\n         * generated by the write operation or an error.\n         */\n        final CompletableFuture<Void> future;\n\n        /**\n         * The time this event was created.\n         */\n        private final long createdTimeMs;\n\n        CoordinatorCompleteTransactionEvent(\n            String name,\n            TopicPartition tp,\n            long producerId,\n            short producerEpoch,\n            int coordinatorEpoch,\n            TransactionResult result,\n            Duration writeTimeout\n        ) {\n            this.name = name;\n            this.tp = tp;\n            this.producerId = producerId;\n            this.producerEpoch = producerEpoch;\n            this.coordinatorEpoch = coordinatorEpoch;\n            this.result = result;\n            this.writeTimeout = writeTimeout;\n            this.future = new CompletableFuture<>();\n            this.createdTimeMs = time.milliseconds();\n        }\n\n        /**\n         * @return The key used by the CoordinatorEventProcessor to ensure\n         * that events with the same key are not processed concurrently.\n         */\n        @Override\n        public TopicPartition key() {\n            return tp;\n        }\n\n        /**\n         * Called by the CoordinatorEventProcessor when the event is executed.\n         */\n        @Override\n        public void run() {\n            try {\n                withActiveContextOrThrow(tp, context -> {\n                    long prevLastWrittenOffset = context.coordinator.lastWrittenOffset();\n\n                    try {\n                        context.coordinator.replayEndTransactionMarker(\n                            producerId,\n                            producerEpoch,\n                            result\n                        );\n\n                        long offset = partitionWriter.append(\n                            tp,\n                            VerificationGuard.SENTINEL,\n                            MemoryRecords.withEndTransactionMarker(\n                                time.milliseconds(),\n                                producerId,\n                                producerEpoch,\n                                new EndTransactionMarker(\n                                    result == TransactionResult.COMMIT ? ControlRecordType.COMMIT : ControlRecordType.ABORT,\n                                    coordinatorEpoch\n                                )\n                            )\n                        );\n                        context.coordinator.updateLastWrittenOffset(offset);\n\n                        if (!future.isDone()) {\n                            context.deferredEventQueue.add(offset, this);\n                            operationTimeout = new OperationTimeout(tp, this, writeTimeout.toMillis());\n                            timer.add(operationTimeout);\n                        } else {\n                            complete(null);\n                        }\n                    } catch (Throwable t) {\n                        context.coordinator.revertLastWrittenOffset(prevLastWrittenOffset);\n                        complete(t);\n                    }\n                });\n            } catch (Throwable t) {\n                complete(t);\n            }\n        }\n\n        /**\n         * Completes the future with either the result of the write operation\n         * or the provided exception.\n         *\n         * @param exception The exception to complete the future with.\n         */\n        @Override\n        public void complete(Throwable exception) {\n            if (exception == null) {\n                future.complete(null);\n            } else {\n                future.completeExceptionally(exception);\n            }\n\n            if (operationTimeout != null) {\n                operationTimeout.cancel();\n                operationTimeout = null;\n            }\n        }\n\n        @Override\n        public long createdTimeMs() {\n            return createdTimeMs;\n        }\n\n        @Override\n        public String toString() {\n            return \"CoordinatorCompleteTransactionEvent(name=\" + name + \")\";\n        }\n    }\n\n    /**\n     * A coordinator internal event.\n     */\n    class CoordinatorInternalEvent implements CoordinatorEvent {\n        /**\n         * The topic partition that this internal event is applied to.\n         */\n        final TopicPartition tp;\n\n        /**\n         * The operation name.\n         */\n        final String name;\n\n        /**\n         * The internal operation to execute.\n         */\n        final Runnable op;\n\n        /**\n         * The time this event was created.\n         */\n        private final long createdTimeMs;\n\n        /**\n         * Constructor.\n         *\n         * @param name  The operation name.\n         * @param tp    The topic partition that the operation is applied to.\n         * @param op    The operation.\n         */\n        CoordinatorInternalEvent(\n            String name,\n            TopicPartition tp,\n            Runnable op\n        ) {\n            this.tp = tp;\n            this.name = name;\n            this.op = op;\n            this.createdTimeMs = time.milliseconds();\n        }\n\n        /**\n         * @return The key used by the CoordinatorEventProcessor to ensure\n         * that events with the same key are not processed concurrently.\n         */\n        @Override\n        public TopicPartition key() {\n            return tp;\n        }\n\n        /**\n         * Called by the CoordinatorEventProcessor when the event is executed.\n         */\n        @Override\n        public void run() {\n            try {\n                op.run();\n            } catch (Throwable t) {\n                complete(t);\n            }\n        }\n\n        /**\n         * Logs any exceptions thrown while the event is executed.\n         *\n         * @param exception The exception.\n         */\n        @Override\n        public void complete(Throwable exception) {\n            if (exception != null) {\n                log.error(\"Execution of {} failed due to {}.\", name, exception.getMessage(), exception);\n            }\n        }\n\n        @Override\n        public long createdTimeMs() {\n            return this.createdTimeMs;\n        }\n\n        @Override\n        public String toString() {\n            return \"InternalEvent(name=\" + name + \")\";\n        }\n    }\n\n    /**\n     * Partition listener to be notified when the high watermark of the partitions\n     * backing the coordinator are updated.\n     */\n    class HighWatermarkListener implements PartitionWriter.Listener {\n\n        private static final long NO_OFFSET = -1L;\n\n        /**\n         * The atomic long is used to store the last and unprocessed high watermark\n         * received from the partition. The atomic value is replaced by -1L when\n         * the high watermark is taken to update the context.\n         */\n        private final AtomicLong lastHighWatermark = new AtomicLong(NO_OFFSET);\n\n        /**\n         * @return The last high watermark received or NO_OFFSET if none is pending.\n         */\n        public long lastHighWatermark() {\n            return lastHighWatermark.get();\n        }\n\n        /**\n         * Updates the high watermark of the corresponding coordinator.\n         *\n         * @param tp        The topic partition.\n         * @param offset    The new high watermark.\n         */\n        @Override\n        public void onHighWatermarkUpdated(\n            TopicPartition tp,\n            long offset\n        ) {\n            log.debug(\"High watermark of {} incremented to {}.\", tp, offset);\n            if (lastHighWatermark.getAndSet(offset) == NO_OFFSET) {\n                // An event to apply the new high watermark is pushed to the front of the\n                // queue only if the previous value was -1L. If it was not, it means that\n                // there is already an event waiting to process the last value.\n                enqueueFirst(new CoordinatorInternalEvent(\"HighWatermarkUpdate\", tp, () -> {\n                    long newHighWatermark = lastHighWatermark.getAndSet(NO_OFFSET);\n\n                    CoordinatorContext context = coordinators.get(tp);\n                    if (context != null) {\n                        context.lock.lock();\n                        try {\n                            if (context.state == CoordinatorState.ACTIVE) {\n                                // The updated high watermark can be applied to the coordinator only if the coordinator\n                                // exists and is in the active state.\n                                log.debug(\"Updating high watermark of {} to {}.\", tp, newHighWatermark);\n                                context.coordinator.updateLastCommittedOffset(newHighWatermark);\n                                context.deferredEventQueue.completeUpTo(newHighWatermark);\n                                coordinatorMetrics.onUpdateLastCommittedOffset(tp, newHighWatermark);\n                            } else {\n                                log.debug(\"Ignored high watermark updated for {} to {} because the coordinator is not active.\",\n                                    tp, newHighWatermark);\n                            }\n                        } finally {\n                            context.lock.unlock();\n                        }\n                    } else {\n                        log.debug(\"Ignored high watermark updated for {} to {} because the coordinator does not exist.\",\n                            tp, newHighWatermark);\n                    }\n                }));\n            }\n        }\n    }\n\n    /**\n     * 16KB. Used for initial buffer size for write operations.\n     */\n    static final int MIN_BUFFER_SIZE = 16384;\n\n    /**\n     * The log prefix.\n     */\n    private final String logPrefix;\n\n    /**\n     * The log context.\n     */\n    private final LogContext logContext;\n\n    /**\n     * The logger.\n     */\n    private final Logger log;\n\n    /**\n     * The system time.\n     */\n    private final Time time;\n\n    /**\n     * The system timer.\n     */\n    private final Timer timer;\n\n    /**\n     * The write operation timeout\n     */\n    private final Duration defaultWriteTimeout;\n\n    /**\n     * The coordinators keyed by topic partition.\n     */\n    private final ConcurrentHashMap<TopicPartition, CoordinatorContext> coordinators;\n\n    /**\n     * The event processor used by the runtime.\n     */\n    private final CoordinatorEventProcessor processor;\n\n    /**\n     * The partition writer used by the runtime to persist records.\n     */\n    private final PartitionWriter partitionWriter;\n\n    /**\n     * The coordinator loaded used by the runtime.\n     */\n    private final CoordinatorLoader<U> loader;\n\n    /**\n     * The coordinator state machine builder used by the runtime\n     * to instantiate a coordinator.\n     */\n    private final CoordinatorShardBuilderSupplier<S, U> coordinatorShardBuilderSupplier;\n\n    /**\n     * The coordinator runtime metrics.\n     */\n    private final CoordinatorRuntimeMetrics runtimeMetrics;\n\n    /**\n     * The coordinator metrics.\n     */\n    private final CoordinatorMetrics coordinatorMetrics;\n\n    /**\n     * The serializer used to serialize records.\n     */\n    private final Serializer<U> serializer;\n\n    /**\n     * The compression codec used when writing records.\n     */\n    private final Compression compression;\n\n    /**\n     * Atomic boolean indicating whether the runtime is running.\n     */\n    private final AtomicBoolean isRunning = new AtomicBoolean(true);\n\n    /**\n     * The latest known metadata image.\n     */\n    private volatile MetadataImage metadataImage = MetadataImage.EMPTY;\n\n    /**\n     * Constructor.\n     *\n     * @param logPrefix                         The log prefix.\n     * @param logContext                        The log context.\n     * @param processor                         The event processor.\n     * @param partitionWriter                   The partition writer.\n     * @param loader                            The coordinator loader.\n     * @param coordinatorShardBuilderSupplier   The coordinator builder.\n     * @param time                              The system time.\n     * @param timer                             The system timer.\n     * @param defaultWriteTimeout               The write operation timeout.\n     * @param runtimeMetrics                    The runtime metrics.\n     * @param coordinatorMetrics                The coordinator metrics.\n     * @param serializer                        The serializer.\n     * @param compression                       The compression codec.\n     */\n    private CoordinatorRuntime(\n        String logPrefix,\n        LogContext logContext,\n        CoordinatorEventProcessor processor,\n        PartitionWriter partitionWriter,\n        CoordinatorLoader<U> loader,\n        CoordinatorShardBuilderSupplier<S, U> coordinatorShardBuilderSupplier,\n        Time time,\n        Timer timer,\n        Duration defaultWriteTimeout,\n        CoordinatorRuntimeMetrics runtimeMetrics,\n        CoordinatorMetrics coordinatorMetrics,\n        Serializer<U> serializer,\n        Compression compression\n    ) {\n        this.logPrefix = logPrefix;\n        this.logContext = logContext;\n        this.log = logContext.logger(CoordinatorRuntime.class);\n        this.time = time;\n        this.timer = timer;\n        this.defaultWriteTimeout = defaultWriteTimeout;\n        this.coordinators = new ConcurrentHashMap<>();\n        this.processor = processor;\n        this.partitionWriter = partitionWriter;\n        this.loader = loader;\n        this.coordinatorShardBuilderSupplier = coordinatorShardBuilderSupplier;\n        this.runtimeMetrics = runtimeMetrics;\n        this.coordinatorMetrics = coordinatorMetrics;\n        this.serializer = serializer;\n        this.compression = compression;\n    }\n\n    /**\n     * Throws a NotCoordinatorException exception if the runtime is not\n     * running.\n     */\n    private void throwIfNotRunning() {\n        if (!isRunning.get()) {\n            throw Errors.NOT_COORDINATOR.exception();\n        }\n    }\n\n    /**\n     * Enqueues a new event at the end of the processing queue.\n     *\n     * @param event The event.\n     * @throws NotCoordinatorException If the event processor is closed.\n     */\n    private void enqueueLast(CoordinatorEvent event) {\n        try {\n            processor.enqueueLast(event);\n        } catch (RejectedExecutionException ex) {\n            throw new NotCoordinatorException(\"Can't accept an event because the processor is closed\", ex);\n        }\n    }\n\n    /**\n     * Enqueues a new event at the front of the processing queue.\n     *\n     * @param event The event.\n     * @throws NotCoordinatorException If the event processor is closed.\n     */\n    private void enqueueFirst(CoordinatorEvent event) {\n        try {\n            processor.enqueueFirst(event);\n        } catch (RejectedExecutionException ex) {\n            throw new NotCoordinatorException(\"Can't accept an event because the processor is closed\", ex);\n        }\n    }\n\n    /**\n     * @return The coordinator context or a new context if it does not exist.\n     * Package private for testing.\n     */\n    CoordinatorContext maybeCreateContext(TopicPartition tp) {\n        return coordinators.computeIfAbsent(tp, CoordinatorContext::new);\n    }\n\n    /**\n     * @return The coordinator context or thrown an exception if it does\n     * not exist.\n     * @throws NotCoordinatorException\n     * Package private for testing.\n     */\n    CoordinatorContext contextOrThrow(TopicPartition tp) throws NotCoordinatorException {\n        CoordinatorContext context = coordinators.get(tp);\n\n        if (context == null) {\n            throw Errors.NOT_COORDINATOR.exception();\n        } else {\n            return context;\n        }\n    }\n\n    /**\n     * Calls the provided function with the context iff the context is active; throws\n     * an exception otherwise. This method ensures that the context lock is acquired\n     * before calling the function and releases afterwards.\n     *\n     * @param tp    The topic partition.\n     * @param func  The function that will receive the context.\n     * @throws NotCoordinatorException\n     * @throws CoordinatorLoadInProgressException\n     */\n    private void withActiveContextOrThrow(\n        TopicPartition tp,\n        Consumer<CoordinatorContext> func\n    ) throws NotCoordinatorException, CoordinatorLoadInProgressException {\n        CoordinatorContext context = contextOrThrow(tp);\n\n        try {\n            context.lock.lock();\n            if (context.state == CoordinatorState.ACTIVE) {\n                func.accept(context);\n            } else if (context.state == CoordinatorState.LOADING) {\n                throw Errors.COORDINATOR_LOAD_IN_PROGRESS.exception();\n            } else {\n                throw Errors.NOT_COORDINATOR.exception();\n            }\n        } finally {\n            context.lock.unlock();\n        }\n    }\n\n    /**\n     * Schedules a write operation.\n     *\n     * @param name      The name of the write operation.\n     * @param tp        The address of the coordinator (aka its topic-partitions).\n     * @param timeout   The write operation timeout.\n     * @param op        The write operation.\n     *\n     * @return A future that will be completed with the result of the write operation\n     * when the operation is completed or an exception if the write operation failed.\n     *\n     * @param <T> The type of the result.\n     */\n    public <T> CompletableFuture<T> scheduleWriteOperation(\n        String name,\n        TopicPartition tp,\n        Duration timeout,\n        CoordinatorWriteOperation<S, T, U> op\n    ) {\n        throwIfNotRunning();\n        log.debug(\"Scheduled execution of write operation {}.\", name);\n        CoordinatorWriteEvent<T> event = new CoordinatorWriteEvent<>(name, tp, timeout, op);\n        enqueueLast(event);\n        return event.future;\n    }\n\n    /**\n     * Schedule a write operation for each coordinator.\n     *\n     * @param name      The name of the write operation.\n     * @param timeout   The write operation timeout.\n     * @param op        The write operation.\n     *\n     * @return A list of futures where each future will be completed with the result of the write operation\n     * when the operation is completed or an exception if the write operation failed.\n     *\n     * @param <T> The type of the result.\n     */\n    public <T> List<CompletableFuture<T>> scheduleWriteAllOperation(\n        String name,\n        Duration timeout,\n        CoordinatorWriteOperation<S, T, U> op\n    ) {\n        throwIfNotRunning();\n        log.debug(\"Scheduled execution of write all operation {}.\", name);\n        return coordinators\n            .keySet()\n            .stream()\n            .map(tp -> scheduleWriteOperation(name, tp, timeout, op))\n            .collect(Collectors.toList());\n    }\n\n    /**\n     * Schedules a transactional write operation.\n     *\n     * @param name              The name of the write operation.\n     * @param tp                The address of the coordinator (aka its topic-partitions).\n     * @param transactionalId   The transactional id.\n     * @param producerId        The producer id.\n     * @param producerEpoch     The producer epoch.\n     * @param timeout           The write operation timeout.\n     * @param op                The write operation.\n     * @param apiVersion        The Version of the Txn_Offset_Commit request\n     *\n     * @return A future that will be completed with the result of the write operation\n     * when the operation is completed or an exception if the write operation failed.\n     *\n     * @param <T> The type of the result.\n     */\n    public <T> CompletableFuture<T> scheduleTransactionalWriteOperation(\n        String name,\n        TopicPartition tp,\n        String transactionalId,\n        long producerId,\n        short producerEpoch,\n        Duration timeout,\n        CoordinatorWriteOperation<S, T, U> op,\n        Short apiVersion\n    ) {\n        throwIfNotRunning();\n        log.debug(\"Scheduled execution of transactional write operation {}.\", name);\n        return partitionWriter.maybeStartTransactionVerification(\n            tp,\n            transactionalId,\n            producerId,\n            producerEpoch,\n            apiVersion\n        ).thenCompose(verificationGuard -> {\n            CoordinatorWriteEvent<T> event = new CoordinatorWriteEvent<>(\n                name,\n                tp,\n                transactionalId,\n                producerId,\n                producerEpoch,\n                verificationGuard,\n                timeout,\n                op\n            );\n            enqueueLast(event);\n            return event.future;\n        });\n    }\n\n    /**\n     * Schedules the transaction completion.\n     *\n     * @param name              The name of the operation.\n     * @param tp                The address of the coordinator (aka its topic-partitions).\n     * @param producerId        The producer id.\n     * @param producerEpoch     The producer epoch.\n     * @param coordinatorEpoch  The epoch of the transaction coordinator.\n     * @param result            The transaction result.\n     *\n     * @return A future that will be completed with null when the operation is\n     * completed or an exception if the operation failed.\n     */\n    public CompletableFuture<Void> scheduleTransactionCompletion(\n        String name,\n        TopicPartition tp,\n        long producerId,\n        short producerEpoch,\n        int coordinatorEpoch,\n        TransactionResult result,\n        Duration timeout\n    ) {\n        throwIfNotRunning();\n        log.debug(\"Scheduled execution of transaction completion for {} with producer id={}, producer epoch={}, \" +\n            \"coordinator epoch={} and transaction result={}.\", tp, producerId, producerEpoch, coordinatorEpoch, result);\n        CoordinatorCompleteTransactionEvent event = new CoordinatorCompleteTransactionEvent(\n            name,\n            tp,\n            producerId,\n            producerEpoch,\n            coordinatorEpoch,\n            result,\n            timeout\n        );\n        enqueueLast(event);\n        return event.future;\n    }\n\n    /**\n     * Schedules a read operation.\n     *\n     * @param name  The name of the read operation.\n     * @param tp    The address of the coordinator (aka its topic-partitions).\n     * @param op    The read operation.\n     *\n     * @return A future that will be completed with the result of the read operation\n     * when the operation is completed or an exception if the read operation failed.\n     *\n     * @param <T> The type of the result.\n     */\n    public <T> CompletableFuture<T> scheduleReadOperation(\n        String name,\n        TopicPartition tp,\n        CoordinatorReadOperation<S, T> op\n    ) {\n        throwIfNotRunning();\n        log.debug(\"Scheduled execution of read operation {}.\", name);\n        CoordinatorReadEvent<T> event = new CoordinatorReadEvent<>(name, tp, op);\n        enqueueLast(event);\n        return event.future;\n    }\n\n    /**\n     * Schedules a read operation for each coordinator.\n     *\n     * @param name  The name of the read operation.\n     * @param op    The read operation.\n     *\n     * @return A list of futures where each future will be completed with the result of the read operation\n     * when the operation is completed or an exception if the read operation failed.\n     *\n     * @param <T> The type of the result.\n     */\n    public <T> List<CompletableFuture<T>> scheduleReadAllOperation(\n        String name,\n        CoordinatorReadOperation<S, T> op\n    ) {\n        throwIfNotRunning();\n        log.debug(\"Scheduled execution of read all operation {}.\", name);\n        return coordinators\n            .keySet()\n            .stream()\n            .map(tp -> scheduleReadOperation(name, tp, op))\n            .collect(Collectors.toList());\n    }\n\n    /**\n     * Schedules an internal event.\n     *\n     * @param name  The name of the write operation.\n     * @param tp    The address of the coordinator (aka its topic-partitions).\n     * @param op    The operation.\n     */\n    private void scheduleInternalOperation(\n        String name,\n        TopicPartition tp,\n        Runnable op\n    ) {\n        log.debug(\"Scheduled execution of internal operation {}.\", name);\n        enqueueLast(new CoordinatorInternalEvent(name, tp, op));\n    }\n\n    /**\n     * Schedules the loading of a coordinator. This is called when the broker is elected as\n     * the leader for a partition.\n     *\n     * @param tp                The topic partition of the coordinator. Records from this\n     *                          partitions will be read and applied to the coordinator.\n     * @param partitionEpoch    The epoch of the partition.\n     */\n    public void scheduleLoadOperation(\n        TopicPartition tp,\n        int partitionEpoch\n    ) {\n        throwIfNotRunning();\n        log.info(\"Scheduling loading of metadata from {} with epoch {}\", tp, partitionEpoch);\n\n        // Touch the state to make the runtime immediately aware of the new coordinator.\n        maybeCreateContext(tp);\n\n        scheduleInternalOperation(\"Load(tp=\" + tp + \", epoch=\" + partitionEpoch + \")\", tp, () -> {\n            // The context is re-created if it does not exist.\n            CoordinatorContext context = maybeCreateContext(tp);\n\n            context.lock.lock();\n            try {\n                if (context.epoch < partitionEpoch) {\n                    context.epoch = partitionEpoch;\n\n                    switch (context.state) {\n                        case FAILED:\n                        case INITIAL:\n                            context.transitionTo(CoordinatorState.LOADING);\n                            loader.load(tp, context.coordinator).whenComplete((summary, exception) -> {\n                                scheduleInternalOperation(\"CompleteLoad(tp=\" + tp + \", epoch=\" + partitionEpoch + \")\", tp, () -> {\n                                    CoordinatorContext ctx = coordinators.get(tp);\n                                    if (ctx != null)  {\n                                        if (ctx.state != CoordinatorState.LOADING) {\n                                            log.info(\"Ignored load completion from {} because context is in {} state.\",\n                                                ctx.tp, ctx.state);\n                                            return;\n                                        }\n                                        try {\n                                            if (exception != null) throw exception;\n                                            ctx.transitionTo(CoordinatorState.ACTIVE);\n                                            if (summary != null) {\n                                                runtimeMetrics.recordPartitionLoadSensor(summary.startTimeMs(), summary.endTimeMs());\n                                                log.info(\"Finished loading of metadata from {} with epoch {} in {}ms where {}ms \" +\n                                                         \"was spent in the scheduler. Loaded {} records which total to {} bytes.\",\n                                                    tp, partitionEpoch, summary.endTimeMs() - summary.startTimeMs(),\n                                                    summary.schedulerQueueTimeMs(), summary.numRecords(), summary.numBytes());\n                                            }\n                                        } catch (Throwable ex) {\n                                            log.error(\"Failed to load metadata from {} with epoch {} due to {}.\",\n                                                tp, partitionEpoch, ex.toString());\n                                            ctx.transitionTo(CoordinatorState.FAILED);\n                                        }\n                                    } else {\n                                        log.debug(\"Failed to complete the loading of metadata for {} in epoch {} since the coordinator does not exist.\",\n                                            tp, partitionEpoch);\n                                    }\n                                });\n                            });\n                            break;\n\n                        case LOADING:\n                            log.info(\"The coordinator {} is already loading metadata.\", tp);\n                            break;\n\n                        case ACTIVE:\n                            log.info(\"The coordinator {} is already active.\", tp);\n                            break;\n\n                        default:\n                            log.error(\"Cannot load coordinator {} in state {}.\", tp, context.state);\n                    }\n                } else {\n                    log.info(\"Ignored loading metadata from {} since current epoch {} is larger than or equals to {}.\",\n                        context.tp, context.epoch, partitionEpoch);\n                }\n            } finally {\n                context.lock.unlock();\n            }\n        });\n    }\n\n    /**\n     * Schedules the unloading of a coordinator. This is called when the broker is not the\n     * leader anymore.\n     *\n     * @param tp                The topic partition of the coordinator.\n     * @param partitionEpoch    The partition epoch as an optional value.\n     *                          An empty value means that the topic was deleted.\n     */\n    public void scheduleUnloadOperation(\n        TopicPartition tp,\n        OptionalInt partitionEpoch\n    ) {\n        throwIfNotRunning();\n        log.info(\"Scheduling unloading of metadata for {} with epoch {}\", tp, partitionEpoch);\n\n        scheduleInternalOperation(\"UnloadCoordinator(tp=\" + tp + \", epoch=\" + partitionEpoch + \")\", tp, () -> {\n            CoordinatorContext context = coordinators.get(tp);\n            if (context != null) {\n                context.lock.lock();\n                try {\n                    if (!partitionEpoch.isPresent() || context.epoch < partitionEpoch.getAsInt()) {\n                        log.info(\"Started unloading metadata for {} with epoch {}.\", tp, partitionEpoch);\n                        context.transitionTo(CoordinatorState.CLOSED);\n                        coordinators.remove(tp, context);\n                        log.info(\"Finished unloading metadata for {} with epoch {}.\", tp, partitionEpoch);\n                    } else {\n                        log.info(\"Ignored unloading metadata for {} in epoch {} since current epoch is {}.\",\n                            tp, partitionEpoch, context.epoch);\n                    }\n                } finally {\n                    context.lock.unlock();\n                }\n            } else {\n                log.info(\"Ignored unloading metadata for {} in epoch {} since metadata was never loaded.\",\n                    tp, partitionEpoch);\n            }\n        });\n    }\n\n    /**\n     * A new metadata image is available.\n     *\n     * @param newImage  The new metadata image.\n     * @param delta     The metadata delta.\n     */\n    public void onNewMetadataImage(\n        MetadataImage newImage,\n        MetadataDelta delta\n    ) {\n        throwIfNotRunning();\n        log.debug(\"Scheduling applying of a new metadata image with offset {}.\", newImage.offset());\n\n        // Update global image.\n        metadataImage = newImage;\n\n        // Push an event for each coordinator.\n        coordinators.keySet().forEach(tp -> {\n            scheduleInternalOperation(\"UpdateImage(tp=\" + tp + \", offset=\" + newImage.offset() + \")\", tp, () -> {\n                CoordinatorContext context = coordinators.get(tp);\n                if (context != null) {\n                    context.lock.lock();\n                    try {\n                        if (context.state == CoordinatorState.ACTIVE) {\n                            // The new image can be applied to the coordinator only if the coordinator\n                            // exists and is in the active state.\n                            log.debug(\"Applying new metadata image with offset {} to {}.\", newImage.offset(), tp);\n                            context.coordinator.onNewMetadataImage(newImage, delta);\n                        } else {\n                            log.debug(\"Ignored new metadata image with offset {} for {} because the coordinator is not active.\",\n                                newImage.offset(), tp);\n                        }\n                    } finally {\n                        context.lock.unlock();\n                    }\n                } else {\n                    log.debug(\"Ignored new metadata image with offset {} for {} because the coordinator does not exist.\",\n                        newImage.offset(), tp);\n                }\n            });\n        });\n    }\n\n    /**\n     * Closes the runtime. This closes all the coordinators currently registered\n     * in the runtime.\n     *\n     * @throws Exception\n     */\n    public void close() throws Exception {\n        if (!isRunning.compareAndSet(true, false)) {\n            log.warn(\"Coordinator runtime is already shutting down.\");\n            return;\n        }\n\n        log.info(\"Closing coordinator runtime.\");\n        Utils.closeQuietly(loader, \"loader\");\n        Utils.closeQuietly(timer, \"timer\");\n        // This close the processor, drain all the pending events and\n        // reject any new events.\n        Utils.closeQuietly(processor, \"event processor\");\n        // Unload all the coordinators.\n        coordinators.forEach((tp, context) -> {\n            context.lock.lock();\n            try {\n                context.transitionTo(CoordinatorState.CLOSED);\n            } finally {\n                context.lock.unlock();\n            }\n        });\n        coordinators.clear();\n        Utils.closeQuietly(runtimeMetrics, \"runtime metrics\");\n        log.info(\"Coordinator runtime closed.\");\n    }\n}",
                "methodCount": 81
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "processingTime": {
                "llmResponseTime": -1,
                "pluginProcessingTime": -1,
                "totalTime": 50644
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "withLogPrefix",
                            "method_signature": "public withLogPrefix(String logPrefix)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withLogContext",
                            "method_signature": "public withLogContext(LogContext logContext)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withEventProcessor",
                            "method_signature": "public withEventProcessor(CoordinatorEventProcessor eventProcessor)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withPartitionWriter",
                            "method_signature": "public withPartitionWriter(PartitionWriter partitionWriter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withLoader",
                            "method_signature": "public withLoader(CoordinatorLoader<U> loader)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withCoordinatorShardBuilderSupplier",
                            "method_signature": "public withCoordinatorShardBuilderSupplier(CoordinatorShardBuilderSupplier<S, U> coordinatorShardBuilderSupplier)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withTime",
                            "method_signature": "public withTime(Time time)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withTimer",
                            "method_signature": "public withTimer(Timer timer)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withDefaultWriteTimeOut",
                            "method_signature": "public withDefaultWriteTimeOut(Duration defaultWriteTimeout)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withCoordinatorRuntimeMetrics",
                            "method_signature": "public withCoordinatorRuntimeMetrics(CoordinatorRuntimeMetrics runtimeMetrics)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withCoordinatorMetrics",
                            "method_signature": "public withCoordinatorMetrics(CoordinatorMetrics coordinatorMetrics)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withSerializer",
                            "method_signature": "public withSerializer(Serializer<U> serializer)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withCompression",
                            "method_signature": "public withCompression(Compression compression)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "cancelAll",
                            "method_signature": "public cancelAll()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "size",
                            "method_signature": "public size()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionTo",
                            "method_signature": "private transitionTo(\n            CoordinatorState newState\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "unload",
                            "method_signature": "private unload()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "append",
                            "method_signature": "private append(CoordinatorContext context)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "lastHighWatermark",
                            "method_signature": "public lastHighWatermark()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "throwIfNotRunning",
                            "method_signature": "private throwIfNotRunning()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "enqueueLast",
                            "method_signature": "private enqueueLast(CoordinatorEvent event)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "enqueueFirst",
                            "method_signature": "private enqueueFirst(CoordinatorEvent event)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeCreateContext",
                            "method_signature": " maybeCreateContext(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "contextOrThrow",
                            "method_signature": " contextOrThrow(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withActiveContextOrThrow",
                            "method_signature": "private withActiveContextOrThrow(\n        TopicPartition tp,\n        Consumer<CoordinatorContext> func\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "scheduleWriteOperation",
                            "method_signature": "public scheduleWriteOperation(\n        String name,\n        TopicPartition tp,\n        Duration timeout,\n        CoordinatorWriteOperation<S, T, U> op\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "scheduleWriteAllOperation",
                            "method_signature": "public scheduleWriteAllOperation(\n        String name,\n        Duration timeout,\n        CoordinatorWriteOperation<S, T, U> op\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "scheduleTransactionalWriteOperation",
                            "method_signature": "public scheduleTransactionalWriteOperation(\n        String name,\n        TopicPartition tp,\n        String transactionalId,\n        long producerId,\n        short producerEpoch,\n        Duration timeout,\n        CoordinatorWriteOperation<S, T, U> op,\n        Short apiVersion\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "scheduleTransactionCompletion",
                            "method_signature": "public scheduleTransactionCompletion(\n        String name,\n        TopicPartition tp,\n        long producerId,\n        short producerEpoch,\n        int coordinatorEpoch,\n        TransactionResult result,\n        Duration timeout\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "scheduleReadOperation",
                            "method_signature": "public scheduleReadOperation(\n        String name,\n        TopicPartition tp,\n        CoordinatorReadOperation<S, T> op\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "scheduleReadAllOperation",
                            "method_signature": "public scheduleReadAllOperation(\n        String name,\n        CoordinatorReadOperation<S, T> op\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "scheduleInternalOperation",
                            "method_signature": "private scheduleInternalOperation(\n        String name,\n        TopicPartition tp,\n        Runnable op\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "scheduleLoadOperation",
                            "method_signature": "public scheduleLoadOperation(\n        TopicPartition tp,\n        int partitionEpoch\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "scheduleUnloadOperation",
                            "method_signature": "public scheduleUnloadOperation(\n        TopicPartition tp,\n        OptionalInt partitionEpoch\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onNewMetadataImage",
                            "method_signature": "public onNewMetadataImage(\n        MetadataImage newImage,\n        MetadataDelta delta\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "public close()",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withCoordinatorShardBuilderSupplier",
                            "method_signature": "public withCoordinatorShardBuilderSupplier(CoordinatorShardBuilderSupplier<S, U> coordinatorShardBuilderSupplier)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "size",
                            "method_signature": "public size()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withEventProcessor",
                            "method_signature": "public withEventProcessor(CoordinatorEventProcessor eventProcessor)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withTime",
                            "method_signature": "public withTime(Time time)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withLogPrefix",
                            "method_signature": "public withLogPrefix(String logPrefix)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withPartitionWriter",
                            "method_signature": "public withPartitionWriter(PartitionWriter partitionWriter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withLoader",
                            "method_signature": "public withLoader(CoordinatorLoader<U> loader)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withDefaultWriteTimeOut",
                            "method_signature": "public withDefaultWriteTimeOut(Duration defaultWriteTimeout)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withCoordinatorRuntimeMetrics",
                            "method_signature": "public withCoordinatorRuntimeMetrics(CoordinatorRuntimeMetrics runtimeMetrics)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withCoordinatorMetrics",
                            "method_signature": "public withCoordinatorMetrics(CoordinatorMetrics coordinatorMetrics)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withSerializer",
                            "method_signature": "public withSerializer(Serializer<U> serializer)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withCompression",
                            "method_signature": "public withCompression(Compression compression)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withTimer",
                            "method_signature": "public withTimer(Timer timer)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withLogContext",
                            "method_signature": "public withLogContext(LogContext logContext)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withCoordinatorShardBuilderSupplier",
                            "method_signature": "public withCoordinatorShardBuilderSupplier(CoordinatorShardBuilderSupplier<S, U> coordinatorShardBuilderSupplier)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "size",
                            "method_signature": "public size()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withEventProcessor",
                            "method_signature": "public withEventProcessor(CoordinatorEventProcessor eventProcessor)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withTime",
                            "method_signature": "public withTime(Time time)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withLogPrefix",
                            "method_signature": "public withLogPrefix(String logPrefix)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withPartitionWriter",
                            "method_signature": "public withPartitionWriter(PartitionWriter partitionWriter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withLoader",
                            "method_signature": "public withLoader(CoordinatorLoader<U> loader)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withDefaultWriteTimeOut",
                            "method_signature": "public withDefaultWriteTimeOut(Duration defaultWriteTimeout)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withCoordinatorRuntimeMetrics",
                            "method_signature": "public withCoordinatorRuntimeMetrics(CoordinatorRuntimeMetrics runtimeMetrics)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withCoordinatorMetrics",
                            "method_signature": "public withCoordinatorMetrics(CoordinatorMetrics coordinatorMetrics)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withSerializer",
                            "method_signature": "public withSerializer(Serializer<U> serializer)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withCompression",
                            "method_signature": "public withCompression(Compression compression)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withTimer",
                            "method_signature": "public withTimer(Timer timer)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withLogContext",
                            "method_signature": "public withLogContext(LogContext logContext)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "public build()": {
                    "first": {
                        "method_name": "build",
                        "method_signature": "public build()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.13484573902550048
                },
                "public withCoordinatorShardBuilderSupplier(CoordinatorShardBuilderSupplier<S, U> coordinatorShardBuilderSupplier)": {
                    "first": {
                        "method_name": "withCoordinatorShardBuilderSupplier",
                        "method_signature": "public withCoordinatorShardBuilderSupplier(CoordinatorShardBuilderSupplier<S, U> coordinatorShardBuilderSupplier)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.24297596088847223
                },
                "public size()": {
                    "first": {
                        "method_name": "size",
                        "method_signature": "public size()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.24718238929584802
                },
                "public withEventProcessor(CoordinatorEventProcessor eventProcessor)": {
                    "first": {
                        "method_name": "withEventProcessor",
                        "method_signature": "public withEventProcessor(CoordinatorEventProcessor eventProcessor)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.25866703476981456
                },
                "public withTime(Time time)": {
                    "first": {
                        "method_name": "withTime",
                        "method_signature": "public withTime(Time time)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2594309706388152
                },
                "public withLogPrefix(String logPrefix)": {
                    "first": {
                        "method_name": "withLogPrefix",
                        "method_signature": "public withLogPrefix(String logPrefix)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.25981294057600274
                },
                "public withPartitionWriter(PartitionWriter partitionWriter)": {
                    "first": {
                        "method_name": "withPartitionWriter",
                        "method_signature": "public withPartitionWriter(PartitionWriter partitionWriter)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.25981294057600274
                },
                "public withLoader(CoordinatorLoader<U> loader)": {
                    "first": {
                        "method_name": "withLoader",
                        "method_signature": "public withLoader(CoordinatorLoader<U> loader)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.25981294057600274
                },
                "public withDefaultWriteTimeOut(Duration defaultWriteTimeout)": {
                    "first": {
                        "method_name": "withDefaultWriteTimeOut",
                        "method_signature": "public withDefaultWriteTimeOut(Duration defaultWriteTimeout)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.25981294057600274
                },
                "public withCoordinatorRuntimeMetrics(CoordinatorRuntimeMetrics runtimeMetrics)": {
                    "first": {
                        "method_name": "withCoordinatorRuntimeMetrics",
                        "method_signature": "public withCoordinatorRuntimeMetrics(CoordinatorRuntimeMetrics runtimeMetrics)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.25981294057600274
                },
                "public withCoordinatorMetrics(CoordinatorMetrics coordinatorMetrics)": {
                    "first": {
                        "method_name": "withCoordinatorMetrics",
                        "method_signature": "public withCoordinatorMetrics(CoordinatorMetrics coordinatorMetrics)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.25981294057600274
                },
                "public withSerializer(Serializer<U> serializer)": {
                    "first": {
                        "method_name": "withSerializer",
                        "method_signature": "public withSerializer(Serializer<U> serializer)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.25981294057600274
                },
                "public withCompression(Compression compression)": {
                    "first": {
                        "method_name": "withCompression",
                        "method_signature": "public withCompression(Compression compression)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.25981294057600274
                },
                "public withTimer(Timer timer)": {
                    "first": {
                        "method_name": "withTimer",
                        "method_signature": "public withTimer(Timer timer)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.26057688445580474
                },
                "public withLogContext(LogContext logContext)": {
                    "first": {
                        "method_name": "withLogContext",
                        "method_signature": "public withLogContext(LogContext logContext)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2609588583984403
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "public build()",
                    "public withCoordinatorShardBuilderSupplier(CoordinatorShardBuilderSupplier<S, U> coordinatorShardBuilderSupplier)",
                    "public withEventProcessor(CoordinatorEventProcessor eventProcessor)",
                    "public withPartitionWriter(PartitionWriter partitionWriter)",
                    "public withLoader(CoordinatorLoader<U> loader)",
                    "public withLogContext(LogContext logContext)",
                    "public withCoordinatorRuntimeMetrics(CoordinatorRuntimeMetrics runtimeMetrics)",
                    "public withCoordinatorMetrics(CoordinatorMetrics coordinatorMetrics)",
                    "public withSerializer(Serializer<U> serializer)",
                    "public withCompression(Compression compression)",
                    "public withTimer(Timer timer)",
                    "public withLogPrefix(String logPrefix)",
                    "public withDefaultWriteTimeOut(Duration defaultWriteTimeout)",
                    "public withTime(Time time)",
                    "public size()"
                ],
                "llm_response_time": 7509
            },
            "targetClassMap": {
                "build": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2223,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withCoordinatorShardBuilderSupplier": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3569,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withEventProcessor": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 1789,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withPartitionWriter": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 1965,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withLoader": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2929,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withLogContext": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2380,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withCoordinatorRuntimeMetrics": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 1699,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withCoordinatorMetrics": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2187,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withSerializer": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3913,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withCompression": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3336,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withTimer": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 4598,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withLogPrefix": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3080,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withDefaultWriteTimeOut": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2103,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withTime": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3110,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "size": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3401,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "638844f833b165d6f9ca52c173858d26b7254fac",
        "url": "https://github.com/apache/kafka/commit/638844f833b165d6f9ca52c173858d26b7254fac",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method private completeTransaction(producerId long, producerEpoch short, coordinatorEpoch int, result TransactionResult, event DeferredEvent) : void extracted from public run() : void in class org.apache.kafka.coordinator.group.runtime.CoordinatorRuntime.CoordinatorCompleteTransactionEvent & moved to class org.apache.kafka.coordinator.group.runtime.CoordinatorRuntime.CoordinatorContext",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1138,
                    "endLine": 1184,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public run() : void"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1145,
                    "endLine": 1145,
                    "startColumn": 21,
                    "endColumn": 90,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1148,
                    "endLine": 1152,
                    "startColumn": 25,
                    "endColumn": 27,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1154,
                    "endLine": 1166,
                    "startColumn": 25,
                    "endColumn": 27,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1167,
                    "endLine": 1167,
                    "startColumn": 25,
                    "endColumn": 77,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1170,
                    "endLine": 1170,
                    "startColumn": 29,
                    "endColumn": 74,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1177,
                    "endLine": 1177,
                    "startColumn": 25,
                    "endColumn": 92,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1178,
                    "endLine": 1178,
                    "startColumn": 25,
                    "endColumn": 37,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1147,
                    "endLine": 1179,
                    "startColumn": 21,
                    "endColumn": 22,
                    "codeElementType": "TRY_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1176,
                    "endLine": 1179,
                    "startColumn": 23,
                    "endColumn": 22,
                    "codeElementType": "CATCH_CLAUSE",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1010,
                    "endLine": 1063,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "private completeTransaction(producerId long, producerEpoch short, coordinatorEpoch int, result TransactionResult, event DeferredEvent) : void"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1035,
                    "endLine": 1035,
                    "startColumn": 13,
                    "endColumn": 74,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1037,
                    "endLine": 1041,
                    "startColumn": 17,
                    "endColumn": 19,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1043,
                    "endLine": 1055,
                    "startColumn": 17,
                    "endColumn": 19,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1056,
                    "endLine": 1056,
                    "startColumn": 17,
                    "endColumn": 61,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1058,
                    "endLine": 1058,
                    "startColumn": 17,
                    "endColumn": 55,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1060,
                    "endLine": 1060,
                    "startColumn": 17,
                    "endColumn": 76,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1061,
                    "endLine": 1061,
                    "startColumn": 17,
                    "endColumn": 35,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1036,
                    "endLine": 1062,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "TRY_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1059,
                    "endLine": 1062,
                    "startColumn": 15,
                    "endColumn": 14,
                    "codeElementType": "CATCH_CLAUSE",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1531,
                    "endLine": 1554,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public run() : void"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1538,
                    "endLine": 1544,
                    "startColumn": 21,
                    "endColumn": 22,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "context.completeTransaction(producerId,producerEpoch,coordinatorEpoch,result,this)"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1028,
                    "endLine": 1028,
                    "startColumn": 17,
                    "endColumn": 105,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1033,
                    "endLine": 1033,
                    "startColumn": 13,
                    "endColumn": 33,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1027,
                    "endLine": 1029,
                    "startColumn": 51,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1027,
                    "endLine": 1029,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": false
        },
        "ref_id": 584,
        "extraction_results": {
            "success": true,
            "newCommitHash": "fe9c6822775e39d22c74b77d5df030ccc5b570b0",
            "newBranchName": "extract-completeTransaction-run-39ffdea"
        },
        "telemetry": {
            "id": "62ebad49-88bc-4a28-b587-bc226c633c8c",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1939,
                "lineStart": 71,
                "lineEnd": 2009,
                "bodyLineStart": 71,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                "sourceCode": "/**\n * The CoordinatorRuntime provides a framework to implement coordinators such as the group coordinator\n * or the transaction coordinator.\n *\n * The runtime framework maps each underlying partitions (e.g. __consumer_offsets) that that broker is a\n * leader of to a coordinator replicated state machine. A replicated state machine holds the hard and soft\n * state of all the objects (e.g. groups or offsets) assigned to the partition. The hard state is stored in\n * timeline datastructures backed by a SnapshotRegistry. The runtime supports two type of operations\n * on state machines: (1) Writes and (2) Reads.\n *\n * (1) A write operation, aka a request, can read the full and potentially **uncommitted** state from state\n * machine to handle the operation. A write operation typically generates a response and a list of\n * records. The records are applied to the state machine and persisted to the partition. The response\n * is parked until the records are committed and delivered when they are.\n *\n * (2) A read operation, aka a request, can only read the committed state from the state machine to handle\n * the operation. A read operation typically generates a response that is immediately completed.\n *\n * The runtime framework exposes an asynchronous, future based, API to the world. All the operations\n * are executed by an CoordinatorEventProcessor. The processor guarantees that operations for a\n * single partition or state machine are not processed concurrently.\n *\n * @param <S> The type of the state machine.\n * @param <U> The type of the record.\n */\npublic class CoordinatorRuntime<S extends CoordinatorShard<U>, U> implements AutoCloseable {\n\n    /**\n     * Builder to create a CoordinatorRuntime.\n     *\n     * @param <S> The type of the state machine.\n     * @param <U> The type of the record.\n     */\n    public static class Builder<S extends CoordinatorShard<U>, U> {\n        private String logPrefix;\n        private LogContext logContext;\n        private CoordinatorEventProcessor eventProcessor;\n        private PartitionWriter partitionWriter;\n        private CoordinatorLoader<U> loader;\n        private CoordinatorShardBuilderSupplier<S, U> coordinatorShardBuilderSupplier;\n        private Time time = Time.SYSTEM;\n        private Timer timer;\n        private Duration defaultWriteTimeout;\n        private CoordinatorRuntimeMetrics runtimeMetrics;\n        private CoordinatorMetrics coordinatorMetrics;\n        private Serializer<U> serializer;\n        private Compression compression;\n\n        public Builder<S, U> withLogPrefix(String logPrefix) {\n            this.logPrefix = logPrefix;\n            return this;\n        }\n\n        public Builder<S, U> withLogContext(LogContext logContext) {\n            this.logContext = logContext;\n            return this;\n        }\n\n        public Builder<S, U> withEventProcessor(CoordinatorEventProcessor eventProcessor) {\n            this.eventProcessor = eventProcessor;\n            return this;\n        }\n\n        public Builder<S, U> withPartitionWriter(PartitionWriter partitionWriter) {\n            this.partitionWriter = partitionWriter;\n            return this;\n        }\n\n        public Builder<S, U> withLoader(CoordinatorLoader<U> loader) {\n            this.loader = loader;\n            return this;\n        }\n\n        public Builder<S, U> withCoordinatorShardBuilderSupplier(CoordinatorShardBuilderSupplier<S, U> coordinatorShardBuilderSupplier) {\n            this.coordinatorShardBuilderSupplier = coordinatorShardBuilderSupplier;\n            return this;\n        }\n\n        public Builder<S, U> withTime(Time time) {\n            this.time = time;\n            return this;\n        }\n\n        public Builder<S, U> withTimer(Timer timer) {\n            this.timer = timer;\n            return this;\n        }\n\n        public Builder<S, U> withDefaultWriteTimeOut(Duration defaultWriteTimeout) {\n            this.defaultWriteTimeout = defaultWriteTimeout;\n            return this;\n        }\n\n        public Builder<S, U> withCoordinatorRuntimeMetrics(CoordinatorRuntimeMetrics runtimeMetrics) {\n            this.runtimeMetrics = runtimeMetrics;\n            return this;\n        }\n\n        public Builder<S, U> withCoordinatorMetrics(CoordinatorMetrics coordinatorMetrics) {\n            this.coordinatorMetrics = coordinatorMetrics;\n            return this;\n        }\n\n        public Builder<S, U> withSerializer(Serializer<U> serializer) {\n            this.serializer = serializer;\n            return this;\n        }\n\n        public Builder<S, U> withCompression(Compression compression) {\n            this.compression = compression;\n            return this;\n        }\n\n        public CoordinatorRuntime<S, U> build() {\n            if (logPrefix == null)\n                logPrefix = \"\";\n            if (logContext == null)\n                logContext = new LogContext(logPrefix);\n            if (eventProcessor == null)\n                throw new IllegalArgumentException(\"Event processor must be set.\");\n            if (partitionWriter == null)\n                throw new IllegalArgumentException(\"Partition write must be set.\");\n            if (loader == null)\n                throw new IllegalArgumentException(\"Loader must be set.\");\n            if (coordinatorShardBuilderSupplier == null)\n                throw new IllegalArgumentException(\"State machine supplier must be set.\");\n            if (time == null)\n                throw new IllegalArgumentException(\"Time must be set.\");\n            if (timer == null)\n                throw new IllegalArgumentException(\"Timer must be set.\");\n            if (runtimeMetrics == null)\n                throw new IllegalArgumentException(\"CoordinatorRuntimeMetrics must be set.\");\n            if (coordinatorMetrics == null)\n                throw new IllegalArgumentException(\"CoordinatorMetrics must be set.\");\n            if (serializer == null)\n                throw new IllegalArgumentException(\"Serializer must be set.\");\n            if (compression == null)\n                compression = Compression.NONE;\n\n            return new CoordinatorRuntime<>(\n                logPrefix,\n                logContext,\n                eventProcessor,\n                partitionWriter,\n                loader,\n                coordinatorShardBuilderSupplier,\n                time,\n                timer,\n                defaultWriteTimeout,\n                runtimeMetrics,\n                coordinatorMetrics,\n                serializer,\n                compression\n            );\n        }\n    }\n\n    /**\n     * The various state that a coordinator for a partition can be in.\n     */\n    public enum CoordinatorState {\n        /**\n         * Initial state when a coordinator is created.\n         */\n        INITIAL {\n            @Override\n            boolean canTransitionFrom(CoordinatorState state) {\n                return false;\n            }\n        },\n\n        /**\n         * The coordinator is being loaded.\n         */\n        LOADING {\n            @Override\n            boolean canTransitionFrom(CoordinatorState state) {\n                return state == INITIAL || state == FAILED;\n            }\n        },\n\n        /**\n         * The coordinator is active and can service requests.\n         */\n        ACTIVE {\n            @Override\n            boolean canTransitionFrom(CoordinatorState state) {\n                return state == ACTIVE || state == LOADING;\n            }\n        },\n\n        /**\n         * The coordinator is closed.\n         */\n        CLOSED {\n            @Override\n            boolean canTransitionFrom(CoordinatorState state) {\n                return true;\n            }\n        },\n\n        /**\n         * The coordinator loading has failed.\n         */\n        FAILED {\n            @Override\n            boolean canTransitionFrom(CoordinatorState state) {\n                return state == LOADING;\n            }\n        };\n\n        abstract boolean canTransitionFrom(CoordinatorState state);\n    }\n\n    /**\n     * The EventBasedCoordinatorTimer implements the CoordinatorTimer interface and provides an event based\n     * timer which turns timeouts of a regular {@link Timer} into {@link CoordinatorWriteEvent} events which\n     * are executed by the {@link CoordinatorEventProcessor} used by this coordinator runtime. This is done\n     * to ensure that the timer respects the threading model of the coordinator runtime.\n     *\n     * The {@link CoordinatorWriteEvent} events pushed by the coordinator timer wraps the\n     * {@link TimeoutOperation} operations scheduled by the coordinators.\n     *\n     * It also keeps track of all the scheduled {@link TimerTask}. This allows timeout operations to be\n     * cancelled or rescheduled. When a timer is cancelled or overridden, the previous timer is guaranteed to\n     * not be executed even if it already expired and got pushed to the event processor.\n     *\n     * When a timer fails with an unexpected exception, the timer is rescheduled with a backoff.\n     */\n    class EventBasedCoordinatorTimer implements CoordinatorTimer<Void, U> {\n        /**\n         * The logger.\n         */\n        final Logger log;\n\n        /**\n         * The topic partition.\n         */\n        final TopicPartition tp;\n\n        /**\n         * The scheduled timers keyed by their key.\n         */\n        final Map<String, TimerTask> tasks = new HashMap<>();\n\n        EventBasedCoordinatorTimer(TopicPartition tp, LogContext logContext) {\n            this.tp = tp;\n            this.log = logContext.logger(EventBasedCoordinatorTimer.class);\n        }\n\n        @Override\n        public void schedule(\n            String key,\n            long delay,\n            TimeUnit unit,\n            boolean retry,\n            TimeoutOperation<Void, U> operation\n        ) {\n            schedule(key, delay, unit, retry, 500, operation);\n        }\n\n        @Override\n        public void schedule(\n            String key,\n            long delay,\n            TimeUnit unit,\n            boolean retry,\n            long retryBackoff,\n            TimeoutOperation<Void, U> operation\n        ) {\n            // The TimerTask wraps the TimeoutOperation into a CoordinatorWriteEvent. When the TimerTask\n            // expires, the event is pushed to the queue of the coordinator runtime to be executed. This\n            // ensures that the threading model of the runtime is respected.\n            TimerTask task = new TimerTask(unit.toMillis(delay)) {\n                @Override\n                public void run() {\n                    String eventName = \"Timeout(tp=\" + tp + \", key=\" + key + \")\";\n                    CoordinatorWriteEvent<Void> event = new CoordinatorWriteEvent<>(eventName, tp, defaultWriteTimeout, coordinator -> {\n                        log.debug(\"Executing write event {} for timer {}.\", eventName, key);\n\n                        // If the task is different, it means that the timer has been\n                        // cancelled while the event was waiting to be processed.\n                        if (!tasks.remove(key, this)) {\n                            throw new RejectedExecutionException(\"Timer \" + key + \" was overridden or cancelled\");\n                        }\n\n                        // Execute the timeout operation.\n                        return operation.generateRecords();\n                    });\n\n                    // If the write event fails, it is rescheduled with a small backoff except if retry\n                    // is disabled or if the error is fatal.\n                    event.future.exceptionally(ex -> {\n                        if (ex instanceof RejectedExecutionException) {\n                            log.debug(\"The write event {} for the timer {} was not executed because it was \" +\n                                \"cancelled or overridden.\", event.name, key);\n                            return null;\n                        }\n\n                        if (ex instanceof NotCoordinatorException || ex instanceof CoordinatorLoadInProgressException) {\n                            log.debug(\"The write event {} for the timer {} failed due to {}. Ignoring it because \" +\n                                \"the coordinator is not active.\", event.name, key, ex.getMessage());\n                            return null;\n                        }\n\n                        if (retry) {\n                            log.info(\"The write event {} for the timer {} failed due to {}. Rescheduling it. \",\n                                event.name, key, ex.getMessage());\n                            schedule(key, retryBackoff, TimeUnit.MILLISECONDS, true, retryBackoff, operation);\n                        } else {\n                            log.error(\"The write event {} for the timer {} failed due to {}. Ignoring it. \",\n                                event.name, key, ex.getMessage());\n                        }\n\n                        return null;\n                    });\n\n                    log.debug(\"Scheduling write event {} for timer {}.\", event.name, key);\n                    try {\n                        enqueueLast(event);\n                    } catch (NotCoordinatorException ex) {\n                        log.info(\"Failed to enqueue write event {} for timer {} because the runtime is closed. Ignoring it.\",\n                            event.name, key);\n                    }\n                }\n            };\n\n            log.debug(\"Registering timer {} with delay of {}ms.\", key, unit.toMillis(delay));\n            TimerTask prevTask = tasks.put(key, task);\n            if (prevTask != null) prevTask.cancel();\n\n            timer.add(task);\n        }\n\n        @Override\n        public void scheduleIfAbsent(\n            String key,\n            long delay,\n            TimeUnit unit,\n            boolean retry,\n            TimeoutOperation<Void, U> operation\n        ) {\n            if (!tasks.containsKey(key)) {\n                schedule(key, delay, unit, retry, 500, operation);\n            }\n        }\n\n        @Override\n        public void cancel(String key) {\n            TimerTask prevTask = tasks.remove(key);\n            if (prevTask != null) prevTask.cancel();\n        }\n\n        public void cancelAll() {\n            Iterator<Map.Entry<String, TimerTask>> iterator = tasks.entrySet().iterator();\n            while (iterator.hasNext()) {\n                iterator.next().getValue().cancel();\n                iterator.remove();\n            }\n        }\n\n        public int size() {\n            return tasks.size();\n        }\n    }\n\n    /**\n     * CoordinatorContext holds all the metadata around a coordinator state machine.\n     */\n    class CoordinatorContext {\n        /**\n         * The lock which protects all data in the context. Note that the context\n         * is never accessed concurrently, but it is accessed by multiple threads.\n         */\n        final ReentrantLock lock;\n\n        /**\n         * The topic partition backing the coordinator.\n         */\n        final TopicPartition tp;\n\n        /**\n         * The log context.\n         */\n        final LogContext logContext;\n\n        /**\n         * The deferred event queue used to park events waiting\n         * on records to be committed.\n         */\n        final DeferredEventQueue deferredEventQueue;\n\n        /**\n         * The coordinator timer.\n         */\n        final EventBasedCoordinatorTimer timer;\n\n        /**\n         * The current state.\n         */\n        CoordinatorState state;\n\n        /**\n         * The current epoch of the coordinator. This represents\n         * the epoch of the partition leader.\n         */\n        int epoch;\n\n        /**\n         * The state machine and the metadata that can be accessed by\n         * other threads.\n         */\n        SnapshottableCoordinator<S, U> coordinator;\n\n        /**\n         * The high watermark listener registered to all the partitions\n         * backing the coordinators.\n         */\n        HighWatermarkListener highWatermarklistener;\n\n        /**\n         * The buffer supplier used to write records to the log.\n         */\n        BufferSupplier bufferSupplier;\n\n        /**\n         * Constructor.\n         *\n         * @param tp The topic partition of the coordinator.\n         */\n        private CoordinatorContext(\n            TopicPartition tp\n        ) {\n            this.lock = new ReentrantLock();\n            this.tp = tp;\n            this.logContext = new LogContext(String.format(\"[%s topic=%s partition=%d] \",\n                logPrefix,\n                tp.topic(),\n                tp.partition()\n            ));\n            this.state = CoordinatorState.INITIAL;\n            this.epoch = -1;\n            this.deferredEventQueue = new DeferredEventQueue(logContext);\n            this.timer = new EventBasedCoordinatorTimer(tp, logContext);\n            this.bufferSupplier = new BufferSupplier.GrowableBufferSupplier();\n        }\n\n        /**\n         * Transitions to the new state.\n         *\n         * @param newState The new state.\n         */\n        private void transitionTo(\n            CoordinatorState newState\n        ) {\n            if (!newState.canTransitionFrom(state)) {\n                throw new IllegalStateException(\"Cannot transition from \" + state + \" to \" + newState);\n            }\n            CoordinatorState oldState = state;\n\n            log.debug(\"Transition from {} to {}.\", state, newState);\n            switch (newState) {\n                case LOADING:\n                    state = CoordinatorState.LOADING;\n                    SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n                    coordinator = new SnapshottableCoordinator<>(\n                        logContext,\n                        snapshotRegistry,\n                        coordinatorShardBuilderSupplier\n                            .get()\n                            .withLogContext(logContext)\n                            .withSnapshotRegistry(snapshotRegistry)\n                            .withTime(time)\n                            .withTimer(timer)\n                            .withCoordinatorMetrics(coordinatorMetrics)\n                            .withTopicPartition(tp)\n                            .build(),\n                        tp\n                    );\n                    break;\n\n                case ACTIVE:\n                    state = CoordinatorState.ACTIVE;\n                    highWatermarklistener = new HighWatermarkListener();\n                    partitionWriter.registerListener(tp, highWatermarklistener);\n                    coordinator.onLoaded(metadataImage);\n                    break;\n\n                case FAILED:\n                    state = CoordinatorState.FAILED;\n                    unload();\n                    break;\n\n                case CLOSED:\n                    state = CoordinatorState.CLOSED;\n                    unload();\n                    break;\n\n                default:\n                    throw new IllegalArgumentException(\"Transitioning to \" + newState + \" is not supported.\");\n            }\n\n            runtimeMetrics.recordPartitionStateChange(oldState, state);\n        }\n\n        /**\n         * Unloads the coordinator.\n         */\n        private void unload() {\n            if (highWatermarklistener != null) {\n                partitionWriter.deregisterListener(tp, highWatermarklistener);\n                highWatermarklistener = null;\n            }\n            timer.cancelAll();\n            deferredEventQueue.failAll(Errors.NOT_COORDINATOR.exception());\n            if (coordinator != null) {\n                coordinator.onUnloaded();\n            }\n            coordinator = null;\n        }\n    }\n\n    class OperationTimeout extends TimerTask {\n        private final TopicPartition tp;\n        private final DeferredEvent event;\n\n        public OperationTimeout(\n            TopicPartition tp,\n            DeferredEvent event,\n            long delayMs\n        ) {\n            super(delayMs);\n            this.event = event;\n            this.tp = tp;\n        }\n\n        @Override\n        public void run() {\n            String name = event.toString();\n            scheduleInternalOperation(\"OperationTimeout(name=\" + name + \", tp=\" + tp + \")\", tp,\n                () -> event.complete(new TimeoutException(name + \" timed out after \" + delayMs + \"ms\")));\n        }\n    }\n\n    /**\n     * A coordinator write operation.\n     *\n     * @param <S> The type of the coordinator state machine.\n     * @param <T> The type of the response.\n     * @param <U> The type of the records.\n     */\n    public interface CoordinatorWriteOperation<S, T, U> {\n        /**\n         * Generates the records needed to implement this coordinator write operation. In general,\n         * this operation should not modify the hard state of the coordinator. That modifications\n         * will happen later on, when the records generated by this function are applied to the\n         * coordinator.\n         *\n         * @param coordinator The coordinator state machine.\n         * @return A result containing a list of records and the RPC result.\n         * @throws KafkaException\n         */\n        CoordinatorResult<T, U> generateRecordsAndResult(S coordinator) throws KafkaException;\n    }\n\n    /**\n     * A coordinator event that modifies the coordinator state.\n     *\n     * @param <T> The type of the response.\n     */\n    class CoordinatorWriteEvent<T> implements CoordinatorEvent, DeferredEvent {\n        /**\n         * The topic partition that this write event is applied to.\n         */\n        final TopicPartition tp;\n\n        /**\n         * The operation name.\n         */\n        final String name;\n\n        /**\n         * The transactional id.\n         */\n        final String transactionalId;\n\n        /**\n         * The producer id.\n         */\n        final long producerId;\n\n        /**\n         * The producer epoch.\n         */\n        final short producerEpoch;\n\n        /**\n         * The verification guard.\n         */\n        final VerificationGuard verificationGuard;\n\n        /**\n         * The write operation to execute.\n         */\n        final CoordinatorWriteOperation<S, T, U> op;\n\n        /**\n         * The future that will be completed with the response\n         * generated by the write operation or an error.\n         */\n        final CompletableFuture<T> future;\n\n        /**\n         * Timeout value for the write operation.\n         */\n        final Duration writeTimeout;\n\n        /**\n         * The operation timeout.\n         */\n        private OperationTimeout operationTimeout = null;\n\n        /**\n         * The result of the write operation. It could be null\n         * if an exception is thrown before it is assigned.\n         */\n        CoordinatorResult<T, U> result;\n\n        /**\n         * The time this event was created.\n         */\n        private final long createdTimeMs;\n\n        /**\n         * Constructor.\n         *\n         * @param name                  The operation name.\n         * @param tp                    The topic partition that the operation is applied to.\n         * @param writeTimeout          The write operation timeout\n         * @param op                    The write operation.\n         */\n        CoordinatorWriteEvent(\n            String name,\n            TopicPartition tp,\n            Duration writeTimeout,\n            CoordinatorWriteOperation<S, T, U> op\n        ) {\n            this(\n                name,\n                tp,\n                null,\n                RecordBatch.NO_PRODUCER_ID,\n                RecordBatch.NO_PRODUCER_EPOCH,\n                VerificationGuard.SENTINEL,\n                writeTimeout,\n                op\n            );\n        }\n\n        /**\n         * Constructor.\n         *\n         * @param name                      The operation name.\n         * @param tp                        The topic partition that the operation is applied to.\n         * @param transactionalId           The transactional id.\n         * @param producerId                The producer id.\n         * @param producerEpoch             The producer epoch.\n         * @param verificationGuard         The verification guard.\n         * @param writeTimeout              The write operation timeout\n         * @param op                        The write operation.\n         */\n        CoordinatorWriteEvent(\n            String name,\n            TopicPartition tp,\n            String transactionalId,\n            long producerId,\n            short producerEpoch,\n            VerificationGuard verificationGuard,\n            Duration writeTimeout,\n            CoordinatorWriteOperation<S, T, U> op\n        ) {\n            this.tp = tp;\n            this.name = name;\n            this.op = op;\n            this.transactionalId = transactionalId;\n            this.producerId = producerId;\n            this.producerEpoch = producerEpoch;\n            this.verificationGuard = verificationGuard;\n            this.future = new CompletableFuture<>();\n            this.createdTimeMs = time.milliseconds();\n            this.writeTimeout = writeTimeout;\n        }\n\n        /**\n         * @return The key used by the CoordinatorEventProcessor to ensure\n         * that events with the same key are not processed concurrently.\n         */\n        @Override\n        public TopicPartition key() {\n            return tp;\n        }\n\n        /**\n         * Called by the CoordinatorEventProcessor when the event is executed.\n         */\n        @Override\n        public void run() {\n            try {\n                // Get the context of the coordinator or fail if the coordinator is not in active state.\n                withActiveContextOrThrow(tp, context -> {\n                    // Execute the operation.\n                    result = op.generateRecordsAndResult(context.coordinator.coordinator());\n\n                    if (result.records().isEmpty()) {\n                        // If the records are empty, it was a read operation after all. In this case,\n                        // the response can be returned directly iff there are no pending write operations;\n                        // otherwise, the read needs to wait on the last write operation to be completed.\n                        OptionalLong pendingOffset = context.deferredEventQueue.highestPendingOffset();\n                        if (pendingOffset.isPresent()) {\n                            context.deferredEventQueue.add(pendingOffset.getAsLong(), this);\n                        } else {\n                            complete(null);\n                        }\n                    } else {\n                        // If the records are not empty, first, they are applied to the state machine,\n                        // second, then are written to the partition/log, and finally, the response\n                        // is put into the deferred event queue.\n                        long prevLastWrittenOffset = context.coordinator.lastWrittenOffset();\n                        LogConfig logConfig = partitionWriter.config(tp);\n                        byte magic = logConfig.recordVersion().value;\n                        int maxBatchSize = logConfig.maxMessageSize();\n                        long currentTimeMs = time.milliseconds();\n                        ByteBuffer buffer = context.bufferSupplier.get(Math.min(MIN_BUFFER_SIZE, maxBatchSize));\n\n                        try {\n                            MemoryRecordsBuilder builder = new MemoryRecordsBuilder(\n                                buffer,\n                                magic,\n                                compression,\n                                TimestampType.CREATE_TIME,\n                                0L,\n                                currentTimeMs,\n                                producerId,\n                                producerEpoch,\n                                0,\n                                producerId != RecordBatch.NO_PRODUCER_ID,\n                                false,\n                                RecordBatch.NO_PARTITION_LEADER_EPOCH,\n                                maxBatchSize\n                            );\n\n                            // Apply the records to the state machine and add them to the batch.\n                            for (int i = 0; i < result.records().size(); i++) {\n                                U record = result.records().get(i);\n\n                                if (result.replayRecords()) {\n                                    // We compute the offset of the record based on the last written offset. The\n                                    // coordinator is the single writer to the underlying partition so we can\n                                    // deduce it like this.\n                                    context.coordinator.replay(\n                                        prevLastWrittenOffset + i,\n                                        producerId,\n                                        producerEpoch,\n                                        record\n                                    );\n                                }\n\n                                byte[] keyBytes = serializer.serializeKey(record);\n                                byte[] valBytes = serializer.serializeValue(record);\n\n                                if (builder.hasRoomFor(currentTimeMs, keyBytes, valBytes, EMPTY_HEADERS)) {\n                                    builder.append(\n                                        currentTimeMs,\n                                        keyBytes,\n                                        valBytes,\n                                        EMPTY_HEADERS\n                                    );\n                                } else {\n                                    throw new RecordTooLargeException(\"Message batch size is \" + builder.estimatedSizeInBytes() +\n                                        \" bytes in append to partition \" + tp + \" which exceeds the maximum \" +\n                                        \"configured size of \" + maxBatchSize + \".\");\n                                }\n                            }\n\n                            // Write the records to the log and update the last written\n                            // offset.\n                            long offset = partitionWriter.append(\n                                tp,\n                                verificationGuard,\n                                builder.build()\n                            );\n                            context.coordinator.updateLastWrittenOffset(offset);\n\n                            // Add the response to the deferred queue.\n                            if (!future.isDone()) {\n                                context.deferredEventQueue.add(offset, this);\n                                operationTimeout = new OperationTimeout(tp, this, writeTimeout.toMillis());\n                                timer.add(operationTimeout);\n                            } else {\n                                complete(null);\n                            }\n                        } catch (Throwable t) {\n                            context.coordinator.revertLastWrittenOffset(prevLastWrittenOffset);\n                            complete(t);\n                        } finally {\n                            context.bufferSupplier.release(buffer);\n                        }\n                    }\n                });\n            } catch (Throwable t) {\n                complete(t);\n            }\n        }\n\n        /**\n         * Completes the future with either the result of the write operation\n         * or the provided exception.\n         *\n         * @param exception The exception to complete the future with.\n         */\n        @Override\n        public void complete(Throwable exception) {\n            CompletableFuture<Void> appendFuture = result != null ? result.appendFuture() : null;\n\n            if (exception == null) {\n                if (appendFuture != null) result.appendFuture().complete(null);\n                future.complete(result.response());\n            } else {\n                if (appendFuture != null) result.appendFuture().completeExceptionally(exception);\n                future.completeExceptionally(exception);\n            }\n\n            if (operationTimeout != null) {\n                operationTimeout.cancel();\n                operationTimeout = null;\n            }\n        }\n\n        @Override\n        public long createdTimeMs() {\n            return this.createdTimeMs;\n        }\n\n        @Override\n        public String toString() {\n            return \"CoordinatorWriteEvent(name=\" + name + \")\";\n        }\n    }\n\n    /**\n     * A coordinator read operation.\n     *\n     * @param <S> The type of the coordinator state machine.\n     * @param <T> The type of the response.\n     */\n    public interface CoordinatorReadOperation<S, T> {\n        /**\n         * Generates the response to implement this coordinator read operation. A read\n         * operation received the last committed offset. It must use it to ensure that\n         * it does not read uncommitted data from the timeline data structures.\n         *\n         * @param state     The coordinator state machine.\n         * @param offset    The last committed offset.\n         * @return A response.\n         * @throws KafkaException\n         */\n        T generateResponse(S state, long offset) throws KafkaException;\n    }\n\n    /**\n     * A coordinator that reads the committed coordinator state.\n     *\n     * @param <T> The type of the response.\n     */\n    class CoordinatorReadEvent<T> implements CoordinatorEvent {\n        /**\n         * The topic partition that this read event is applied to.\n         */\n        final TopicPartition tp;\n\n        /**\n         * The operation name.\n         */\n        final String name;\n\n        /**\n         * The read operation to execute.\n         */\n        final CoordinatorReadOperation<S, T> op;\n\n        /**\n         * The future that will be completed with the response\n         * generated by the read operation or an error.\n         */\n        final CompletableFuture<T> future;\n\n        /**\n         * The result of the read operation. It could be null\n         * if an exception is thrown before it is assigned.\n         */\n        T response;\n\n        /**\n         * The time this event was created.\n         */\n        private final long createdTimeMs;\n\n        /**\n         * Constructor.\n         *\n         * @param name  The operation name.\n         * @param tp    The topic partition that the operation is applied to.\n         * @param op    The read operation.\n         */\n        CoordinatorReadEvent(\n            String name,\n            TopicPartition tp,\n            CoordinatorReadOperation<S, T> op\n        ) {\n            this.tp = tp;\n            this.name = name;\n            this.op = op;\n            this.future = new CompletableFuture<>();\n            this.createdTimeMs = time.milliseconds();\n        }\n\n        /**\n         * @return The key used by the CoordinatorEventProcessor to ensure\n         * that events with the same key are not processed concurrently.\n         */\n        @Override\n        public TopicPartition key() {\n            return tp;\n        }\n\n        /**\n         * Called by the CoordinatorEventProcessor when the event is executed.\n         */\n        @Override\n        public void run() {\n            try {\n                // Get the context of the coordinator or fail if the coordinator is not in active state.\n                withActiveContextOrThrow(tp, context -> {\n                    // Execute the read operation.\n                    response = op.generateResponse(\n                        context.coordinator.coordinator(),\n                        context.coordinator.lastCommittedOffset()\n                    );\n\n                    // The response can be completed immediately.\n                    complete(null);\n                });\n            } catch (Throwable t) {\n                complete(t);\n            }\n        }\n\n        /**\n         * Completes the future with either the result of the read operation\n         * or the provided exception.\n         *\n         * @param exception The exception to complete the future with.\n         */\n        @Override\n        public void complete(Throwable exception) {\n            if (exception == null) {\n                future.complete(response);\n            } else {\n                future.completeExceptionally(exception);\n            }\n        }\n\n        @Override\n        public long createdTimeMs() {\n            return this.createdTimeMs;\n        }\n\n        @Override\n        public String toString() {\n            return \"CoordinatorReadEvent(name=\" + name + \")\";\n        }\n    }\n\n    /**\n     * A coordinator event that applies and writes a transaction end marker.\n     */\n    class CoordinatorCompleteTransactionEvent implements CoordinatorEvent, DeferredEvent {\n        /**\n         * The topic partition that this write event is applied to.\n         */\n        final TopicPartition tp;\n\n        /**\n         * The operation name.\n         */\n        final String name;\n\n        /**\n         * The producer id.\n         */\n        final long producerId;\n\n        /**\n         * The producer epoch.\n         */\n        final short producerEpoch;\n\n        /**\n         * The coordinator epoch of the transaction coordinator.\n         */\n        final int coordinatorEpoch;\n\n        /**\n         * The transaction result.\n         */\n        final TransactionResult result;\n\n        /**\n         * Timeout value for the write operation.\n         */\n        final Duration writeTimeout;\n\n        /**\n         * The operation timeout.\n         */\n        private OperationTimeout operationTimeout = null;\n\n        /**\n         * The future that will be completed with the response\n         * generated by the write operation or an error.\n         */\n        final CompletableFuture<Void> future;\n\n        /**\n         * The time this event was created.\n         */\n        private final long createdTimeMs;\n\n        CoordinatorCompleteTransactionEvent(\n            String name,\n            TopicPartition tp,\n            long producerId,\n            short producerEpoch,\n            int coordinatorEpoch,\n            TransactionResult result,\n            Duration writeTimeout\n        ) {\n            this.name = name;\n            this.tp = tp;\n            this.producerId = producerId;\n            this.producerEpoch = producerEpoch;\n            this.coordinatorEpoch = coordinatorEpoch;\n            this.result = result;\n            this.writeTimeout = writeTimeout;\n            this.future = new CompletableFuture<>();\n            this.createdTimeMs = time.milliseconds();\n        }\n\n        /**\n         * @return The key used by the CoordinatorEventProcessor to ensure\n         * that events with the same key are not processed concurrently.\n         */\n        @Override\n        public TopicPartition key() {\n            return tp;\n        }\n\n        /**\n         * Called by the CoordinatorEventProcessor when the event is executed.\n         */\n        @Override\n        public void run() {\n            try {\n                withActiveContextOrThrow(tp, context -> {\n                    long prevLastWrittenOffset = context.coordinator.lastWrittenOffset();\n\n                    completeTransaction(context, prevLastWrittenOffset);\n                });\n            } catch (Throwable t) {\n                complete(t);\n            }\n        }\n\n        private void completeTransaction(CoordinatorContext context, long prevLastWrittenOffset) {\n            try {\n                context.coordinator.replayEndTransactionMarker(\n                    producerId,\n                    producerEpoch,\n                    result\n                );\n\n                long offset = partitionWriter.append(\n                    tp,\n                    VerificationGuard.SENTINEL,\n                    MemoryRecords.withEndTransactionMarker(\n                        time.milliseconds(),\n                        producerId,\n                        producerEpoch,\n                        new EndTransactionMarker(\n                            result == TransactionResult.COMMIT ? ControlRecordType.COMMIT : ControlRecordType.ABORT,\n                            coordinatorEpoch\n                        )\n                    )\n                );\n                context.coordinator.updateLastWrittenOffset(offset);\n\n                if (!future.isDone()) {\n                    context.deferredEventQueue.add(offset, this);\n                    operationTimeout = new OperationTimeout(tp, this, writeTimeout.toMillis());\n                    timer.add(operationTimeout);\n                } else {\n                    complete(null);\n                }\n            } catch (Throwable t) {\n                context.coordinator.revertLastWrittenOffset(prevLastWrittenOffset);\n                complete(t);\n            }\n        }\n\n        /**\n         * Completes the future with either the result of the write operation\n         * or the provided exception.\n         *\n         * @param exception The exception to complete the future with.\n         */\n        @Override\n        public void complete(Throwable exception) {\n            if (exception == null) {\n                future.complete(null);\n            } else {\n                future.completeExceptionally(exception);\n            }\n\n            if (operationTimeout != null) {\n                operationTimeout.cancel();\n                operationTimeout = null;\n            }\n        }\n\n        @Override\n        public long createdTimeMs() {\n            return createdTimeMs;\n        }\n\n        @Override\n        public String toString() {\n            return \"CoordinatorCompleteTransactionEvent(name=\" + name + \")\";\n        }\n    }\n\n    /**\n     * A coordinator internal event.\n     */\n    class CoordinatorInternalEvent implements CoordinatorEvent {\n        /**\n         * The topic partition that this internal event is applied to.\n         */\n        final TopicPartition tp;\n\n        /**\n         * The operation name.\n         */\n        final String name;\n\n        /**\n         * The internal operation to execute.\n         */\n        final Runnable op;\n\n        /**\n         * The time this event was created.\n         */\n        private final long createdTimeMs;\n\n        /**\n         * Constructor.\n         *\n         * @param name  The operation name.\n         * @param tp    The topic partition that the operation is applied to.\n         * @param op    The operation.\n         */\n        CoordinatorInternalEvent(\n            String name,\n            TopicPartition tp,\n            Runnable op\n        ) {\n            this.tp = tp;\n            this.name = name;\n            this.op = op;\n            this.createdTimeMs = time.milliseconds();\n        }\n\n        /**\n         * @return The key used by the CoordinatorEventProcessor to ensure\n         * that events with the same key are not processed concurrently.\n         */\n        @Override\n        public TopicPartition key() {\n            return tp;\n        }\n\n        /**\n         * Called by the CoordinatorEventProcessor when the event is executed.\n         */\n        @Override\n        public void run() {\n            try {\n                op.run();\n            } catch (Throwable t) {\n                complete(t);\n            }\n        }\n\n        /**\n         * Logs any exceptions thrown while the event is executed.\n         *\n         * @param exception The exception.\n         */\n        @Override\n        public void complete(Throwable exception) {\n            if (exception != null) {\n                log.error(\"Execution of {} failed due to {}.\", name, exception.getMessage(), exception);\n            }\n        }\n\n        @Override\n        public long createdTimeMs() {\n            return this.createdTimeMs;\n        }\n\n        @Override\n        public String toString() {\n            return \"InternalEvent(name=\" + name + \")\";\n        }\n    }\n\n    /**\n     * Partition listener to be notified when the high watermark of the partitions\n     * backing the coordinator are updated.\n     */\n    class HighWatermarkListener implements PartitionWriter.Listener {\n\n        private static final long NO_OFFSET = -1L;\n\n        /**\n         * The atomic long is used to store the last and unprocessed high watermark\n         * received from the partition. The atomic value is replaced by -1L when\n         * the high watermark is taken to update the context.\n         */\n        private final AtomicLong lastHighWatermark = new AtomicLong(NO_OFFSET);\n\n        /**\n         * @return The last high watermark received or NO_OFFSET if none is pending.\n         */\n        public long lastHighWatermark() {\n            return lastHighWatermark.get();\n        }\n\n        /**\n         * Updates the high watermark of the corresponding coordinator.\n         *\n         * @param tp        The topic partition.\n         * @param offset    The new high watermark.\n         */\n        @Override\n        public void onHighWatermarkUpdated(\n            TopicPartition tp,\n            long offset\n        ) {\n            log.debug(\"High watermark of {} incremented to {}.\", tp, offset);\n            if (lastHighWatermark.getAndSet(offset) == NO_OFFSET) {\n                // An event to apply the new high watermark is pushed to the front of the\n                // queue only if the previous value was -1L. If it was not, it means that\n                // there is already an event waiting to process the last value.\n                enqueueFirst(new CoordinatorInternalEvent(\"HighWatermarkUpdate\", tp, () -> {\n                    long newHighWatermark = lastHighWatermark.getAndSet(NO_OFFSET);\n\n                    CoordinatorContext context = coordinators.get(tp);\n                    if (context != null) {\n                        context.lock.lock();\n                        try {\n                            if (context.state == CoordinatorState.ACTIVE) {\n                                // The updated high watermark can be applied to the coordinator only if the coordinator\n                                // exists and is in the active state.\n                                log.debug(\"Updating high watermark of {} to {}.\", tp, newHighWatermark);\n                                context.coordinator.updateLastCommittedOffset(newHighWatermark);\n                                context.deferredEventQueue.completeUpTo(newHighWatermark);\n                                coordinatorMetrics.onUpdateLastCommittedOffset(tp, newHighWatermark);\n                            } else {\n                                log.debug(\"Ignored high watermark updated for {} to {} because the coordinator is not active.\",\n                                    tp, newHighWatermark);\n                            }\n                        } finally {\n                            context.lock.unlock();\n                        }\n                    } else {\n                        log.debug(\"Ignored high watermark updated for {} to {} because the coordinator does not exist.\",\n                            tp, newHighWatermark);\n                    }\n                }));\n            }\n        }\n    }\n\n    /**\n     * 16KB. Used for initial buffer size for write operations.\n     */\n    static final int MIN_BUFFER_SIZE = 16384;\n\n    /**\n     * The log prefix.\n     */\n    private final String logPrefix;\n\n    /**\n     * The log context.\n     */\n    private final LogContext logContext;\n\n    /**\n     * The logger.\n     */\n    private final Logger log;\n\n    /**\n     * The system time.\n     */\n    private final Time time;\n\n    /**\n     * The system timer.\n     */\n    private final Timer timer;\n\n    /**\n     * The write operation timeout\n     */\n    private final Duration defaultWriteTimeout;\n\n    /**\n     * The coordinators keyed by topic partition.\n     */\n    private final ConcurrentHashMap<TopicPartition, CoordinatorContext> coordinators;\n\n    /**\n     * The event processor used by the runtime.\n     */\n    private final CoordinatorEventProcessor processor;\n\n    /**\n     * The partition writer used by the runtime to persist records.\n     */\n    private final PartitionWriter partitionWriter;\n\n    /**\n     * The coordinator loaded used by the runtime.\n     */\n    private final CoordinatorLoader<U> loader;\n\n    /**\n     * The coordinator state machine builder used by the runtime\n     * to instantiate a coordinator.\n     */\n    private final CoordinatorShardBuilderSupplier<S, U> coordinatorShardBuilderSupplier;\n\n    /**\n     * The coordinator runtime metrics.\n     */\n    private final CoordinatorRuntimeMetrics runtimeMetrics;\n\n    /**\n     * The coordinator metrics.\n     */\n    private final CoordinatorMetrics coordinatorMetrics;\n\n    /**\n     * The serializer used to serialize records.\n     */\n    private final Serializer<U> serializer;\n\n    /**\n     * The compression codec used when writing records.\n     */\n    private final Compression compression;\n\n    /**\n     * Atomic boolean indicating whether the runtime is running.\n     */\n    private final AtomicBoolean isRunning = new AtomicBoolean(true);\n\n    /**\n     * The latest known metadata image.\n     */\n    private volatile MetadataImage metadataImage = MetadataImage.EMPTY;\n\n    /**\n     * Constructor.\n     *\n     * @param logPrefix                         The log prefix.\n     * @param logContext                        The log context.\n     * @param processor                         The event processor.\n     * @param partitionWriter                   The partition writer.\n     * @param loader                            The coordinator loader.\n     * @param coordinatorShardBuilderSupplier   The coordinator builder.\n     * @param time                              The system time.\n     * @param timer                             The system timer.\n     * @param defaultWriteTimeout               The write operation timeout.\n     * @param runtimeMetrics                    The runtime metrics.\n     * @param coordinatorMetrics                The coordinator metrics.\n     * @param serializer                        The serializer.\n     * @param compression                       The compression codec.\n     */\n    private CoordinatorRuntime(\n        String logPrefix,\n        LogContext logContext,\n        CoordinatorEventProcessor processor,\n        PartitionWriter partitionWriter,\n        CoordinatorLoader<U> loader,\n        CoordinatorShardBuilderSupplier<S, U> coordinatorShardBuilderSupplier,\n        Time time,\n        Timer timer,\n        Duration defaultWriteTimeout,\n        CoordinatorRuntimeMetrics runtimeMetrics,\n        CoordinatorMetrics coordinatorMetrics,\n        Serializer<U> serializer,\n        Compression compression\n    ) {\n        this.logPrefix = logPrefix;\n        this.logContext = logContext;\n        this.log = logContext.logger(CoordinatorRuntime.class);\n        this.time = time;\n        this.timer = timer;\n        this.defaultWriteTimeout = defaultWriteTimeout;\n        this.coordinators = new ConcurrentHashMap<>();\n        this.processor = processor;\n        this.partitionWriter = partitionWriter;\n        this.loader = loader;\n        this.coordinatorShardBuilderSupplier = coordinatorShardBuilderSupplier;\n        this.runtimeMetrics = runtimeMetrics;\n        this.coordinatorMetrics = coordinatorMetrics;\n        this.serializer = serializer;\n        this.compression = compression;\n    }\n\n    /**\n     * Throws a NotCoordinatorException exception if the runtime is not\n     * running.\n     */\n    private void throwIfNotRunning() {\n        if (!isRunning.get()) {\n            throw Errors.NOT_COORDINATOR.exception();\n        }\n    }\n\n    /**\n     * Enqueues a new event at the end of the processing queue.\n     *\n     * @param event The event.\n     * @throws NotCoordinatorException If the event processor is closed.\n     */\n    private void enqueueLast(CoordinatorEvent event) {\n        try {\n            processor.enqueueLast(event);\n        } catch (RejectedExecutionException ex) {\n            throw new NotCoordinatorException(\"Can't accept an event because the processor is closed\", ex);\n        }\n    }\n\n    /**\n     * Enqueues a new event at the front of the processing queue.\n     *\n     * @param event The event.\n     * @throws NotCoordinatorException If the event processor is closed.\n     */\n    private void enqueueFirst(CoordinatorEvent event) {\n        try {\n            processor.enqueueFirst(event);\n        } catch (RejectedExecutionException ex) {\n            throw new NotCoordinatorException(\"Can't accept an event because the processor is closed\", ex);\n        }\n    }\n\n    /**\n     * @return The coordinator context or a new context if it does not exist.\n     * Package private for testing.\n     */\n    CoordinatorContext maybeCreateContext(TopicPartition tp) {\n        return coordinators.computeIfAbsent(tp, CoordinatorContext::new);\n    }\n\n    /**\n     * @return The coordinator context or thrown an exception if it does\n     * not exist.\n     * @throws NotCoordinatorException\n     * Package private for testing.\n     */\n    CoordinatorContext contextOrThrow(TopicPartition tp) throws NotCoordinatorException {\n        CoordinatorContext context = coordinators.get(tp);\n\n        if (context == null) {\n            throw Errors.NOT_COORDINATOR.exception();\n        } else {\n            return context;\n        }\n    }\n\n    /**\n     * Calls the provided function with the context iff the context is active; throws\n     * an exception otherwise. This method ensures that the context lock is acquired\n     * before calling the function and releases afterwards.\n     *\n     * @param tp    The topic partition.\n     * @param func  The function that will receive the context.\n     * @throws NotCoordinatorException\n     * @throws CoordinatorLoadInProgressException\n     */\n    private void withActiveContextOrThrow(\n        TopicPartition tp,\n        Consumer<CoordinatorContext> func\n    ) throws NotCoordinatorException, CoordinatorLoadInProgressException {\n        CoordinatorContext context = contextOrThrow(tp);\n\n        try {\n            context.lock.lock();\n            if (context.state == CoordinatorState.ACTIVE) {\n                func.accept(context);\n            } else if (context.state == CoordinatorState.LOADING) {\n                throw Errors.COORDINATOR_LOAD_IN_PROGRESS.exception();\n            } else {\n                throw Errors.NOT_COORDINATOR.exception();\n            }\n        } finally {\n            context.lock.unlock();\n        }\n    }\n\n    /**\n     * Schedules a write operation.\n     *\n     * @param name      The name of the write operation.\n     * @param tp        The address of the coordinator (aka its topic-partitions).\n     * @param timeout   The write operation timeout.\n     * @param op        The write operation.\n     *\n     * @return A future that will be completed with the result of the write operation\n     * when the operation is completed or an exception if the write operation failed.\n     *\n     * @param <T> The type of the result.\n     */\n    public <T> CompletableFuture<T> scheduleWriteOperation(\n        String name,\n        TopicPartition tp,\n        Duration timeout,\n        CoordinatorWriteOperation<S, T, U> op\n    ) {\n        throwIfNotRunning();\n        log.debug(\"Scheduled execution of write operation {}.\", name);\n        CoordinatorWriteEvent<T> event = new CoordinatorWriteEvent<>(name, tp, timeout, op);\n        enqueueLast(event);\n        return event.future;\n    }\n\n    /**\n     * Schedule a write operation for each coordinator.\n     *\n     * @param name      The name of the write operation.\n     * @param timeout   The write operation timeout.\n     * @param op        The write operation.\n     *\n     * @return A list of futures where each future will be completed with the result of the write operation\n     * when the operation is completed or an exception if the write operation failed.\n     *\n     * @param <T> The type of the result.\n     */\n    public <T> List<CompletableFuture<T>> scheduleWriteAllOperation(\n        String name,\n        Duration timeout,\n        CoordinatorWriteOperation<S, T, U> op\n    ) {\n        throwIfNotRunning();\n        log.debug(\"Scheduled execution of write all operation {}.\", name);\n        return coordinators\n            .keySet()\n            .stream()\n            .map(tp -> scheduleWriteOperation(name, tp, timeout, op))\n            .collect(Collectors.toList());\n    }\n\n    /**\n     * Schedules a transactional write operation.\n     *\n     * @param name              The name of the write operation.\n     * @param tp                The address of the coordinator (aka its topic-partitions).\n     * @param transactionalId   The transactional id.\n     * @param producerId        The producer id.\n     * @param producerEpoch     The producer epoch.\n     * @param timeout           The write operation timeout.\n     * @param op                The write operation.\n     * @param apiVersion        The Version of the Txn_Offset_Commit request\n     *\n     * @return A future that will be completed with the result of the write operation\n     * when the operation is completed or an exception if the write operation failed.\n     *\n     * @param <T> The type of the result.\n     */\n    public <T> CompletableFuture<T> scheduleTransactionalWriteOperation(\n        String name,\n        TopicPartition tp,\n        String transactionalId,\n        long producerId,\n        short producerEpoch,\n        Duration timeout,\n        CoordinatorWriteOperation<S, T, U> op,\n        Short apiVersion\n    ) {\n        throwIfNotRunning();\n        log.debug(\"Scheduled execution of transactional write operation {}.\", name);\n        return partitionWriter.maybeStartTransactionVerification(\n            tp,\n            transactionalId,\n            producerId,\n            producerEpoch,\n            apiVersion\n        ).thenCompose(verificationGuard -> {\n            CoordinatorWriteEvent<T> event = new CoordinatorWriteEvent<>(\n                name,\n                tp,\n                transactionalId,\n                producerId,\n                producerEpoch,\n                verificationGuard,\n                timeout,\n                op\n            );\n            enqueueLast(event);\n            return event.future;\n        });\n    }\n\n    /**\n     * Schedules the transaction completion.\n     *\n     * @param name              The name of the operation.\n     * @param tp                The address of the coordinator (aka its topic-partitions).\n     * @param producerId        The producer id.\n     * @param producerEpoch     The producer epoch.\n     * @param coordinatorEpoch  The epoch of the transaction coordinator.\n     * @param result            The transaction result.\n     *\n     * @return A future that will be completed with null when the operation is\n     * completed or an exception if the operation failed.\n     */\n    public CompletableFuture<Void> scheduleTransactionCompletion(\n        String name,\n        TopicPartition tp,\n        long producerId,\n        short producerEpoch,\n        int coordinatorEpoch,\n        TransactionResult result,\n        Duration timeout\n    ) {\n        throwIfNotRunning();\n        log.debug(\"Scheduled execution of transaction completion for {} with producer id={}, producer epoch={}, \" +\n            \"coordinator epoch={} and transaction result={}.\", tp, producerId, producerEpoch, coordinatorEpoch, result);\n        CoordinatorCompleteTransactionEvent event = new CoordinatorCompleteTransactionEvent(\n            name,\n            tp,\n            producerId,\n            producerEpoch,\n            coordinatorEpoch,\n            result,\n            timeout\n        );\n        enqueueLast(event);\n        return event.future;\n    }\n\n    /**\n     * Schedules a read operation.\n     *\n     * @param name  The name of the read operation.\n     * @param tp    The address of the coordinator (aka its topic-partitions).\n     * @param op    The read operation.\n     *\n     * @return A future that will be completed with the result of the read operation\n     * when the operation is completed or an exception if the read operation failed.\n     *\n     * @param <T> The type of the result.\n     */\n    public <T> CompletableFuture<T> scheduleReadOperation(\n        String name,\n        TopicPartition tp,\n        CoordinatorReadOperation<S, T> op\n    ) {\n        throwIfNotRunning();\n        log.debug(\"Scheduled execution of read operation {}.\", name);\n        CoordinatorReadEvent<T> event = new CoordinatorReadEvent<>(name, tp, op);\n        enqueueLast(event);\n        return event.future;\n    }\n\n    /**\n     * Schedules a read operation for each coordinator.\n     *\n     * @param name  The name of the read operation.\n     * @param op    The read operation.\n     *\n     * @return A list of futures where each future will be completed with the result of the read operation\n     * when the operation is completed or an exception if the read operation failed.\n     *\n     * @param <T> The type of the result.\n     */\n    public <T> List<CompletableFuture<T>> scheduleReadAllOperation(\n        String name,\n        CoordinatorReadOperation<S, T> op\n    ) {\n        throwIfNotRunning();\n        log.debug(\"Scheduled execution of read all operation {}.\", name);\n        return coordinators\n            .keySet()\n            .stream()\n            .map(tp -> scheduleReadOperation(name, tp, op))\n            .collect(Collectors.toList());\n    }\n\n    /**\n     * Schedules an internal event.\n     *\n     * @param name  The name of the write operation.\n     * @param tp    The address of the coordinator (aka its topic-partitions).\n     * @param op    The operation.\n     */\n    private void scheduleInternalOperation(\n        String name,\n        TopicPartition tp,\n        Runnable op\n    ) {\n        log.debug(\"Scheduled execution of internal operation {}.\", name);\n        enqueueLast(new CoordinatorInternalEvent(name, tp, op));\n    }\n\n    /**\n     * Schedules the loading of a coordinator. This is called when the broker is elected as\n     * the leader for a partition.\n     *\n     * @param tp                The topic partition of the coordinator. Records from this\n     *                          partitions will be read and applied to the coordinator.\n     * @param partitionEpoch    The epoch of the partition.\n     */\n    public void scheduleLoadOperation(\n        TopicPartition tp,\n        int partitionEpoch\n    ) {\n        throwIfNotRunning();\n        log.info(\"Scheduling loading of metadata from {} with epoch {}\", tp, partitionEpoch);\n\n        // Touch the state to make the runtime immediately aware of the new coordinator.\n        maybeCreateContext(tp);\n\n        scheduleInternalOperation(\"Load(tp=\" + tp + \", epoch=\" + partitionEpoch + \")\", tp, () -> {\n            // The context is re-created if it does not exist.\n            CoordinatorContext context = maybeCreateContext(tp);\n\n            context.lock.lock();\n            try {\n                if (context.epoch < partitionEpoch) {\n                    context.epoch = partitionEpoch;\n\n                    switch (context.state) {\n                        case FAILED:\n                        case INITIAL:\n                            context.transitionTo(CoordinatorState.LOADING);\n                            loader.load(tp, context.coordinator).whenComplete((summary, exception) -> {\n                                scheduleInternalOperation(\"CompleteLoad(tp=\" + tp + \", epoch=\" + partitionEpoch + \")\", tp, () -> {\n                                    CoordinatorContext ctx = coordinators.get(tp);\n                                    if (ctx != null)  {\n                                        if (ctx.state != CoordinatorState.LOADING) {\n                                            log.info(\"Ignored load completion from {} because context is in {} state.\",\n                                                ctx.tp, ctx.state);\n                                            return;\n                                        }\n                                        try {\n                                            if (exception != null) throw exception;\n                                            ctx.transitionTo(CoordinatorState.ACTIVE);\n                                            if (summary != null) {\n                                                runtimeMetrics.recordPartitionLoadSensor(summary.startTimeMs(), summary.endTimeMs());\n                                                log.info(\"Finished loading of metadata from {} with epoch {} in {}ms where {}ms \" +\n                                                         \"was spent in the scheduler. Loaded {} records which total to {} bytes.\",\n                                                    tp, partitionEpoch, summary.endTimeMs() - summary.startTimeMs(),\n                                                    summary.schedulerQueueTimeMs(), summary.numRecords(), summary.numBytes());\n                                            }\n                                        } catch (Throwable ex) {\n                                            log.error(\"Failed to load metadata from {} with epoch {} due to {}.\",\n                                                tp, partitionEpoch, ex.toString());\n                                            ctx.transitionTo(CoordinatorState.FAILED);\n                                        }\n                                    } else {\n                                        log.debug(\"Failed to complete the loading of metadata for {} in epoch {} since the coordinator does not exist.\",\n                                            tp, partitionEpoch);\n                                    }\n                                });\n                            });\n                            break;\n\n                        case LOADING:\n                            log.info(\"The coordinator {} is already loading metadata.\", tp);\n                            break;\n\n                        case ACTIVE:\n                            log.info(\"The coordinator {} is already active.\", tp);\n                            break;\n\n                        default:\n                            log.error(\"Cannot load coordinator {} in state {}.\", tp, context.state);\n                    }\n                } else {\n                    log.info(\"Ignored loading metadata from {} since current epoch {} is larger than or equals to {}.\",\n                        context.tp, context.epoch, partitionEpoch);\n                }\n            } finally {\n                context.lock.unlock();\n            }\n        });\n    }\n\n    /**\n     * Schedules the unloading of a coordinator. This is called when the broker is not the\n     * leader anymore.\n     *\n     * @param tp                The topic partition of the coordinator.\n     * @param partitionEpoch    The partition epoch as an optional value.\n     *                          An empty value means that the topic was deleted.\n     */\n    public void scheduleUnloadOperation(\n        TopicPartition tp,\n        OptionalInt partitionEpoch\n    ) {\n        throwIfNotRunning();\n        log.info(\"Scheduling unloading of metadata for {} with epoch {}\", tp, partitionEpoch);\n\n        scheduleInternalOperation(\"UnloadCoordinator(tp=\" + tp + \", epoch=\" + partitionEpoch + \")\", tp, () -> {\n            CoordinatorContext context = coordinators.get(tp);\n            if (context != null) {\n                context.lock.lock();\n                try {\n                    if (!partitionEpoch.isPresent() || context.epoch < partitionEpoch.getAsInt()) {\n                        log.info(\"Started unloading metadata for {} with epoch {}.\", tp, partitionEpoch);\n                        context.transitionTo(CoordinatorState.CLOSED);\n                        coordinators.remove(tp, context);\n                        log.info(\"Finished unloading metadata for {} with epoch {}.\", tp, partitionEpoch);\n                    } else {\n                        log.info(\"Ignored unloading metadata for {} in epoch {} since current epoch is {}.\",\n                            tp, partitionEpoch, context.epoch);\n                    }\n                } finally {\n                    context.lock.unlock();\n                }\n            } else {\n                log.info(\"Ignored unloading metadata for {} in epoch {} since metadata was never loaded.\",\n                    tp, partitionEpoch);\n            }\n        });\n    }\n\n    /**\n     * A new metadata image is available.\n     *\n     * @param newImage  The new metadata image.\n     * @param delta     The metadata delta.\n     */\n    public void onNewMetadataImage(\n        MetadataImage newImage,\n        MetadataDelta delta\n    ) {\n        throwIfNotRunning();\n        log.debug(\"Scheduling applying of a new metadata image with offset {}.\", newImage.offset());\n\n        // Update global image.\n        metadataImage = newImage;\n\n        // Push an event for each coordinator.\n        coordinators.keySet().forEach(tp -> {\n            scheduleInternalOperation(\"UpdateImage(tp=\" + tp + \", offset=\" + newImage.offset() + \")\", tp, () -> {\n                CoordinatorContext context = coordinators.get(tp);\n                if (context != null) {\n                    context.lock.lock();\n                    try {\n                        if (context.state == CoordinatorState.ACTIVE) {\n                            // The new image can be applied to the coordinator only if the coordinator\n                            // exists and is in the active state.\n                            log.debug(\"Applying new metadata image with offset {} to {}.\", newImage.offset(), tp);\n                            context.coordinator.onNewMetadataImage(newImage, delta);\n                        } else {\n                            log.debug(\"Ignored new metadata image with offset {} for {} because the coordinator is not active.\",\n                                newImage.offset(), tp);\n                        }\n                    } finally {\n                        context.lock.unlock();\n                    }\n                } else {\n                    log.debug(\"Ignored new metadata image with offset {} for {} because the coordinator does not exist.\",\n                        newImage.offset(), tp);\n                }\n            });\n        });\n    }\n\n    /**\n     * Closes the runtime. This closes all the coordinators currently registered\n     * in the runtime.\n     *\n     * @throws Exception\n     */\n    public void close() throws Exception {\n        if (!isRunning.compareAndSet(true, false)) {\n            log.warn(\"Coordinator runtime is already shutting down.\");\n            return;\n        }\n\n        log.info(\"Closing coordinator runtime.\");\n        Utils.closeQuietly(loader, \"loader\");\n        Utils.closeQuietly(timer, \"timer\");\n        // This close the processor, drain all the pending events and\n        // reject any new events.\n        Utils.closeQuietly(processor, \"event processor\");\n        // Unload all the coordinators.\n        coordinators.forEach((tp, context) -> {\n            context.lock.lock();\n            try {\n                context.transitionTo(CoordinatorState.CLOSED);\n            } finally {\n                context.lock.unlock();\n            }\n        });\n        coordinators.clear();\n        Utils.closeQuietly(runtimeMetrics, \"runtime metrics\");\n        log.info(\"Coordinator runtime closed.\");\n    }\n}",
                "methodCount": 81
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "processingTime": {
                "llmResponseTime": -1,
                "pluginProcessingTime": -1,
                "totalTime": 8351
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "withLogPrefix",
                            "method_signature": "public withLogPrefix(String logPrefix)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withLogContext",
                            "method_signature": "public withLogContext(LogContext logContext)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withEventProcessor",
                            "method_signature": "public withEventProcessor(CoordinatorEventProcessor eventProcessor)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withPartitionWriter",
                            "method_signature": "public withPartitionWriter(PartitionWriter partitionWriter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withLoader",
                            "method_signature": "public withLoader(CoordinatorLoader<U> loader)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withCoordinatorShardBuilderSupplier",
                            "method_signature": "public withCoordinatorShardBuilderSupplier(CoordinatorShardBuilderSupplier<S, U> coordinatorShardBuilderSupplier)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withTime",
                            "method_signature": "public withTime(Time time)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withTimer",
                            "method_signature": "public withTimer(Timer timer)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withDefaultWriteTimeOut",
                            "method_signature": "public withDefaultWriteTimeOut(Duration defaultWriteTimeout)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withCoordinatorRuntimeMetrics",
                            "method_signature": "public withCoordinatorRuntimeMetrics(CoordinatorRuntimeMetrics runtimeMetrics)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withCoordinatorMetrics",
                            "method_signature": "public withCoordinatorMetrics(CoordinatorMetrics coordinatorMetrics)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withSerializer",
                            "method_signature": "public withSerializer(Serializer<U> serializer)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withCompression",
                            "method_signature": "public withCompression(Compression compression)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "cancelAll",
                            "method_signature": "public cancelAll()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "size",
                            "method_signature": "public size()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionTo",
                            "method_signature": "private transitionTo(\n            CoordinatorState newState\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "unload",
                            "method_signature": "private unload()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeTransaction",
                            "method_signature": "private completeTransaction(CoordinatorContext context, long prevLastWrittenOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "lastHighWatermark",
                            "method_signature": "public lastHighWatermark()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "throwIfNotRunning",
                            "method_signature": "private throwIfNotRunning()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "enqueueLast",
                            "method_signature": "private enqueueLast(CoordinatorEvent event)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "enqueueFirst",
                            "method_signature": "private enqueueFirst(CoordinatorEvent event)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeCreateContext",
                            "method_signature": " maybeCreateContext(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "contextOrThrow",
                            "method_signature": " contextOrThrow(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withActiveContextOrThrow",
                            "method_signature": "private withActiveContextOrThrow(\n        TopicPartition tp,\n        Consumer<CoordinatorContext> func\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "scheduleWriteOperation",
                            "method_signature": "public scheduleWriteOperation(\n        String name,\n        TopicPartition tp,\n        Duration timeout,\n        CoordinatorWriteOperation<S, T, U> op\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "scheduleWriteAllOperation",
                            "method_signature": "public scheduleWriteAllOperation(\n        String name,\n        Duration timeout,\n        CoordinatorWriteOperation<S, T, U> op\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "scheduleTransactionalWriteOperation",
                            "method_signature": "public scheduleTransactionalWriteOperation(\n        String name,\n        TopicPartition tp,\n        String transactionalId,\n        long producerId,\n        short producerEpoch,\n        Duration timeout,\n        CoordinatorWriteOperation<S, T, U> op,\n        Short apiVersion\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "scheduleTransactionCompletion",
                            "method_signature": "public scheduleTransactionCompletion(\n        String name,\n        TopicPartition tp,\n        long producerId,\n        short producerEpoch,\n        int coordinatorEpoch,\n        TransactionResult result,\n        Duration timeout\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "scheduleReadOperation",
                            "method_signature": "public scheduleReadOperation(\n        String name,\n        TopicPartition tp,\n        CoordinatorReadOperation<S, T> op\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "scheduleReadAllOperation",
                            "method_signature": "public scheduleReadAllOperation(\n        String name,\n        CoordinatorReadOperation<S, T> op\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "scheduleInternalOperation",
                            "method_signature": "private scheduleInternalOperation(\n        String name,\n        TopicPartition tp,\n        Runnable op\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "scheduleLoadOperation",
                            "method_signature": "public scheduleLoadOperation(\n        TopicPartition tp,\n        int partitionEpoch\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "scheduleUnloadOperation",
                            "method_signature": "public scheduleUnloadOperation(\n        TopicPartition tp,\n        OptionalInt partitionEpoch\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onNewMetadataImage",
                            "method_signature": "public onNewMetadataImage(\n        MetadataImage newImage,\n        MetadataDelta delta\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "public close()",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withCoordinatorShardBuilderSupplier",
                            "method_signature": "public withCoordinatorShardBuilderSupplier(CoordinatorShardBuilderSupplier<S, U> coordinatorShardBuilderSupplier)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "size",
                            "method_signature": "public size()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withEventProcessor",
                            "method_signature": "public withEventProcessor(CoordinatorEventProcessor eventProcessor)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withTime",
                            "method_signature": "public withTime(Time time)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withLogPrefix",
                            "method_signature": "public withLogPrefix(String logPrefix)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withPartitionWriter",
                            "method_signature": "public withPartitionWriter(PartitionWriter partitionWriter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withLoader",
                            "method_signature": "public withLoader(CoordinatorLoader<U> loader)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withDefaultWriteTimeOut",
                            "method_signature": "public withDefaultWriteTimeOut(Duration defaultWriteTimeout)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withCoordinatorRuntimeMetrics",
                            "method_signature": "public withCoordinatorRuntimeMetrics(CoordinatorRuntimeMetrics runtimeMetrics)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withCoordinatorMetrics",
                            "method_signature": "public withCoordinatorMetrics(CoordinatorMetrics coordinatorMetrics)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withSerializer",
                            "method_signature": "public withSerializer(Serializer<U> serializer)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withCompression",
                            "method_signature": "public withCompression(Compression compression)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withTimer",
                            "method_signature": "public withTimer(Timer timer)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withLogContext",
                            "method_signature": "public withLogContext(LogContext logContext)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withCoordinatorShardBuilderSupplier",
                            "method_signature": "public withCoordinatorShardBuilderSupplier(CoordinatorShardBuilderSupplier<S, U> coordinatorShardBuilderSupplier)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "size",
                            "method_signature": "public size()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withEventProcessor",
                            "method_signature": "public withEventProcessor(CoordinatorEventProcessor eventProcessor)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withTime",
                            "method_signature": "public withTime(Time time)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withLogPrefix",
                            "method_signature": "public withLogPrefix(String logPrefix)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withPartitionWriter",
                            "method_signature": "public withPartitionWriter(PartitionWriter partitionWriter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withLoader",
                            "method_signature": "public withLoader(CoordinatorLoader<U> loader)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withDefaultWriteTimeOut",
                            "method_signature": "public withDefaultWriteTimeOut(Duration defaultWriteTimeout)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withCoordinatorRuntimeMetrics",
                            "method_signature": "public withCoordinatorRuntimeMetrics(CoordinatorRuntimeMetrics runtimeMetrics)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withCoordinatorMetrics",
                            "method_signature": "public withCoordinatorMetrics(CoordinatorMetrics coordinatorMetrics)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withSerializer",
                            "method_signature": "public withSerializer(Serializer<U> serializer)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withCompression",
                            "method_signature": "public withCompression(Compression compression)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withTimer",
                            "method_signature": "public withTimer(Timer timer)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withLogContext",
                            "method_signature": "public withLogContext(LogContext logContext)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "public build()": {
                    "first": {
                        "method_name": "build",
                        "method_signature": "public build()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.1348384824975538
                },
                "public withCoordinatorShardBuilderSupplier(CoordinatorShardBuilderSupplier<S, U> coordinatorShardBuilderSupplier)": {
                    "first": {
                        "method_name": "withCoordinatorShardBuilderSupplier",
                        "method_signature": "public withCoordinatorShardBuilderSupplier(CoordinatorShardBuilderSupplier<S, U> coordinatorShardBuilderSupplier)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2429630163571894
                },
                "public size()": {
                    "first": {
                        "method_name": "size",
                        "method_signature": "public size()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.24716923082631437
                },
                "public withEventProcessor(CoordinatorEventProcessor eventProcessor)": {
                    "first": {
                        "method_name": "withEventProcessor",
                        "method_signature": "public withEventProcessor(CoordinatorEventProcessor eventProcessor)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2586532560062145
                },
                "public withTime(Time time)": {
                    "first": {
                        "method_name": "withTime",
                        "method_signature": "public withTime(Time time)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.25941715108507585
                },
                "public withLogPrefix(String logPrefix)": {
                    "first": {
                        "method_name": "withLogPrefix",
                        "method_signature": "public withLogPrefix(String logPrefix)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.25979910062687334
                },
                "public withPartitionWriter(PartitionWriter partitionWriter)": {
                    "first": {
                        "method_name": "withPartitionWriter",
                        "method_signature": "public withPartitionWriter(PartitionWriter partitionWriter)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.25979910062687334
                },
                "public withLoader(CoordinatorLoader<U> loader)": {
                    "first": {
                        "method_name": "withLoader",
                        "method_signature": "public withLoader(CoordinatorLoader<U> loader)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.25979910062687334
                },
                "public withDefaultWriteTimeOut(Duration defaultWriteTimeout)": {
                    "first": {
                        "method_name": "withDefaultWriteTimeOut",
                        "method_signature": "public withDefaultWriteTimeOut(Duration defaultWriteTimeout)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.25979910062687334
                },
                "public withCoordinatorRuntimeMetrics(CoordinatorRuntimeMetrics runtimeMetrics)": {
                    "first": {
                        "method_name": "withCoordinatorRuntimeMetrics",
                        "method_signature": "public withCoordinatorRuntimeMetrics(CoordinatorRuntimeMetrics runtimeMetrics)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.25979910062687334
                },
                "public withCoordinatorMetrics(CoordinatorMetrics coordinatorMetrics)": {
                    "first": {
                        "method_name": "withCoordinatorMetrics",
                        "method_signature": "public withCoordinatorMetrics(CoordinatorMetrics coordinatorMetrics)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.25979910062687334
                },
                "public withSerializer(Serializer<U> serializer)": {
                    "first": {
                        "method_name": "withSerializer",
                        "method_signature": "public withSerializer(Serializer<U> serializer)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.25979910062687334
                },
                "public withCompression(Compression compression)": {
                    "first": {
                        "method_name": "withCompression",
                        "method_signature": "public withCompression(Compression compression)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.25979910062687334
                },
                "public withTimer(Timer timer)": {
                    "first": {
                        "method_name": "withTimer",
                        "method_signature": "public withTimer(Timer timer)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2605630037152544
                },
                "public withLogContext(LogContext logContext)": {
                    "first": {
                        "method_name": "withLogContext",
                        "method_signature": "public withLogContext(LogContext logContext)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.26094495726185907
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "public build()",
                    "public size()",
                    "public withEventProcessor(CoordinatorEventProcessor eventProcessor)",
                    "public withPartitionWriter(PartitionWriter partitionWriter)",
                    "public withLoader(CoordinatorLoader<U> loader)",
                    "public withCoordinatorShardBuilderSupplier(CoordinatorShardBuilderSupplier<S, U> coordinatorShardBuilderSupplier)",
                    "public withTime(Time time)",
                    "public withTimer(Timer timer)",
                    "public withDefaultWriteTimeOut(Duration defaultWriteTimeout)",
                    "public withCoordinatorRuntimeMetrics(CoordinatorRuntimeMetrics runtimeMetrics)",
                    "public withCoordinatorMetrics(CoordinatorMetrics coordinatorMetrics)",
                    "public withSerializer(Serializer<U> serializer)",
                    "public withCompression(Compression compression)",
                    "public withLogPrefix(String logPrefix)",
                    "public withLogContext(LogContext logContext)"
                ],
                "llm_response_time": 7406
            },
            "targetClassMap": {
                "build": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "size": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withEventProcessor": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withPartitionWriter": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withLoader": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withCoordinatorShardBuilderSupplier": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withTime": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withTimer": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withDefaultWriteTimeOut": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withCoordinatorRuntimeMetrics": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withCoordinatorMetrics": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withSerializer": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withCompression": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withLogPrefix": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "withLogContext": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "9a239c6142a8f2eb36f1600d1012224c31e58e71",
        "url": "https://github.com/apache/kafka/commit/9a239c6142a8f2eb36f1600d1012224c31e58e71",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method private registerStreamThread(streamThread StreamThread) : void extracted from private createAndAddStreamThread(cacheSizePerThread long, threadIdx int) : StreamThread in class org.apache.kafka.streams.KafkaStreams & moved to class org.apache.kafka.streams.KafkaStreams.StreamStateListener",
            "leftSideLocations": [
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java",
                    "startLine": 1068,
                    "endLine": 1092,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private createAndAddStreamThread(cacheSizePerThread long, threadIdx int) : StreamThread"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java",
                    "startLine": 1089,
                    "endLine": 1089,
                    "startColumn": 9,
                    "endColumn": 69,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java",
                    "startLine": 699,
                    "endLine": 701,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "private registerStreamThread(streamThread StreamThread) : void"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java",
                    "startLine": 700,
                    "endLine": 700,
                    "startColumn": 13,
                    "endColumn": 73,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java",
                    "startLine": 1064,
                    "endLine": 1088,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private createAndAddStreamThread(cacheSizePerThread long, threadIdx int) : StreamThread"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java",
                    "startLine": 1083,
                    "endLine": 1083,
                    "startColumn": 9,
                    "endColumn": 63,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "streamStateListener.registerStreamThread(streamThread)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 585,
        "extraction_results": {
            "success": true,
            "newCommitHash": "d737539bfd2a4db119995ea4d2d5e7126c5d9c02",
            "newBranchName": "extract-registerStreamThread-createAndAddStreamThread-fc6f8b6"
        },
        "telemetry": {
            "id": "d3622fe7-d6a7-49f5-8b95-6ecffef909cb",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 2141,
                "lineStart": 118,
                "lineEnd": 2258,
                "bodyLineStart": 118,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java",
                "sourceCode": "/**\n * A Kafka client that allows for performing continuous computation on input coming from one or more input topics and\n * sends output to zero, one, or more output topics.\n * <p>\n * The computational logic can be specified either by using the {@link Topology} to define a DAG topology of\n * {@link org.apache.kafka.streams.processor.api.Processor}s or by using the {@link StreamsBuilder} which provides the high-level DSL to define\n * transformations.\n * <p>\n * One {@code KafkaStreams} instance can contain one or more threads specified in the configs for the processing work.\n * <p>\n * A {@code KafkaStreams} instance can co-ordinate with any other instances with the same\n * {@link StreamsConfig#APPLICATION_ID_CONFIG application ID} (whether in the same process, on other processes on this\n * machine, or on remote machines) as a single (possibly distributed) stream processing application.\n * These instances will divide up the work based on the assignment of the input topic partitions so that all partitions\n * are being consumed.\n * If instances are added or fail, all (remaining) instances will rebalance the partition assignment among themselves\n * to balance processing load and ensure that all input topic partitions are processed.\n * <p>\n * Internally a {@code KafkaStreams} instance contains a normal {@link KafkaProducer} and {@link KafkaConsumer} instance\n * that is used for reading input and writing output.\n * <p>\n * A simple example might look like this:\n * <pre>{@code\n * Properties props = new Properties();\n * props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"my-stream-processing-application\");\n * props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n * props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n * props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n *\n * StreamsBuilder builder = new StreamsBuilder();\n * builder.<String, String>stream(\"my-input-topic\").mapValues(value -> String.valueOf(value.length())).to(\"my-output-topic\");\n *\n * KafkaStreams streams = new KafkaStreams(builder.build(), props);\n * streams.start();\n * }</pre>\n *\n * @see org.apache.kafka.streams.StreamsBuilder\n * @see org.apache.kafka.streams.Topology\n */\npublic class KafkaStreams implements AutoCloseable {\n\n    private static final String JMX_PREFIX = \"kafka.streams\";\n\n    // processId is expected to be unique across JVMs and to be used\n    // in userData of the subscription request to allow assignor be aware\n    // of the co-location of stream thread's consumers. It is for internal\n    // usage only and should not be exposed to users at all.\n    private final Time time;\n    private final Logger log;\n    protected final String clientId;\n    private final Metrics metrics;\n    protected final StreamsConfig applicationConfigs;\n    protected final List<StreamThread> threads;\n    protected final StreamsMetadataState streamsMetadataState;\n    private final ScheduledExecutorService stateDirCleaner;\n    private final ScheduledExecutorService rocksDBMetricsRecordingService;\n    protected final Admin adminClient;\n    private final StreamsMetricsImpl streamsMetrics;\n    private final long totalCacheSize;\n    private final StreamStateListener streamStateListener;\n    private final DelegatingStateRestoreListener delegatingStateRestoreListener;\n    private final Map<Long, StreamThread.State> threadState;\n    private final UUID processId;\n    private final KafkaClientSupplier clientSupplier;\n    protected final TopologyMetadata topologyMetadata;\n    private final QueryableStoreProvider queryableStoreProvider;\n    private final DelegatingStandbyUpdateListener delegatingStandbyUpdateListener;\n\n    GlobalStreamThread globalStreamThread;\n    protected StateDirectory stateDirectory = null;\n    private KafkaStreams.StateListener stateListener;\n    private boolean oldHandler;\n    private BiConsumer<Throwable, Boolean> streamsUncaughtExceptionHandler;\n    private final Object changeThreadCount = new Object();\n\n    // container states\n    /**\n     * Kafka Streams states are the possible state that a Kafka Streams instance can be in.\n     * An instance must only be in one state at a time.\n     * The expected state transition with the following defined states is:\n     *\n     * <pre>\n     *                 +--------------+\n     *         +&lt;----- | Created (0)  |\n     *         |       +-----+--------+\n     *         |             |\n     *         |             v\n     *         |       +----+--+------+\n     *         |       | Re-          |\n     *         +&lt;----- | Balancing (1)| --------&gt;+\n     *         |       +-----+-+------+          |\n     *         |             | ^                 |\n     *         |             v |                 |\n     *         |       +--------------+          v\n     *         |       | Running (2)  | --------&gt;+\n     *         |       +------+-------+          |\n     *         |              |                  |\n     *         |              v                  |\n     *         |       +------+-------+     +----+-------+\n     *         +-----&gt; | Pending      |     | Pending    |\n     *                 | Shutdown (3) |     | Error (5)  |\n     *                 +------+-------+     +-----+------+\n     *                        |                   |\n     *                        v                   v\n     *                 +------+-------+     +-----+--------+\n     *                 | Not          |     | Error (6)    |\n     *                 | Running (4)  |     +--------------+\n     *                 +--------------+\n     *\n     *\n     * </pre>\n     * Note the following:\n     * - RUNNING state will transit to REBALANCING if any of its threads is in PARTITION_REVOKED or PARTITIONS_ASSIGNED state\n     * - REBALANCING state will transit to RUNNING if all of its threads are in RUNNING state\n     * - Any state except NOT_RUNNING, PENDING_ERROR or ERROR can go to PENDING_SHUTDOWN (whenever close is called)\n     * - Of special importance: If the global stream thread dies, or all stream threads die (or both) then\n     *   the instance will be in the ERROR state. The user will not need to close it.\n     */\n    public enum State {\n        // Note: if you add a new state, check the below methods and how they are used within Streams to see if\n        // any of them should be updated to include the new state. For example a new shutdown path or terminal\n        // state would likely need to be included in methods like isShuttingDown(), hasCompletedShutdown(), etc.\n        CREATED(1, 3),          // 0\n        REBALANCING(2, 3, 5),   // 1\n        RUNNING(1, 2, 3, 5),    // 2\n        PENDING_SHUTDOWN(4),    // 3\n        NOT_RUNNING,            // 4\n        PENDING_ERROR(6),       // 5\n        ERROR;                  // 6\n\n        private final Set<Integer> validTransitions = new HashSet<>();\n\n        State(final Integer... validTransitions) {\n            this.validTransitions.addAll(Arrays.asList(validTransitions));\n        }\n\n        public boolean hasNotStarted() {\n            return equals(CREATED);\n        }\n\n        public boolean isRunningOrRebalancing() {\n            return equals(RUNNING) || equals(REBALANCING);\n        }\n\n        public boolean isShuttingDown() {\n            return equals(PENDING_SHUTDOWN) || equals(PENDING_ERROR);\n        }\n\n        public boolean hasCompletedShutdown() {\n            return equals(NOT_RUNNING) || equals(ERROR);\n        }\n\n        public boolean hasStartedOrFinishedShuttingDown() {\n            return isShuttingDown() || hasCompletedShutdown();\n        }\n\n        public boolean isValidTransition(final State newState) {\n            return validTransitions.contains(newState.ordinal());\n        }\n    }\n\n    private final Object stateLock = new Object();\n    protected volatile State state = State.CREATED;\n\n    private boolean waitOnState(final State targetState, final long waitMs) {\n        final long begin = time.milliseconds();\n        synchronized (stateLock) {\n            boolean interrupted = false;\n            long elapsedMs = 0L;\n            try {\n                while (state != targetState) {\n                    if (waitMs > elapsedMs) {\n                        final long remainingMs = waitMs - elapsedMs;\n                        try {\n                            stateLock.wait(remainingMs);\n                        } catch (final InterruptedException e) {\n                            interrupted = true;\n                        }\n                    } else {\n                        log.debug(\"Cannot transit to {} within {}ms\", targetState, waitMs);\n                        return false;\n                    }\n                    elapsedMs = time.milliseconds() - begin;\n                }\n            } finally {\n                // Make sure to restore the interruption status before returning.\n                // We do not always own the current thread that executes this method, i.e., we do not know the\n                // interruption policy of the thread. The least we can do is restore the interruption status before\n                // the current thread exits this method.\n                if (interrupted) {\n                    Thread.currentThread().interrupt();\n                }\n            }\n            return true;\n        }\n    }\n\n    /**\n     * Sets the state\n     * @param newState New state\n     */\n    private boolean setState(final State newState) {\n        final State oldState;\n\n        synchronized (stateLock) {\n            oldState = state;\n\n            if (state == State.PENDING_SHUTDOWN && newState != State.NOT_RUNNING) {\n                // when the state is already in PENDING_SHUTDOWN, all other transitions than NOT_RUNNING (due to thread dying) will be\n                // refused but we do not throw exception here, to allow appropriate error handling\n                return false;\n            } else if (state == State.NOT_RUNNING && (newState == State.PENDING_SHUTDOWN || newState == State.NOT_RUNNING)) {\n                // when the state is already in NOT_RUNNING, its transition to PENDING_SHUTDOWN or NOT_RUNNING (due to consecutive close calls)\n                // will be refused but we do not throw exception here, to allow idempotent close calls\n                return false;\n            } else if (state == State.REBALANCING && newState == State.REBALANCING) {\n                // when the state is already in REBALANCING, it should not transit to REBALANCING again\n                return false;\n            } else if (state == State.ERROR && (newState == State.PENDING_ERROR || newState == State.ERROR)) {\n                // when the state is already in ERROR, its transition to PENDING_ERROR or ERROR (due to consecutive close calls)\n                return false;\n            } else if (state == State.PENDING_ERROR && newState != State.ERROR) {\n                // when the state is already in PENDING_ERROR, all other transitions than ERROR (due to thread dying) will be\n                // refused but we do not throw exception here, to allow appropriate error handling\n                return false;\n            } else if (!state.isValidTransition(newState)) {\n                throw new IllegalStateException(\"Stream-client \" + clientId + \": Unexpected state transition from \" + oldState + \" to \" + newState);\n            } else {\n                log.info(\"State transition from {} to {}\", oldState, newState);\n            }\n            state = newState;\n            stateLock.notifyAll();\n        }\n\n        // we need to call the user customized state listener outside the state lock to avoid potential deadlocks\n        if (stateListener != null) {\n            stateListener.onChange(newState, oldState);\n        }\n\n        return true;\n    }\n\n    /**\n     * Return the current {@link State} of this {@code KafkaStreams} instance.\n     *\n     * @return the current state of this Kafka Streams instance\n     */\n    public State state() {\n        return state;\n    }\n\n    protected boolean isRunningOrRebalancing() {\n        synchronized (stateLock) {\n            return state.isRunningOrRebalancing();\n        }\n    }\n\n    protected boolean hasStartedOrFinishedShuttingDown() {\n        synchronized (stateLock) {\n            return state.hasStartedOrFinishedShuttingDown();\n        }\n    }\n\n    protected void validateIsRunningOrRebalancing() {\n        synchronized (stateLock) {\n            if (state.hasNotStarted()) {\n                throw new StreamsNotStartedException(\"KafkaStreams has not been started, you can retry after calling start()\");\n            }\n            if (!state.isRunningOrRebalancing()) {\n                throw new IllegalStateException(\"KafkaStreams is not running. State is \" + state + \".\");\n            }\n        }\n    }\n\n    /**\n     * Listen to {@link State} change events.\n     */\n    public interface StateListener {\n\n        /**\n         * Called when state changes.\n         *\n         * @param newState new state\n         * @param oldState previous state\n         */\n        void onChange(final State newState, final State oldState);\n    }\n\n    /**\n     * An app can set a single {@link KafkaStreams.StateListener} so that the app is notified when state changes.\n     *\n     * @param listener a new state listener\n     * @throws IllegalStateException if this {@code KafkaStreams} instance has already been started.\n     */\n    public void setStateListener(final KafkaStreams.StateListener listener) {\n        synchronized (stateLock) {\n            if (state.hasNotStarted()) {\n                stateListener = listener;\n            } else {\n                throw new IllegalStateException(\"Can only set StateListener before calling start(). Current state is: \" + state);\n            }\n        }\n    }\n\n    /**\n     * Set the handler invoked when an internal {@link StreamsConfig#NUM_STREAM_THREADS_CONFIG stream thread} abruptly\n     * terminates due to an uncaught exception.\n     *\n     * @param uncaughtExceptionHandler the uncaught exception handler for all internal threads; {@code null} deletes the current handler\n     * @throws IllegalStateException if this {@code KafkaStreams} instance has already been started.\n     *\n     * @deprecated Since 2.8.0. Use {@link KafkaStreams#setUncaughtExceptionHandler(StreamsUncaughtExceptionHandler)} instead.\n     *\n     */\n    @Deprecated\n    public void setUncaughtExceptionHandler(final Thread.UncaughtExceptionHandler uncaughtExceptionHandler) {\n        synchronized (stateLock) {\n            if (state.hasNotStarted()) {\n                oldHandler = true;\n                processStreamThread(thread -> thread.setUncaughtExceptionHandler(uncaughtExceptionHandler));\n\n                if (globalStreamThread != null) {\n                    globalStreamThread.setUncaughtExceptionHandler(uncaughtExceptionHandler);\n                }\n            } else {\n                throw new IllegalStateException(\"Can only set UncaughtExceptionHandler before calling start(). \" +\n                    \"Current state is: \" + state);\n            }\n        }\n    }\n\n    /**\n     * Set the handler invoked when an internal {@link StreamsConfig#NUM_STREAM_THREADS_CONFIG stream thread}\n     * throws an unexpected exception.\n     * These might be exceptions indicating rare bugs in Kafka Streams, or they\n     * might be exceptions thrown by your code, for example a NullPointerException thrown from your processor logic.\n     * The handler will execute on the thread that produced the exception.\n     * In order to get the thread that threw the exception, use {@code Thread.currentThread()}.\n     * <p>\n     * Note, this handler must be threadsafe, since it will be shared among all threads, and invoked from any\n     * thread that encounters such an exception.\n     *\n     * @param userStreamsUncaughtExceptionHandler the uncaught exception handler of type {@link StreamsUncaughtExceptionHandler} for all internal threads\n     * @throws IllegalStateException if this {@code KafkaStreams} instance has already been started.\n     * @throws NullPointerException if userStreamsUncaughtExceptionHandler is null.\n     */\n    public void setUncaughtExceptionHandler(final StreamsUncaughtExceptionHandler userStreamsUncaughtExceptionHandler) {\n        synchronized (stateLock) {\n            if (state.hasNotStarted()) {\n                Objects.requireNonNull(userStreamsUncaughtExceptionHandler);\n                streamsUncaughtExceptionHandler =\n                    (exception, skipThreadReplacement) ->\n                        handleStreamsUncaughtException(exception, userStreamsUncaughtExceptionHandler, skipThreadReplacement);\n                processStreamThread(thread -> thread.setStreamsUncaughtExceptionHandler(streamsUncaughtExceptionHandler));\n                if (globalStreamThread != null) {\n                    globalStreamThread.setUncaughtExceptionHandler(\n                        exception -> handleStreamsUncaughtException(exception, userStreamsUncaughtExceptionHandler, false)\n                    );\n                }\n                processStreamThread(thread -> thread.setUncaughtExceptionHandler((t, e) -> { }\n                ));\n\n                if (globalStreamThread != null) {\n                    globalStreamThread.setUncaughtExceptionHandler((t, e) -> { }\n                    );\n                }\n            } else {\n                throw new IllegalStateException(\"Can only set UncaughtExceptionHandler before calling start(). \" +\n                    \"Current state is: \" + state);\n            }\n        }\n    }\n\n    private void defaultStreamsUncaughtExceptionHandler(final Throwable throwable, final boolean skipThreadReplacement) {\n        if (oldHandler) {\n            threads.remove(Thread.currentThread());\n            if (throwable instanceof RuntimeException) {\n                throw (RuntimeException) throwable;\n            } else if (throwable instanceof Error) {\n                throw (Error) throwable;\n            } else {\n                throw new RuntimeException(\"Unexpected checked exception caught in the uncaught exception handler\", throwable);\n            }\n        } else {\n            handleStreamsUncaughtException(throwable, t -> SHUTDOWN_CLIENT, skipThreadReplacement);\n        }\n    }\n\n    private void replaceStreamThread(final Throwable throwable) {\n        if (globalStreamThread != null && Thread.currentThread().getName().equals(globalStreamThread.getName())) {\n            log.warn(\"The global thread cannot be replaced. Reverting to shutting down the client.\");\n            log.error(\"Encountered the following exception during processing \" +\n                    \" The streams client is going to shut down now. \", throwable);\n            closeToError();\n        }\n        final StreamThread deadThread = (StreamThread) Thread.currentThread();\n        deadThread.shutdown();\n        addStreamThread();\n        if (throwable instanceof RuntimeException) {\n            throw (RuntimeException) throwable;\n        } else if (throwable instanceof Error) {\n            throw (Error) throwable;\n        } else {\n            throw new RuntimeException(\"Unexpected checked exception caught in the uncaught exception handler\", throwable);\n        }\n    }\n\n    private void handleStreamsUncaughtException(final Throwable throwable,\n                                                final StreamsUncaughtExceptionHandler streamsUncaughtExceptionHandler,\n                                                final boolean skipThreadReplacement) {\n        final StreamsUncaughtExceptionHandler.StreamThreadExceptionResponse action = streamsUncaughtExceptionHandler.handle(throwable);\n        if (oldHandler) {\n            log.warn(\"Stream's new uncaught exception handler is set as well as the deprecated old handler.\" +\n                    \"The old handler will be ignored as long as a new handler is set.\");\n        }\n        switch (action) {\n            case REPLACE_THREAD:\n                if (!skipThreadReplacement) {\n                    log.error(\"Replacing thread in the streams uncaught exception handler\", throwable);\n                    replaceStreamThread(throwable);\n                } else {\n                    log.debug(\"Skipping thread replacement for recoverable error\");\n                }\n                break;\n            case SHUTDOWN_CLIENT:\n                log.error(\"Encountered the following exception during processing \" +\n                        \"and the registered exception handler opted to \" + action + \".\" +\n                        \" The streams client is going to shut down now. \", throwable);\n                closeToError();\n                break;\n            case SHUTDOWN_APPLICATION:\n                if (getNumLiveStreamThreads() == 1) {\n                    log.warn(\"Attempt to shut down the application requires adding a thread to communicate the shutdown. No processing will be done on this thread\");\n                    addStreamThread();\n                }\n                if (throwable instanceof Error) {\n                    log.error(\"This option requires running threads to shut down the application.\" +\n                            \"but the uncaught exception was an Error, which means this runtime is no \" +\n                            \"longer in a well-defined state. Attempting to send the shutdown command anyway.\", throwable);\n                }\n                if (Thread.currentThread().equals(globalStreamThread) && getNumLiveStreamThreads() == 0) {\n                    log.error(\"Exception in global thread caused the application to attempt to shutdown.\" +\n                            \" This action will succeed only if there is at least one StreamThread running on this client.\" +\n                            \" Currently there are no running threads so will now close the client.\");\n                    closeToError();\n                    break;\n                }\n                processStreamThread(thread -> thread.sendShutdownRequest(AssignorError.SHUTDOWN_REQUESTED));\n                log.error(\"Encountered the following exception during processing \" +\n                        \"and sent shutdown request for the entire application.\", throwable);\n                break;\n        }\n    }\n\n    /**\n     * Set the listener which is triggered whenever a {@link StateStore} is being restored in order to resume\n     * processing.\n     *\n     * @param globalStateRestoreListener The listener triggered when {@link StateStore} is being restored.\n     * @throws IllegalStateException if this {@code KafkaStreams} instance has already been started.\n     */\n    public void setGlobalStateRestoreListener(final StateRestoreListener globalStateRestoreListener) {\n        synchronized (stateLock) {\n            if (state.hasNotStarted()) {\n                delegatingStateRestoreListener.setUserStateRestoreListener(globalStateRestoreListener);\n            } else {\n                throw new IllegalStateException(\"Can only set GlobalStateRestoreListener before calling start(). \" +\n                    \"Current state is: \" + state);\n            }\n        }\n    }\n\n    /**\n     * Set the listener which is triggered whenever a standby task is updated\n     *\n     * @param standbyListener The listener triggered when a standby task is updated.\n     * @throws IllegalStateException if this {@code KafkaStreams} instance has already been started.\n     */\n    public void setStandbyUpdateListener(final StandbyUpdateListener standbyListener) {\n        synchronized (stateLock) {\n            if (state.hasNotStarted()) {\n                this.delegatingStandbyUpdateListener.setUserStandbyListener(standbyListener);\n            } else {\n                throw new IllegalStateException(\"Can only set StandbyUpdateListener before calling start(). \" +\n                        \"Current state is: \" + state);\n            }\n        }\n    }\n\n    /**\n     * Get read-only handle on global metrics registry, including streams client's own metrics plus\n     * its embedded producer, consumer and admin clients' metrics.\n     *\n     * @return Map of all metrics.\n     */\n    public Map<MetricName, ? extends Metric> metrics() {\n        final Map<MetricName, Metric> result = new LinkedHashMap<>();\n        // producer and consumer clients are per-thread\n        processStreamThread(thread -> {\n            result.putAll(thread.producerMetrics());\n            result.putAll(thread.consumerMetrics());\n            // admin client is shared, so we can actually move it\n            // to result.putAll(adminClient.metrics()).\n            // we did it intentionally just for flexibility.\n            result.putAll(thread.adminClientMetrics());\n        });\n        // global thread's consumer client\n        if (globalStreamThread != null) {\n            result.putAll(globalStreamThread.consumerMetrics());\n        }\n        // self streams metrics\n        result.putAll(metrics.metrics());\n        return Collections.unmodifiableMap(result);\n    }\n\n    /**\n     * Class that handles stream thread transitions\n     */\n    final class StreamStateListener implements StreamThread.StateListener {\n        private final Map<Long, StreamThread.State> threadState;\n        private GlobalStreamThread.State globalThreadState;\n        // this lock should always be held before the state lock\n        private final Object threadStatesLock;\n\n        StreamStateListener(final Map<Long, StreamThread.State> threadState,\n                            final GlobalStreamThread.State globalThreadState) {\n            this.threadState = threadState;\n            this.globalThreadState = globalThreadState;\n            this.threadStatesLock = new Object();\n        }\n\n        /**\n         * If all threads are up, including the global thread, set to RUNNING\n         */\n        private void maybeSetRunning() {\n            // state can be transferred to RUNNING if\n            // 1) all threads are either RUNNING or DEAD\n            // 2) thread is pending-shutdown and there are still other threads running\n            final boolean hasRunningThread = threadState.values().stream().anyMatch(s -> s == StreamThread.State.RUNNING);\n            for (final StreamThread.State state : threadState.values()) {\n                if (state == StreamThread.State.PENDING_SHUTDOWN && hasRunningThread) continue;\n                if (state != StreamThread.State.RUNNING && state != StreamThread.State.DEAD) {\n                    return;\n                }\n            }\n\n            // the global state thread is relevant only if it is started. There are cases\n            // when we don't have a global state thread at all, e.g., when we don't have global KTables\n            if (globalThreadState != null && globalThreadState != GlobalStreamThread.State.RUNNING) {\n                return;\n            }\n\n            setState(State.RUNNING);\n        }\n\n\n        @Override\n        public synchronized void onChange(final Thread thread,\n                                          final ThreadStateTransitionValidator abstractNewState,\n                                          final ThreadStateTransitionValidator abstractOldState) {\n            synchronized (threadStatesLock) {\n                // StreamThreads first\n                if (thread instanceof StreamThread) {\n                    final StreamThread.State newState = (StreamThread.State) abstractNewState;\n                    threadState.put(thread.getId(), newState);\n\n                    if (newState == StreamThread.State.PARTITIONS_REVOKED || newState == StreamThread.State.PARTITIONS_ASSIGNED) {\n                        setState(State.REBALANCING);\n                    } else if (newState == StreamThread.State.RUNNING) {\n                        maybeSetRunning();\n                    }\n                } else if (thread instanceof GlobalStreamThread) {\n                    // global stream thread has different invariants\n                    final GlobalStreamThread.State newState = (GlobalStreamThread.State) abstractNewState;\n                    globalThreadState = newState;\n\n                    if (newState == GlobalStreamThread.State.RUNNING) {\n                        maybeSetRunning();\n                    } else if (newState == GlobalStreamThread.State.DEAD) {\n                        if (state != State.PENDING_SHUTDOWN) {\n                            log.error(\"Global thread has died. The streams application or client will now close to ERROR.\");\n                            closeToError();\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    static final class DelegatingStateRestoreListener implements StateRestoreListener {\n        private StateRestoreListener userStateRestoreListener;\n\n        private void throwOnFatalException(final Exception fatalUserException,\n                                           final TopicPartition topicPartition,\n                                           final String storeName) {\n            throw new StreamsException(\n                    String.format(\"Fatal user code error in store restore listener for store %s, partition %s.\",\n                            storeName,\n                            topicPartition),\n                    fatalUserException);\n        }\n\n        void setUserStateRestoreListener(final StateRestoreListener userStateRestoreListener) {\n            this.userStateRestoreListener = userStateRestoreListener;\n        }\n\n        @Override\n        public void onRestoreStart(final TopicPartition topicPartition,\n                                   final String storeName,\n                                   final long startingOffset,\n                                   final long endingOffset) {\n            if (userStateRestoreListener != null) {\n                try {\n                    userStateRestoreListener.onRestoreStart(topicPartition, storeName, startingOffset, endingOffset);\n                } catch (final Exception fatalUserException) {\n                    throwOnFatalException(fatalUserException, topicPartition, storeName);\n                }\n            }\n        }\n\n        @Override\n        public void onBatchRestored(final TopicPartition topicPartition,\n                                    final String storeName,\n                                    final long batchEndOffset,\n                                    final long numRestored) {\n            if (userStateRestoreListener != null) {\n                try {\n                    userStateRestoreListener.onBatchRestored(topicPartition, storeName, batchEndOffset, numRestored);\n                } catch (final Exception fatalUserException) {\n                    throwOnFatalException(fatalUserException, topicPartition, storeName);\n                }\n            }\n        }\n\n        @Override\n        public void onRestoreEnd(final TopicPartition topicPartition, final String storeName, final long totalRestored) {\n            if (userStateRestoreListener != null) {\n                try {\n                    userStateRestoreListener.onRestoreEnd(topicPartition, storeName, totalRestored);\n                } catch (final Exception fatalUserException) {\n                    throwOnFatalException(fatalUserException, topicPartition, storeName);\n                }\n            }\n        }\n\n        @Override\n        public void onRestoreSuspended(final TopicPartition topicPartition, final String storeName, final long totalRestored) {\n            if (userStateRestoreListener != null) {\n                try {\n                    userStateRestoreListener.onRestoreSuspended(topicPartition, storeName, totalRestored);\n                } catch (final Exception fatalUserException) {\n                    throwOnFatalException(fatalUserException, topicPartition, storeName);\n                }\n            }\n        }\n    }\n\n    static final class DelegatingStandbyUpdateListener implements StandbyUpdateListener {\n        private StandbyUpdateListener userStandbyListener;\n\n        private void throwOnFatalException(final Exception fatalUserException,\n                                           final TopicPartition topicPartition,\n                                           final String storeName) {\n            throw new StreamsException(\n                    String.format(\"Fatal user code error in standby update listener for store %s, partition %s.\",\n                            storeName,\n                            topicPartition),\n                    fatalUserException);\n        }\n\n        @Override\n        public void onUpdateStart(final TopicPartition topicPartition,\n                          final String storeName, \n                          final long startingOffset) {\n            if (userStandbyListener != null) {\n                try {\n                    userStandbyListener.onUpdateStart(topicPartition, storeName, startingOffset);\n                } catch (final Exception fatalUserException) {\n                    throwOnFatalException(fatalUserException, topicPartition, storeName);\n                }\n            }\n        }\n\n        @Override\n        public void onBatchLoaded(final TopicPartition topicPartition, final String storeName, final TaskId taskId, final long batchEndOffset, final long batchSize, final long currentEndOffset) {\n            if (userStandbyListener != null) {\n                try {\n                    userStandbyListener.onBatchLoaded(topicPartition, storeName, taskId, batchEndOffset, batchSize, currentEndOffset);\n                } catch (final Exception fatalUserException) {\n                    throwOnFatalException(fatalUserException, topicPartition, storeName);\n                }\n            }\n        }\n\n        @Override\n        public void onUpdateSuspended(final TopicPartition topicPartition, final String storeName, final long storeOffset, final long currentEndOffset, final SuspendReason reason) {\n            if (userStandbyListener != null) {\n                try {\n                    userStandbyListener.onUpdateSuspended(topicPartition, storeName, storeOffset, currentEndOffset, reason);\n                } catch (final Exception fatalUserException) {\n                    throwOnFatalException(fatalUserException, topicPartition, storeName);\n                }\n            }\n        }\n\n        void setUserStandbyListener(final StandbyUpdateListener userStandbyListener) {\n            this.userStandbyListener = userStandbyListener;\n        }\n    }\n\n    /**\n     * Create a {@code KafkaStreams} instance.\n     * <p>\n     * Note: even if you never call {@link #start()} on a {@code KafkaStreams} instance,\n     * you still must {@link #close()} it to avoid resource leaks.\n     *\n     * @param topology the topology specifying the computational logic\n     * @param props    properties for {@link StreamsConfig}\n     * @throws StreamsException if any fatal error occurs\n     */\n    public KafkaStreams(final Topology topology,\n                        final Properties props) {\n        this(topology, new StreamsConfig(props));\n    }\n\n    /**\n     * Create a {@code KafkaStreams} instance.\n     * <p>\n     * Note: even if you never call {@link #start()} on a {@code KafkaStreams} instance,\n     * you still must {@link #close()} it to avoid resource leaks.\n     *\n     * @param topology       the topology specifying the computational logic\n     * @param props          properties for {@link StreamsConfig}\n     * @param clientSupplier the Kafka clients supplier which provides underlying producer and consumer clients\n     *                       for the new {@code KafkaStreams} instance\n     * @throws StreamsException if any fatal error occurs\n     */\n    public KafkaStreams(final Topology topology,\n                        final Properties props,\n                        final KafkaClientSupplier clientSupplier) {\n        this(topology, new StreamsConfig(props), clientSupplier, Time.SYSTEM);\n    }\n\n    /**\n     * Create a {@code KafkaStreams} instance.\n     * <p>\n     * Note: even if you never call {@link #start()} on a {@code KafkaStreams} instance,\n     * you still must {@link #close()} it to avoid resource leaks.\n     *\n     * @param topology       the topology specifying the computational logic\n     * @param props          properties for {@link StreamsConfig}\n     * @param time           {@code Time} implementation; cannot be null\n     * @throws StreamsException if any fatal error occurs\n     */\n    public KafkaStreams(final Topology topology,\n                        final Properties props,\n                        final Time time) {\n        this(topology, new StreamsConfig(props), time);\n    }\n\n    /**\n     * Create a {@code KafkaStreams} instance.\n     * <p>\n     * Note: even if you never call {@link #start()} on a {@code KafkaStreams} instance,\n     * you still must {@link #close()} it to avoid resource leaks.\n     *\n     * @param topology       the topology specifying the computational logic\n     * @param props          properties for {@link StreamsConfig}\n     * @param clientSupplier the Kafka clients supplier which provides underlying producer and consumer clients\n     *                       for the new {@code KafkaStreams} instance\n     * @param time           {@code Time} implementation; cannot be null\n     * @throws StreamsException if any fatal error occurs\n     */\n    public KafkaStreams(final Topology topology,\n                        final Properties props,\n                        final KafkaClientSupplier clientSupplier,\n                        final Time time) {\n        this(topology, new StreamsConfig(props), clientSupplier, time);\n    }\n\n    /**\n     * Create a {@code KafkaStreams} instance.\n     * <p>\n     * Note: even if you never call {@link #start()} on a {@code KafkaStreams} instance,\n     * you still must {@link #close()} it to avoid resource leaks.\n     *\n     * @param topology  the topology specifying the computational logic\n     * @param applicationConfigs    configs for Kafka Streams\n     * @throws StreamsException if any fatal error occurs\n     */\n    public KafkaStreams(final Topology topology,\n                        final StreamsConfig applicationConfigs) {\n        this(topology, applicationConfigs, applicationConfigs.getKafkaClientSupplier());\n    }\n\n    /**\n     * Create a {@code KafkaStreams} instance.\n     * <p>\n     * Note: even if you never call {@link #start()} on a {@code KafkaStreams} instance,\n     * you still must {@link #close()} it to avoid resource leaks.\n     *\n     * @param topology       the topology specifying the computational logic\n     * @param applicationConfigs         configs for Kafka Streams\n     * @param clientSupplier the Kafka clients supplier which provides underlying producer and consumer clients\n     *                       for the new {@code KafkaStreams} instance\n     * @throws StreamsException if any fatal error occurs\n     */\n    public KafkaStreams(final Topology topology,\n                        final StreamsConfig applicationConfigs,\n                        final KafkaClientSupplier clientSupplier) {\n        this(new TopologyMetadata(topology.internalTopologyBuilder, applicationConfigs), applicationConfigs, clientSupplier);\n    }\n\n    /**\n     * Create a {@code KafkaStreams} instance.\n     * <p>\n     * Note: even if you never call {@link #start()} on a {@code KafkaStreams} instance,\n     * you still must {@link #close()} it to avoid resource leaks.\n     *\n     * @param topology       the topology specifying the computational logic\n     * @param applicationConfigs         configs for Kafka Streams\n     * @param time           {@code Time} implementation; cannot be null\n     * @throws StreamsException if any fatal error occurs\n     */\n    public KafkaStreams(final Topology topology,\n                        final StreamsConfig applicationConfigs,\n                        final Time time) {\n        this(new TopologyMetadata(topology.internalTopologyBuilder, applicationConfigs), applicationConfigs, applicationConfigs.getKafkaClientSupplier(), time);\n    }\n\n    private KafkaStreams(final Topology topology,\n                         final StreamsConfig applicationConfigs,\n                         final KafkaClientSupplier clientSupplier,\n                         final Time time) throws StreamsException {\n        this(new TopologyMetadata(topology.internalTopologyBuilder, applicationConfigs), applicationConfigs, clientSupplier, time);\n    }\n\n    protected KafkaStreams(final TopologyMetadata topologyMetadata,\n                           final StreamsConfig applicationConfigs,\n                           final KafkaClientSupplier clientSupplier) throws StreamsException {\n        this(topologyMetadata, applicationConfigs, clientSupplier, Time.SYSTEM);\n    }\n\n    @SuppressWarnings(\"this-escape\")\n    private KafkaStreams(final TopologyMetadata topologyMetadata,\n                         final StreamsConfig applicationConfigs,\n                         final KafkaClientSupplier clientSupplier,\n                         final Time time) throws StreamsException {\n        this.applicationConfigs = applicationConfigs;\n        this.time = time;\n\n        this.topologyMetadata = topologyMetadata;\n        this.topologyMetadata.buildAndRewriteTopology();\n\n        final boolean hasGlobalTopology = topologyMetadata.hasGlobalTopology();\n\n        try {\n            stateDirectory = new StateDirectory(applicationConfigs, time, topologyMetadata.hasPersistentStores(), topologyMetadata.hasNamedTopologies());\n            processId = stateDirectory.initializeProcessId();\n        } catch (final ProcessorStateException fatal) {\n            Utils.closeQuietly(stateDirectory, \"streams state directory\");\n            throw new StreamsException(fatal);\n        }\n\n        // The application ID is a required config and hence should always have value\n        final String userClientId = applicationConfigs.getString(StreamsConfig.CLIENT_ID_CONFIG);\n        final String applicationId = applicationConfigs.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n        if (userClientId.length() <= 0) {\n            clientId = applicationId + \"-\" + processId;\n        } else {\n            clientId = userClientId;\n        }\n        final LogContext logContext = new LogContext(String.format(\"stream-client [%s] \", clientId));\n        this.log = logContext.logger(getClass());\n        topologyMetadata.setLog(logContext);\n\n        // use client id instead of thread client id since this admin client may be shared among threads\n        this.clientSupplier = clientSupplier;\n        adminClient = clientSupplier.getAdmin(applicationConfigs.getAdminConfigs(ClientUtils.getSharedAdminClientId(clientId)));\n\n        log.info(\"Kafka Streams version: {}\", ClientMetrics.version());\n        log.info(\"Kafka Streams commit ID: {}\", ClientMetrics.commitId());\n\n        metrics = getMetrics(applicationConfigs, time, clientId);\n        streamsMetrics = new StreamsMetricsImpl(\n            metrics,\n            clientId,\n            applicationConfigs.getString(StreamsConfig.BUILT_IN_METRICS_VERSION_CONFIG),\n            time\n        );\n\n        ClientMetrics.addVersionMetric(streamsMetrics);\n        ClientMetrics.addCommitIdMetric(streamsMetrics);\n        ClientMetrics.addApplicationIdMetric(streamsMetrics, applicationConfigs.getString(StreamsConfig.APPLICATION_ID_CONFIG));\n        ClientMetrics.addTopologyDescriptionMetric(streamsMetrics, (metricsConfig, now) -> this.topologyMetadata.topologyDescriptionString());\n        ClientMetrics.addStateMetric(streamsMetrics, (metricsConfig, now) -> state);\n        threads = Collections.synchronizedList(new LinkedList<>());\n        ClientMetrics.addNumAliveStreamThreadMetric(streamsMetrics, (metricsConfig, now) -> getNumLiveStreamThreads());\n\n        streamsMetadataState = new StreamsMetadataState(\n            this.topologyMetadata,\n            parseHostInfo(applicationConfigs.getString(StreamsConfig.APPLICATION_SERVER_CONFIG)),\n            logContext\n        );\n\n        oldHandler = false;\n        streamsUncaughtExceptionHandler = this::defaultStreamsUncaughtExceptionHandler;\n        delegatingStateRestoreListener = new DelegatingStateRestoreListener();\n        delegatingStandbyUpdateListener = new DelegatingStandbyUpdateListener();\n\n        totalCacheSize = getTotalCacheSize(applicationConfigs);\n        final int numStreamThreads = topologyMetadata.getNumStreamThreads(applicationConfigs);\n        final long cacheSizePerThread = getCacheSizePerThread(numStreamThreads);\n\n        GlobalStreamThread.State globalThreadState = null;\n        if (hasGlobalTopology) {\n            final String globalThreadId = clientId + \"-GlobalStreamThread\";\n            globalStreamThread = new GlobalStreamThread(\n                topologyMetadata.globalTaskTopology(),\n                applicationConfigs,\n                clientSupplier.getGlobalConsumer(applicationConfigs.getGlobalConsumerConfigs(clientId)),\n                stateDirectory,\n                cacheSizePerThread,\n                streamsMetrics,\n                time,\n                globalThreadId,\n                delegatingStateRestoreListener,\n                exception -> defaultStreamsUncaughtExceptionHandler(exception, false)\n            );\n            globalThreadState = globalStreamThread.state();\n        }\n\n        threadState = new HashMap<>(numStreamThreads);\n        streamStateListener = new StreamStateListener(threadState, globalThreadState);\n\n        final GlobalStateStoreProvider globalStateStoreProvider = new GlobalStateStoreProvider(this.topologyMetadata.globalStateStores());\n\n        if (hasGlobalTopology) {\n            globalStreamThread.setStateListener(streamStateListener);\n        }\n\n        queryableStoreProvider = new QueryableStoreProvider(globalStateStoreProvider);\n        for (int i = 1; i <= numStreamThreads; i++) {\n            createAndAddStreamThread(cacheSizePerThread, i);\n        }\n\n        stateDirCleaner = setupStateDirCleaner();\n        rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, applicationConfigs);\n    }\n\n    private StreamThread createAndAddStreamThread(final long cacheSizePerThread, final int threadIdx) {\n        final StreamThread streamThread = StreamThread.create(\n            topologyMetadata,\n            applicationConfigs,\n            clientSupplier,\n            adminClient,\n            processId,\n            clientId,\n            streamsMetrics,\n            time,\n            streamsMetadataState,\n            cacheSizePerThread,\n            stateDirectory,\n            delegatingStateRestoreListener,\n            delegatingStandbyUpdateListener,\n            threadIdx,\n            KafkaStreams.this::closeToError,\n            streamsUncaughtExceptionHandler\n        );\n        streamThread.setStateListener(streamStateListener);\n        threads.add(streamThread);\n        registerStreamThread(streamThread);\n        queryableStoreProvider.addStoreProviderForThread(streamThread.getName(), new StreamThreadStateStoreProvider(streamThread));\n        return streamThread;\n    }\n\n    private void registerStreamThread(StreamThread streamThread) {\n        threadState.put(streamThread.getId(), streamThread.state());\n    }\n\n    static Metrics getMetrics(final StreamsConfig config, final Time time, final String clientId) {\n        final MetricConfig metricConfig = new MetricConfig()\n            .samples(config.getInt(StreamsConfig.METRICS_NUM_SAMPLES_CONFIG))\n            .recordLevel(Sensor.RecordingLevel.forName(config.getString(StreamsConfig.METRICS_RECORDING_LEVEL_CONFIG)))\n            .timeWindow(config.getLong(StreamsConfig.METRICS_SAMPLE_WINDOW_MS_CONFIG), TimeUnit.MILLISECONDS);\n        final List<MetricsReporter> reporters = CommonClientConfigs.metricsReporters(clientId, config);\n\n        final MetricsContext metricsContext = new KafkaMetricsContext(JMX_PREFIX,\n                                                                      config.originalsWithPrefix(CommonClientConfigs.METRICS_CONTEXT_PREFIX));\n        return new Metrics(metricConfig, reporters, time, metricsContext);\n    }\n\n    /**\n     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n     * Kafka Streams client.\n     * <p>\n     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n     * threads does not exceed the total cache size specified in configuration\n     * {@link StreamsConfig#STATESTORE_CACHE_MAX_BYTES_CONFIG}.\n     * <p>\n     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n     *\n     * @return name of the added stream thread or empty if a new stream thread could not be added\n     */\n    public Optional<String> addStreamThread() {\n        if (isRunningOrRebalancing()) {\n            final StreamThread streamThread;\n            synchronized (changeThreadCount) {\n                final int threadIdx = getNextThreadIndex();\n                final int numLiveThreads = getNumLiveStreamThreads();\n                final long cacheSizePerThread = getCacheSizePerThread(numLiveThreads + 1);\n                log.info(\"Adding StreamThread-{}, there will now be {} live threads and the new cache size per thread is {}\",\n                         threadIdx, numLiveThreads + 1, cacheSizePerThread);\n                resizeThreadCache(cacheSizePerThread);\n                // Creating thread should hold the lock in order to avoid duplicate thread index.\n                // If the duplicate index happen, the metadata of thread may be duplicate too.\n                streamThread = createAndAddStreamThread(cacheSizePerThread, threadIdx);\n            }\n\n            synchronized (stateLock) {\n                if (isRunningOrRebalancing()) {\n                    streamThread.start();\n                    return Optional.of(streamThread.getName());\n                } else {\n                    log.warn(\"Terminating the new thread because the Kafka Streams client is in state {}\", state);\n                    streamThread.shutdown();\n                    threads.remove(streamThread);\n                    final long cacheSizePerThread = getCacheSizePerThread(getNumLiveStreamThreads());\n                    log.info(\"Resizing thread cache due to terminating added thread, new cache size per thread is {}\", cacheSizePerThread);\n                    resizeThreadCache(cacheSizePerThread);\n                    return Optional.empty();\n                }\n            }\n        } else {\n            log.warn(\"Cannot add a stream thread when Kafka Streams client is in state {}\", state);\n            return Optional.empty();\n        }\n    }\n\n    /**\n     * Removes one stream thread out of the running stream threads from this Kafka Streams client.\n     * <p>\n     * The removed stream thread is gracefully shut down. This method does not specify which stream\n     * thread is shut down.\n     * <p>\n     * Since the number of stream threads decreases, the sizes of the caches in the remaining stream\n     * threads are adapted so that the sum of the cache sizes over all stream threads equals the total\n     * cache size specified in configuration {@link StreamsConfig#STATESTORE_CACHE_MAX_BYTES_CONFIG}.\n     *\n     * @return name of the removed stream thread or empty if a stream thread could not be removed because\n     *         no stream threads are alive\n     */\n    public Optional<String> removeStreamThread() {\n        return removeStreamThread(Long.MAX_VALUE);\n    }\n\n    /**\n     * Removes one stream thread out of the running stream threads from this Kafka Streams client.\n     * <p>\n     * The removed stream thread is gracefully shut down. This method does not specify which stream\n     * thread is shut down.\n     * <p>\n     * Since the number of stream threads decreases, the sizes of the caches in the remaining stream\n     * threads are adapted so that the sum of the cache sizes over all stream threads equals the total\n     * cache size specified in configuration {@link StreamsConfig#STATESTORE_CACHE_MAX_BYTES_CONFIG}.\n     *\n     * @param timeout The length of time to wait for the thread to shut down\n     * @throws org.apache.kafka.common.errors.TimeoutException if the thread does not stop in time\n     * @return name of the removed stream thread or empty if a stream thread could not be removed because\n     *         no stream threads are alive\n     */\n    public Optional<String> removeStreamThread(final Duration timeout) {\n        final String msgPrefix = prepareMillisCheckFailMsgPrefix(timeout, \"timeout\");\n        final long timeoutMs = validateMillisecondDuration(timeout, msgPrefix);\n        return removeStreamThread(timeoutMs);\n    }\n\n    private Optional<String> removeStreamThread(final long timeoutMs) throws TimeoutException {\n        final long startMs = time.milliseconds();\n\n        if (isRunningOrRebalancing()) {\n            synchronized (changeThreadCount) {\n                // make a copy of threads to avoid holding lock\n                for (final StreamThread streamThread : new ArrayList<>(threads)) {\n                    final boolean callingThreadIsNotCurrentStreamThread = !streamThread.getName().equals(Thread.currentThread().getName());\n                    if (streamThread.isThreadAlive() && (callingThreadIsNotCurrentStreamThread || getNumLiveStreamThreads() == 1)) {\n                        log.info(\"Removing StreamThread \" + streamThread.getName());\n                        final Optional<String> groupInstanceID = streamThread.getGroupInstanceID();\n                        streamThread.requestLeaveGroupDuringShutdown();\n                        streamThread.shutdown();\n                        if (!streamThread.getName().equals(Thread.currentThread().getName())) {\n                            final long remainingTimeMs = timeoutMs - (time.milliseconds() - startMs);\n                            if (remainingTimeMs <= 0 || !streamThread.waitOnThreadState(StreamThread.State.DEAD, remainingTimeMs)) {\n                                log.warn(\"{} did not shutdown in the allotted time.\", streamThread.getName());\n                                // Don't remove from threads until shutdown is complete. We will trim it from the\n                                // list once it reaches DEAD, and if for some reason it's hanging indefinitely in the\n                                // shutdown then we should just consider this thread.id to be burned\n                            } else {\n                                log.info(\"Successfully removed {} in {}ms\", streamThread.getName(), time.milliseconds() - startMs);\n                                threads.remove(streamThread);\n                                queryableStoreProvider.removeStoreProviderForThread(streamThread.getName());\n                            }\n                        } else {\n                            log.info(\"{} is the last remaining thread and must remove itself, therefore we cannot wait \"\n                                + \"for it to complete shutdown as this will result in deadlock.\", streamThread.getName());\n                        }\n\n                        final long cacheSizePerThread = getCacheSizePerThread(getNumLiveStreamThreads());\n                        log.info(\"Resizing thread cache due to thread removal, new cache size per thread is {}\", cacheSizePerThread);\n                        resizeThreadCache(cacheSizePerThread);\n                        if (groupInstanceID.isPresent() && callingThreadIsNotCurrentStreamThread) {\n                            final MemberToRemove memberToRemove = new MemberToRemove(groupInstanceID.get());\n                            final Collection<MemberToRemove> membersToRemove = Collections.singletonList(memberToRemove);\n                            final RemoveMembersFromConsumerGroupResult removeMembersFromConsumerGroupResult = \n                                adminClient.removeMembersFromConsumerGroup(\n                                    applicationConfigs.getString(StreamsConfig.APPLICATION_ID_CONFIG),\n                                    new RemoveMembersFromConsumerGroupOptions(membersToRemove)\n                                );\n                            try {\n                                final long remainingTimeMs = timeoutMs - (time.milliseconds() - startMs);\n                                removeMembersFromConsumerGroupResult.memberResult(memberToRemove).get(remainingTimeMs, TimeUnit.MILLISECONDS);\n                            } catch (final java.util.concurrent.TimeoutException exception) {\n                                log.error(\n                                    String.format(\n                                        \"Could not remove static member %s from consumer group %s due to a timeout:\",\n                                        groupInstanceID.get(),\n                                        applicationConfigs.getString(StreamsConfig.APPLICATION_ID_CONFIG)\n                                    ),\n                                        exception\n                                );\n                                throw new TimeoutException(exception.getMessage(), exception);\n                            } catch (final InterruptedException e) {\n                                Thread.currentThread().interrupt();\n                            } catch (final ExecutionException exception) {\n                                log.error(\n                                    String.format(\n                                        \"Could not remove static member %s from consumer group %s due to:\",\n                                        groupInstanceID.get(),\n                                        applicationConfigs.getString(StreamsConfig.APPLICATION_ID_CONFIG)\n                                    ),\n                                        exception\n                                );\n                                throw new StreamsException(\n                                        \"Could not remove static member \" + groupInstanceID.get()\n                                            + \" from consumer group \" + applicationConfigs.getString(StreamsConfig.APPLICATION_ID_CONFIG)\n                                            + \" for the following reason: \",\n                                        exception.getCause()\n                                );\n                            }\n                        }\n                        final long remainingTimeMs = timeoutMs - (time.milliseconds() - startMs);\n                        if (remainingTimeMs <= 0) {\n                            throw new TimeoutException(\"Thread \" + streamThread.getName() + \" did not stop in the allotted time\");\n                        }\n                        return Optional.of(streamThread.getName());\n                    }\n                }\n            }\n            log.warn(\"There are no threads eligible for removal\");\n        } else {\n            log.warn(\"Cannot remove a stream thread when Kafka Streams client is in state  \" + state());\n        }\n        return Optional.empty();\n    }\n\n    /*\n     * Takes a snapshot and counts the number of stream threads which are not in PENDING_SHUTDOWN or DEAD\n     *\n     * note: iteration over SynchronizedList is not thread safe so it must be manually synchronized. However, we may\n     * require other locks when looping threads and it could cause deadlock. Hence, we create a copy to avoid holding\n     * threads lock when looping threads.\n     * @return number of alive stream threads\n     */\n    private int getNumLiveStreamThreads() {\n        final AtomicInteger numLiveThreads = new AtomicInteger(0);\n\n        synchronized (threads) {\n            processStreamThread(thread -> {\n                if (thread.state() == StreamThread.State.DEAD) {\n                    log.debug(\"Trimming thread {} from the threads list since it's state is {}\", thread.getName(), StreamThread.State.DEAD);\n                    threads.remove(thread);\n                } else if (thread.state() == StreamThread.State.PENDING_SHUTDOWN) {\n                    log.debug(\"Skipping thread {} from num live threads computation since it's state is {}\",\n                              thread.getName(), StreamThread.State.PENDING_SHUTDOWN);\n                } else {\n                    numLiveThreads.incrementAndGet();\n                }\n            });\n            return numLiveThreads.get();\n        }\n    }\n\n    private int getNextThreadIndex() {\n        final HashSet<String> allLiveThreadNames = new HashSet<>();\n        final AtomicInteger maxThreadId = new AtomicInteger(1);\n        synchronized (threads) {\n            processStreamThread(thread -> {\n                // trim any DEAD threads from the list so we can reuse the thread.id\n                // this is only safe to do once the thread has fully completed shutdown\n                if (thread.state() == StreamThread.State.DEAD) {\n                    threads.remove(thread);\n                } else {\n                    allLiveThreadNames.add(thread.getName());\n                    // Assume threads are always named with the \"-StreamThread-<threadId>\" suffix\n                    final int threadId = Integer.parseInt(thread.getName().substring(thread.getName().lastIndexOf(\"-\") + 1));\n                    if (threadId > maxThreadId.get()) {\n                        maxThreadId.set(threadId);\n                    }\n                }\n            });\n\n            final String baseName = clientId + \"-StreamThread-\";\n            for (int i = 1; i <= maxThreadId.get(); i++) {\n                final String name = baseName + i;\n                if (!allLiveThreadNames.contains(name)) {\n                    return i;\n                }\n            }\n            // It's safe to use threads.size() rather than getNumLiveStreamThreads() to infer the number of threads\n            // here since we trimmed any DEAD threads earlier in this method while holding the lock\n            return threads.size() + 1;\n        }\n    }\n\n    private long getCacheSizePerThread(final int numStreamThreads) {\n        if (numStreamThreads == 0) {\n            return totalCacheSize;\n        }\n        return totalCacheSize / (numStreamThreads + (topologyMetadata.hasGlobalTopology() ? 1 : 0));\n    }\n\n    private void resizeThreadCache(final long cacheSizePerThread) {\n        processStreamThread(thread -> thread.resizeCache(cacheSizePerThread));\n        if (globalStreamThread != null) {\n            globalStreamThread.resize(cacheSizePerThread);\n        }\n    }\n\n    private ScheduledExecutorService setupStateDirCleaner() {\n        return Executors.newSingleThreadScheduledExecutor(r -> {\n            final Thread thread = new Thread(r, clientId + \"-CleanupThread\");\n            thread.setDaemon(true);\n            return thread;\n        });\n    }\n\n    private static ScheduledExecutorService maybeCreateRocksDBMetricsRecordingService(final String clientId,\n                                                                                      final StreamsConfig config) {\n        if (RecordingLevel.forName(config.getString(StreamsConfig.METRICS_RECORDING_LEVEL_CONFIG)) == RecordingLevel.DEBUG) {\n            return Executors.newSingleThreadScheduledExecutor(r -> {\n                final Thread thread = new Thread(r, clientId + \"-RocksDBMetricsRecordingTrigger\");\n                thread.setDaemon(true);\n                return thread;\n            });\n        }\n        return null;\n    }\n\n    private static HostInfo parseHostInfo(final String endPoint) {\n        final HostInfo hostInfo = HostInfo.buildFromEndpoint(endPoint);\n        if (hostInfo == null) {\n            return StreamsMetadataState.UNKNOWN_HOST;\n        } else {\n            return hostInfo;\n        }\n    }\n\n    /**\n     * Start the {@code KafkaStreams} instance by starting all its threads.\n     * This function is expected to be called only once during the life cycle of the client.\n     * <p>\n     * Because threads are started in the background, this method does not block.\n     * However, if you have global stores in your topology, this method blocks until all global stores are restored.\n     * As a consequence, any fatal exception that happens during processing is by default only logged.\n     * If you want to be notified about dying threads, you can\n     * {@link #setUncaughtExceptionHandler(Thread.UncaughtExceptionHandler) register an uncaught exception handler}\n     * before starting the {@code KafkaStreams} instance.\n     * <p>\n     * Note, for brokers with version {@code 0.9.x} or lower, the broker version cannot be checked.\n     * There will be no error and the client will hang and retry to verify the broker version until it\n     * {@link StreamsConfig#REQUEST_TIMEOUT_MS_CONFIG times out}.\n\n     * @throws IllegalStateException if process was already started\n     * @throws StreamsException if the Kafka brokers have version 0.10.0.x or\n     *                          if {@link StreamsConfig#PROCESSING_GUARANTEE_CONFIG exactly-once} is enabled for pre 0.11.0.x brokers\n     */\n    public synchronized void start() throws IllegalStateException, StreamsException {\n        if (setState(State.REBALANCING)) {\n            log.debug(\"Starting Streams client\");\n\n            if (globalStreamThread != null) {\n                globalStreamThread.start();\n            }\n\n            final int numThreads = processStreamThread(StreamThread::start);\n\n            log.info(\"Started {} stream threads\", numThreads);\n\n            final Long cleanupDelay = applicationConfigs.getLong(StreamsConfig.STATE_CLEANUP_DELAY_MS_CONFIG);\n            stateDirCleaner.scheduleAtFixedRate(() -> {\n                // we do not use lock here since we only read on the value and act on it\n                if (state == State.RUNNING) {\n                    stateDirectory.cleanRemovedTasks(cleanupDelay);\n                }\n            }, cleanupDelay, cleanupDelay, TimeUnit.MILLISECONDS);\n\n            final long recordingDelay = 0;\n            final long recordingInterval = 1;\n            if (rocksDBMetricsRecordingService != null) {\n                rocksDBMetricsRecordingService.scheduleAtFixedRate(\n                    streamsMetrics.rocksDBMetricsRecordingTrigger(),\n                    recordingDelay,\n                    recordingInterval,\n                    TimeUnit.MINUTES\n                );\n            }\n        } else {\n            throw new IllegalStateException(\"The client is either already started or already stopped, cannot re-start\");\n        }\n    }\n\n    /**\n     * Class that handles options passed in case of {@code KafkaStreams} instance scale down\n     */\n    public static class CloseOptions {\n        private Duration timeout = Duration.ofMillis(Long.MAX_VALUE);\n        private boolean leaveGroup = false;\n\n        public CloseOptions timeout(final Duration timeout) {\n            this.timeout = timeout;\n            return this;\n        }\n\n        public CloseOptions leaveGroup(final boolean leaveGroup) {\n            this.leaveGroup = leaveGroup;\n            return this;\n        }\n    }\n\n    /**\n     * Shutdown this {@code KafkaStreams} instance by signaling all the threads to stop, and then wait for them to join.\n     * This will block until all threads have stopped.\n     */\n    public void close() {\n        close(Long.MAX_VALUE, false);\n    }\n\n    private Thread shutdownHelper(final boolean error, final long timeoutMs, final boolean leaveGroup) {\n        stateDirCleaner.shutdownNow();\n        if (rocksDBMetricsRecordingService != null) {\n            rocksDBMetricsRecordingService.shutdownNow();\n        }\n\n        // wait for all threads to join in a separate thread;\n        // save the current thread so that if it is a stream thread\n        // we don't attempt to join it and cause a deadlock\n        return new Thread(() -> {\n            // notify all the threads to stop; avoid deadlocks by stopping any\n            // further state reports from the thread since we're shutting down\n            int numStreamThreads = processStreamThread(StreamThread::shutdown);\n\n            log.info(\"Shutting down {} stream threads\", numStreamThreads);\n\n            topologyMetadata.wakeupThreads();\n\n            numStreamThreads = processStreamThread(thread -> {\n                try {\n                    if (!thread.isRunning()) {\n                        log.debug(\"Shutdown {} complete\", thread.getName());\n\n                        thread.join();\n                    }\n                } catch (final InterruptedException ex) {\n                    log.warn(\"Shutdown {} interrupted\", thread.getName());\n\n                    Thread.currentThread().interrupt();\n                }\n            });\n\n            if (leaveGroup) {\n                processStreamThread(streamThreadLeaveConsumerGroup(timeoutMs));\n            }\n\n            log.info(\"Shutdown {} stream threads complete\", numStreamThreads);\n\n            if (globalStreamThread != null) {\n                log.info(\"Shutting down the global stream threads\");\n\n                globalStreamThread.shutdown();\n            }\n\n            if (globalStreamThread != null && !globalStreamThread.stillRunning()) {\n                try {\n                    globalStreamThread.join();\n                } catch (final InterruptedException e) {\n                    log.warn(\"Shutdown the global stream thread interrupted\");\n\n                    Thread.currentThread().interrupt();\n                }\n                globalStreamThread = null;\n\n                log.info(\"Shutdown global stream threads complete\");\n            }\n\n            stateDirectory.close();\n            adminClient.close();\n\n            streamsMetrics.removeAllClientLevelSensorsAndMetrics();\n            metrics.close();\n            if (!error) {\n                setState(State.NOT_RUNNING);\n            } else {\n                setState(State.ERROR);\n            }\n        }, clientId + \"-CloseThread\");\n    }\n\n    private boolean close(final long timeoutMs, final boolean leaveGroup) {\n        if (state.hasCompletedShutdown()) {\n            log.info(\"Streams client is already in the terminal {} state, all resources are closed and the client has stopped.\", state);\n            return true;\n        }\n        if (state.isShuttingDown()) {\n            log.info(\"Streams client is in {}, all resources are being closed and the client will be stopped.\", state);\n            if (state == State.PENDING_ERROR && waitOnState(State.ERROR, timeoutMs)) {\n                log.info(\"Streams client stopped to ERROR completely\");\n                return true;\n            } else if (state == State.PENDING_SHUTDOWN && waitOnState(State.NOT_RUNNING, timeoutMs)) {\n                log.info(\"Streams client stopped to NOT_RUNNING completely\");\n                return true;\n            } else {\n                log.warn(\"Streams client cannot transition to {} completely within the timeout\",\n                         state == State.PENDING_SHUTDOWN ? State.NOT_RUNNING : State.ERROR);\n                return false;\n            }\n        }\n\n        if (!setState(State.PENDING_SHUTDOWN)) {\n            // if we can't transition to PENDING_SHUTDOWN but not because we're already shutting down, then it must be fatal\n            log.error(\"Failed to transition to PENDING_SHUTDOWN, current state is {}\", state);\n            throw new StreamsException(\"Failed to shut down while in state \" + state);\n        } else {\n\n            final Thread shutdownThread = shutdownHelper(false, timeoutMs, leaveGroup);\n\n            shutdownThread.setDaemon(true);\n            shutdownThread.start();\n        }\n\n        if (waitOnState(State.NOT_RUNNING, timeoutMs)) {\n            log.info(\"Streams client stopped completely\");\n            return true;\n        } else {\n            log.info(\"Streams client cannot stop completely within the {}ms timeout\", timeoutMs);\n            return false;\n        }\n    }\n\n    private void closeToError() {\n        if (!setState(State.PENDING_ERROR)) {\n            log.info(\"Skipping shutdown since we are already in \" + state());\n        } else {\n            final Thread shutdownThread = shutdownHelper(true, -1, false);\n\n            shutdownThread.setDaemon(true);\n            shutdownThread.start();\n        }\n    }\n\n    /**\n     * Shutdown this {@code KafkaStreams} by signaling all the threads to stop, and then wait up to the timeout for the\n     * threads to join.\n     * A {@code timeout} of Duration.ZERO (or any other zero duration) makes the close operation asynchronous.\n     * Negative-duration timeouts are rejected.\n     *\n     * @param timeout  how long to wait for the threads to shutdown\n     * @return {@code true} if all threads were successfully stopped&mdash;{@code false} if the timeout was reached\n     * before all threads stopped\n     * Note that this method must not be called in the {@link StateListener#onChange(KafkaStreams.State, KafkaStreams.State)} callback of {@link StateListener}.\n     * @throws IllegalArgumentException if {@code timeout} can't be represented as {@code long milliseconds}\n     */\n    public synchronized boolean close(final Duration timeout) throws IllegalArgumentException {\n        final String msgPrefix = prepareMillisCheckFailMsgPrefix(timeout, \"timeout\");\n        final long timeoutMs = validateMillisecondDuration(timeout, msgPrefix);\n        if (timeoutMs < 0) {\n            throw new IllegalArgumentException(\"Timeout can't be negative.\");\n        }\n\n        log.debug(\"Stopping Streams client with timeoutMillis = {} ms.\", timeoutMs);\n\n        return close(timeoutMs, false);\n    }\n\n    /**\n     * Shutdown this {@code KafkaStreams} by signaling all the threads to stop, and then wait up to the timeout for the\n     * threads to join.\n     * @param options  contains timeout to specify how long to wait for the threads to shutdown, and a flag leaveGroup to\n     *                 trigger consumer leave call\n     * @return {@code true} if all threads were successfully stopped&mdash;{@code false} if the timeout was reached\n     * before all threads stopped\n     * Note that this method must not be called in the {@link StateListener#onChange(KafkaStreams.State, KafkaStreams.State)} callback of {@link StateListener}.\n     * @throws IllegalArgumentException if {@code timeout} can't be represented as {@code long milliseconds}\n     */\n    public synchronized boolean close(final CloseOptions options) throws IllegalArgumentException {\n        Objects.requireNonNull(options, \"options cannot be null\");\n        final String msgPrefix = prepareMillisCheckFailMsgPrefix(options.timeout, \"timeout\");\n        final long timeoutMs = validateMillisecondDuration(options.timeout, msgPrefix);\n        if (timeoutMs < 0) {\n            throw new IllegalArgumentException(\"Timeout can't be negative.\");\n        }\n        log.debug(\"Stopping Streams client with timeoutMillis = {} ms.\", timeoutMs);\n        return close(timeoutMs, options.leaveGroup);\n    }\n\n    private Consumer<StreamThread> streamThreadLeaveConsumerGroup(final long remainingTimeMs) {\n        return thread -> {\n            final Optional<String> groupInstanceId = thread.getGroupInstanceID();\n            if (groupInstanceId.isPresent()) {\n                log.debug(\"Sending leave group trigger to removing instance from consumer group: {}.\",\n                    groupInstanceId.get());\n                final MemberToRemove memberToRemove = new MemberToRemove(groupInstanceId.get());\n                final Collection<MemberToRemove> membersToRemove = Collections.singletonList(memberToRemove);\n\n                final RemoveMembersFromConsumerGroupResult removeMembersFromConsumerGroupResult = adminClient\n                    .removeMembersFromConsumerGroup(\n                        applicationConfigs.getString(StreamsConfig.APPLICATION_ID_CONFIG),\n                        new RemoveMembersFromConsumerGroupOptions(membersToRemove)\n                    );\n\n                try {\n                    removeMembersFromConsumerGroupResult.memberResult(memberToRemove)\n                        .get(remainingTimeMs, TimeUnit.MILLISECONDS);\n                } catch (final Exception e) {\n                    final String msg = String.format(\"Could not remove static member %s from consumer group %s.\",\n                                                     groupInstanceId.get(), applicationConfigs.getString(StreamsConfig.APPLICATION_ID_CONFIG));\n                    log.error(msg, e);\n                }\n            }\n        };\n    }\n\n    /**\n     * Do a clean up of the local {@link StateStore} directory ({@link StreamsConfig#STATE_DIR_CONFIG}) by deleting all\n     * data with regard to the {@link StreamsConfig#APPLICATION_ID_CONFIG application ID}.\n     * <p>\n     * May only be called either before this {@code KafkaStreams} instance is {@link #start() started} or after the\n     * instance is {@link #close() closed}.\n     * <p>\n     * Calling this method triggers a restore of local {@link StateStore}s on the next {@link #start() application start}.\n     *\n     * @throws IllegalStateException if this {@code KafkaStreams} instance has been started and hasn't fully shut down\n     * @throws StreamsException if cleanup failed\n     */\n    public void cleanUp() {\n        if (!(state.hasNotStarted() || state.hasCompletedShutdown())) {\n            throw new IllegalStateException(\"Cannot clean up while running.\");\n        }\n        stateDirectory.clean();\n    }\n\n    /**\n     * Find all currently running {@code KafkaStreams} instances (potentially remotely) that use the same\n     * {@link StreamsConfig#APPLICATION_ID_CONFIG application ID} as this instance (i.e., all instances that belong to\n     * the same Kafka Streams application) and return {@link StreamsMetadata} for each discovered instance.\n     * <p>\n     * Note: this is a point in time view and it may change due to partition reassignment.\n     *\n     * @return {@link StreamsMetadata} for each {@code KafkaStreams} instances of this application\n     * @deprecated since 3.0.0 use {@link KafkaStreams#metadataForAllStreamsClients}\n     */\n    @Deprecated\n    public Collection<org.apache.kafka.streams.state.StreamsMetadata> allMetadata() {\n        validateIsRunningOrRebalancing();\n        return streamsMetadataState.getAllMetadata().stream().map(streamsMetadata ->\n                new org.apache.kafka.streams.state.StreamsMetadata(streamsMetadata.hostInfo(),\n                        streamsMetadata.stateStoreNames(),\n                        streamsMetadata.topicPartitions(),\n                        streamsMetadata.standbyStateStoreNames(),\n                        streamsMetadata.standbyTopicPartitions()))\n                .collect(Collectors.toSet());\n    }\n\n    /**\n     * Find all currently running {@code KafkaStreams} instances (potentially remotely) that use the same\n     * {@link StreamsConfig#APPLICATION_ID_CONFIG application ID} as this instance (i.e., all instances that belong to\n     * the same Kafka Streams application) and return {@link StreamsMetadata} for each discovered instance.\n     * <p>\n     * Note: this is a point in time view and it may change due to partition reassignment.\n     *\n     * @return {@link StreamsMetadata} for each {@code KafkaStreams} instances of this application\n     */\n    public Collection<StreamsMetadata> metadataForAllStreamsClients() {\n        validateIsRunningOrRebalancing();\n        return streamsMetadataState.getAllMetadata();\n    }\n\n    /**\n     * Find all currently running {@code KafkaStreams} instances (potentially remotely) that\n     * <ul>\n     *   <li>use the same {@link StreamsConfig#APPLICATION_ID_CONFIG application ID} as this instance (i.e., all\n     *       instances that belong to the same Kafka Streams application)</li>\n     *   <li>and that contain a {@link StateStore} with the given {@code storeName}</li>\n     * </ul>\n     * and return {@link StreamsMetadata} for each discovered instance.\n     * <p>\n     * Note: this is a point in time view and it may change due to partition reassignment.\n     *\n     * @param storeName the {@code storeName} to find metadata for\n     * @return {@link StreamsMetadata} for each {@code KafkaStreams} instances with the provide {@code storeName} of\n     * this application\n     * @deprecated since 3.0.0 use {@link KafkaStreams#streamsMetadataForStore} instead\n     */\n    @Deprecated\n    public Collection<org.apache.kafka.streams.state.StreamsMetadata> allMetadataForStore(final String storeName) {\n        validateIsRunningOrRebalancing();\n        return streamsMetadataState.getAllMetadataForStore(storeName).stream().map(streamsMetadata ->\n                new org.apache.kafka.streams.state.StreamsMetadata(streamsMetadata.hostInfo(),\n                        streamsMetadata.stateStoreNames(),\n                        streamsMetadata.topicPartitions(),\n                        streamsMetadata.standbyStateStoreNames(),\n                        streamsMetadata.standbyTopicPartitions()))\n                .collect(Collectors.toSet());\n    }\n\n    /**\n     * Find all currently running {@code KafkaStreams} instances (potentially remotely) that\n     * <ul>\n     *   <li>use the same {@link StreamsConfig#APPLICATION_ID_CONFIG application ID} as this instance (i.e., all\n     *       instances that belong to the same Kafka Streams application)</li>\n     *   <li>and that contain a {@link StateStore} with the given {@code storeName}</li>\n     * </ul>\n     * and return {@link StreamsMetadata} for each discovered instance.\n     * <p>\n     * Note: this is a point in time view and it may change due to partition reassignment.\n     *\n     * @param storeName the {@code storeName} to find metadata for\n     * @return {@link StreamsMetadata} for each {@code KafkaStreams} instances with the provide {@code storeName} of\n     * this application\n     */\n    public Collection<StreamsMetadata> streamsMetadataForStore(final String storeName) {\n        validateIsRunningOrRebalancing();\n        return streamsMetadataState.getAllMetadataForStore(storeName);\n    }\n\n    /**\n     * Finds the metadata containing the active hosts and standby hosts where the key being queried would reside.\n     *\n     * @param storeName     the {@code storeName} to find metadata for\n     * @param key           the key to find metadata for\n     * @param keySerializer serializer for the key\n     * @param <K>           key type\n     * Returns {@link KeyQueryMetadata} containing all metadata about hosting the given key for the given store,\n     * or {@code null} if no matching metadata could be found.\n     */\n    public <K> KeyQueryMetadata queryMetadataForKey(final String storeName,\n                                                    final K key,\n                                                    final Serializer<K> keySerializer) {\n        validateIsRunningOrRebalancing();\n        return streamsMetadataState.getKeyQueryMetadataForKey(storeName, key, keySerializer);\n    }\n\n    /**\n     * Finds the metadata containing the active hosts and standby hosts where the key being queried would reside.\n     *\n     * @param storeName     the {@code storeName} to find metadata for\n     * @param key           the key to find metadata for\n     * @param partitioner the partitioner to be use to locate the host for the key\n     * @param <K>           key type\n     * Returns {@link KeyQueryMetadata} containing all metadata about hosting the given key for the given store, using the\n     * the supplied partitioner, or {@code null} if no matching metadata could be found.\n     */\n    public <K> KeyQueryMetadata queryMetadataForKey(final String storeName,\n                                                    final K key,\n                                                    final StreamPartitioner<? super K, ?> partitioner) {\n        validateIsRunningOrRebalancing();\n        return streamsMetadataState.getKeyQueryMetadataForKey(storeName, key, partitioner);\n    }\n\n    /**\n     * Get a facade wrapping the local {@link StateStore} instances with the provided {@link StoreQueryParameters}.\n     * The returned object can be used to query the {@link StateStore} instances.\n     *\n     * @param storeQueryParameters   the parameters used to fetch a queryable store\n     * @return A facade wrapping the local {@link StateStore} instances\n     * @throws StreamsNotStartedException If Streams has not yet been started. Just call {@link KafkaStreams#start()}\n     *                                    and then retry this call.\n     * @throws UnknownStateStoreException If the specified store name does not exist in the topology.\n     * @throws InvalidStateStorePartitionException If the specified partition does not exist.\n     * @throws InvalidStateStoreException If the Streams instance isn't in a queryable state.\n     *                                    If the store's type does not match the QueryableStoreType,\n     *                                    the Streams instance is not in a queryable state with respect\n     *                                    to the parameters, or if the store is not available locally, then\n     *                                    an InvalidStateStoreException is thrown upon store access.\n     */\n    public <T> T store(final StoreQueryParameters<T> storeQueryParameters) {\n        validateIsRunningOrRebalancing();\n        final String storeName = storeQueryParameters.storeName();\n        if (!topologyMetadata.hasStore(storeName)) {\n            throw new UnknownStateStoreException(\n                \"Cannot get state store \" + storeName + \" because no such store is registered in the topology.\"\n            );\n        }\n        return queryableStoreProvider.getStore(storeQueryParameters);\n    }\n\n    /**\n     *  This method pauses processing for the KafkaStreams instance.\n     *\n     *  <p>Paused topologies will only skip over a) processing, b) punctuation, and c) standby tasks.\n     *  Notably, paused topologies will still poll Kafka consumers, and commit offsets.\n     *  This method sets transient state that is not maintained or managed among instances.\n     *  Note that pause() can be called before start() in order to start a KafkaStreams instance\n     *  in a manner where the processing is paused as described, but the consumers are started up.\n     */\n    public void pause() {\n        if (topologyMetadata.hasNamedTopologies()) {\n            for (final NamedTopology namedTopology : topologyMetadata.getAllNamedTopologies()) {\n                topologyMetadata.pauseTopology(namedTopology.name());\n            }\n        } else {\n            topologyMetadata.pauseTopology(UNNAMED_TOPOLOGY);\n        }\n    }\n\n    /**\n     * @return true when the KafkaStreams instance has its processing paused.\n     */\n    public boolean isPaused() {\n        if (topologyMetadata.hasNamedTopologies()) {\n            return topologyMetadata.getAllNamedTopologies().stream()\n                .map(NamedTopology::name)\n                .allMatch(topologyMetadata::isPaused);\n        } else {\n            return topologyMetadata.isPaused(UNNAMED_TOPOLOGY);\n        }\n    }\n\n    /**\n     * This method resumes processing for the KafkaStreams instance.\n     */\n    public void resume() {\n        if (topologyMetadata.hasNamedTopologies()) {\n            for (final NamedTopology namedTopology : topologyMetadata.getAllNamedTopologies()) {\n                topologyMetadata.resumeTopology(namedTopology.name());\n            }\n        } else {\n            topologyMetadata.resumeTopology(UNNAMED_TOPOLOGY);\n        }\n        threads.forEach(StreamThread::signalResume);\n    }\n\n    /**\n     * handle each stream thread in a snapshot of threads.\n     * noted: iteration over SynchronizedList is not thread safe so it must be manually synchronized. However, we may\n     * require other locks when looping threads and it could cause deadlock. Hence, we create a copy to avoid holding\n     * threads lock when looping threads.\n     * @param consumer handler\n     */\n    protected int processStreamThread(final Consumer<StreamThread> consumer) {\n        final List<StreamThread> copy = new ArrayList<>(threads);\n        for (final StreamThread thread : copy) consumer.accept(thread);\n\n        return copy.size();\n    }\n\n    /**\n     * Returns the internal clients' assigned {@code client instance ids}.\n     *\n     * @return The internal clients' assigned instance ids used for metrics collection.\n     *\n     * @throws IllegalArgumentException If {@code timeout} is negative.\n     * @throws IllegalStateException If {@code KafkaStreams} is not running.\n     * @throws TimeoutException Indicates that a request timed out.\n     * @throws StreamsException For any other error that might occur.\n     */\n    public synchronized ClientInstanceIds clientInstanceIds(final Duration timeout) {\n        if (timeout.isNegative()) {\n            throw new IllegalArgumentException(\"The timeout cannot be negative.\");\n        }\n        if (state().hasNotStarted()) {\n            throw new IllegalStateException(\"KafkaStreams has not been started, you can retry after calling start().\");\n        }\n        if (state().isShuttingDown() || state.hasCompletedShutdown()) {\n            throw new IllegalStateException(\"KafkaStreams has been stopped (\" + state + \").\");\n        }\n\n        final Timer remainingTime = time.timer(timeout.toMillis());\n        final ClientInstanceIdsImpl clientInstanceIds = new ClientInstanceIdsImpl();\n\n        // (1) fan-out calls to threads\n\n        // StreamThread for main/restore consumers and producer(s)\n        final Map<String, KafkaFuture<Uuid>> consumerFutures = new HashMap<>();\n        final Map<String, KafkaFuture<Map<String, KafkaFuture<Uuid>>>> producerFutures = new HashMap<>();\n        synchronized (changeThreadCount) {\n            for (final StreamThread streamThread : threads) {\n                consumerFutures.putAll(streamThread.consumerClientInstanceIds(timeout));\n                producerFutures.put(streamThread.getName(), streamThread.producersClientInstanceIds(timeout));\n            }\n        }\n\n        // GlobalThread\n        KafkaFuture<Uuid> globalThreadFuture = null;\n        if (globalStreamThread != null) {\n            globalThreadFuture = globalStreamThread.globalConsumerInstanceId(timeout);\n        }\n\n        // (2) get admin client instance id in a blocking fashion, while Stream/GlobalThreads work in parallel\n        try {\n            clientInstanceIds.setAdminInstanceId(adminClient.clientInstanceId(timeout));\n            remainingTime.update(time.milliseconds());\n        } catch (final IllegalStateException telemetryDisabledError) {\n            // swallow\n            log.debug(\"Telemetry is disabled on the admin client.\");\n        } catch (final TimeoutException timeoutException) {\n            throw timeoutException;\n        } catch (final Exception error) {\n            throw new StreamsException(\"Could not retrieve admin client instance id.\", error);\n        }\n\n        // (3) collect client instance ids from threads\n\n        // (3a) collect consumers from StreamsThread\n        for (final Map.Entry<String, KafkaFuture<Uuid>> consumerFuture : consumerFutures.entrySet()) {\n            final Uuid instanceId = getOrThrowException(\n                consumerFuture.getValue(),\n                remainingTime.remainingMs(),\n                () -> String.format(\n                    \"Could not retrieve consumer instance id for %s.\",\n                    consumerFuture.getKey()\n                )\n            );\n            remainingTime.update(time.milliseconds());\n\n            // could be `null` if telemetry is disabled on the consumer itself\n            if (instanceId != null) {\n                clientInstanceIds.addConsumerInstanceId(\n                    consumerFuture.getKey(),\n                    instanceId\n                );\n            } else {\n                log.debug(String.format(\"Telemetry is disabled for %s.\", consumerFuture.getKey()));\n            }\n        }\n\n        // (3b) collect producers from StreamsThread\n        for (final Map.Entry<String, KafkaFuture<Map<String, KafkaFuture<Uuid>>>> threadProducerFuture : producerFutures.entrySet()) {\n            final Map<String, KafkaFuture<Uuid>> streamThreadProducerFutures = getOrThrowException(\n                threadProducerFuture.getValue(),\n                remainingTime.remainingMs(),\n                () -> String.format(\n                    \"Could not retrieve producer instance id for %s.\",\n                    threadProducerFuture.getKey()\n                )\n            );\n            remainingTime.update(time.milliseconds());\n\n            for (final Map.Entry<String, KafkaFuture<Uuid>> producerFuture : streamThreadProducerFutures.entrySet()) {\n                final Uuid instanceId = getOrThrowException(\n                    producerFuture.getValue(),\n                    remainingTime.remainingMs(),\n                    () -> String.format(\n                        \"Could not retrieve producer instance id for %s.\",\n                        producerFuture.getKey()\n                    )\n                );\n                remainingTime.update(time.milliseconds());\n\n                // could be `null` if telemetry is disabled on the producer itself\n                if (instanceId != null) {\n                    clientInstanceIds.addProducerInstanceId(\n                        producerFuture.getKey(),\n                        instanceId\n                    );\n                } else {\n                    log.debug(String.format(\"Telemetry is disabled for %s.\", producerFuture.getKey()));\n                }\n            }\n        }\n\n        // (3c) collect from GlobalThread\n        if (globalThreadFuture != null) {\n            final Uuid instanceId = getOrThrowException(\n                globalThreadFuture,\n                remainingTime.remainingMs(),\n                () -> \"Could not retrieve global consumer client instance id.\"\n            );\n            remainingTime.update(time.milliseconds());\n\n            // could be `null` if telemetry is disabled on the client itself\n            if (instanceId != null) {\n                clientInstanceIds.addConsumerInstanceId(\n                    globalStreamThread.getName(),\n                    instanceId\n                );\n            } else {\n                log.debug(\"Telemetry is disabled for the global consumer.\");\n            }\n        }\n\n        return clientInstanceIds;\n    }\n\n    private <T> T getOrThrowException(\n        final KafkaFuture<T> future,\n        final long timeoutMs,\n        final Supplier<String> errorMessage) {\n        final Throwable cause;\n\n        try {\n            return future.get(timeoutMs, TimeUnit.MILLISECONDS);\n        } catch (final java.util.concurrent.TimeoutException timeout) {\n            throw new TimeoutException(errorMessage.get(), timeout);\n        } catch (final ExecutionException exception) {\n            cause = exception.getCause();\n            if (cause instanceof TimeoutException) {\n                throw (TimeoutException) cause;\n            }\n        } catch (final InterruptedException error) {\n            cause = error;\n        }\n\n        throw new StreamsException(errorMessage.get(), cause);\n    }\n\n    /**\n     * Returns runtime information about the local threads of this {@link KafkaStreams} instance.\n     *\n     * @return the set of {@link org.apache.kafka.streams.processor.ThreadMetadata}.\n     * @deprecated since 3.0 use {@link #metadataForLocalThreads()}\n     */\n    @Deprecated\n    public Set<org.apache.kafka.streams.processor.ThreadMetadata> localThreadsMetadata() {\n        return metadataForLocalThreads().stream().map(threadMetadata -> new org.apache.kafka.streams.processor.ThreadMetadata(\n                threadMetadata.threadName(),\n                threadMetadata.threadState(),\n                threadMetadata.consumerClientId(),\n                threadMetadata.restoreConsumerClientId(),\n                threadMetadata.producerClientIds(),\n                threadMetadata.adminClientId(),\n                threadMetadata.activeTasks().stream().map(taskMetadata -> new org.apache.kafka.streams.processor.TaskMetadata(\n                        taskMetadata.taskId().toString(),\n                        taskMetadata.topicPartitions(),\n                        taskMetadata.committedOffsets(),\n                        taskMetadata.endOffsets(),\n                        taskMetadata.timeCurrentIdlingStarted())\n                ).collect(Collectors.toSet()),\n                threadMetadata.standbyTasks().stream().map(taskMetadata -> new org.apache.kafka.streams.processor.TaskMetadata(\n                        taskMetadata.taskId().toString(),\n                        taskMetadata.topicPartitions(),\n                        taskMetadata.committedOffsets(),\n                        taskMetadata.endOffsets(),\n                        taskMetadata.timeCurrentIdlingStarted())\n                ).collect(Collectors.toSet())))\n                .collect(Collectors.toSet());\n    }\n\n    /**\n     * Returns runtime information about the local threads of this {@link KafkaStreams} instance.\n     *\n     * @return the set of {@link ThreadMetadata}.\n     */\n    public Set<ThreadMetadata> metadataForLocalThreads() {\n        final Set<ThreadMetadata> threadMetadata = new HashSet<>();\n        processStreamThread(thread -> {\n            synchronized (thread.getStateLock()) {\n                if (thread.state() != StreamThread.State.DEAD) {\n                    threadMetadata.add(thread.threadMetadata());\n                }\n            }\n        });\n        return threadMetadata;\n    }\n\n    /**\n     * Returns {@link LagInfo}, for all store partitions (active or standby) local to this Streams instance. Note that the\n     * values returned are just estimates and meant to be used for making soft decisions on whether the data in the store\n     * partition is fresh enough for querying.\n     *\n     * <p>Note: Each invocation of this method issues a call to the Kafka brokers. Thus, it's advisable to limit the frequency\n     * of invocation to once every few seconds.\n     *\n     * @return map of store names to another map of partition to {@link LagInfo}s\n     * @throws StreamsException if the admin client request throws exception\n     */\n    public Map<String, Map<Integer, LagInfo>> allLocalStorePartitionLags() {\n        final List<Task> allTasks = new ArrayList<>();\n        processStreamThread(thread -> allTasks.addAll(thread.readyOnlyAllTasks()));\n        return allLocalStorePartitionLags(allTasks);\n    }\n\n    protected Map<String, Map<Integer, LagInfo>> allLocalStorePartitionLags(final List<Task> tasksToCollectLagFor) {\n        final Map<String, Map<Integer, LagInfo>> localStorePartitionLags = new TreeMap<>();\n        final Collection<TopicPartition> allPartitions = new LinkedList<>();\n        final Map<TopicPartition, Long> allChangelogPositions = new HashMap<>();\n\n        // Obtain the current positions, of all the active-restoring and standby tasks\n        for (final Task task : tasksToCollectLagFor) {\n            allPartitions.addAll(task.changelogPartitions());\n            // Note that not all changelog partitions, will have positions; since some may not have started\n            allChangelogPositions.putAll(task.changelogOffsets());\n        }\n\n        log.debug(\"Current changelog positions: {}\", allChangelogPositions);\n        final Map<TopicPartition, ListOffsetsResultInfo> allEndOffsets;\n        allEndOffsets = fetchEndOffsets(allPartitions, adminClient);\n        log.debug(\"Current end offsets :{}\", allEndOffsets);\n\n        for (final Map.Entry<TopicPartition, ListOffsetsResultInfo> entry : allEndOffsets.entrySet()) {\n            // Avoiding an extra admin API lookup by computing lags for not-yet-started restorations\n            // from zero instead of the real \"earliest offset\" for the changelog.\n            // This will yield the correct relative order of lagginess for the tasks in the cluster,\n            // but it is an over-estimate of how much work remains to restore the task from scratch.\n            final long earliestOffset = 0L;\n            final long changelogPosition = allChangelogPositions.getOrDefault(entry.getKey(), earliestOffset);\n            final long latestOffset = entry.getValue().offset();\n            final LagInfo lagInfo = new LagInfo(changelogPosition == Task.LATEST_OFFSET ? latestOffset : changelogPosition, latestOffset);\n            final String storeName = streamsMetadataState.getStoreForChangelogTopic(entry.getKey().topic());\n            localStorePartitionLags.computeIfAbsent(storeName, ignored -> new TreeMap<>())\n                .put(entry.getKey().partition(), lagInfo);\n        }\n\n        return Collections.unmodifiableMap(localStorePartitionLags);\n    }\n\n    /**\n     * Run an interactive query against a state store.\n     * <p>\n     * This method allows callers outside of the Streams runtime to access the internal state of\n     * stateful processors. See <a href=\"https://kafka.apache.org/documentation/streams/developer-guide/interactive-queries.html\">IQ docs</a>\n     * for more information.\n     * <p>\n     * NOTICE: This functionality is {@link Evolving} and subject to change in minor versions.\n     * Once it is stabilized, this notice and the evolving annotation will be removed.\n     *\n     * @param <R> The result type specified by the query.\n     * @throws StreamsNotStartedException If Streams has not yet been started. Just call {@link\n     *                                    KafkaStreams#start()} and then retry this call.\n     * @throws StreamsStoppedException    If Streams is in a terminal state like PENDING_SHUTDOWN,\n     *                                    NOT_RUNNING, PENDING_ERROR, or ERROR. The caller should\n     *                                    discover a new instance to query.\n     * @throws UnknownStateStoreException If the specified store name does not exist in the\n     *                                    topology.\n     */\n    @Evolving\n    public <R> StateQueryResult<R> query(final StateQueryRequest<R> request) {\n        final String storeName = request.getStoreName();\n        if (!topologyMetadata.hasStore(storeName)) {\n            throw new UnknownStateStoreException(\n                \"Cannot get state store \"\n                    + storeName\n                    + \" because no such store is registered in the topology.\"\n            );\n        }\n        if (state().hasNotStarted()) {\n            throw new StreamsNotStartedException(\n                \"KafkaStreams has not been started, you can retry after calling start().\"\n            );\n        }\n        if (state().isShuttingDown() || state.hasCompletedShutdown()) {\n            throw new StreamsStoppedException(\n                \"KafkaStreams has been stopped (\" + state + \").\"\n                    + \" This instance can no longer serve queries.\"\n            );\n        }\n        final StateQueryResult<R> result = new StateQueryResult<>();\n\n        final Map<String, StateStore> globalStateStores = topologyMetadata.globalStateStores();\n        if (globalStateStores.containsKey(storeName)) {\n            // See KAFKA-13523\n            result.setGlobalResult(\n                QueryResult.forFailure(\n                    FailureReason.UNKNOWN_QUERY_TYPE,\n                    \"Global stores do not yet support the KafkaStreams#query API. Use KafkaStreams#store instead.\"\n                )\n            );\n        } else {\n            for (final StreamThread thread : threads) {\n                final Set<Task> tasks = thread.readyOnlyAllTasks();\n                for (final Task task : tasks) {\n\n                    final TaskId taskId = task.id();\n                    final int partition = taskId.partition();\n                    if (request.isAllPartitions() || request.getPartitions().contains(partition)) {\n                        final StateStore store = task.getStore(storeName);\n                        if (store != null) {\n                            final StreamThread.State state = thread.state();\n                            final boolean active = task.isActive();\n                            if (request.isRequireActive()\n                                && (state != StreamThread.State.RUNNING || !active)) {\n\n                                result.addResult(\n                                    partition,\n                                    QueryResult.forFailure(\n                                        FailureReason.NOT_ACTIVE,\n                                        \"Query requires a running active task,\"\n                                            + \" but partition was in state \"\n                                            + state + \" and was \"\n                                            + (active ? \"active\" : \"not active\") + \".\"\n                                    )\n                                );\n                            } else {\n                                final QueryResult<R> r = store.query(\n                                    request.getQuery(),\n                                    request.isRequireActive()\n                                        ? PositionBound.unbounded()\n                                        : request.getPositionBound(),\n                                    new QueryConfig(request.executionInfoEnabled())\n                                );\n                                result.addResult(partition, r);\n                            }\n\n\n                            // optimization: if we have handled all the requested partitions,\n                            // we can return right away.\n                            if (!request.isAllPartitions()\n                                && result.getPartitionResults().keySet().containsAll(request.getPartitions())) {\n                                return result;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n\n        if (!request.isAllPartitions()) {\n            for (final Integer partition : request.getPartitions()) {\n                if (!result.getPartitionResults().containsKey(partition)) {\n                    result.addResult(partition, QueryResult.forFailure(\n                        FailureReason.NOT_PRESENT,\n                        \"The requested partition was not present at the time of the query.\"\n                    ));\n                }\n            }\n        }\n\n        return result;\n    }\n\n}",
                "methodCount": 90
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 5,
                "candidates": [
                    {
                        "lineStart": 1067,
                        "lineEnd": 1091,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method createAndAddStreamThread to class StreamStateListener",
                        "description": "Move method createAndAddStreamThread to org.apache.kafka.streams.KafkaStreams.StreamStateListener\nRationale: The createAndAddStreamThread() method is closely tied to the lifecycle and state transitions of StreamThread objects, which naturally fit into the responsibilities of the StreamStateListener class. The StreamStateListener class already manages thread state changes and transitions, so introducing the creation and registration of StreamThread objects within it would be a coherent and logical enhancement of its existing functionality.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1093,
                        "lineEnd": 1095,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method registerStreamThread to class DelegatingStandbyUpdateListener",
                        "description": "Move method registerStreamThread to org.apache.kafka.streams.KafkaStreams.DelegatingStandbyUpdateListener\nRationale: The `registerStreamThread` method is responsible for managing the state of `StreamThread` objects, which closely aligns with the responsibilities of the `DelegatingStandbyUpdateListener`. This class is tasked with handling updates related to standby replicas for state stores, and it manages similar concerns regarding thread states and partition updates. Therefore, moving `registerStreamThread` to `DelegatingStandbyUpdateListener` will help consolidate state management and thread handling in one appropriate location. This method does not fit the `Time` class since it is unrelated to timekeeping or scheduling tasks, and `DelegatingStateRestoreListener` focuses more on state restoration rather than ongoing thread state management.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1097,
                        "lineEnd": 1107,
                        "refactoringType": "MyMoveStaticMethodRefactoring",
                        "refactoringInfo": "Move Static method getMetrics to class StreamStateListener",
                        "description": "move method getMetrics to PsiClass:StreamStateListener\nRationale: The getMetrics() method appears to be dealing with stream configurations and metrics, which aligns with the responsibilities of the StreamStateListener class that handles stream thread transitions. Metrics are an integral part of monitoring and managing stream states, making StreamStateListener a suitable target class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1342,
                        "lineEnd": 1347,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method getCacheSizePerThread to class StreamStateListener",
                        "description": "Move method getCacheSizePerThread to org.apache.kafka.streams.KafkaStreams.StreamStateListener\nRationale: The method 'getCacheSizePerThread' calculates cache size per thread, which is directly related to stream threads. The 'StreamStateListener' class already handles stream thread transitions and state management. As such, it has relevant context and supporting properties (like thread states and global thread state) which align well with the method's intent. Moving this method to 'StreamStateListener' makes the code more cohesive by keeping stream thread-related logic in one place.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1356,
                        "lineEnd": 1362,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method setupStateDirCleaner to class StreamStateListener",
                        "description": "Move method setupStateDirCleaner to org.apache.kafka.streams.KafkaStreams.StreamStateListener\nRationale: The method setupStateDirCleaner() is related to managing state transitions and stream thread operations, which falls within the responsibility of the StreamStateListener class. The StreamStateListener class handles stream thread transitions and states, making it a suitable location for such a method. Moreover, this positioning would allow easier and more direct access to thread states and transitions, ensuring that the cleanup of state directories is integrated into the overall state management process.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "hasNotStarted",
                            "method_signature": "public hasNotStarted()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isRunningOrRebalancing",
                            "method_signature": "public isRunningOrRebalancing()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isShuttingDown",
                            "method_signature": "public isShuttingDown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasCompletedShutdown",
                            "method_signature": "public hasCompletedShutdown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasStartedOrFinishedShuttingDown",
                            "method_signature": "public hasStartedOrFinishedShuttingDown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isValidTransition",
                            "method_signature": "public isValidTransition(final State newState)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "waitOnState",
                            "method_signature": "private waitOnState(final State targetState, final long waitMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isRunningOrRebalancing",
                            "method_signature": "protected isRunningOrRebalancing()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasStartedOrFinishedShuttingDown",
                            "method_signature": "protected hasStartedOrFinishedShuttingDown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validateIsRunningOrRebalancing",
                            "method_signature": "protected validateIsRunningOrRebalancing()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "defaultStreamsUncaughtExceptionHandler",
                            "method_signature": "private defaultStreamsUncaughtExceptionHandler(final Throwable throwable, final boolean skipThreadReplacement)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replaceStreamThread",
                            "method_signature": "private replaceStreamThread(final Throwable throwable)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleStreamsUncaughtException",
                            "method_signature": "private handleStreamsUncaughtException(final Throwable throwable,\n                                                final StreamsUncaughtExceptionHandler streamsUncaughtExceptionHandler,\n                                                final boolean skipThreadReplacement)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "metrics",
                            "method_signature": "public metrics()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSetRunning",
                            "method_signature": "private maybeSetRunning()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "throwOnFatalException",
                            "method_signature": "private throwOnFatalException(final Exception fatalUserException,\n                                           final TopicPartition topicPartition,\n                                           final String storeName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "throwOnFatalException",
                            "method_signature": "private throwOnFatalException(final Exception fatalUserException,\n                                           final TopicPartition topicPartition,\n                                           final String storeName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createAndAddStreamThread",
                            "method_signature": "private createAndAddStreamThread(final long cacheSizePerThread, final int threadIdx)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "registerStreamThread",
                            "method_signature": "private registerStreamThread(StreamThread streamThread)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getMetrics",
                            "method_signature": "static getMetrics(final StreamsConfig config, final Time time, final String clientId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addStreamThread",
                            "method_signature": "public addStreamThread()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "removeStreamThread",
                            "method_signature": "public removeStreamThread()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "removeStreamThread",
                            "method_signature": "public removeStreamThread(final Duration timeout)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "removeStreamThread",
                            "method_signature": "private removeStreamThread(final long timeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getCacheSizePerThread",
                            "method_signature": "private getCacheSizePerThread(final int numStreamThreads)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resizeThreadCache",
                            "method_signature": "private resizeThreadCache(final long cacheSizePerThread)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "setupStateDirCleaner",
                            "method_signature": "private setupStateDirCleaner()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeCreateRocksDBMetricsRecordingService",
                            "method_signature": "private static maybeCreateRocksDBMetricsRecordingService(final String clientId,\n                                                                                      final StreamsConfig config)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "parseHostInfo",
                            "method_signature": "private static parseHostInfo(final String endPoint)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "start",
                            "method_signature": "public synchronized start()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "timeout",
                            "method_signature": "public timeout(final Duration timeout)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "leaveGroup",
                            "method_signature": "public leaveGroup(final boolean leaveGroup)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "public close()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "shutdownHelper",
                            "method_signature": "private shutdownHelper(final boolean error, final long timeoutMs, final boolean leaveGroup)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "private close(final long timeoutMs, final boolean leaveGroup)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "closeToError",
                            "method_signature": "private closeToError()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "public synchronized close(final Duration timeout)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "public synchronized close(final CloseOptions options)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "streamThreadLeaveConsumerGroup",
                            "method_signature": "private streamThreadLeaveConsumerGroup(final long remainingTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "cleanUp",
                            "method_signature": "public cleanUp()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "allMetadata",
                            "method_signature": "@Deprecated\n    public allMetadata()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "metadataForAllStreamsClients",
                            "method_signature": "public metadataForAllStreamsClients()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "allMetadataForStore",
                            "method_signature": "@Deprecated\n    public allMetadataForStore(final String storeName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "streamsMetadataForStore",
                            "method_signature": "public streamsMetadataForStore(final String storeName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "queryMetadataForKey",
                            "method_signature": "public queryMetadataForKey(final String storeName,\n                                                    final K key,\n                                                    final Serializer<K> keySerializer)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "queryMetadataForKey",
                            "method_signature": "public queryMetadataForKey(final String storeName,\n                                                    final K key,\n                                                    final StreamPartitioner<? super K, ?> partitioner)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "store",
                            "method_signature": "public store(final StoreQueryParameters<T> storeQueryParameters)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pause",
                            "method_signature": "public pause()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isPaused",
                            "method_signature": "public isPaused()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resume",
                            "method_signature": "public resume()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "processStreamThread",
                            "method_signature": "protected processStreamThread(final Consumer<StreamThread> consumer)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "clientInstanceIds",
                            "method_signature": "public synchronized clientInstanceIds(final Duration timeout)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getOrThrowException",
                            "method_signature": "private getOrThrowException(\n        final KafkaFuture<T> future,\n        final long timeoutMs,\n        final Supplier<String> errorMessage)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "localThreadsMetadata",
                            "method_signature": "@Deprecated\n    public localThreadsMetadata()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "metadataForLocalThreads",
                            "method_signature": "public metadataForLocalThreads()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "allLocalStorePartitionLags",
                            "method_signature": "public allLocalStorePartitionLags()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "allLocalStorePartitionLags",
                            "method_signature": "protected allLocalStorePartitionLags(final List<Task> tasksToCollectLagFor)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "query",
                            "method_signature": "@Evolving\n    public query(final StateQueryRequest<R> request)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "createAndAddStreamThread",
                            "method_signature": "private createAndAddStreamThread(final long cacheSizePerThread, final int threadIdx)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "registerStreamThread",
                            "method_signature": "private registerStreamThread(StreamThread streamThread)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "throwOnFatalException",
                            "method_signature": "private throwOnFatalException(final Exception fatalUserException,\n                                           final TopicPartition topicPartition,\n                                           final String storeName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasCompletedShutdown",
                            "method_signature": "public hasCompletedShutdown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isRunningOrRebalancing",
                            "method_signature": "public isRunningOrRebalancing()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isShuttingDown",
                            "method_signature": "public isShuttingDown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasStartedOrFinishedShuttingDown",
                            "method_signature": "public hasStartedOrFinishedShuttingDown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "timeout",
                            "method_signature": "public timeout(final Duration timeout)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "throwOnFatalException",
                            "method_signature": "private throwOnFatalException(final Exception fatalUserException,\n                                           final TopicPartition topicPartition,\n                                           final String storeName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "leaveGroup",
                            "method_signature": "public leaveGroup(final boolean leaveGroup)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getMetrics",
                            "method_signature": "static getMetrics(final StreamsConfig config, final Time time, final String clientId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isValidTransition",
                            "method_signature": "public isValidTransition(final State newState)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasNotStarted",
                            "method_signature": "public hasNotStarted()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getCacheSizePerThread",
                            "method_signature": "private getCacheSizePerThread(final int numStreamThreads)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "setupStateDirCleaner",
                            "method_signature": "private setupStateDirCleaner()",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "createAndAddStreamThread",
                            "method_signature": "private createAndAddStreamThread(final long cacheSizePerThread, final int threadIdx)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "registerStreamThread",
                            "method_signature": "private registerStreamThread(StreamThread streamThread)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "throwOnFatalException",
                            "method_signature": "private throwOnFatalException(final Exception fatalUserException,\n                                           final TopicPartition topicPartition,\n                                           final String storeName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasCompletedShutdown",
                            "method_signature": "public hasCompletedShutdown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isRunningOrRebalancing",
                            "method_signature": "public isRunningOrRebalancing()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isShuttingDown",
                            "method_signature": "public isShuttingDown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasStartedOrFinishedShuttingDown",
                            "method_signature": "public hasStartedOrFinishedShuttingDown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "timeout",
                            "method_signature": "public timeout(final Duration timeout)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "throwOnFatalException",
                            "method_signature": "private throwOnFatalException(final Exception fatalUserException,\n                                           final TopicPartition topicPartition,\n                                           final String storeName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "leaveGroup",
                            "method_signature": "public leaveGroup(final boolean leaveGroup)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getMetrics",
                            "method_signature": "static getMetrics(final StreamsConfig config, final Time time, final String clientId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isValidTransition",
                            "method_signature": "public isValidTransition(final State newState)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasNotStarted",
                            "method_signature": "public hasNotStarted()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getCacheSizePerThread",
                            "method_signature": "private getCacheSizePerThread(final int numStreamThreads)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "setupStateDirCleaner",
                            "method_signature": "private setupStateDirCleaner()",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private createAndAddStreamThread(final long cacheSizePerThread, final int threadIdx)": {
                    "first": {
                        "method_name": "createAndAddStreamThread",
                        "method_signature": "private createAndAddStreamThread(final long cacheSizePerThread, final int threadIdx)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.25696134676569427
                },
                "private registerStreamThread(StreamThread streamThread)": {
                    "first": {
                        "method_name": "registerStreamThread",
                        "method_signature": "private registerStreamThread(StreamThread streamThread)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.29763449627441263
                },
                "private throwOnFatalException(final Exception fatalUserException,\n                                           final TopicPartition topicPartition,\n                                           final String storeName)": {
                    "first": {
                        "method_name": "throwOnFatalException",
                        "method_signature": "private throwOnFatalException(final Exception fatalUserException,\n                                           final TopicPartition topicPartition,\n                                           final String storeName)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.31680266571710364
                },
                "public hasCompletedShutdown()": {
                    "first": {
                        "method_name": "hasCompletedShutdown",
                        "method_signature": "public hasCompletedShutdown()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3139355570463819
                },
                "public isRunningOrRebalancing()": {
                    "first": {
                        "method_name": "isRunningOrRebalancing",
                        "method_signature": "public isRunningOrRebalancing()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.31432307780588564
                },
                "public isShuttingDown()": {
                    "first": {
                        "method_name": "isShuttingDown",
                        "method_signature": "public isShuttingDown()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.31432307780588564
                },
                "public hasStartedOrFinishedShuttingDown()": {
                    "first": {
                        "method_name": "hasStartedOrFinishedShuttingDown",
                        "method_signature": "public hasStartedOrFinishedShuttingDown()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3147105996111831
                },
                "public timeout(final Duration timeout)": {
                    "first": {
                        "method_name": "timeout",
                        "method_signature": "public timeout(final Duration timeout)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.31551276904248693
                },
                "public leaveGroup(final boolean leaveGroup)": {
                    "first": {
                        "method_name": "leaveGroup",
                        "method_signature": "public leaveGroup(final boolean leaveGroup)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.32222736703228017
                },
                "static getMetrics(final StreamsConfig config, final Time time, final String clientId)": {
                    "first": {
                        "method_name": "getMetrics",
                        "method_signature": "static getMetrics(final StreamsConfig config, final Time time, final String clientId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3252175375378335
                },
                "public isValidTransition(final State newState)": {
                    "first": {
                        "method_name": "isValidTransition",
                        "method_signature": "public isValidTransition(final State newState)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.33680107141645255
                },
                "public hasNotStarted()": {
                    "first": {
                        "method_name": "hasNotStarted",
                        "method_signature": "public hasNotStarted()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.35025680359571343
                },
                "private getCacheSizePerThread(final int numStreamThreads)": {
                    "first": {
                        "method_name": "getCacheSizePerThread",
                        "method_signature": "private getCacheSizePerThread(final int numStreamThreads)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.37289391567267377
                },
                "private setupStateDirCleaner()": {
                    "first": {
                        "method_name": "setupStateDirCleaner",
                        "method_signature": "private setupStateDirCleaner()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.411429684342156
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "private createAndAddStreamThread(final long cacheSizePerThread, final int threadIdx)",
                    "private registerStreamThread(StreamThread streamThread)",
                    "public leaveGroup(final boolean leaveGroup)",
                    "static getMetrics(final StreamsConfig config, final Time time, final String clientId)",
                    "private getCacheSizePerThread(final int numStreamThreads)",
                    "private setupStateDirCleaner()",
                    "public hasNotStarted()",
                    "public isValidTransition(final State newState)",
                    "public isRunningOrRebalancing()",
                    "public isShuttingDown()",
                    "public hasCompletedShutdown()",
                    "public hasStartedOrFinishedShuttingDown()",
                    "public timeout(final Duration timeout)"
                ],
                "llm_response_time": 6818
            },
            "targetClassMap": {
                "createAndAddStreamThread": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.08800141237968083
                        },
                        {
                            "class_name": "Admin",
                            "similarity_score": 0.025348039886047998
                        },
                        {
                            "class_name": "StreamStateListener",
                            "similarity_score": 0.22924821597026102
                        },
                        {
                            "class_name": "DelegatingStateRestoreListener",
                            "similarity_score": 0.273138984094458
                        },
                        {
                            "class_name": "KafkaClientSupplier",
                            "similarity_score": 0.014173351371926584
                        },
                        {
                            "class_name": "DelegatingStandbyUpdateListener",
                            "similarity_score": 0.2980564646168641
                        },
                        {
                            "class_name": "StateListener",
                            "similarity_score": 0.07669649888473705
                        },
                        {
                            "class_name": "State",
                            "similarity_score": 0.060090203007582506
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "StreamStateListener",
                        "DelegatingStandbyUpdateListener",
                        "DelegatingStateRestoreListener"
                    ],
                    "llm_response_time": 4119,
                    "similarity_computation_time": 4,
                    "similarity_metric": "cosine"
                },
                "registerStreamThread": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.12222418386066781
                        },
                        {
                            "class_name": "Admin",
                            "similarity_score": 0.028656331856469708
                        },
                        {
                            "class_name": "DelegatingStateRestoreListener",
                            "similarity_score": 0.4244727455521982
                        },
                        {
                            "class_name": "KafkaClientSupplier",
                            "similarity_score": 0.02834670274385317
                        },
                        {
                            "class_name": "DelegatingStandbyUpdateListener",
                            "similarity_score": 0.40544445554499886
                        },
                        {
                            "class_name": "StateListener",
                            "similarity_score": 0.11504474832710555
                        },
                        {
                            "class_name": "State",
                            "similarity_score": 0.08512778759407522
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "DelegatingStandbyUpdateListener",
                        "DelegatingStateRestoreListener",
                        "Time"
                    ],
                    "llm_response_time": 4313,
                    "similarity_computation_time": 5,
                    "similarity_metric": "cosine"
                },
                "leaveGroup": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2508,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "getMetrics": {
                    "target_classes": [
                        {
                            "class_name": "StreamStateListener",
                            "similarity_score": 0.29904043619843756
                        },
                        {
                            "class_name": "DelegatingStandbyUpdateListener",
                            "similarity_score": 0.38463838326272454
                        },
                        {
                            "class_name": "DelegatingStateRestoreListener",
                            "similarity_score": 0.3355751701525133
                        },
                        {
                            "class_name": "CloseOptions",
                            "similarity_score": 0.2302277300680952
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "StreamStateListener",
                        "DelegatingStandbyUpdateListener",
                        "DelegatingStateRestoreListener"
                    ],
                    "llm_response_time": 3606,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "getCacheSizePerThread": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.168501278681587
                        },
                        {
                            "class_name": "Admin",
                            "similarity_score": 0.05448752981302172
                        },
                        {
                            "class_name": "StreamStateListener",
                            "similarity_score": 0.39976215003186305
                        },
                        {
                            "class_name": "DelegatingStateRestoreListener",
                            "similarity_score": 0.39981830569128424
                        },
                        {
                            "class_name": "KafkaClientSupplier",
                            "similarity_score": 0.02791390324071063
                        },
                        {
                            "class_name": "DelegatingStandbyUpdateListener",
                            "similarity_score": 0.3830681232979435
                        },
                        {
                            "class_name": "StateListener",
                            "similarity_score": 0.0755254912049361
                        },
                        {
                            "class_name": "State",
                            "similarity_score": 0.12574207558471995
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "StreamStateListener",
                        "DelegatingStateRestoreListener",
                        "DelegatingStandbyUpdateListener"
                    ],
                    "llm_response_time": 3312,
                    "similarity_computation_time": 3,
                    "similarity_metric": "cosine"
                },
                "setupStateDirCleaner": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.13719630611540523
                        },
                        {
                            "class_name": "StreamsConfig",
                            "similarity_score": 0.289704781485348
                        },
                        {
                            "class_name": "Admin",
                            "similarity_score": 0.048941132493683946
                        },
                        {
                            "class_name": "StreamStateListener",
                            "similarity_score": 0.4135668496183326
                        },
                        {
                            "class_name": "DelegatingStateRestoreListener",
                            "similarity_score": 0.36499732051891387
                        },
                        {
                            "class_name": "KafkaClientSupplier",
                            "similarity_score": 0.026515912428055843
                        },
                        {
                            "class_name": "DelegatingStandbyUpdateListener",
                            "similarity_score": 0.3690083291391448
                        },
                        {
                            "class_name": "StateListener",
                            "similarity_score": 0.10249000771134847
                        },
                        {
                            "class_name": "State",
                            "similarity_score": 0.12044836711932148
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "StreamStateListener",
                        "DelegatingStandbyUpdateListener",
                        "DelegatingStateRestoreListener"
                    ],
                    "llm_response_time": 3877,
                    "similarity_computation_time": 17,
                    "similarity_metric": "cosine"
                },
                "hasNotStarted": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3758,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "isValidTransition": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3313,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "isRunningOrRebalancing": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2022,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "isShuttingDown": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2794,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "hasCompletedShutdown": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2073,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "hasStartedOrFinishedShuttingDown": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 1926,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "timeout": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 1444,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public of(id int, directoryId Uuid) : ReplicaKey extracted from private handleVoteRequest(requestMetadata RaftRequest.Inbound) : VoteResponseData in class org.apache.kafka.raft.KafkaRaftClient & moved to class org.apache.kafka.raft.internals.ReplicaKey",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 634,
                    "endLine": 693,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private handleVoteRequest(requestMetadata RaftRequest.Inbound) : VoteResponseData"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 681,
                    "endLine": 681,
                    "startColumn": 9,
                    "endColumn": 80,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 681,
                    "endLine": 681,
                    "startColumn": 62,
                    "endColumn": 78,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 76,
                    "endLine": 81,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public of(id int, directoryId Uuid) : ReplicaKey"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 77,
                    "endLine": 80,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 79,
                    "endLine": 79,
                    "startColumn": 51,
                    "endColumn": 67,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 646,
                    "endLine": 740,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private handleVoteRequest(requestMetadata RaftRequest.Inbound) : VoteResponseData"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 720,
                    "endLine": 723,
                    "startColumn": 35,
                    "endColumn": 10,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(candidateId,partitionRequest.candidateDirectoryId())"
                }
            ],
            "isStatic": true
        },
        "ref_id": 586,
        "extraction_results": {
            "success": true,
            "newCommitHash": "a1a79576cfdda1fadb6e45c03ef22dde5ee09052",
            "newBranchName": "extract-of-handleVoteRequest-5b0e96d"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public of(id int, directoryId Uuid) : ReplicaKey extracted from public fromVotersRecord(voters VotersRecord) : VoterSet in class org.apache.kafka.raft.internals.VoterSet & moved to class org.apache.kafka.raft.internals.ReplicaKey",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 338,
                    "endLine": 376,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public fromVotersRecord(voters VotersRecord) : VoterSet"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 351,
                    "endLine": 351,
                    "startColumn": 17,
                    "endColumn": 48,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 351,
                    "endLine": 351,
                    "startColumn": 31,
                    "endColumn": 47,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 76,
                    "endLine": 81,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public of(id int, directoryId Uuid) : ReplicaKey"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 77,
                    "endLine": 80,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 79,
                    "endLine": 79,
                    "startColumn": 51,
                    "endColumn": 67,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 368,
                    "endLine": 391,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public fromVotersRecord(voters VotersRecord) : VoterSet"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 380,
                    "endLine": 380,
                    "startColumn": 21,
                    "endColumn": 77,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(voter.voterId(),voter.voterDirectoryId())"
                }
            ],
            "isStatic": true
        },
        "ref_id": 587,
        "extraction_results": {
            "success": true,
            "newCommitHash": "07604225fbb2b54ae0e2274780d4869c6135d8c4",
            "newBranchName": "extract-of-fromVotersRecord-5b0e96d"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public of(id int, directoryId Uuid) : ReplicaKey extracted from package testVotedCandidateWithoutVotedDirectoryId() : void in class org.apache.kafka.raft.ElectionStateTest & moved to class org.apache.kafka.raft.internals.ReplicaKey",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/ElectionStateTest.java",
                    "startLine": 44,
                    "endLine": 55,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package testVotedCandidateWithoutVotedDirectoryId() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/ElectionStateTest.java",
                    "startLine": 46,
                    "endLine": 50,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/ElectionStateTest.java",
                    "startLine": 48,
                    "endLine": 48,
                    "startColumn": 30,
                    "endColumn": 46,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 76,
                    "endLine": 81,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public of(id int, directoryId Uuid) : ReplicaKey"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 77,
                    "endLine": 80,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 79,
                    "endLine": 79,
                    "startColumn": 51,
                    "endColumn": 67,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/ElectionStateTest.java",
                    "startLine": 43,
                    "endLine": 54,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package testVotedCandidateWithoutVotedDirectoryId() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/ElectionStateTest.java",
                    "startLine": 47,
                    "endLine": 47,
                    "startColumn": 13,
                    "endColumn": 57,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(1,ReplicaKey.NO_DIRECTORY_ID)"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/ElectionStateTest.java",
                    "startLine": 50,
                    "endLine": 50,
                    "startColumn": 51,
                    "endColumn": 95,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(1,ReplicaKey.NO_DIRECTORY_ID)"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/ElectionStateTest.java",
                    "startLine": 52,
                    "endLine": 52,
                    "startColumn": 44,
                    "endColumn": 79,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(1,Uuid.randomUuid())"
                }
            ],
            "isStatic": true
        },
        "ref_id": 588,
        "extraction_results": {
            "success": true,
            "newCommitHash": "1213ba4ab5900b98825d3181ce4aaaade032185b",
            "newBranchName": "extract-of-testVotedCandidateWithoutVotedDirectoryId-5b0e96d"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public of(id int, directoryId Uuid) : ReplicaKey extracted from public testRejectVotesFromSameEpochAfterResigningCandidacy() : void in class org.apache.kafka.raft.KafkaRaftClientTest & moved to class org.apache.kafka.raft.internals.ReplicaKey",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 133,
                    "endLine": 154,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testRejectVotesFromSameEpochAfterResigningCandidacy() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 140,
                    "endLine": 143,
                    "startColumn": 9,
                    "endColumn": 22,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 142,
                    "endLine": 142,
                    "startColumn": 63,
                    "endColumn": 79,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 76,
                    "endLine": 81,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public of(id int, directoryId Uuid) : ReplicaKey"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 77,
                    "endLine": 80,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 79,
                    "endLine": 79,
                    "startColumn": 51,
                    "endColumn": 67,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 140,
                    "endLine": 169,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testRejectVotesFromSameEpochAfterResigningCandidacy(withKip853Rpc boolean) : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 150,
                    "endLine": 150,
                    "startColumn": 40,
                    "endColumn": 90,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(localId,ReplicaKey.NO_DIRECTORY_ID)"
                }
            ],
            "isStatic": true
        },
        "ref_id": 589,
        "extraction_results": {
            "success": true,
            "newCommitHash": "c09364c40b313e17967ae8aecd005823c6716904",
            "newBranchName": "extract-of-testRejectVotesFromSameEpochAfterResigningCandidacy-5b0e96d"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public of(id int, directoryId Uuid) : ReplicaKey extracted from public testGrantVotesFromHigherEpochAfterResigningCandidacy() : void in class org.apache.kafka.raft.KafkaRaftClientTest & moved to class org.apache.kafka.raft.internals.ReplicaKey",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 184,
                    "endLine": 210,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testGrantVotesFromHigherEpochAfterResigningCandidacy() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 191,
                    "endLine": 194,
                    "startColumn": 9,
                    "endColumn": 22,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 193,
                    "endLine": 193,
                    "startColumn": 63,
                    "endColumn": 79,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 76,
                    "endLine": 81,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public of(id int, directoryId Uuid) : ReplicaKey"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 77,
                    "endLine": 80,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 79,
                    "endLine": 79,
                    "startColumn": 51,
                    "endColumn": 67,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 207,
                    "endLine": 241,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testGrantVotesFromHigherEpochAfterResigningCandidacy(withKip853Rpc boolean) : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 40,
                    "endColumn": 90,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(localId,ReplicaKey.NO_DIRECTORY_ID)"
                }
            ],
            "isStatic": true
        },
        "ref_id": 590,
        "extraction_results": {
            "success": true,
            "newCommitHash": "4a35ba7fa81b0d225add0b085934b0ac100d809c",
            "newBranchName": "extract-of-testGrantVotesFromHigherEpochAfterResigningCandidacy-5b0e96d"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public of(id int, directoryId Uuid) : ReplicaKey extracted from public testCandidateIgnoreVoteRequestOnSameEpoch() : void in class org.apache.kafka.raft.KafkaRaftClientTest & moved to class org.apache.kafka.raft.internals.ReplicaKey",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 1333,
                    "endLine": 1350,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testCandidateIgnoreVoteRequestOnSameEpoch() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 1340,
                    "endLine": 1342,
                    "startColumn": 9,
                    "endColumn": 22,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 1341,
                    "endLine": 1341,
                    "startColumn": 69,
                    "endColumn": 85,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 76,
                    "endLine": 81,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public of(id int, directoryId Uuid) : ReplicaKey"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 77,
                    "endLine": 80,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 79,
                    "endLine": 79,
                    "startColumn": 51,
                    "endColumn": 67,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 1465,
                    "endLine": 1484,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testCandidateIgnoreVoteRequestOnSameEpoch(withKip853Rpc boolean) : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 1474,
                    "endLine": 1474,
                    "startColumn": 46,
                    "endColumn": 96,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(localId,ReplicaKey.NO_DIRECTORY_ID)"
                }
            ],
            "isStatic": true
        },
        "ref_id": 591,
        "extraction_results": {
            "success": true,
            "newCommitHash": "69922171830e9631f9ae21792260660ce528b49b",
            "newBranchName": "extract-of-testCandidateIgnoreVoteRequestOnSameEpoch-5b0e96d"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public of(id int, directoryId Uuid) : ReplicaKey extracted from private voterNode(id int, directoryId Uuid) : VoterSet.VoterNode in class org.apache.kafka.raft.LeaderStateTest & moved to class org.apache.kafka.raft.internals.ReplicaKey",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/LeaderStateTest.java",
                    "startLine": 924,
                    "endLine": 926,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private voterNode(id int, directoryId Uuid) : VoterSet.VoterNode"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/LeaderStateTest.java",
                    "startLine": 925,
                    "endLine": 925,
                    "startColumn": 9,
                    "endColumn": 96,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/LeaderStateTest.java",
                    "startLine": 925,
                    "endLine": 925,
                    "startColumn": 57,
                    "endColumn": 81,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 76,
                    "endLine": 81,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public of(id int, directoryId Uuid) : ReplicaKey"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 77,
                    "endLine": 80,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 79,
                    "endLine": 79,
                    "startColumn": 70,
                    "endColumn": 94,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/LeaderStateTest.java",
                    "startLine": 1152,
                    "endLine": 1155,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private replicaKey(id int, withDirectoryId boolean) : ReplicaKey"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/LeaderStateTest.java",
                    "startLine": 1154,
                    "endLine": 1154,
                    "startColumn": 16,
                    "endColumn": 46,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(id,directoryId)"
                }
            ],
            "isStatic": true
        },
        "ref_id": 592,
        "extraction_results": {
            "success": true,
            "newCommitHash": "921989f3bae571a01a8168533f4a097f8463c251",
            "newBranchName": "extract-of-voterNode-5b0e96d"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public of(id int, directoryId Uuid) : ReplicaKey extracted from package assertVotedCandidate(epoch int, candidateId int) : void in class org.apache.kafka.raft.RaftClientTestContext & moved to class org.apache.kafka.raft.internals.ReplicaKey",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java",
                    "startLine": 471,
                    "endLine": 480,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package assertVotedCandidate(epoch int, candidateId int) : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java",
                    "startLine": 472,
                    "endLine": 479,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java",
                    "startLine": 475,
                    "endLine": 475,
                    "startColumn": 44,
                    "endColumn": 60,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 76,
                    "endLine": 81,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public of(id int, directoryId Uuid) : ReplicaKey"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 77,
                    "endLine": 80,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 79,
                    "endLine": 79,
                    "startColumn": 51,
                    "endColumn": 67,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java",
                    "startLine": 504,
                    "endLine": 513,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package assertVotedCandidate(epoch int, candidateId int) : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java",
                    "startLine": 508,
                    "endLine": 508,
                    "startColumn": 17,
                    "endColumn": 71,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(candidateId,ReplicaKey.NO_DIRECTORY_ID)"
                }
            ],
            "isStatic": true
        },
        "ref_id": 593,
        "extraction_results": {
            "success": true,
            "newCommitHash": "1010b38457f76212ac13126d380b383dd5a07816",
            "newBranchName": "extract-of-assertVotedCandidate-5b0e96d"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public of(id int, directoryId Uuid) : ReplicaKey extracted from public testCanGrantVoteWithoutDirectoryId(isLogUpToDate boolean) : void in class org.apache.kafka.raft.VotedStateTest & moved to class org.apache.kafka.raft.internals.ReplicaKey",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 81,
                    "endLine": 99,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testCanGrantVoteWithoutDirectoryId(isLogUpToDate boolean) : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 86,
                    "endLine": 88,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 87,
                    "endLine": 87,
                    "startColumn": 55,
                    "endColumn": 71,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 76,
                    "endLine": 81,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public of(id int, directoryId Uuid) : ReplicaKey"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 77,
                    "endLine": 80,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 79,
                    "endLine": 79,
                    "startColumn": 51,
                    "endColumn": 67,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 80,
                    "endLine": 98,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testCanGrantVoteWithoutDirectoryId(isLogUpToDate boolean) : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 86,
                    "endLine": 86,
                    "startColumn": 32,
                    "endColumn": 82,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(votedId,ReplicaKey.NO_DIRECTORY_ID)"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 90,
                    "endLine": 90,
                    "startColumn": 17,
                    "endColumn": 58,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(votedId,Uuid.randomUuid())"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 96,
                    "endLine": 96,
                    "startColumn": 32,
                    "endColumn": 86,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(votedId + 1,ReplicaKey.NO_DIRECTORY_ID)"
                }
            ],
            "isStatic": true
        },
        "ref_id": 594,
        "extraction_results": {
            "success": true,
            "newCommitHash": "a3dcbc8e1113d76e79c62cdfefc1178927822949",
            "newBranchName": "extract-of-testCanGrantVoteWithoutDirectoryId-5b0e96d"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public of(id int, directoryId Uuid) : ReplicaKey extracted from package testCanGrantVoteWithDirectoryId() : void in class org.apache.kafka.raft.VotedStateTest & moved to class org.apache.kafka.raft.internals.ReplicaKey",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 101,
                    "endLine": 115,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package testCanGrantVoteWithDirectoryId() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 111,
                    "endLine": 111,
                    "startColumn": 9,
                    "endColumn": 90,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 111,
                    "endLine": 111,
                    "startColumn": 63,
                    "endColumn": 79,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 76,
                    "endLine": 81,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public of(id int, directoryId Uuid) : ReplicaKey"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 77,
                    "endLine": 80,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 79,
                    "endLine": 79,
                    "startColumn": 51,
                    "endColumn": 67,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 100,
                    "endLine": 114,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package testCanGrantVoteWithDirectoryId() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 108,
                    "endLine": 108,
                    "startColumn": 32,
                    "endColumn": 73,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(votedId,Uuid.randomUuid())"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 110,
                    "endLine": 110,
                    "startColumn": 40,
                    "endColumn": 90,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(votedId,ReplicaKey.NO_DIRECTORY_ID)"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 113,
                    "endLine": 113,
                    "startColumn": 40,
                    "endColumn": 94,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(votedId + 1,ReplicaKey.NO_DIRECTORY_ID)"
                }
            ],
            "isStatic": true
        },
        "ref_id": 595,
        "extraction_results": {
            "success": true,
            "newCommitHash": "a37b52583af45c77835ce95b947b845eb8ca5b72",
            "newBranchName": "extract-of-testCanGrantVoteWithDirectoryId-5b0e96d"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public of(id int, directoryId Uuid) : ReplicaKey extracted from package testRemoveVoter() : void in class org.apache.kafka.raft.internals.VoterSetTest & moved to class org.apache.kafka.raft.internals.ReplicaKey",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/internals/VoterSetTest.java",
                    "startLine": 99,
                    "endLine": 112,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package testRemoveVoter() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/internals/VoterSetTest.java",
                    "startLine": 104,
                    "endLine": 104,
                    "startColumn": 9,
                    "endColumn": 98,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 76,
                    "endLine": 81,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public of(id int, directoryId Uuid) : ReplicaKey"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 77,
                    "endLine": 80,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/internals/VoterSetTest.java",
                    "startLine": 103,
                    "endLine": 116,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package testRemoveVoter() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/internals/VoterSetTest.java",
                    "startLine": 108,
                    "endLine": 108,
                    "startColumn": 61,
                    "endColumn": 105,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(4,ReplicaKey.NO_DIRECTORY_ID)"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/internals/VoterSetTest.java",
                    "startLine": 109,
                    "endLine": 109,
                    "startColumn": 61,
                    "endColumn": 96,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(4,Uuid.randomUuid())"
                }
            ],
            "isStatic": true
        },
        "ref_id": 596,
        "extraction_results": {
            "success": true,
            "newCommitHash": "a067ca529f03eb9895605805a5db023875822a95",
            "newBranchName": "extract-of-testRemoveVoter-5b0e96d"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public of(id int, directoryId Uuid) : ReplicaKey extracted from package testIsVoterWithoutDirectoryId() : void in class org.apache.kafka.raft.internals.VoterSetTest & moved to class org.apache.kafka.raft.internals.ReplicaKey",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/internals/VoterSetTest.java",
                    "startLine": 131,
                    "endLine": 140,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package testIsVoterWithoutDirectoryId() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/internals/VoterSetTest.java",
                    "startLine": 136,
                    "endLine": 136,
                    "startColumn": 9,
                    "endColumn": 74,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/internals/VoterSetTest.java",
                    "startLine": 136,
                    "endLine": 136,
                    "startColumn": 54,
                    "endColumn": 70,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 76,
                    "endLine": 81,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public of(id int, directoryId Uuid) : ReplicaKey"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 77,
                    "endLine": 80,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 79,
                    "endLine": 79,
                    "startColumn": 51,
                    "endColumn": 67,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/internals/VoterSetTest.java",
                    "startLine": 135,
                    "endLine": 144,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package testIsVoterWithoutDirectoryId() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/internals/VoterSetTest.java",
                    "startLine": 140,
                    "endLine": 140,
                    "startColumn": 37,
                    "endColumn": 81,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(1,ReplicaKey.NO_DIRECTORY_ID)"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/internals/VoterSetTest.java",
                    "startLine": 141,
                    "endLine": 141,
                    "startColumn": 37,
                    "endColumn": 72,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(1,Uuid.randomUuid())"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/internals/VoterSetTest.java",
                    "startLine": 142,
                    "endLine": 142,
                    "startColumn": 38,
                    "endColumn": 73,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(4,Uuid.randomUuid())"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/internals/VoterSetTest.java",
                    "startLine": 143,
                    "endLine": 143,
                    "startColumn": 38,
                    "endColumn": 82,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(4,ReplicaKey.NO_DIRECTORY_ID)"
                }
            ],
            "isStatic": true
        },
        "ref_id": 597,
        "extraction_results": {
            "success": true,
            "newCommitHash": "2a5f8c1975fbb531ec70cf01da080908a94bf7ce",
            "newBranchName": "extract-of-testIsVoterWithoutDirectoryId-5b0e96d"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public isVoter(replicaKey ReplicaKey) : boolean extracted from public isVoter(nodeKey ReplicaKey) : boolean in class org.apache.kafka.raft.internals.VoterSet & moved to class org.apache.kafka.raft.internals.VoterSet.VoterNode",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 97,
                    "endLine": 122,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public isVoter(nodeKey ReplicaKey) : boolean"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 117,
                    "endLine": 117,
                    "startColumn": 17,
                    "endColumn": 29,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 113,
                    "endLine": 113,
                    "startColumn": 17,
                    "endColumn": 84,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 112,
                    "endLine": 118,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 112,
                    "endLine": 114,
                    "startColumn": 60,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 114,
                    "endLine": 118,
                    "startColumn": 20,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 303,
                    "endLine": 325,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public isVoter(replicaKey ReplicaKey) : boolean"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 323,
                    "endLine": 323,
                    "startColumn": 17,
                    "endColumn": 29,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 319,
                    "endLine": 319,
                    "startColumn": 17,
                    "endColumn": 80,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 318,
                    "endLine": 324,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 318,
                    "endLine": 320,
                    "startColumn": 53,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 320,
                    "endLine": 324,
                    "startColumn": 20,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 98,
                    "endLine": 114,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public isVoter(replicaKey ReplicaKey) : boolean"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 112,
                    "endLine": 112,
                    "startColumn": 26,
                    "endColumn": 50,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "node.isVoter(replicaKey)"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 316,
                    "endLine": 316,
                    "startColumn": 51,
                    "endColumn": 64,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 316,
                    "endLine": 316,
                    "startColumn": 13,
                    "endColumn": 64,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": false
        },
        "ref_id": 598,
        "extraction_results": {
            "success": true,
            "newCommitHash": "a59dce86eeab4914e3eae243b556d5c0ee07379a",
            "newBranchName": "extract-isVoter-isVoter-5b0e96d"
        },
        "telemetry": {
            "id": "d41e0cdf-37b9-4b00-b308-07067e0bb95d",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 367,
                "lineStart": 40,
                "lineEnd": 406,
                "bodyLineStart": 40,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                "sourceCode": "/**\n * A type for representing the set of voters for a topic partition.\n *\n * It encapsulates static information like a voter's endpoint and their supported kraft.version.\n *\n * It provides functionality for converting to and from {@code VotersRecord} and for converting\n * from the static configuration.\n */\npublic final class VoterSet {\n    private final Map<Integer, VoterNode> voters;\n\n    VoterSet(Map<Integer, VoterNode> voters) {\n        if (voters.isEmpty()) {\n            throw new IllegalArgumentException(\"Voters cannot be empty\");\n        }\n\n        this.voters = voters;\n    }\n\n    /**\n     * Returns the node information for all the given voter ids and listener.\n     *\n     * @param voterIds the ids of the voters\n     * @param listenerName the name of the listener\n     * @return the node information for all of the voter ids\n     * @throws IllegalArgumentException if there are missing endpoints\n     */\n    public Set<Node> voterNodes(Stream<Integer> voterIds, ListenerName listenerName) {\n        return voterIds\n            .map(voterId ->\n                voterNode(voterId, listenerName).orElseThrow(() ->\n                    new IllegalArgumentException(\n                        String.format(\n                            \"Unable to find endpoint for voter %d and listener %s in %s\",\n                            voterId,\n                            listenerName,\n                            voters\n                        )\n                    )\n                )\n            )\n            .collect(Collectors.toSet());\n    }\n\n    /**\n     * Returns the node information for a given voter id and listener.\n     *\n     * @param voterId the id of the voter\n     * @param listenerName the name of the listener\n     * @return the node information if it exists, otherwise {@code Optional.empty()}\n     */\n    public Optional<Node> voterNode(int voterId, ListenerName listenerName) {\n        return Optional.ofNullable(voters.get(voterId))\n            .flatMap(voterNode -> voterNode.address(listenerName))\n            .map(address -> new Node(voterId, address.getHostString(), address.getPort()));\n    }\n\n    /**\n     * Returns if the node is a voter in the set of voters.\n     *\n     * If the voter set includes the directory id, the {@code nodeKey} directory id must match the\n     * directory id specified by the voter set.\n     *\n     * If the voter set doesn't include the directory id ({@code Optional.empty()}), a node is in\n     * the voter set as long as the node id matches. The directory id is not checked.\n     *\n     * @param nodeKey the node's id and directory id\n     * @return true if the node is a voter in the voter set, otherwise false\n     */\n    public boolean isVoter(ReplicaKey nodeKey) {\n        VoterNode node = voters.get(nodeKey.id());\n        if (node != null) {\n            return isVoter(nodeKey, node);\n        } else {\n            return false;\n        }\n    }\n\n    private boolean isVoter(ReplicaKey nodeKey, VoterNode node) {\n        if (node.voterKey().directoryId().isPresent()) {\n            return node.voterKey().directoryId().equals(nodeKey.directoryId());\n        } else {\n            // configured voter set doesn't include a directory id so it is a voter as long as the node id\n            // matches\n            return true;\n        }\n    }\n\n    /**\n     * Returns if the node is the only voter in the set of voters.\n     *\n     * @param nodeKey the node's id and directory id\n     * @return true if the node is the only voter in the voter set, otherwise false\n     */\n    public boolean isOnlyVoter(ReplicaKey nodeKey) {\n        return voters.size() == 1 && isVoter(nodeKey);\n    }\n\n    /**\n     * Returns all of the voter ids.\n     */\n    public Set<Integer> voterIds() {\n        return voters.keySet();\n    }\n\n    public Map<Integer, VoterNode> voters() {\n        return voters;\n    }\n\n    /**\n     * Adds a voter to the voter set.\n     *\n     * This object is immutable. A new voter set is returned if the voter was added.\n     *\n     * A new voter can be added to a voter set if its id doesn't already exist in the voter set.\n     *\n     * @param voter the new voter to add\n     * @return a new voter set if the voter was added, otherwise {@code Optional.empty()}\n     */\n    public Optional<VoterSet> addVoter(VoterNode voter) {\n        if (voters.containsKey(voter.voterKey().id())) {\n            return Optional.empty();\n        }\n\n        HashMap<Integer, VoterNode> newVoters = new HashMap<>(voters);\n        newVoters.put(voter.voterKey().id(), voter);\n\n        return Optional.of(new VoterSet(newVoters));\n    }\n\n    /**\n     * Remove a voter from the voter set.\n     *\n     * This object is immutable. A new voter set is returned if the voter was removed.\n     *\n     * A voter can be removed from the voter set if its id and directory id match.\n     *\n     * @param voterKey the voter key\n     * @return a new voter set if the voter was removed, otherwise {@code Optional.empty()}\n     */\n    public Optional<VoterSet> removeVoter(ReplicaKey voterKey) {\n        VoterNode oldVoter = voters.get(voterKey.id());\n        if (oldVoter != null && Objects.equals(oldVoter.voterKey(), voterKey)) {\n            HashMap<Integer, VoterNode> newVoters = new HashMap<>(voters);\n            newVoters.remove(voterKey.id());\n\n            return Optional.of(new VoterSet(newVoters));\n        }\n\n        return Optional.empty();\n    }\n\n    /**\n     * Converts a voter set to a voters record for a given version.\n     *\n     * @param version the version of the voters record\n     */\n    public VotersRecord toVotersRecord(short version) {\n        Function<VoterNode, VotersRecord.Voter> voterConvertor = voter -> {\n            Iterator<VotersRecord.Endpoint> endpoints = voter\n                .listeners()\n                .entrySet()\n                .stream()\n                .map(entry ->\n                    new VotersRecord.Endpoint()\n                        .setName(entry.getKey().value())\n                        .setHost(entry.getValue().getHostString())\n                        .setPort(entry.getValue().getPort())\n                )\n                .iterator();\n\n            VotersRecord.KRaftVersionFeature kraftVersionFeature = new VotersRecord.KRaftVersionFeature()\n                .setMinSupportedVersion(voter.supportedKRaftVersion().min())\n                .setMaxSupportedVersion(voter.supportedKRaftVersion().max());\n\n            return new VotersRecord.Voter()\n                .setVoterId(voter.voterKey().id())\n                .setVoterDirectoryId(voter.voterKey().directoryId().orElse(Uuid.ZERO_UUID))\n                .setEndpoints(new VotersRecord.EndpointCollection(endpoints))\n                .setKRaftVersionFeature(kraftVersionFeature);\n        };\n\n        List<VotersRecord.Voter> voterRecordVoters = voters\n            .values()\n            .stream()\n            .map(voterConvertor)\n            .collect(Collectors.toList());\n\n        return new VotersRecord()\n            .setVersion(version)\n            .setVoters(voterRecordVoters);\n    }\n\n    /**\n     * Determines if two sets of voters have an overlapping majority.\n     *\n     * An overlapping majority means that for all majorities in {@code this} set of voters and for\n     * all majority in {@code that} set of voters, they have at least one voter in common.\n     *\n     * If this function returns true, it means that if one of the set of voters commits an offset,\n     * the other set of voters cannot commit a conflicting offset.\n     *\n     * @param that the other voter set to compare\n     * @return true if they have an overlapping majority, false otherwise\n     */\n    public boolean hasOverlappingMajority(VoterSet that) {\n        Set<ReplicaKey> thisReplicaKeys = voters\n            .values()\n            .stream()\n            .map(VoterNode::voterKey)\n            .collect(Collectors.toSet());\n\n        Set<ReplicaKey> thatReplicaKeys = that.voters\n            .values()\n            .stream()\n            .map(VoterNode::voterKey)\n            .collect(Collectors.toSet());\n\n        if (Utils.diff(HashSet::new, thisReplicaKeys, thatReplicaKeys).size() > 1) return false;\n        return Utils.diff(HashSet::new, thatReplicaKeys, thisReplicaKeys).size() <= 1;\n    }\n\n    @Override\n    public boolean equals(Object o) {\n        if (this == o) return true;\n        if (o == null || getClass() != o.getClass()) return false;\n\n        VoterSet that = (VoterSet) o;\n\n        return voters.equals(that.voters);\n    }\n\n    @Override\n    public int hashCode() {\n        return Objects.hashCode(voters);\n    }\n\n    @Override\n    public String toString() {\n        return String.format(\"VoterSet(voters=%s)\", voters);\n    }\n\n    public static final class VoterNode {\n        private final ReplicaKey voterKey;\n        private final Map<ListenerName, InetSocketAddress> listeners;\n        private final SupportedVersionRange supportedKRaftVersion;\n\n        public VoterNode(\n            ReplicaKey voterKey,\n            Map<ListenerName, InetSocketAddress> listeners,\n            SupportedVersionRange supportedKRaftVersion\n        ) {\n            this.voterKey = voterKey;\n            this.listeners = listeners;\n            this.supportedKRaftVersion = supportedKRaftVersion;\n        }\n\n        public ReplicaKey voterKey() {\n            return voterKey;\n        }\n\n        Map<ListenerName, InetSocketAddress> listeners() {\n            return listeners;\n        }\n\n        SupportedVersionRange supportedKRaftVersion() {\n            return supportedKRaftVersion;\n        }\n\n\n        Optional<InetSocketAddress> address(ListenerName listener) {\n            return Optional.ofNullable(listeners.get(listener));\n        }\n\n        @Override\n        public boolean equals(Object o) {\n            if (this == o) return true;\n            if (o == null || getClass() != o.getClass()) return false;\n\n            VoterNode that = (VoterNode) o;\n\n            if (!Objects.equals(voterKey, that.voterKey)) return false;\n            if (!Objects.equals(supportedKRaftVersion, that.supportedKRaftVersion)) return false;\n            return Objects.equals(listeners, that.listeners);\n        }\n\n        @Override\n        public int hashCode() {\n            return Objects.hash(voterKey, listeners, supportedKRaftVersion);\n        }\n\n        @Override\n        public String toString() {\n            return String.format(\n                \"VoterNode(voterKey=%s, listeners=%s, supportedKRaftVersion=%s)\",\n                voterKey,\n                listeners,\n                supportedKRaftVersion\n            );\n        }\n    }\n\n    /**\n     * Converts a {@code VotersRecord} to a {@code VoterSet}.\n     *\n     * @param voters the set of voters control record\n     * @return the voter set\n     */\n    public static VoterSet fromVotersRecord(VotersRecord voters) {\n        HashMap<Integer, VoterNode> voterNodes = new HashMap<>(voters.voters().size());\n        for (VotersRecord.Voter voter: voters.voters()) {\n            final Optional<Uuid> directoryId;\n            if (!voter.voterDirectoryId().equals(Uuid.ZERO_UUID)) {\n                directoryId = Optional.of(voter.voterDirectoryId());\n            } else {\n                directoryId = Optional.empty();\n            }\n\n            Map<ListenerName, InetSocketAddress> listeners = new HashMap<>(voter.endpoints().size());\n            for (VotersRecord.Endpoint endpoint : voter.endpoints()) {\n                listeners.put(\n                    ListenerName.normalised(endpoint.name()),\n                    InetSocketAddress.createUnresolved(endpoint.host(), endpoint.port())\n                );\n            }\n\n            voterNodes.put(\n                voter.voterId(),\n                new VoterNode(\n                    ReplicaKey.of(voter.voterId(), directoryId),\n                    listeners,\n                    new SupportedVersionRange(\n                        voter.kRaftVersionFeature().minSupportedVersion(),\n                        voter.kRaftVersionFeature().maxSupportedVersion()\n                    )\n                )\n            );\n        }\n\n        return new VoterSet(voterNodes);\n    }\n\n    /**\n     * Creates a voter set from a map of socket addresses.\n     *\n     * @param listener the listener name for all of the endpoints\n     * @param voters the socket addresses by voter id\n     * @return the voter set\n     */\n    public static VoterSet fromInetSocketAddresses(ListenerName listener, Map<Integer, InetSocketAddress> voters) {\n        Map<Integer, VoterNode> voterNodes = voters\n            .entrySet()\n            .stream()\n            .collect(\n                Collectors.toMap(\n                    Map.Entry::getKey,\n                    entry -> new VoterNode(\n                        ReplicaKey.of(entry.getKey(), Optional.empty()),\n                        Collections.singletonMap(listener, entry.getValue()),\n                        new SupportedVersionRange((short) 0, (short) 0)\n                    )\n                )\n            );\n\n        return new VoterSet(voterNodes);\n    }\n}",
                "methodCount": 25
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 4,
                "candidates": [
                    {
                        "lineStart": 341,
                        "lineEnd": 379,
                        "refactoringType": "MyMoveStaticMethodRefactoring",
                        "refactoringInfo": "Move Static method fromVotersRecord to class KRaftControlRecordStateMachine",
                        "description": "move method fromVotersRecord to PsiClass:KRaftControlRecordStateMachine\nRationale: The method `fromVotersRecord` is designed to convert a `VotersRecord` into a `VoterSet`. This functionality directly relates to managing and maintaining voter sets, which is a core responsibility of the `KRaftControlRecordStateMachine` class. `KRaftControlRecordStateMachine` already has operations that leverage `VoterSet` and manage control records for KRaft (Kafka Raft). Therefore, transferring this method to `KRaftControlRecordStateMachine` will reduce coupling and keep related functionality closer together, enhancing maintainability and coherence. Additionally, the method is unrelated to batch accumulation or record iteration, making `BatchAccumulator` and `RecordsIterator` unsuitable choices.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 381,
                        "lineEnd": 404,
                        "refactoringType": "MyMoveStaticMethodRefactoring",
                        "refactoringInfo": "Move Static method fromInetSocketAddresses to class VoterSetHistory",
                        "description": "move method fromInetSocketAddresses to PsiClass:VoterSetHistory\nRationale: The purpose of the method fromInetSocketAddresses() is to create a VoterSet from a map of socket addresses. Given the class VoterSetHistory is focused on managing VoterSet objects and their histories, it's logical to move the method here. This method fits well within VoterSetHistory's responsibilities of dealing with VoterSet objects. Also, VoterSetHistory already has a close relationship with VoterSet, further justifying the relocation.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 117,
                        "lineEnd": 125,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method isVoter to class VoterNode",
                        "description": "Move method isVoter to org.apache.kafka.raft.internals.VoterSet.VoterNode\nRationale: The isVoter() method is more closely related to the VoterNode class as it performs a check specifically related to its voting eligibility based on the voterKey and potentially a directoryId. The logic inside this method also accesses the data members like voterKey() from VoterNode. Therefore, it is more appropriate and semantically correct to place this method within the VoterNode class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 148,
                        "lineEnd": 167,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method addVoter to class VoterNode",
                        "description": "Move method addVoter to org.apache.kafka.raft.internals.VoterSet.VoterNode\nRationale: null",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "voterNodes",
                            "method_signature": "public voterNodes(Stream<Integer> voterIds, ListenerName listenerName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voterNode",
                            "method_signature": "public voterNode(int voterId, ListenerName listenerName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isVoter",
                            "method_signature": "public isVoter(ReplicaKey nodeKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isVoter",
                            "method_signature": "private isVoter(ReplicaKey nodeKey, VoterNode node)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isOnlyVoter",
                            "method_signature": "public isOnlyVoter(ReplicaKey nodeKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voterIds",
                            "method_signature": "public voterIds()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addVoter",
                            "method_signature": "public addVoter(VoterNode voter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "removeVoter",
                            "method_signature": "public removeVoter(ReplicaKey voterKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "toVotersRecord",
                            "method_signature": "public toVotersRecord(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasOverlappingMajority",
                            "method_signature": "public hasOverlappingMajority(VoterSet that)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "address",
                            "method_signature": " address(ListenerName listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fromVotersRecord",
                            "method_signature": "public static fromVotersRecord(VotersRecord voters)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fromInetSocketAddresses",
                            "method_signature": "public static fromInetSocketAddresses(ListenerName listener, Map<Integer, InetSocketAddress> voters)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "address",
                            "method_signature": " address(ListenerName listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isVoter",
                            "method_signature": "private isVoter(ReplicaKey nodeKey, VoterNode node)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasOverlappingMajority",
                            "method_signature": "public hasOverlappingMajority(VoterSet that)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "toVotersRecord",
                            "method_signature": "public toVotersRecord(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voterIds",
                            "method_signature": "public voterIds()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fromVotersRecord",
                            "method_signature": "public static fromVotersRecord(VotersRecord voters)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voterNodes",
                            "method_signature": "public voterNodes(Stream<Integer> voterIds, ListenerName listenerName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fromInetSocketAddresses",
                            "method_signature": "public static fromInetSocketAddresses(ListenerName listener, Map<Integer, InetSocketAddress> voters)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voterNode",
                            "method_signature": "public voterNode(int voterId, ListenerName listenerName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isOnlyVoter",
                            "method_signature": "public isOnlyVoter(ReplicaKey nodeKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isVoter",
                            "method_signature": "public isVoter(ReplicaKey nodeKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addVoter",
                            "method_signature": "public addVoter(VoterNode voter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "removeVoter",
                            "method_signature": "public removeVoter(ReplicaKey voterKey)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "address",
                            "method_signature": " address(ListenerName listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isVoter",
                            "method_signature": "private isVoter(ReplicaKey nodeKey, VoterNode node)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasOverlappingMajority",
                            "method_signature": "public hasOverlappingMajority(VoterSet that)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "toVotersRecord",
                            "method_signature": "public toVotersRecord(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voterIds",
                            "method_signature": "public voterIds()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fromVotersRecord",
                            "method_signature": "public static fromVotersRecord(VotersRecord voters)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voterNodes",
                            "method_signature": "public voterNodes(Stream<Integer> voterIds, ListenerName listenerName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fromInetSocketAddresses",
                            "method_signature": "public static fromInetSocketAddresses(ListenerName listener, Map<Integer, InetSocketAddress> voters)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voterNode",
                            "method_signature": "public voterNode(int voterId, ListenerName listenerName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isOnlyVoter",
                            "method_signature": "public isOnlyVoter(ReplicaKey nodeKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isVoter",
                            "method_signature": "public isVoter(ReplicaKey nodeKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addVoter",
                            "method_signature": "public addVoter(VoterNode voter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "removeVoter",
                            "method_signature": "public removeVoter(ReplicaKey voterKey)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                " address(ListenerName listener)": {
                    "first": {
                        "method_name": "address",
                        "method_signature": " address(ListenerName listener)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.27535262953636336
                },
                "private isVoter(ReplicaKey nodeKey, VoterNode node)": {
                    "first": {
                        "method_name": "isVoter",
                        "method_signature": "private isVoter(ReplicaKey nodeKey, VoterNode node)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5250976466655825
                },
                "public hasOverlappingMajority(VoterSet that)": {
                    "first": {
                        "method_name": "hasOverlappingMajority",
                        "method_signature": "public hasOverlappingMajority(VoterSet that)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5910342605513871
                },
                "public toVotersRecord(short version)": {
                    "first": {
                        "method_name": "toVotersRecord",
                        "method_signature": "public toVotersRecord(short version)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5969578432683813
                },
                "public voterIds()": {
                    "first": {
                        "method_name": "voterIds",
                        "method_signature": "public voterIds()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.599922411402502
                },
                "public static fromVotersRecord(VotersRecord voters)": {
                    "first": {
                        "method_name": "fromVotersRecord",
                        "method_signature": "public static fromVotersRecord(VotersRecord voters)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6531231583801658
                },
                "public voterNodes(Stream<Integer> voterIds, ListenerName listenerName)": {
                    "first": {
                        "method_name": "voterNodes",
                        "method_signature": "public voterNodes(Stream<Integer> voterIds, ListenerName listenerName)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.663328561495416
                },
                "public static fromInetSocketAddresses(ListenerName listener, Map<Integer, InetSocketAddress> voters)": {
                    "first": {
                        "method_name": "fromInetSocketAddresses",
                        "method_signature": "public static fromInetSocketAddresses(ListenerName listener, Map<Integer, InetSocketAddress> voters)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.7177645871971503
                },
                "public voterNode(int voterId, ListenerName listenerName)": {
                    "first": {
                        "method_name": "voterNode",
                        "method_signature": "public voterNode(int voterId, ListenerName listenerName)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.7281878658067636
                },
                "public isOnlyVoter(ReplicaKey nodeKey)": {
                    "first": {
                        "method_name": "isOnlyVoter",
                        "method_signature": "public isOnlyVoter(ReplicaKey nodeKey)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.7370304643520827
                },
                "public isVoter(ReplicaKey nodeKey)": {
                    "first": {
                        "method_name": "isVoter",
                        "method_signature": "public isVoter(ReplicaKey nodeKey)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.7520933032747849
                },
                "public addVoter(VoterNode voter)": {
                    "first": {
                        "method_name": "addVoter",
                        "method_signature": "public addVoter(VoterNode voter)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.756175773318763
                },
                "public removeVoter(ReplicaKey voterKey)": {
                    "first": {
                        "method_name": "removeVoter",
                        "method_signature": "public removeVoter(ReplicaKey voterKey)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.7980538348554548
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "public hasOverlappingMajority(VoterSet that)",
                    "public toVotersRecord(short version)",
                    "public static fromVotersRecord(VotersRecord voters)",
                    "public static fromInetSocketAddresses(ListenerName listener, Map<Integer, InetSocketAddress> voters)",
                    "public voterNodes(Stream<Integer> voterIds, ListenerName listenerName)",
                    "public voterNode(int voterId, ListenerName listenerName)",
                    "private isVoter(ReplicaKey nodeKey, VoterNode node)",
                    "public addVoter(VoterNode voter)",
                    "public removeVoter(ReplicaKey voterKey)",
                    "public isVoter(ReplicaKey nodeKey)",
                    "public isOnlyVoter(ReplicaKey nodeKey)",
                    "public voterIds()"
                ],
                "llm_response_time": 4061
            },
            "targetClassMap": {
                "hasOverlappingMajority": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 1713,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "toVotersRecord": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3648,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "fromVotersRecord": {
                    "target_classes": [
                        {
                            "class_name": "MemoryBatchReader",
                            "similarity_score": 0.46940354265485384
                        },
                        {
                            "class_name": "ReplicaKey",
                            "similarity_score": 0.43780091086244954
                        },
                        {
                            "class_name": "RecordsBatchReader",
                            "similarity_score": 0.5527613203990577
                        },
                        {
                            "class_name": "RecordsIterator",
                            "similarity_score": 0.6399641295823094
                        },
                        {
                            "class_name": "KafkaRaftMetrics",
                            "similarity_score": 0.527153058955667
                        },
                        {
                            "class_name": "KRaftControlRecordStateMachine",
                            "similarity_score": 0.6031716388154883
                        },
                        {
                            "class_name": "IdentitySerde",
                            "similarity_score": 0.3697557379316652
                        },
                        {
                            "class_name": "CompletedBatch",
                            "similarity_score": 0.40705392790793005
                        },
                        {
                            "class_name": "Entry",
                            "similarity_score": 0.4020924292725762
                        },
                        {
                            "class_name": "BlockingMessageQueue",
                            "similarity_score": 0.5260116800583926
                        },
                        {
                            "class_name": "BatchAccumulator",
                            "similarity_score": 0.6688721890352016
                        },
                        {
                            "class_name": "BatchBuilder",
                            "similarity_score": 0.5909074420901153
                        },
                        {
                            "class_name": "BatchMemoryPool",
                            "similarity_score": 0.5839709333507171
                        },
                        {
                            "class_name": "ThresholdPurgatory",
                            "similarity_score": 0.5542842135038807
                        },
                        {
                            "class_name": "VoterNode",
                            "similarity_score": 0.4506945701674536
                        },
                        {
                            "class_name": "VoterSetHistory",
                            "similarity_score": 0.5597584873113235
                        },
                        {
                            "class_name": "TimeRatio",
                            "similarity_score": 0.5451668039066979
                        },
                        {
                            "class_name": "TreeMapLogHistory",
                            "similarity_score": 0.5402078257018265
                        },
                        {
                            "class_name": "StringSerde",
                            "similarity_score": 0.37644264027483737
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "KRaftControlRecordStateMachine",
                        "BatchAccumulator",
                        "RecordsIterator"
                    ],
                    "llm_response_time": 3763,
                    "similarity_computation_time": 4,
                    "similarity_metric": "cosine"
                },
                "fromInetSocketAddresses": {
                    "target_classes": [
                        {
                            "class_name": "MemoryBatchReader",
                            "similarity_score": 0.1952100584619064
                        },
                        {
                            "class_name": "ReplicaKey",
                            "similarity_score": 0.1598844120297036
                        },
                        {
                            "class_name": "RecordsBatchReader",
                            "similarity_score": 0.1899052894697335
                        },
                        {
                            "class_name": "RecordsIterator",
                            "similarity_score": 0.3184390418582076
                        },
                        {
                            "class_name": "ThresholdPurgatory",
                            "similarity_score": 0.19306396717321508
                        },
                        {
                            "class_name": "TimeRatio",
                            "similarity_score": 0.3995261043519034
                        },
                        {
                            "class_name": "VoterNode",
                            "similarity_score": 0.17254242808037282
                        },
                        {
                            "class_name": "VoterSetHistory",
                            "similarity_score": 0.6485831580034436
                        },
                        {
                            "class_name": "StringSerde",
                            "similarity_score": 0.13719216180104993
                        },
                        {
                            "class_name": "TreeMapLogHistory",
                            "similarity_score": 0.2801317053127485
                        },
                        {
                            "class_name": "CompletedBatch",
                            "similarity_score": 0.234027255727688
                        },
                        {
                            "class_name": "Entry",
                            "similarity_score": 0.14533024159093358
                        },
                        {
                            "class_name": "BatchAccumulator",
                            "similarity_score": 0.49095761276934125
                        },
                        {
                            "class_name": "BatchBuilder",
                            "similarity_score": 0.5229793280529779
                        },
                        {
                            "class_name": "BatchMemoryPool",
                            "similarity_score": 0.4842512625630111
                        },
                        {
                            "class_name": "BlockingMessageQueue",
                            "similarity_score": 0.19354119452957066
                        },
                        {
                            "class_name": "KRaftControlRecordStateMachine",
                            "similarity_score": 0.5375429399796611
                        },
                        {
                            "class_name": "KafkaRaftMetrics",
                            "similarity_score": 0.25446537859445423
                        },
                        {
                            "class_name": "IdentitySerde",
                            "similarity_score": 0.1418475390906605
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "VoterSetHistory",
                        "KRaftControlRecordStateMachine",
                        "BatchBuilder"
                    ],
                    "llm_response_time": 4333,
                    "similarity_computation_time": 4,
                    "similarity_metric": "cosine"
                },
                "voterNodes": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 4168,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "voterNode": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2955,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "isVoter": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3126,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "addVoter": {
                    "target_classes": [
                        {
                            "class_name": "VoterNode",
                            "similarity_score": 0.19311110245460245
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "VoterNode"
                    ],
                    "llm_response_time": 2261,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "removeVoter": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3415,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "isOnlyVoter": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 1976,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "voterIds": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2984,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public voterKeys() : Set<ReplicaKey> extracted from public advanceLocalLeaderHighWatermarkToLogEndOffset() : void in class org.apache.kafka.raft.RaftClientTestContext & moved to class org.apache.kafka.raft.internals.VoterSet",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java",
                    "startLine": 1156,
                    "endLine": 1171,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public advanceLocalLeaderHighWatermarkToLogEndOffset() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java",
                    "startLine": 1159,
                    "endLine": 1159,
                    "startColumn": 9,
                    "endColumn": 123,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 133,
                    "endLine": 142,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public voterKeys() : Set<ReplicaKey>"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 137,
                    "endLine": 141,
                    "startColumn": 9,
                    "endColumn": 42,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java",
                    "startLine": 1466,
                    "endLine": 1487,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public advanceLocalLeaderHighWatermarkToLogEndOffset() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java",
                    "startLine": 1470,
                    "endLine": 1471,
                    "startColumn": 48,
                    "endColumn": 25,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "voters.voterKeys()"
                }
            ],
            "isStatic": false
        },
        "ref_id": 599,
        "extraction_results": {
            "success": true,
            "newCommitHash": "62284eacd3fc8ac2c2d1c96aec657cfc93a1434c",
            "newBranchName": "extract-voterKeys-advanceLocalLeaderHighWatermarkToLogEndOffset-5b0e96d"
        },
        "telemetry": {
            "id": "2f386bc5-ce30-4aaf-a358-20bb5f9aa129",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1218,
                "lineStart": 94,
                "lineEnd": 1311,
                "bodyLineStart": 94,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java",
                "sourceCode": "public final class RaftClientTestContext {\n    public final RecordSerde<String> serde = Builder.SERDE;\n    final TopicPartition metadataPartition = Builder.METADATA_PARTITION;\n    final Uuid metadataTopicId = Uuid.METADATA_TOPIC_ID;\n    final int electionBackoffMaxMs = Builder.ELECTION_BACKOFF_MAX_MS;\n    final int fetchMaxWaitMs = Builder.FETCH_MAX_WAIT_MS;\n    final int fetchTimeoutMs = Builder.FETCH_TIMEOUT_MS;\n    final int checkQuorumTimeoutMs = (int) (fetchTimeoutMs * CHECK_QUORUM_TIMEOUT_FACTOR);\n    final int retryBackoffMs = Builder.RETRY_BACKOFF_MS;\n\n    private int electionTimeoutMs;\n    private int requestTimeoutMs;\n    private int appendLingerMs;\n\n    private final QuorumStateStore quorumStateStore;\n    final Uuid clusterId;\n    private final OptionalInt localId;\n    public final KafkaRaftClient<String> client;\n    final Metrics metrics;\n    public final MockLog log;\n    final MockNetworkChannel channel;\n    final MockMessageQueue messageQueue;\n    final MockTime time;\n    final MockListener listener;\n    final Set<Integer> voters;\n    final Set<Integer> bootstrapIds;\n\n    private final List<RaftResponse.Outbound> sentResponses = new ArrayList<>();\n\n    public static final class Builder {\n        static final int DEFAULT_ELECTION_TIMEOUT_MS = 10000;\n\n        private static final RecordSerde<String> SERDE = new StringSerde();\n        private static final TopicPartition METADATA_PARTITION = new TopicPartition(\"metadata\", 0);\n        private static final int ELECTION_BACKOFF_MAX_MS = 100;\n        private static final int FETCH_MAX_WAIT_MS = 0;\n        // fetch timeout is usually larger than election timeout\n        private static final int FETCH_TIMEOUT_MS = 50000;\n        private static final int DEFAULT_REQUEST_TIMEOUT_MS = 5000;\n        private static final int RETRY_BACKOFF_MS = 50;\n        private static final int DEFAULT_APPEND_LINGER_MS = 0;\n\n        private final MockMessageQueue messageQueue = new MockMessageQueue();\n        private final MockTime time = new MockTime();\n        private final QuorumStateStore quorumStateStore = new MockQuorumStateStore();\n        private final MockableRandom random = new MockableRandom(1L);\n        private final LogContext logContext = new LogContext();\n        private final MockLog log = new MockLog(METADATA_PARTITION, Uuid.METADATA_TOPIC_ID, logContext);\n        private final Uuid clusterId = Uuid.randomUuid();\n        private final Set<Integer> voters;\n        private final OptionalInt localId;\n        private final Uuid localDirectoryId = Uuid.randomUuid();\n        private final short kraftVersion = 0;\n\n        private int requestTimeoutMs = DEFAULT_REQUEST_TIMEOUT_MS;\n        private int electionTimeoutMs = DEFAULT_ELECTION_TIMEOUT_MS;\n        private int appendLingerMs = DEFAULT_APPEND_LINGER_MS;\n        private MemoryPool memoryPool = MemoryPool.NONE;\n        private List<InetSocketAddress> bootstrapServers = Collections.emptyList();\n\n        public Builder(int localId, Set<Integer> voters) {\n            this(OptionalInt.of(localId), voters);\n        }\n\n        public Builder(OptionalInt localId, Set<Integer> voters) {\n            this.voters = voters;\n            this.localId = localId;\n        }\n\n        Builder withElectedLeader(int epoch, int leaderId) {\n            quorumStateStore.writeElectionState(\n                ElectionState.withElectedLeader(epoch, leaderId, voters),\n                kraftVersion\n            );\n            return this;\n        }\n\n        Builder withUnknownLeader(int epoch) {\n            quorumStateStore.writeElectionState(\n                ElectionState.withUnknownLeader(epoch, voters),\n                kraftVersion\n            );\n            return this;\n        }\n\n        Builder withVotedCandidate(int epoch, ReplicaKey votedKey) {\n            quorumStateStore.writeElectionState(\n                ElectionState.withVotedCandidate(epoch, votedKey, voters),\n                kraftVersion\n            );\n            return this;\n        }\n\n        Builder updateRandom(Consumer<MockableRandom> consumer) {\n            consumer.accept(random);\n            return this;\n        }\n\n        Builder withMemoryPool(MemoryPool pool) {\n            this.memoryPool = pool;\n            return this;\n        }\n\n        Builder withAppendLingerMs(int appendLingerMs) {\n            this.appendLingerMs = appendLingerMs;\n            return this;\n        }\n\n        public Builder appendToLog(int epoch, List<String> records) {\n            MemoryRecords batch = buildBatch(\n                time.milliseconds(),\n                log.endOffset().offset,\n                epoch,\n                records\n            );\n            log.appendAsLeader(batch, epoch);\n            // Need to flush the log to update the last flushed offset. This is always correct\n            // because append operation was done in the Builder which represent the state of the\n            // log before the replica starts.\n            log.flush(false);\n\n            // Reset the value of this method since \"flush\" before the replica start should not\n            // count when checking for flushes by the KRaft client.\n            log.flushedSinceLastChecked();\n            return this;\n        }\n\n        Builder withEmptySnapshot(OffsetAndEpoch snapshotId) {\n            try (RawSnapshotWriter snapshot = log.createNewSnapshotUnchecked(snapshotId).get()) {\n                snapshot.freeze();\n            }\n            return this;\n        }\n\n        Builder deleteBeforeSnapshot(OffsetAndEpoch snapshotId) {\n            if (snapshotId.offset() > log.highWatermark().offset) {\n                log.updateHighWatermark(new LogOffsetMetadata(snapshotId.offset()));\n            }\n            log.deleteBeforeSnapshot(snapshotId);\n\n            return this;\n        }\n\n        Builder withElectionTimeoutMs(int electionTimeoutMs) {\n            this.electionTimeoutMs = electionTimeoutMs;\n            return this;\n        }\n\n        Builder withRequestTimeoutMs(int requestTimeoutMs) {\n            this.requestTimeoutMs = requestTimeoutMs;\n            return this;\n        }\n\n        Builder withBootstrapServers(List<InetSocketAddress> bootstrapServers) {\n            this.bootstrapServers = bootstrapServers;\n            return this;\n        }\n\n        public RaftClientTestContext build() throws IOException {\n            Metrics metrics = new Metrics(time);\n            MockNetworkChannel channel = new MockNetworkChannel();\n            MockListener listener = new MockListener(localId);\n            Map<Integer, InetSocketAddress> voterAddressMap = voters\n                .stream()\n                .collect(Collectors.toMap(Function.identity(), RaftClientTestContext::mockAddress));\n\n            QuorumConfig quorumConfig = new QuorumConfig(\n                requestTimeoutMs,\n                RETRY_BACKOFF_MS,\n                electionTimeoutMs,\n                ELECTION_BACKOFF_MAX_MS,\n                FETCH_TIMEOUT_MS,\n                appendLingerMs\n            );\n\n            KafkaRaftClient<String> client = new KafkaRaftClient<>(\n                localId,\n                localDirectoryId,\n                SERDE,\n                channel,\n                messageQueue,\n                log,\n                memoryPool,\n                time,\n                new MockExpirationService(time),\n                FETCH_MAX_WAIT_MS,\n                clusterId.toString(),\n                bootstrapServers,\n                logContext,\n                random,\n                quorumConfig\n            );\n\n            client.register(listener);\n            client.initialize(\n                voterAddressMap,\n                quorumStateStore,\n                metrics\n            );\n\n            RaftClientTestContext context = new RaftClientTestContext(\n                clusterId,\n                localId,\n                client,\n                log,\n                channel,\n                messageQueue,\n                time,\n                quorumStateStore,\n                voters,\n                IntStream\n                    .iterate(-2, id -> id - 1)\n                    .limit(bootstrapServers.size())\n                    .boxed()\n                    .collect(Collectors.toSet()),\n                metrics,\n                listener\n            );\n\n            context.electionTimeoutMs = electionTimeoutMs;\n            context.requestTimeoutMs = requestTimeoutMs;\n            context.appendLingerMs = appendLingerMs;\n\n            return context;\n        }\n    }\n\n    private RaftClientTestContext(\n        Uuid clusterId,\n        OptionalInt localId,\n        KafkaRaftClient<String> client,\n        MockLog log,\n        MockNetworkChannel channel,\n        MockMessageQueue messageQueue,\n        MockTime time,\n        QuorumStateStore quorumStateStore,\n        Set<Integer> voters,\n        Set<Integer> bootstrapIds,\n        Metrics metrics,\n        MockListener listener\n    ) {\n        this.clusterId = clusterId;\n        this.localId = localId;\n        this.client = client;\n        this.log = log;\n        this.channel = channel;\n        this.messageQueue = messageQueue;\n        this.time = time;\n        this.quorumStateStore = quorumStateStore;\n        this.voters = voters;\n        this.bootstrapIds = bootstrapIds;\n        this.metrics = metrics;\n        this.listener = listener;\n    }\n\n    int electionTimeoutMs() {\n        return electionTimeoutMs;\n    }\n\n    int requestTimeoutMs() {\n        return requestTimeoutMs;\n    }\n\n    int appendLingerMs() {\n        return appendLingerMs;\n    }\n\n    MemoryRecords buildBatch(\n        long baseOffset,\n        int epoch,\n        List<String> records\n    ) {\n        return buildBatch(time.milliseconds(), baseOffset, epoch, records);\n    }\n\n    static MemoryRecords buildBatch(\n        long timestamp,\n        long baseOffset,\n        int epoch,\n        List<String> records\n    ) {\n        ByteBuffer buffer = ByteBuffer.allocate(512);\n        BatchBuilder<String> builder = new BatchBuilder<>(\n            buffer,\n            Builder.SERDE,\n            Compression.NONE,\n            baseOffset,\n            timestamp,\n            false,\n            epoch,\n            512\n        );\n\n        for (String record : records) {\n            builder.appendRecord(record, null);\n        }\n\n        return builder.build();\n    }\n\n    static RaftClientTestContext initializeAsLeader(int localId, Set<Integer> voters, int epoch) throws Exception {\n        if (epoch <= 0) {\n            throw new IllegalArgumentException(\"Cannot become leader in epoch \" + epoch);\n        }\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n            .withUnknownLeader(epoch - 1)\n            .build();\n\n        context.assertUnknownLeader(epoch - 1);\n        context.becomeLeader();\n        return context;\n    }\n\n    public void becomeLeader() throws Exception {\n        int currentEpoch = currentEpoch();\n        time.sleep(electionTimeoutMs * 2L);\n        expectAndGrantVotes(currentEpoch + 1);\n        expectBeginEpoch(currentEpoch + 1);\n    }\n\n    public OptionalInt currentLeader() {\n        return currentLeaderAndEpoch().leaderId();\n    }\n\n    public int currentEpoch() {\n        return currentLeaderAndEpoch().epoch();\n    }\n\n    LeaderAndEpoch currentLeaderAndEpoch() {\n        ElectionState election = quorumStateStore.readElectionState().get();\n        return new LeaderAndEpoch(election.optionalLeaderId(), election.epoch());\n    }\n\n    void expectAndGrantVotes(int epoch) throws Exception {\n        pollUntilRequest();\n\n        List<RaftRequest.Outbound> voteRequests = collectVoteRequests(epoch,\n            log.lastFetchedEpoch(), log.endOffset().offset);\n\n        for (RaftRequest.Outbound request : voteRequests) {\n            VoteResponseData voteResponse = voteResponse(true, Optional.empty(), epoch);\n            deliverResponse(request.correlationId(), request.destination(), voteResponse);\n        }\n\n        client.poll();\n        assertElectedLeader(epoch, localIdOrThrow());\n    }\n\n    private int localIdOrThrow() {\n        return localId.orElseThrow(() -> new AssertionError(\"Required local id is not defined\"));\n    }\n\n    private void expectBeginEpoch(int epoch) throws Exception {\n        pollUntilRequest();\n        for (RaftRequest.Outbound request : collectBeginEpochRequests(epoch)) {\n            BeginQuorumEpochResponseData beginEpochResponse = beginEpochResponse(epoch, localIdOrThrow());\n            deliverResponse(request.correlationId(), request.destination(), beginEpochResponse);\n        }\n        client.poll();\n    }\n\n    public void pollUntil(TestCondition condition) throws InterruptedException {\n        TestUtils.waitForCondition(() -> {\n            client.poll();\n            return condition.conditionMet();\n        }, 5000, \"Condition failed to be satisfied before timeout\");\n    }\n\n    void pollUntilResponse() throws InterruptedException {\n        pollUntil(() -> !sentResponses.isEmpty());\n    }\n\n    void pollUntilRequest() throws InterruptedException {\n        pollUntil(channel::hasSentRequests);\n    }\n\n    void assertVotedCandidate(int epoch, int candidateId) {\n        assertEquals(\n            ElectionState.withVotedCandidate(\n                epoch,\n                ReplicaKey.of(candidateId, Optional.empty()),\n                voters\n            ),\n            quorumStateStore.readElectionState().get()\n        );\n    }\n\n    public void assertElectedLeader(int epoch, int leaderId) {\n        assertEquals(\n            ElectionState.withElectedLeader(epoch, leaderId, voters),\n            quorumStateStore.readElectionState().get()\n        );\n    }\n\n    void assertUnknownLeader(int epoch) {\n        assertEquals(\n            ElectionState.withUnknownLeader(epoch, voters),\n            quorumStateStore.readElectionState().get()\n        );\n    }\n\n    void assertResignedLeader(int epoch, int leaderId) {\n        assertTrue(client.quorum().isResigned());\n        assertEquals(\n            ElectionState.withElectedLeader(epoch, leaderId, voters),\n            quorumStateStore.readElectionState().get()\n        );\n    }\n\n    DescribeQuorumResponseData collectDescribeQuorumResponse() {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.DESCRIBE_QUORUM);\n        assertEquals(1, sentMessages.size());\n        RaftResponse.Outbound raftMessage = sentMessages.get(0);\n        assertInstanceOf(\n            DescribeQuorumResponseData.class,\n            raftMessage.data(),\n            \"Unexpected request type \" + raftMessage.data());\n        return (DescribeQuorumResponseData) raftMessage.data();\n    }\n\n    void assertSentDescribeQuorumResponse(\n        int leaderId,\n        int leaderEpoch,\n        long highWatermark,\n        List<ReplicaState> voterStates,\n        List<ReplicaState> observerStates\n    ) {\n        DescribeQuorumResponseData response = collectDescribeQuorumResponse();\n\n        DescribeQuorumResponseData.PartitionData partitionData = new DescribeQuorumResponseData.PartitionData()\n            .setErrorCode(Errors.NONE.code())\n            .setLeaderId(leaderId)\n            .setLeaderEpoch(leaderEpoch)\n            .setHighWatermark(highWatermark)\n            .setCurrentVoters(voterStates)\n            .setObservers(observerStates);\n\n        DescribeQuorumResponseData.NodeCollection nodes = new DescribeQuorumResponseData.NodeCollection();\n\n        Consumer<DescribeQuorumResponseData.ReplicaState> addToNodes = replicaState -> {\n            if (nodes.find(replicaState.replicaId()) != null)\n                return;\n\n            nodes.add(new DescribeQuorumResponseData.Node()\n                .setNodeId(replicaState.replicaId()));\n        };\n\n        voterStates.forEach(addToNodes);\n        observerStates.forEach(addToNodes);\n\n        DescribeQuorumResponseData expectedResponse = DescribeQuorumResponse.singletonResponse(\n            metadataPartition,\n            partitionData,\n            nodes\n        );\n        assertEquals(expectedResponse, response);\n    }\n\n    RaftRequest.Outbound assertSentVoteRequest(int epoch, int lastEpoch, long lastEpochOffset, int numVoteReceivers) {\n        List<RaftRequest.Outbound> voteRequests = collectVoteRequests(epoch, lastEpoch, lastEpochOffset);\n        assertEquals(numVoteReceivers, voteRequests.size());\n        return voteRequests.iterator().next();\n    }\n\n    void assertSentVoteResponse(Errors error) {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.VOTE);\n        assertEquals(1, sentMessages.size());\n        RaftMessage raftMessage = sentMessages.get(0);\n        assertInstanceOf(VoteResponseData.class, raftMessage.data());\n        VoteResponseData response = (VoteResponseData) raftMessage.data();\n\n        assertEquals(error, Errors.forCode(response.errorCode()));\n    }\n\n    void assertSentVoteResponse(\n        Errors error,\n        int epoch,\n        OptionalInt leaderId,\n        boolean voteGranted\n    ) {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.VOTE);\n        assertEquals(1, sentMessages.size());\n        RaftMessage raftMessage = sentMessages.get(0);\n        assertInstanceOf(VoteResponseData.class, raftMessage.data());\n        VoteResponseData response = (VoteResponseData) raftMessage.data();\n        assertTrue(hasValidTopicPartition(response, metadataPartition));\n\n        VoteResponseData.PartitionData partitionResponse = response.topics().get(0).partitions().get(0);\n\n        assertEquals(voteGranted, partitionResponse.voteGranted());\n        assertEquals(error, Errors.forCode(partitionResponse.errorCode()));\n        assertEquals(epoch, partitionResponse.leaderEpoch());\n        assertEquals(leaderId.orElse(-1), partitionResponse.leaderId());\n    }\n\n    List<RaftRequest.Outbound> collectVoteRequests(\n        int epoch,\n        int lastEpoch,\n        long lastEpochOffset\n    ) {\n        List<RaftRequest.Outbound> voteRequests = new ArrayList<>();\n        for (RaftRequest.Outbound raftMessage : channel.drainSendQueue()) {\n            if (raftMessage.data() instanceof VoteRequestData) {\n                VoteRequestData request = (VoteRequestData) raftMessage.data();\n                VoteRequestData.PartitionData partitionRequest = unwrap(request);\n\n                assertEquals(epoch, partitionRequest.candidateEpoch());\n                assertEquals(localIdOrThrow(), partitionRequest.candidateId());\n                assertEquals(lastEpoch, partitionRequest.lastOffsetEpoch());\n                assertEquals(lastEpochOffset, partitionRequest.lastOffset());\n                voteRequests.add(raftMessage);\n            }\n        }\n        return voteRequests;\n    }\n\n    void deliverRequest(ApiMessage request) {\n        RaftRequest.Inbound inboundRequest = new RaftRequest.Inbound(\n            channel.newCorrelationId(), request.highestSupportedVersion(), request, time.milliseconds());\n        inboundRequest.completion.whenComplete((response, exception) -> {\n            if (exception != null) {\n                throw new RuntimeException(exception);\n            } else {\n                sentResponses.add(response);\n            }\n        });\n        client.handle(inboundRequest);\n    }\n\n    void deliverResponse(int correlationId, Node source, ApiMessage response) {\n        channel.mockReceive(new RaftResponse.Inbound(correlationId, response, source));\n    }\n\n    RaftRequest.Outbound assertSentBeginQuorumEpochRequest(int epoch, int numBeginEpochRequests) {\n        List<RaftRequest.Outbound> requests = collectBeginEpochRequests(epoch);\n        assertEquals(numBeginEpochRequests, requests.size());\n        return requests.get(0);\n    }\n\n    private List<RaftResponse.Outbound> drainSentResponses(\n        ApiKeys apiKey\n    ) {\n        List<RaftResponse.Outbound> res = new ArrayList<>();\n        Iterator<RaftResponse.Outbound> iterator = sentResponses.iterator();\n        while (iterator.hasNext()) {\n            RaftResponse.Outbound response = iterator.next();\n            if (response.data().apiKey() == apiKey.id) {\n                res.add(response);\n                iterator.remove();\n            }\n        }\n        return res;\n    }\n\n    void assertSentBeginQuorumEpochResponse(\n            Errors responseError\n    ) {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.BEGIN_QUORUM_EPOCH);\n        assertEquals(1, sentMessages.size());\n        RaftMessage raftMessage = sentMessages.get(0);\n        assertInstanceOf(BeginQuorumEpochResponseData.class, raftMessage.data());\n        BeginQuorumEpochResponseData response = (BeginQuorumEpochResponseData) raftMessage.data();\n        assertEquals(responseError, Errors.forCode(response.errorCode()));\n    }\n\n    void assertSentBeginQuorumEpochResponse(\n        Errors partitionError,\n        int epoch,\n        OptionalInt leaderId\n    ) {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.BEGIN_QUORUM_EPOCH);\n        assertEquals(1, sentMessages.size());\n        RaftMessage raftMessage = sentMessages.get(0);\n        assertInstanceOf(BeginQuorumEpochResponseData.class, raftMessage.data());\n        BeginQuorumEpochResponseData response = (BeginQuorumEpochResponseData) raftMessage.data();\n        assertEquals(Errors.NONE, Errors.forCode(response.errorCode()));\n\n        BeginQuorumEpochResponseData.PartitionData partitionResponse =\n            response.topics().get(0).partitions().get(0);\n\n        assertEquals(epoch, partitionResponse.leaderEpoch());\n        assertEquals(leaderId.orElse(-1), partitionResponse.leaderId());\n        assertEquals(partitionError, Errors.forCode(partitionResponse.errorCode()));\n    }\n\n    RaftRequest.Outbound assertSentEndQuorumEpochRequest(int epoch, int destinationId) {\n        List<RaftRequest.Outbound> endQuorumRequests = collectEndQuorumRequests(\n            epoch,\n            Collections.singleton(destinationId),\n            Optional.empty()\n        );\n        assertEquals(1, endQuorumRequests.size());\n        return endQuorumRequests.get(0);\n    }\n\n    void assertSentEndQuorumEpochResponse(\n        Errors responseError\n    ) {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.END_QUORUM_EPOCH);\n        assertEquals(1, sentMessages.size());\n        RaftMessage raftMessage = sentMessages.get(0);\n        assertInstanceOf(EndQuorumEpochResponseData.class, raftMessage.data());\n        EndQuorumEpochResponseData response = (EndQuorumEpochResponseData) raftMessage.data();\n        assertEquals(responseError, Errors.forCode(response.errorCode()));\n    }\n\n    void assertSentEndQuorumEpochResponse(\n        Errors partitionError,\n        int epoch,\n        OptionalInt leaderId\n    ) {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.END_QUORUM_EPOCH);\n        assertEquals(1, sentMessages.size());\n        RaftMessage raftMessage = sentMessages.get(0);\n        assertInstanceOf(EndQuorumEpochResponseData.class, raftMessage.data());\n        EndQuorumEpochResponseData response = (EndQuorumEpochResponseData) raftMessage.data();\n        assertEquals(Errors.NONE, Errors.forCode(response.errorCode()));\n\n        EndQuorumEpochResponseData.PartitionData partitionResponse =\n            response.topics().get(0).partitions().get(0);\n\n        assertEquals(epoch, partitionResponse.leaderEpoch());\n        assertEquals(leaderId.orElse(-1), partitionResponse.leaderId());\n        assertEquals(partitionError, Errors.forCode(partitionResponse.errorCode()));\n    }\n\n    RaftRequest.Outbound assertSentFetchRequest() {\n        List<RaftRequest.Outbound> sentRequests = channel.drainSentRequests(Optional.of(ApiKeys.FETCH));\n        assertEquals(1, sentRequests.size());\n        return sentRequests.get(0);\n    }\n\n    RaftRequest.Outbound assertSentFetchRequest(\n        int epoch,\n        long fetchOffset,\n        int lastFetchedEpoch\n    ) {\n        List<RaftRequest.Outbound> sentMessages = channel.drainSendQueue();\n        assertEquals(1, sentMessages.size());\n\n        RaftRequest.Outbound raftRequest = sentMessages.get(0);\n        assertFetchRequestData(raftRequest, epoch, fetchOffset, lastFetchedEpoch);\n        return raftRequest;\n    }\n\n    FetchResponseData.PartitionData assertSentFetchPartitionResponse() {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.FETCH);\n        assertEquals(\n            1, sentMessages.size(), \"Found unexpected sent messages \" + sentMessages);\n        RaftResponse.Outbound raftMessage = sentMessages.get(0);\n        assertEquals(ApiKeys.FETCH.id, raftMessage.data().apiKey());\n        FetchResponseData response = (FetchResponseData) raftMessage.data();\n        assertEquals(Errors.NONE, Errors.forCode(response.errorCode()));\n\n        assertEquals(1, response.responses().size());\n        assertEquals(metadataPartition.topic(), response.responses().get(0).topic());\n        assertEquals(1, response.responses().get(0).partitions().size());\n        return response.responses().get(0).partitions().get(0);\n    }\n\n    void assertSentFetchPartitionResponse(Errors topLevelError) {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.FETCH);\n        assertEquals(\n            1, sentMessages.size(), \"Found unexpected sent messages \" + sentMessages);\n        RaftResponse.Outbound raftMessage = sentMessages.get(0);\n        assertEquals(ApiKeys.FETCH.id, raftMessage.data().apiKey());\n        FetchResponseData response = (FetchResponseData) raftMessage.data();\n        assertEquals(topLevelError, Errors.forCode(response.errorCode()));\n    }\n\n\n    MemoryRecords assertSentFetchPartitionResponse(\n        Errors error,\n        int epoch,\n        OptionalInt leaderId\n    ) {\n        FetchResponseData.PartitionData partitionResponse = assertSentFetchPartitionResponse();\n        assertEquals(error, Errors.forCode(partitionResponse.errorCode()));\n        assertEquals(epoch, partitionResponse.currentLeader().leaderEpoch());\n        assertEquals(leaderId.orElse(-1), partitionResponse.currentLeader().leaderId());\n        assertEquals(-1, partitionResponse.divergingEpoch().endOffset());\n        assertEquals(-1, partitionResponse.divergingEpoch().epoch());\n        assertEquals(-1, partitionResponse.snapshotId().endOffset());\n        assertEquals(-1, partitionResponse.snapshotId().epoch());\n        return (MemoryRecords) partitionResponse.records();\n    }\n\n    MemoryRecords assertSentFetchPartitionResponse(\n        long highWatermark,\n        int leaderEpoch\n    ) {\n        FetchResponseData.PartitionData partitionResponse = assertSentFetchPartitionResponse();\n        assertEquals(Errors.NONE, Errors.forCode(partitionResponse.errorCode()));\n        assertEquals(leaderEpoch, partitionResponse.currentLeader().leaderEpoch());\n        assertEquals(highWatermark, partitionResponse.highWatermark());\n        assertEquals(-1, partitionResponse.divergingEpoch().endOffset());\n        assertEquals(-1, partitionResponse.divergingEpoch().epoch());\n        assertEquals(-1, partitionResponse.snapshotId().endOffset());\n        assertEquals(-1, partitionResponse.snapshotId().epoch());\n        return (MemoryRecords) partitionResponse.records();\n    }\n\n    RaftRequest.Outbound assertSentFetchSnapshotRequest() {\n        List<RaftRequest.Outbound> sentRequests = channel.drainSentRequests(Optional.of(ApiKeys.FETCH_SNAPSHOT));\n        assertEquals(1, sentRequests.size());\n\n        return sentRequests.get(0);\n    }\n\n    void assertSentFetchSnapshotResponse(Errors responseError) {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.FETCH_SNAPSHOT);\n        assertEquals(1, sentMessages.size());\n\n        RaftMessage message = sentMessages.get(0);\n        assertInstanceOf(FetchSnapshotResponseData.class, message.data());\n\n        FetchSnapshotResponseData response = (FetchSnapshotResponseData) message.data();\n        assertEquals(responseError, Errors.forCode(response.errorCode()));\n    }\n\n    Optional<FetchSnapshotResponseData.PartitionSnapshot> assertSentFetchSnapshotResponse(TopicPartition topicPartition) {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.FETCH_SNAPSHOT);\n        assertEquals(1, sentMessages.size());\n\n        RaftMessage message = sentMessages.get(0);\n        assertInstanceOf(FetchSnapshotResponseData.class, message.data());\n\n        FetchSnapshotResponseData response = (FetchSnapshotResponseData) message.data();\n        assertEquals(Errors.NONE, Errors.forCode(response.errorCode()));\n\n        return FetchSnapshotResponse.forTopicPartition(response, topicPartition);\n    }\n\n    List<RaftRequest.Outbound> collectEndQuorumRequests(\n        int epoch,\n        Set<Integer> destinationIdSet,\n        Optional<List<Integer>> preferredSuccessorsOpt\n    ) {\n        List<RaftRequest.Outbound> endQuorumRequests = new ArrayList<>();\n        Set<Integer> collectedDestinationIdSet = new HashSet<>();\n        for (RaftRequest.Outbound raftMessage : channel.drainSendQueue()) {\n            if (raftMessage.data() instanceof EndQuorumEpochRequestData) {\n                EndQuorumEpochRequestData request = (EndQuorumEpochRequestData) raftMessage.data();\n\n                EndQuorumEpochRequestData.PartitionData partitionRequest =\n                    request.topics().get(0).partitions().get(0);\n\n                assertEquals(epoch, partitionRequest.leaderEpoch());\n                assertEquals(localIdOrThrow(), partitionRequest.leaderId());\n                preferredSuccessorsOpt.ifPresent(preferredSuccessors -> {\n                    assertEquals(preferredSuccessors, partitionRequest.preferredSuccessors());\n                });\n\n                collectedDestinationIdSet.add(raftMessage.destination().id());\n                endQuorumRequests.add(raftMessage);\n            }\n        }\n        assertEquals(destinationIdSet, collectedDestinationIdSet);\n        return endQuorumRequests;\n    }\n\n    void discoverLeaderAsObserver(\n        int leaderId,\n        int epoch\n    ) throws Exception {\n        pollUntilRequest();\n        RaftRequest.Outbound fetchRequest = assertSentFetchRequest();\n        int destinationId = fetchRequest.destination().id();\n        assertTrue(\n            voters.contains(destinationId) || bootstrapIds.contains(destinationId),\n            String.format(\"id %d is not in sets %s or %s\", destinationId, voters, bootstrapIds)\n        );\n        assertFetchRequestData(fetchRequest, 0, 0L, 0);\n\n        deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            fetchResponse(epoch, leaderId, MemoryRecords.EMPTY, 0L, Errors.NONE)\n        );\n        client.poll();\n        assertElectedLeader(epoch, leaderId);\n    }\n\n    private List<RaftRequest.Outbound> collectBeginEpochRequests(int epoch) {\n        List<RaftRequest.Outbound> requests = new ArrayList<>();\n        for (RaftRequest.Outbound raftRequest : channel.drainSentRequests(Optional.of(ApiKeys.BEGIN_QUORUM_EPOCH))) {\n            assertInstanceOf(BeginQuorumEpochRequestData.class, raftRequest.data());\n            BeginQuorumEpochRequestData request = (BeginQuorumEpochRequestData) raftRequest.data();\n\n            BeginQuorumEpochRequestData.PartitionData partitionRequest =\n                request.topics().get(0).partitions().get(0);\n\n            assertEquals(epoch, partitionRequest.leaderEpoch());\n            assertEquals(localIdOrThrow(), partitionRequest.leaderId());\n            requests.add(raftRequest);\n        }\n        return requests;\n    }\n\n    public static InetSocketAddress mockAddress(int id) {\n        return new InetSocketAddress(\"localhost\", 9990 + id);\n    }\n\n    EndQuorumEpochResponseData endEpochResponse(\n        int epoch,\n        OptionalInt leaderId\n    ) {\n        return EndQuorumEpochResponse.singletonResponse(\n            Errors.NONE,\n            metadataPartition,\n            Errors.NONE,\n            epoch,\n            leaderId.orElse(-1)\n        );\n    }\n\n    EndQuorumEpochRequestData endEpochRequest(\n        int epoch,\n        int leaderId,\n        List<Integer> preferredSuccessors\n    ) {\n        return EndQuorumEpochRequest.singletonRequest(\n            metadataPartition,\n            epoch,\n            leaderId,\n            preferredSuccessors\n        );\n    }\n\n    EndQuorumEpochRequestData endEpochRequest(\n        String clusterId,\n        int epoch,\n        int leaderId,\n        List<Integer> preferredSuccessors\n    ) {\n        return EndQuorumEpochRequest.singletonRequest(\n            metadataPartition,\n            clusterId,\n            epoch,\n            leaderId,\n            preferredSuccessors\n        );\n    }\n\n    BeginQuorumEpochRequestData beginEpochRequest(String clusterId, int epoch, int leaderId) {\n        return BeginQuorumEpochRequest.singletonRequest(\n            metadataPartition,\n            clusterId,\n            epoch,\n            leaderId\n        );\n    }\n\n    BeginQuorumEpochRequestData beginEpochRequest(int epoch, int leaderId) {\n        return BeginQuorumEpochRequest.singletonRequest(\n            metadataPartition,\n            epoch,\n            leaderId\n        );\n    }\n\n    private BeginQuorumEpochResponseData beginEpochResponse(int epoch, int leaderId) {\n        return BeginQuorumEpochResponse.singletonResponse(\n            Errors.NONE,\n            metadataPartition,\n            Errors.NONE,\n            epoch,\n            leaderId\n        );\n    }\n\n    VoteRequestData voteRequest(int epoch, int candidateId, int lastEpoch, long lastEpochOffset) {\n        return VoteRequest.singletonRequest(\n            metadataPartition,\n            clusterId.toString(),\n            epoch,\n            candidateId,\n            lastEpoch,\n            lastEpochOffset\n        );\n    }\n\n    VoteRequestData voteRequest(\n        String clusterId,\n        int epoch,\n        int candidateId,\n        int lastEpoch,\n        long lastEpochOffset\n    ) {\n        return VoteRequest.singletonRequest(\n                metadataPartition,\n                clusterId,\n                epoch,\n                candidateId,\n                lastEpoch,\n                lastEpochOffset\n        );\n    }\n\n    VoteResponseData voteResponse(boolean voteGranted, Optional<Integer> leaderId, int epoch) {\n        return VoteResponse.singletonResponse(\n            Errors.NONE,\n            metadataPartition,\n            Errors.NONE,\n            epoch,\n            leaderId.orElse(-1),\n            voteGranted\n        );\n    }\n\n    private VoteRequestData.PartitionData unwrap(VoteRequestData voteRequest) {\n        assertTrue(RaftUtil.hasValidTopicPartition(voteRequest, metadataPartition));\n        return voteRequest.topics().get(0).partitions().get(0);\n    }\n\n    static void assertMatchingRecords(\n        String[] expected,\n        Records actual\n    ) {\n        List<Record> recordList = Utils.toList(actual.records());\n        assertEquals(expected.length, recordList.size());\n        for (int i = 0; i < expected.length; i++) {\n            Record record = recordList.get(i);\n            assertEquals(expected[i], Utils.utf8(record.value()),\n                \"Record at offset \" + record.offset() + \" does not match expected\");\n        }\n    }\n\n    static void verifyLeaderChangeMessage(\n        int leaderId,\n        List<Integer> voters,\n        List<Integer> grantingVoters,\n        ByteBuffer recordKey,\n        ByteBuffer recordValue\n    ) {\n        assertEquals(ControlRecordType.LEADER_CHANGE, ControlRecordType.parse(recordKey));\n\n        LeaderChangeMessage leaderChangeMessage = ControlRecordUtils.deserializeLeaderChangeMessage(recordValue);\n        assertEquals(leaderId, leaderChangeMessage.leaderId());\n        assertEquals(voters.stream().map(voterId -> new Voter().setVoterId(voterId)).collect(Collectors.toList()),\n            leaderChangeMessage.voters());\n        assertEquals(grantingVoters.stream().map(voterId -> new Voter().setVoterId(voterId)).collect(Collectors.toSet()),\n            new HashSet<>(leaderChangeMessage.grantingVoters()));\n    }\n\n    void assertFetchRequestData(\n        RaftRequest.Outbound message,\n        int epoch,\n        long fetchOffset,\n        int lastFetchedEpoch\n    ) {\n        assertInstanceOf(\n            FetchRequestData.class,\n            message.data(),\n            \"unexpected request type \" + message.data());\n        FetchRequestData request = (FetchRequestData) message.data();\n        assertEquals(KafkaRaftClient.MAX_FETCH_SIZE_BYTES, request.maxBytes());\n        assertEquals(fetchMaxWaitMs, request.maxWaitMs());\n\n        assertEquals(1, request.topics().size());\n        assertEquals(metadataPartition.topic(), request.topics().get(0).topic());\n        assertEquals(1, request.topics().get(0).partitions().size());\n\n        FetchRequestData.FetchPartition fetchPartition = request.topics().get(0).partitions().get(0);\n        assertEquals(epoch, fetchPartition.currentLeaderEpoch());\n        assertEquals(fetchOffset, fetchPartition.fetchOffset());\n        assertEquals(lastFetchedEpoch, fetchPartition.lastFetchedEpoch());\n        assertEquals(localId.orElse(-1), request.replicaState().replicaId());\n\n        // Assert that voters have flushed up to the fetch offset\n        if (localId.isPresent() && voters.contains(localId.getAsInt())) {\n            assertEquals(\n                log.firstUnflushedOffset(),\n                fetchOffset,\n                String.format(\n                    \"expected voters have the fetch offset (%s) be the same as the unflushed offset (%s)\",\n                    log.firstUnflushedOffset(),\n                    fetchOffset\n                )\n            );\n        } else {\n            assertFalse(log.flushedSinceLastChecked(), \"KRaft client should not explicitly flush when it is an observer\");\n        }\n    }\n\n    FetchRequestData fetchRequest(\n        int epoch,\n        int replicaId,\n        long fetchOffset,\n        int lastFetchedEpoch,\n        int maxWaitTimeMs\n    ) {\n        return fetchRequest(\n            epoch,\n            clusterId.toString(),\n            replicaId,\n            fetchOffset,\n            lastFetchedEpoch,\n            maxWaitTimeMs\n        );\n    }\n\n    FetchRequestData fetchRequest(\n        int epoch,\n        String clusterId,\n        int replicaId,\n        long fetchOffset,\n        int lastFetchedEpoch,\n        int maxWaitTimeMs\n    ) {\n        FetchRequestData request = RaftUtil.singletonFetchRequest(metadataPartition, metadataTopicId, fetchPartition -> {\n            fetchPartition\n                .setCurrentLeaderEpoch(epoch)\n                .setLastFetchedEpoch(lastFetchedEpoch)\n                .setFetchOffset(fetchOffset);\n        });\n        return request\n            .setMaxWaitMs(maxWaitTimeMs)\n            .setClusterId(clusterId)\n            .setReplicaState(new FetchRequestData.ReplicaState().setReplicaId(replicaId));\n    }\n\n    FetchResponseData fetchResponse(\n        int epoch,\n        int leaderId,\n        Records records,\n        long highWatermark,\n        Errors error\n    ) {\n        return RaftUtil.singletonFetchResponse(metadataPartition, metadataTopicId, Errors.NONE, partitionData -> {\n            partitionData\n                .setRecords(records)\n                .setErrorCode(error.code())\n                .setHighWatermark(highWatermark);\n\n            partitionData.currentLeader()\n                .setLeaderEpoch(epoch)\n                .setLeaderId(leaderId);\n        });\n    }\n\n    FetchResponseData divergingFetchResponse(\n        int epoch,\n        int leaderId,\n        long divergingEpochEndOffset,\n        int divergingEpoch,\n        long highWatermark\n    ) {\n        return RaftUtil.singletonFetchResponse(metadataPartition, metadataTopicId, Errors.NONE, partitionData -> {\n            partitionData.setHighWatermark(highWatermark);\n\n            partitionData.currentLeader()\n                .setLeaderEpoch(epoch)\n                .setLeaderId(leaderId);\n\n            partitionData.divergingEpoch()\n                .setEpoch(divergingEpoch)\n                .setEndOffset(divergingEpochEndOffset);\n        });\n    }\n\n    public void advanceLocalLeaderHighWatermarkToLogEndOffset() throws InterruptedException {\n        assertEquals(localId, currentLeader());\n        long localLogEndOffset = log.endOffset().offset;\n        Set<Integer> followers = voterKeys();\n\n        // Send a request from every follower\n        for (int follower : followers) {\n            deliverRequest(\n                fetchRequest(currentEpoch(), follower, localLogEndOffset, currentEpoch(), 0)\n            );\n            pollUntilResponse();\n            assertSentFetchPartitionResponse(Errors.NONE, currentEpoch(), localId);\n        }\n\n        pollUntil(() -> OptionalLong.of(localLogEndOffset).equals(client.highWatermark()));\n    }\n\n    private Set<Integer> voterKeys() {\n        Set<Integer> followers = voters.stream().filter(voter -> voter != localId.getAsInt()).collect(Collectors.toSet());\n        return followers;\n    }\n\n    static class MockListener implements RaftClient.Listener<String> {\n        private final List<Batch<String>> commits = new ArrayList<>();\n        private final List<BatchReader<String>> savedBatches = new ArrayList<>();\n        private final Map<Integer, Long> claimedEpochStartOffsets = new HashMap<>();\n        private LeaderAndEpoch currentLeaderAndEpoch = new LeaderAndEpoch(OptionalInt.empty(), 0);\n        private final OptionalInt localId;\n        private Optional<SnapshotReader<String>> snapshot = Optional.empty();\n        private boolean readCommit = true;\n\n        MockListener(OptionalInt localId) {\n            this.localId = localId;\n        }\n\n        int numCommittedBatches() {\n            return commits.size();\n        }\n\n        Long claimedEpochStartOffset(int epoch) {\n            return claimedEpochStartOffsets.get(epoch);\n        }\n\n        LeaderAndEpoch currentLeaderAndEpoch() {\n            return currentLeaderAndEpoch;\n        }\n\n        List<Batch<String>> committedBatches() {\n            return commits;\n        }\n\n        Batch<String> lastCommit() {\n            if (commits.isEmpty()) {\n                return null;\n            } else {\n                return commits.get(commits.size() - 1);\n            }\n        }\n\n        OptionalLong lastCommitOffset() {\n            if (commits.isEmpty()) {\n                return OptionalLong.empty();\n            } else {\n                return OptionalLong.of(commits.get(commits.size() - 1).lastOffset());\n            }\n        }\n\n        OptionalInt currentClaimedEpoch() {\n            if (localId.isPresent() && currentLeaderAndEpoch.isLeader(localId.getAsInt())) {\n                return OptionalInt.of(currentLeaderAndEpoch.epoch());\n            } else {\n                return OptionalInt.empty();\n            }\n        }\n\n        List<String> commitWithLastOffset(long lastOffset) {\n            return commits.stream()\n                .filter(batch -> batch.lastOffset() == lastOffset)\n                .findFirst()\n                .map(batch -> batch.records())\n                .orElse(null);\n        }\n\n        Optional<SnapshotReader<String>> drainHandledSnapshot() {\n            Optional<SnapshotReader<String>> temp = snapshot;\n            snapshot = Optional.empty();\n            return temp;\n        }\n\n        void updateReadCommit(boolean readCommit) {\n            this.readCommit = readCommit;\n\n            if (readCommit) {\n                for (BatchReader<String> batch : savedBatches) {\n                    readBatch(batch);\n                }\n\n                savedBatches.clear();\n            }\n        }\n\n        void readBatch(BatchReader<String> reader) {\n            try {\n                while (reader.hasNext()) {\n                    long nextOffset = lastCommitOffset().isPresent() ?\n                        lastCommitOffset().getAsLong() + 1 : 0L;\n                    Batch<String> batch = reader.next();\n                    // We expect monotonic offsets, but not necessarily sequential\n                    // offsets since control records will be filtered.\n                    assertTrue(batch.baseOffset() >= nextOffset,\n                        \"Received non-monotonic commit \" + batch +\n                            \". We expected an offset at least as large as \" + nextOffset);\n                    commits.add(batch);\n                }\n            } finally {\n                reader.close();\n            }\n        }\n\n        @Override\n        public void handleLeaderChange(LeaderAndEpoch leaderAndEpoch) {\n            // We record the current committed offset as the claimed epoch's start\n            // offset. This is useful to verify that the `handleLeaderChange` callback\n            // was not received early on the leader.\n            assertTrue(\n                leaderAndEpoch.epoch() >= currentLeaderAndEpoch.epoch(),\n                String.format(\"new epoch (%d) not >= than old epoch (%d)\", leaderAndEpoch.epoch(), currentLeaderAndEpoch.epoch())\n            );\n            assertNotEquals(currentLeaderAndEpoch, leaderAndEpoch);\n            this.currentLeaderAndEpoch = leaderAndEpoch;\n\n            currentClaimedEpoch().ifPresent(claimedEpoch -> {\n                long claimedEpochStartOffset = lastCommitOffset().isPresent() ?\n                    lastCommitOffset().getAsLong() : 0L;\n                this.claimedEpochStartOffsets.put(leaderAndEpoch.epoch(), claimedEpochStartOffset);\n            });\n        }\n\n        @Override\n        public void handleCommit(BatchReader<String> reader) {\n            if (readCommit) {\n                readBatch(reader);\n            } else {\n                savedBatches.add(reader);\n            }\n        }\n\n        @Override\n        public void handleLoadSnapshot(SnapshotReader<String> reader) {\n            snapshot.ifPresent(snapshot -> assertDoesNotThrow(snapshot::close));\n            commits.clear();\n            savedBatches.clear();\n            snapshot = Optional.of(reader);\n        }\n    }\n}",
                "methodCount": 98
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "withElectedLeader",
                            "method_signature": " withElectedLeader(int epoch, int leaderId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withUnknownLeader",
                            "method_signature": " withUnknownLeader(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withVotedCandidate",
                            "method_signature": " withVotedCandidate(int epoch, ReplicaKey votedKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateRandom",
                            "method_signature": " updateRandom(Consumer<MockableRandom> consumer)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withMemoryPool",
                            "method_signature": " withMemoryPool(MemoryPool pool)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withAppendLingerMs",
                            "method_signature": " withAppendLingerMs(int appendLingerMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "appendToLog",
                            "method_signature": "public appendToLog(int epoch, List<String> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withEmptySnapshot",
                            "method_signature": " withEmptySnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteBeforeSnapshot",
                            "method_signature": " deleteBeforeSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withElectionTimeoutMs",
                            "method_signature": " withElectionTimeoutMs(int electionTimeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withRequestTimeoutMs",
                            "method_signature": " withRequestTimeoutMs(int requestTimeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withBootstrapServers",
                            "method_signature": " withBootstrapServers(List<InetSocketAddress> bootstrapServers)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildBatch",
                            "method_signature": " buildBatch(\n        long baseOffset,\n        int epoch,\n        List<String> records\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildBatch",
                            "method_signature": "static buildBatch(\n        long timestamp,\n        long baseOffset,\n        int epoch,\n        List<String> records\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "initializeAsLeader",
                            "method_signature": "static initializeAsLeader(int localId, Set<Integer> voters, int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "becomeLeader",
                            "method_signature": "public becomeLeader()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "currentLeader",
                            "method_signature": "public currentLeader()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "currentEpoch",
                            "method_signature": "public currentEpoch()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "currentLeaderAndEpoch",
                            "method_signature": " currentLeaderAndEpoch()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "expectAndGrantVotes",
                            "method_signature": " expectAndGrantVotes(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "localIdOrThrow",
                            "method_signature": "private localIdOrThrow()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "expectBeginEpoch",
                            "method_signature": "private expectBeginEpoch(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUntil",
                            "method_signature": "public pollUntil(TestCondition condition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUntilResponse",
                            "method_signature": " pollUntilResponse()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUntilRequest",
                            "method_signature": " pollUntilRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertVotedCandidate",
                            "method_signature": " assertVotedCandidate(int epoch, int candidateId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertElectedLeader",
                            "method_signature": "public assertElectedLeader(int epoch, int leaderId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertUnknownLeader",
                            "method_signature": " assertUnknownLeader(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertResignedLeader",
                            "method_signature": " assertResignedLeader(int epoch, int leaderId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "collectDescribeQuorumResponse",
                            "method_signature": " collectDescribeQuorumResponse()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentDescribeQuorumResponse",
                            "method_signature": " assertSentDescribeQuorumResponse(\n        int leaderId,\n        int leaderEpoch,\n        long highWatermark,\n        List<ReplicaState> voterStates,\n        List<ReplicaState> observerStates\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentVoteRequest",
                            "method_signature": " assertSentVoteRequest(int epoch, int lastEpoch, long lastEpochOffset, int numVoteReceivers)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentVoteResponse",
                            "method_signature": " assertSentVoteResponse(Errors error)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentVoteResponse",
                            "method_signature": " assertSentVoteResponse(\n        Errors error,\n        int epoch,\n        OptionalInt leaderId,\n        boolean voteGranted\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "collectVoteRequests",
                            "method_signature": " collectVoteRequests(\n        int epoch,\n        int lastEpoch,\n        long lastEpochOffset\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deliverRequest",
                            "method_signature": " deliverRequest(ApiMessage request)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deliverResponse",
                            "method_signature": " deliverResponse(int correlationId, Node source, ApiMessage response)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentBeginQuorumEpochRequest",
                            "method_signature": " assertSentBeginQuorumEpochRequest(int epoch, int numBeginEpochRequests)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "drainSentResponses",
                            "method_signature": "private drainSentResponses(\n        ApiKeys apiKey\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentBeginQuorumEpochResponse",
                            "method_signature": " assertSentBeginQuorumEpochResponse(\n            Errors responseError\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentBeginQuorumEpochResponse",
                            "method_signature": " assertSentBeginQuorumEpochResponse(\n        Errors partitionError,\n        int epoch,\n        OptionalInt leaderId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentEndQuorumEpochRequest",
                            "method_signature": " assertSentEndQuorumEpochRequest(int epoch, int destinationId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentEndQuorumEpochResponse",
                            "method_signature": " assertSentEndQuorumEpochResponse(\n        Errors responseError\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentEndQuorumEpochResponse",
                            "method_signature": " assertSentEndQuorumEpochResponse(\n        Errors partitionError,\n        int epoch,\n        OptionalInt leaderId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentFetchRequest",
                            "method_signature": " assertSentFetchRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentFetchRequest",
                            "method_signature": " assertSentFetchRequest(\n        int epoch,\n        long fetchOffset,\n        int lastFetchedEpoch\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentFetchPartitionResponse",
                            "method_signature": " assertSentFetchPartitionResponse()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentFetchPartitionResponse",
                            "method_signature": " assertSentFetchPartitionResponse(Errors topLevelError)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentFetchPartitionResponse",
                            "method_signature": " assertSentFetchPartitionResponse(\n        Errors error,\n        int epoch,\n        OptionalInt leaderId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentFetchPartitionResponse",
                            "method_signature": " assertSentFetchPartitionResponse(\n        long highWatermark,\n        int leaderEpoch\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentFetchSnapshotRequest",
                            "method_signature": " assertSentFetchSnapshotRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentFetchSnapshotResponse",
                            "method_signature": " assertSentFetchSnapshotResponse(Errors responseError)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentFetchSnapshotResponse",
                            "method_signature": " assertSentFetchSnapshotResponse(TopicPartition topicPartition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "collectEndQuorumRequests",
                            "method_signature": " collectEndQuorumRequests(\n        int epoch,\n        Set<Integer> destinationIdSet,\n        Optional<List<Integer>> preferredSuccessorsOpt\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "discoverLeaderAsObserver",
                            "method_signature": " discoverLeaderAsObserver(\n        int leaderId,\n        int epoch\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "collectBeginEpochRequests",
                            "method_signature": "private collectBeginEpochRequests(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "mockAddress",
                            "method_signature": "public static mockAddress(int id)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endEpochResponse",
                            "method_signature": " endEpochResponse(\n        int epoch,\n        OptionalInt leaderId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endEpochRequest",
                            "method_signature": " endEpochRequest(\n        int epoch,\n        int leaderId,\n        List<Integer> preferredSuccessors\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endEpochRequest",
                            "method_signature": " endEpochRequest(\n        String clusterId,\n        int epoch,\n        int leaderId,\n        List<Integer> preferredSuccessors\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginEpochRequest",
                            "method_signature": " beginEpochRequest(String clusterId, int epoch, int leaderId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginEpochRequest",
                            "method_signature": " beginEpochRequest(int epoch, int leaderId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginEpochResponse",
                            "method_signature": "private beginEpochResponse(int epoch, int leaderId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voteRequest",
                            "method_signature": " voteRequest(int epoch, int candidateId, int lastEpoch, long lastEpochOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voteRequest",
                            "method_signature": " voteRequest(\n        String clusterId,\n        int epoch,\n        int candidateId,\n        int lastEpoch,\n        long lastEpochOffset\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voteResponse",
                            "method_signature": " voteResponse(boolean voteGranted, Optional<Integer> leaderId, int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "unwrap",
                            "method_signature": "private unwrap(VoteRequestData voteRequest)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertMatchingRecords",
                            "method_signature": "static assertMatchingRecords(\n        String[] expected,\n        Records actual\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyLeaderChangeMessage",
                            "method_signature": "static verifyLeaderChangeMessage(\n        int leaderId,\n        List<Integer> voters,\n        List<Integer> grantingVoters,\n        ByteBuffer recordKey,\n        ByteBuffer recordValue\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertFetchRequestData",
                            "method_signature": " assertFetchRequestData(\n        RaftRequest.Outbound message,\n        int epoch,\n        long fetchOffset,\n        int lastFetchedEpoch\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchRequest",
                            "method_signature": " fetchRequest(\n        int epoch,\n        int replicaId,\n        long fetchOffset,\n        int lastFetchedEpoch,\n        int maxWaitTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchRequest",
                            "method_signature": " fetchRequest(\n        int epoch,\n        String clusterId,\n        int replicaId,\n        long fetchOffset,\n        int lastFetchedEpoch,\n        int maxWaitTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchResponse",
                            "method_signature": " fetchResponse(\n        int epoch,\n        int leaderId,\n        Records records,\n        long highWatermark,\n        Errors error\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "divergingFetchResponse",
                            "method_signature": " divergingFetchResponse(\n        int epoch,\n        int leaderId,\n        long divergingEpochEndOffset,\n        int divergingEpoch,\n        long highWatermark\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "advanceLocalLeaderHighWatermarkToLogEndOffset",
                            "method_signature": "public advanceLocalLeaderHighWatermarkToLogEndOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voterKeys",
                            "method_signature": "private voterKeys()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "numCommittedBatches",
                            "method_signature": " numCommittedBatches()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "claimedEpochStartOffset",
                            "method_signature": " claimedEpochStartOffset(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "lastCommit",
                            "method_signature": " lastCommit()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "lastCommitOffset",
                            "method_signature": " lastCommitOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "currentClaimedEpoch",
                            "method_signature": " currentClaimedEpoch()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitWithLastOffset",
                            "method_signature": " commitWithLastOffset(long lastOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "drainHandledSnapshot",
                            "method_signature": " drainHandledSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateReadCommit",
                            "method_signature": " updateReadCommit(boolean readCommit)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "readBatch",
                            "method_signature": " readBatch(BatchReader<String> reader)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "appendToLog",
                            "method_signature": "public appendToLog(int epoch, List<String> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitWithLastOffset",
                            "method_signature": " commitWithLastOffset(long lastOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deliverResponse",
                            "method_signature": " deliverResponse(int correlationId, Node source, ApiMessage response)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentFetchPartitionResponse",
                            "method_signature": " assertSentFetchPartitionResponse(\n        long highWatermark,\n        int leaderEpoch\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyLeaderChangeMessage",
                            "method_signature": "static verifyLeaderChangeMessage(\n        int leaderId,\n        List<Integer> voters,\n        List<Integer> grantingVoters,\n        ByteBuffer recordKey,\n        ByteBuffer recordValue\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentFetchPartitionResponse",
                            "method_signature": " assertSentFetchPartitionResponse(\n        Errors error,\n        int epoch,\n        OptionalInt leaderId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUntilResponse",
                            "method_signature": " pollUntilResponse()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertUnknownLeader",
                            "method_signature": " assertUnknownLeader(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUntil",
                            "method_signature": "public pollUntil(TestCondition condition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withVotedCandidate",
                            "method_signature": " withVotedCandidate(int epoch, ReplicaKey votedKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "mockAddress",
                            "method_signature": "public static mockAddress(int id)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withUnknownLeader",
                            "method_signature": " withUnknownLeader(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUntilRequest",
                            "method_signature": " pollUntilRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voteResponse",
                            "method_signature": " voteResponse(boolean voteGranted, Optional<Integer> leaderId, int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertVotedCandidate",
                            "method_signature": " assertVotedCandidate(int epoch, int candidateId)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "appendToLog",
                            "method_signature": "public appendToLog(int epoch, List<String> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitWithLastOffset",
                            "method_signature": " commitWithLastOffset(long lastOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deliverResponse",
                            "method_signature": " deliverResponse(int correlationId, Node source, ApiMessage response)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentFetchPartitionResponse",
                            "method_signature": " assertSentFetchPartitionResponse(\n        long highWatermark,\n        int leaderEpoch\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyLeaderChangeMessage",
                            "method_signature": "static verifyLeaderChangeMessage(\n        int leaderId,\n        List<Integer> voters,\n        List<Integer> grantingVoters,\n        ByteBuffer recordKey,\n        ByteBuffer recordValue\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentFetchPartitionResponse",
                            "method_signature": " assertSentFetchPartitionResponse(\n        Errors error,\n        int epoch,\n        OptionalInt leaderId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUntilResponse",
                            "method_signature": " pollUntilResponse()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertUnknownLeader",
                            "method_signature": " assertUnknownLeader(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUntil",
                            "method_signature": "public pollUntil(TestCondition condition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withVotedCandidate",
                            "method_signature": " withVotedCandidate(int epoch, ReplicaKey votedKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "mockAddress",
                            "method_signature": "public static mockAddress(int id)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withUnknownLeader",
                            "method_signature": " withUnknownLeader(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUntilRequest",
                            "method_signature": " pollUntilRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voteResponse",
                            "method_signature": " voteResponse(boolean voteGranted, Optional<Integer> leaderId, int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertVotedCandidate",
                            "method_signature": " assertVotedCandidate(int epoch, int candidateId)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "public appendToLog(int epoch, List<String> records)": {
                    "first": {
                        "method_name": "appendToLog",
                        "method_signature": "public appendToLog(int epoch, List<String> records)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.20037188589374702
                },
                " commitWithLastOffset(long lastOffset)": {
                    "first": {
                        "method_name": "commitWithLastOffset",
                        "method_signature": " commitWithLastOffset(long lastOffset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2666168079088861
                },
                " deliverResponse(int correlationId, Node source, ApiMessage response)": {
                    "first": {
                        "method_name": "deliverResponse",
                        "method_signature": " deliverResponse(int correlationId, Node source, ApiMessage response)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2734065239981689
                },
                " assertSentFetchPartitionResponse(\n        long highWatermark,\n        int leaderEpoch\n    )": {
                    "first": {
                        "method_name": "assertSentFetchPartitionResponse",
                        "method_signature": " assertSentFetchPartitionResponse(\n        long highWatermark,\n        int leaderEpoch\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3388449168690123
                },
                "static verifyLeaderChangeMessage(\n        int leaderId,\n        List<Integer> voters,\n        List<Integer> grantingVoters,\n        ByteBuffer recordKey,\n        ByteBuffer recordValue\n    )": {
                    "first": {
                        "method_name": "verifyLeaderChangeMessage",
                        "method_signature": "static verifyLeaderChangeMessage(\n        int leaderId,\n        List<Integer> voters,\n        List<Integer> grantingVoters,\n        ByteBuffer recordKey,\n        ByteBuffer recordValue\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3495930731199243
                },
                " assertSentFetchPartitionResponse(\n        Errors error,\n        int epoch,\n        OptionalInt leaderId\n    )": {
                    "first": {
                        "method_name": "assertSentFetchPartitionResponse",
                        "method_signature": " assertSentFetchPartitionResponse(\n        Errors error,\n        int epoch,\n        OptionalInt leaderId\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.358944405602769
                },
                " pollUntilResponse()": {
                    "first": {
                        "method_name": "pollUntilResponse",
                        "method_signature": " pollUntilResponse()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3613433277695657
                },
                " assertUnknownLeader(int epoch)": {
                    "first": {
                        "method_name": "assertUnknownLeader",
                        "method_signature": " assertUnknownLeader(int epoch)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3645671106629942
                },
                "public pollUntil(TestCondition condition)": {
                    "first": {
                        "method_name": "pollUntil",
                        "method_signature": "public pollUntil(TestCondition condition)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3692550031861421
                },
                " withVotedCandidate(int epoch, ReplicaKey votedKey)": {
                    "first": {
                        "method_name": "withVotedCandidate",
                        "method_signature": " withVotedCandidate(int epoch, ReplicaKey votedKey)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3781701779397338
                },
                "public static mockAddress(int id)": {
                    "first": {
                        "method_name": "mockAddress",
                        "method_signature": "public static mockAddress(int id)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3816237363900829
                },
                " withUnknownLeader(int epoch)": {
                    "first": {
                        "method_name": "withUnknownLeader",
                        "method_signature": " withUnknownLeader(int epoch)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3867807490686121
                },
                " pollUntilRequest()": {
                    "first": {
                        "method_name": "pollUntilRequest",
                        "method_signature": " pollUntilRequest()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.38743235353300187
                },
                " voteResponse(boolean voteGranted, Optional<Integer> leaderId, int epoch)": {
                    "first": {
                        "method_name": "voteResponse",
                        "method_signature": " voteResponse(boolean voteGranted, Optional<Integer> leaderId, int epoch)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3951597999849905
                },
                " assertVotedCandidate(int epoch, int candidateId)": {
                    "first": {
                        "method_name": "assertVotedCandidate",
                        "method_signature": " assertVotedCandidate(int epoch, int candidateId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.39636501363757765
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [],
                "llm_response_time": 11106
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "9a78122fb0b1b37961139d5c62e49fa758dc08ac",
        "url": "https://github.com/apache/kafka/commit/9a78122fb0b1b37961139d5c62e49fa758dc08ac",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public requiresKnownMemberId(request JoinGroupRequestData, apiVersion short) : boolean extracted from private classicGroupJoinToConsumerGroup(group ConsumerGroup, context RequestContext, request JoinGroupRequestData, responseFuture CompletableFuture<JoinGroupResponseData>) : CoordinatorResult<Void,CoordinatorRecord> in class org.apache.kafka.coordinator.group.GroupMetadataManager & moved to class org.apache.kafka.common.requests.JoinGroupRequest",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java",
                    "startLine": 1528,
                    "endLine": 1729,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private classicGroupJoinToConsumerGroup(group ConsumerGroup, context RequestContext, request JoinGroupRequestData, responseFuture CompletableFuture<JoinGroupResponseData>) : CoordinatorResult<Void,CoordinatorRecord>"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java",
                    "startLine": 1564,
                    "endLine": 1564,
                    "startColumn": 17,
                    "endColumn": 96,
                    "codeElementType": "IF_STATEMENT_CONDITION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/requests/JoinGroupRequest.java",
                    "startLine": 105,
                    "endLine": 126,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public requiresKnownMemberId(request JoinGroupRequestData, apiVersion short) : boolean"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/requests/JoinGroupRequest.java",
                    "startLine": 123,
                    "endLine": 125,
                    "startColumn": 9,
                    "endColumn": 50,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java",
                    "startLine": 1513,
                    "endLine": 1702,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private classicGroupJoinToConsumerGroup(group ConsumerGroup, context RequestContext, request JoinGroupRequestData, responseFuture CompletableFuture<JoinGroupResponseData>) : CoordinatorResult<Void,CoordinatorRecord>"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java",
                    "startLine": 1543,
                    "endLine": 1543,
                    "startColumn": 13,
                    "endColumn": 82,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "JoinGroupRequest.requiresKnownMemberId(request,context.apiVersion())"
                }
            ],
            "isStatic": true
        },
        "ref_id": 600,
        "extraction_results": {
            "success": true,
            "newCommitHash": "63c8e0264cfb6509f0c869bbaa1b5fe682252576",
            "newBranchName": "extract-requiresKnownMemberId-classicGroupJoinToConsumerGroup-15a4501"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "27220d146c5d043da4adc3d636036bd6e7b112d2",
        "url": "https://github.com/apache/kafka/commit/27220d146c5d043da4adc3d636036bd6e7b112d2",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public summarize() : String extracted from public completeRequest(cb FutureCallback<T>) : T in class org.apache.kafka.connect.runtime.rest.HerderRequestHandler & moved to class org.apache.kafka.connect.util.Stage",
            "leftSideLocations": [
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 54,
                    "endLine": 89,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public completeRequest(cb FutureCallback<T>) : T"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 70,
                    "endLine": 73,
                    "startColumn": 17,
                    "endColumn": 67,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 71,
                    "endLine": 71,
                    "startColumn": 49,
                    "endColumn": 68,
                    "codeElementType": "STRING_LITERAL",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 72,
                    "endLine": 72,
                    "startColumn": 67,
                    "endColumn": 87,
                    "codeElementType": "STRING_LITERAL",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 75,
                    "endLine": 77,
                    "startColumn": 17,
                    "endColumn": 65,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 76,
                    "endLine": 76,
                    "startColumn": 49,
                    "endColumn": 68,
                    "codeElementType": "STRING_LITERAL",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 69,
                    "endLine": 78,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 69,
                    "endLine": 74,
                    "startColumn": 44,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 74,
                    "endLine": 78,
                    "startColumn": 20,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 64,
                    "endLine": 76,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public summarize() : String"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 67,
                    "endLine": 70,
                    "startColumn": 13,
                    "endColumn": 63,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 68,
                    "endLine": 68,
                    "startColumn": 39,
                    "endColumn": 58,
                    "codeElementType": "STRING_LITERAL",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 69,
                    "endLine": 69,
                    "startColumn": 57,
                    "endColumn": 77,
                    "codeElementType": "STRING_LITERAL",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 72,
                    "endLine": 74,
                    "startColumn": 13,
                    "endColumn": 61,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 73,
                    "endLine": 73,
                    "startColumn": 39,
                    "endColumn": 58,
                    "codeElementType": "STRING_LITERAL",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 66,
                    "endLine": 75,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 66,
                    "endLine": 71,
                    "startColumn": 32,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 71,
                    "endLine": 75,
                    "startColumn": 16,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 53,
                    "endLine": 78,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public completeRequest(cb FutureCallback<T>) : T"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 67,
                    "endLine": 67,
                    "startColumn": 54,
                    "endColumn": 71,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "stage.summarize()"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 65,
                    "endLine": 65,
                    "startColumn": 9,
                    "endColumn": 41,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": false
        },
        "ref_id": 601,
        "extraction_results": {
            "success": true,
            "newCommitHash": "a0c65163f3c3b5006b942822829c4c7c4acf3987",
            "newBranchName": "extract-summarize-completeRequest-4550550"
        },
        "telemetry": {
            "id": "0d98b9d6-a3f4-416b-bffc-9a43ae5d0a9c",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 133,
                "lineStart": 41,
                "lineEnd": 173,
                "bodyLineStart": 41,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                "sourceCode": "public class HerderRequestHandler {\n\n    private static final Logger log = LoggerFactory.getLogger(HerderRequestHandler.class);\n\n    private final RestClient restClient;\n\n    private final RestRequestTimeout requestTimeout;\n\n    public HerderRequestHandler(RestClient restClient, RestRequestTimeout requestTimeout) {\n        this.restClient = restClient;\n        this.requestTimeout = requestTimeout;\n    }\n\n    /**\n     * Wait for a {@link FutureCallback} to complete and return the result if successful.\n     * @param cb the future callback to wait for\n     * @return the future callback's result if successful\n     * @param <T> the future's result type\n     * @throws Throwable if the future callback isn't successful\n     */\n    public <T> T completeRequest(FutureCallback<T> cb) throws Throwable {\n        try {\n            return cb.get(requestTimeout.timeoutMs(), TimeUnit.MILLISECONDS);\n        } catch (ExecutionException e) {\n            throw e.getCause();\n        } catch (StagedTimeoutException e) {\n            String message;\n            Stage stage = e.stage();\n            message = summarize(stage);\n            // This timeout is for the operation itself. None of the timeout error codes are relevant, so internal server\n            // error is the best option\n            throw new ConnectRestException(Response.Status.INTERNAL_SERVER_ERROR.getStatusCode(), message);\n        } catch (TimeoutException e) {\n            // This timeout is for the operation itself. None of the timeout error codes are relevant, so internal server\n            // error is the best option\n            throw new ConnectRestException(Response.Status.INTERNAL_SERVER_ERROR.getStatusCode(), \"Request timed out\");\n        } catch (InterruptedException e) {\n            throw new ConnectRestException(Response.Status.INTERNAL_SERVER_ERROR.getStatusCode(), \"Request interrupted\");\n        }\n    }\n\n    private String summarize(Stage stage) {\n        String message;\n        if (stage.completed() != null) {\n            message = \"Request timed out. The last operation the worker completed was \"\n                    + stage.description() + \", which began at \"\n                    + Instant.ofEpochMilli(stage.started()) + \" and completed at \"\n                    + Instant.ofEpochMilli(stage.completed());\n        } else {\n            message = \"Request timed out. The worker is currently \"\n                    + stage.description() + \", which began at \"\n                    + Instant.ofEpochMilli(stage.started());\n        }\n        return message;\n    }\n\n    /**\n     * Wait for a {@link FutureCallback} to complete. If it succeeds, return the parsed response. If it fails, try to forward the\n     * request to the indicated target.\n     */\n    public <T, U> T completeOrForwardRequest(FutureCallback<T> cb,\n                                             String path,\n                                             String method,\n                                             HttpHeaders headers,\n                                             Map<String, String> queryParameters,\n                                             Object body,\n                                             TypeReference<U> resultType,\n                                             Translator<T, U> translator,\n                                             Boolean forward) throws Throwable {\n        try {\n            return completeRequest(cb);\n        } catch (RequestTargetException e) {\n            if (forward == null || forward) {\n                // the only time we allow recursive forwarding is when no forward flag has\n                // been set, which should only be seen by the first worker to handle a user request.\n                // this gives two total hops to resolve the request before giving up.\n                boolean recursiveForward = forward == null;\n                String forwardedUrl = e.forwardUrl();\n                if (forwardedUrl == null) {\n                    // the target didn't know of the leader at this moment.\n                    throw new ConnectRestException(Response.Status.CONFLICT.getStatusCode(),\n                            \"Cannot complete request momentarily due to no known leader URL, \"\n                                    + \"likely because a rebalance was underway.\");\n                }\n                UriBuilder uriBuilder = UriBuilder.fromUri(forwardedUrl)\n                        .path(path)\n                        .queryParam(\"forward\", recursiveForward);\n                if (queryParameters != null) {\n                    queryParameters.forEach(uriBuilder::queryParam);\n                }\n                String forwardUrl = uriBuilder.build().toString();\n                log.debug(\"Forwarding request {} {} {}\", forwardUrl, method, body);\n                return translator.translate(restClient.httpRequest(forwardUrl, method, headers, body, resultType));\n            } else {\n                log.error(\"Request '{} {}' failed because it couldn't find the target Connect worker within two hops (between workers).\",\n                        method, path);\n                // we should find the right target for the query within two hops, so if\n                // we don't, it probably means that a rebalance has taken place.\n                throw new ConnectRestException(Response.Status.CONFLICT.getStatusCode(),\n                        \"Cannot complete request because of a conflicting operation (e.g. worker rebalance)\");\n            }\n        } catch (RebalanceNeededException e) {\n            throw new ConnectRestException(Response.Status.CONFLICT.getStatusCode(),\n                    \"Cannot complete request momentarily due to stale configuration (typically caused by a concurrent config change)\");\n        }\n    }\n\n    public <T, U> T completeOrForwardRequest(FutureCallback<T> cb, String path, String method, HttpHeaders headers, Object body,\n                                             TypeReference<U> resultType, Translator<T, U> translator, Boolean forward) throws Throwable {\n        return completeOrForwardRequest(cb, path, method, headers, null, body, resultType, translator, forward);\n    }\n\n    public <T> T completeOrForwardRequest(FutureCallback<T> cb, String path, String method, HttpHeaders headers, Object body,\n                                                 TypeReference<T> resultType, Boolean forward) throws Throwable {\n        return completeOrForwardRequest(cb, path, method, headers, body, resultType, new IdentityTranslator<>(), forward);\n    }\n\n    public void completeOrForwardRequest(FutureCallback<Void> cb, String path, String method, HttpHeaders headers, Object body,\n                                          Boolean forward) throws Throwable {\n        completeOrForwardRequest(cb, path, method, headers, body, new TypeReference<Void>() { }, new IdentityTranslator<>(), forward);\n    }\n\n    public interface Translator<T, U> {\n        T translate(RestClient.HttpResponse<U> response);\n    }\n\n    public static class IdentityTranslator<T> implements Translator<T, T> {\n        @Override\n        public T translate(RestClient.HttpResponse<T> response) {\n            return response.body();\n        }\n    }\n}",
                "methodCount": 9
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 2,
                "candidates": [
                    {
                        "lineStart": 81,
                        "lineEnd": 94,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method summarize to class Stage",
                        "description": "Move method summarize to org.apache.kafka.connect.util.Stage\nRationale: The summarize() method relies heavily on the properties and methods of the Stage class, using its description, started, and completed attributes to construct a summary message. Therefore, placing this method within the Stage class ensures close encapsulation, cohesion, and easier maintenance, as the method's functionality is directly tied to the Stage instance.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 53,
                        "lineEnd": 79,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method completeRequest to class RestRequestTimeout",
                        "description": "Move method completeRequest to org.apache.kafka.connect.runtime.rest.RestRequestTimeout\nRationale: The 'completeRequest' method is heavily dependent on the request timeout, which is a core responsibility of the 'RestRequestTimeout' interface. By moving this method to 'RestRequestTimeout', it will align more closely with the timeout-related functionality that 'RestRequestTimeout' encapsulates. Furthermore, this move will improve encapsulation and cohesion by localizing timeout logic within a single class/interface. The method also directly invokes 'timeoutMs()', a method from 'RestRequestTimeout', indicating that it belongs to a context where timeout management is central. This ensures that the functionality related to REST request timeouts is handled in one class, improving maintainability and readability.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "completeRequest",
                            "method_signature": "public completeRequest(FutureCallback<T> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "summarize",
                            "method_signature": "private summarize(Stage stage)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeOrForwardRequest",
                            "method_signature": "public completeOrForwardRequest(FutureCallback<T> cb,\n                                             String path,\n                                             String method,\n                                             HttpHeaders headers,\n                                             Map<String, String> queryParameters,\n                                             Object body,\n                                             TypeReference<U> resultType,\n                                             Translator<T, U> translator,\n                                             Boolean forward)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeOrForwardRequest",
                            "method_signature": "public completeOrForwardRequest(FutureCallback<T> cb, String path, String method, HttpHeaders headers, Object body,\n                                             TypeReference<U> resultType, Translator<T, U> translator, Boolean forward)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeOrForwardRequest",
                            "method_signature": "public completeOrForwardRequest(FutureCallback<T> cb, String path, String method, HttpHeaders headers, Object body,\n                                                 TypeReference<T> resultType, Boolean forward)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeOrForwardRequest",
                            "method_signature": "public completeOrForwardRequest(FutureCallback<Void> cb, String path, String method, HttpHeaders headers, Object body,\n                                          Boolean forward)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "summarize",
                            "method_signature": "private summarize(Stage stage)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeOrForwardRequest",
                            "method_signature": "public completeOrForwardRequest(FutureCallback<T> cb, String path, String method, HttpHeaders headers, Object body,\n                                             TypeReference<U> resultType, Translator<T, U> translator, Boolean forward)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeOrForwardRequest",
                            "method_signature": "public completeOrForwardRequest(FutureCallback<T> cb, String path, String method, HttpHeaders headers, Object body,\n                                                 TypeReference<T> resultType, Boolean forward)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeOrForwardRequest",
                            "method_signature": "public completeOrForwardRequest(FutureCallback<Void> cb, String path, String method, HttpHeaders headers, Object body,\n                                          Boolean forward)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeRequest",
                            "method_signature": "public completeRequest(FutureCallback<T> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeOrForwardRequest",
                            "method_signature": "public completeOrForwardRequest(FutureCallback<T> cb,\n                                             String path,\n                                             String method,\n                                             HttpHeaders headers,\n                                             Map<String, String> queryParameters,\n                                             Object body,\n                                             TypeReference<U> resultType,\n                                             Translator<T, U> translator,\n                                             Boolean forward)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "summarize",
                            "method_signature": "private summarize(Stage stage)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeOrForwardRequest",
                            "method_signature": "public completeOrForwardRequest(FutureCallback<T> cb, String path, String method, HttpHeaders headers, Object body,\n                                             TypeReference<U> resultType, Translator<T, U> translator, Boolean forward)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeOrForwardRequest",
                            "method_signature": "public completeOrForwardRequest(FutureCallback<T> cb, String path, String method, HttpHeaders headers, Object body,\n                                                 TypeReference<T> resultType, Boolean forward)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeOrForwardRequest",
                            "method_signature": "public completeOrForwardRequest(FutureCallback<Void> cb, String path, String method, HttpHeaders headers, Object body,\n                                          Boolean forward)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeRequest",
                            "method_signature": "public completeRequest(FutureCallback<T> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeOrForwardRequest",
                            "method_signature": "public completeOrForwardRequest(FutureCallback<T> cb,\n                                             String path,\n                                             String method,\n                                             HttpHeaders headers,\n                                             Map<String, String> queryParameters,\n                                             Object body,\n                                             TypeReference<U> resultType,\n                                             Translator<T, U> translator,\n                                             Boolean forward)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private summarize(Stage stage)": {
                    "first": {
                        "method_name": "summarize",
                        "method_signature": "private summarize(Stage stage)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.356238072502898
                },
                "public completeOrForwardRequest(FutureCallback<T> cb, String path, String method, HttpHeaders headers, Object body,\n                                             TypeReference<U> resultType, Translator<T, U> translator, Boolean forward)": {
                    "first": {
                        "method_name": "completeOrForwardRequest",
                        "method_signature": "public completeOrForwardRequest(FutureCallback<T> cb, String path, String method, HttpHeaders headers, Object body,\n                                             TypeReference<U> resultType, Translator<T, U> translator, Boolean forward)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4261801625671017
                },
                "public completeOrForwardRequest(FutureCallback<T> cb, String path, String method, HttpHeaders headers, Object body,\n                                                 TypeReference<T> resultType, Boolean forward)": {
                    "first": {
                        "method_name": "completeOrForwardRequest",
                        "method_signature": "public completeOrForwardRequest(FutureCallback<T> cb, String path, String method, HttpHeaders headers, Object body,\n                                                 TypeReference<T> resultType, Boolean forward)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4586730194735466
                },
                "public completeOrForwardRequest(FutureCallback<Void> cb, String path, String method, HttpHeaders headers, Object body,\n                                          Boolean forward)": {
                    "first": {
                        "method_name": "completeOrForwardRequest",
                        "method_signature": "public completeOrForwardRequest(FutureCallback<Void> cb, String path, String method, HttpHeaders headers, Object body,\n                                          Boolean forward)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4683482399403714
                },
                "public completeRequest(FutureCallback<T> cb)": {
                    "first": {
                        "method_name": "completeRequest",
                        "method_signature": "public completeRequest(FutureCallback<T> cb)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5309984140082287
                },
                "public completeOrForwardRequest(FutureCallback<T> cb,\n                                             String path,\n                                             String method,\n                                             HttpHeaders headers,\n                                             Map<String, String> queryParameters,\n                                             Object body,\n                                             TypeReference<U> resultType,\n                                             Translator<T, U> translator,\n                                             Boolean forward)": {
                    "first": {
                        "method_name": "completeOrForwardRequest",
                        "method_signature": "public completeOrForwardRequest(FutureCallback<T> cb,\n                                             String path,\n                                             String method,\n                                             HttpHeaders headers,\n                                             Map<String, String> queryParameters,\n                                             Object body,\n                                             TypeReference<U> resultType,\n                                             Translator<T, U> translator,\n                                             Boolean forward)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6313481180509612
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "private summarize(Stage stage)",
                    "public completeRequest(FutureCallback<T> cb)"
                ],
                "llm_response_time": 2969
            },
            "targetClassMap": {
                "summarize": {
                    "target_classes": [
                        {
                            "class_name": "Stage",
                            "similarity_score": 0.4417011364860841
                        },
                        {
                            "class_name": "RestClient",
                            "similarity_score": 0.30643583166846416
                        },
                        {
                            "class_name": "RestRequestTimeout",
                            "similarity_score": 0.13075098860631257
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Stage",
                        "RestRequestTimeout",
                        "RestClient"
                    ],
                    "llm_response_time": 4031,
                    "similarity_computation_time": 3,
                    "similarity_metric": "cosine"
                },
                "completeRequest": {
                    "target_classes": [
                        {
                            "class_name": "RestRequestTimeout",
                            "similarity_score": 0.3643597788679312
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "RestRequestTimeout"
                    ],
                    "llm_response_time": 2497,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4de83d38c926bd991ce3e2d2804537a392ae6691",
        "url": "https://github.com/apache/kafka/commit/4de83d38c926bd991ce3e2d2804537a392ae6691",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public init(context InternalProcessorContext<KOut,VOut>, processingExceptionHandler ProcessingExceptionHandler) : void extracted from private initializeTopology() : void in class org.apache.kafka.streams.processor.internals.StreamTask & moved to class org.apache.kafka.streams.processor.internals.ProcessorNode",
            "leftSideLocations": [
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java",
                    "startLine": 1016,
                    "endLine": 1028,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private initializeTopology() : void"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java",
                    "startLine": 1023,
                    "endLine": 1023,
                    "startColumn": 17,
                    "endColumn": 45,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorNode.java",
                    "startLine": 139,
                    "endLine": 142,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public init(context InternalProcessorContext<KOut,VOut>, processingExceptionHandler ProcessingExceptionHandler) : void"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorNode.java",
                    "startLine": 140,
                    "endLine": 140,
                    "startColumn": 9,
                    "endColumn": 23,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java",
                    "startLine": 1025,
                    "endLine": 1037,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private initializeTopology() : void"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java",
                    "startLine": 1032,
                    "endLine": 1032,
                    "startColumn": 17,
                    "endColumn": 72,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "node.init(processorContext,processingExceptionHandler)"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorNode.java",
                    "startLine": 141,
                    "endLine": 141,
                    "startColumn": 9,
                    "endColumn": 70,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": false
        },
        "ref_id": 602,
        "extraction_results": {
            "success": true,
            "newCommitHash": "fa2cb0ebdf44479f6bad42bdb0f0cb5bc1cc37a7",
            "newBranchName": "extract-init-initializeTopology-931bb62"
        },
        "telemetry": {
            "id": "d56fa6a0-847e-4da8-aae2-089bf21a71e0",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1290,
                "lineStart": 67,
                "lineEnd": 1356,
                "bodyLineStart": 67,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java",
                "sourceCode": "/**\n * A StreamTask is associated with a {@link AbstractPartitionGroup}, and is assigned to a StreamThread for processing.\n */\npublic class StreamTask extends AbstractTask implements ProcessorNodePunctuator, Task {\n\n    private final Time time;\n    private final Consumer<byte[], byte[]> mainConsumer;\n\n    // we want to abstract eos logic out of StreamTask, however\n    // there's still an optimization that requires this info to be\n    // leaked into this class, which is to checkpoint after committing if EOS is not enabled.\n    private final boolean eosEnabled;\n\n    private final int maxBufferedSize;\n    private final AbstractPartitionGroup partitionGroup;\n    private final RecordCollector recordCollector;\n    private final AbstractPartitionGroup.RecordInfo recordInfo;\n    private final Map<TopicPartition, Long> consumedOffsets;\n    private final Map<TopicPartition, Long> committedOffsets;\n    private final Map<TopicPartition, Long> highWatermark;\n    private final Set<TopicPartition> resetOffsetsForPartitions;\n    private final Set<TopicPartition> partitionsToResume;\n    private final PunctuationQueue streamTimePunctuationQueue;\n    private final PunctuationQueue systemTimePunctuationQueue;\n    private final StreamsMetricsImpl streamsMetrics;\n\n    private long processTimeMs = 0L;\n\n    private final Sensor closeTaskSensor;\n    private final Sensor processRatioSensor;\n    private final Sensor processLatencySensor;\n    private final Sensor restoreSensor;\n    private final Sensor restoreRemainingSensor;\n    private final Sensor punctuateLatencySensor;\n    private final Sensor bufferedRecordsSensor;\n    private final Map<String, Sensor> e2eLatencySensors = new HashMap<>();\n\n    private final RecordQueueCreator recordQueueCreator;\n\n    @SuppressWarnings(\"rawtypes\")\n    protected final InternalProcessorContext processorContext;\n\n    private StampedRecord record;\n    private boolean commitNeeded = false;\n    private boolean commitRequested = false;\n    private boolean hasPendingTxCommit = false;\n    private Optional<Long> timeCurrentIdlingStarted;\n\n    @SuppressWarnings({\"rawtypes\", \"this-escape\", \"checkstyle:ParameterNumber\"})\n    public StreamTask(final TaskId id,\n                      final Set<TopicPartition> inputPartitions,\n                      final ProcessorTopology topology,\n                      final Consumer<byte[], byte[]> mainConsumer,\n                      final TaskConfig config,\n                      final StreamsMetricsImpl streamsMetrics,\n                      final StateDirectory stateDirectory,\n                      final ThreadCache cache,\n                      final Time time,\n                      final ProcessorStateManager stateMgr,\n                      final RecordCollector recordCollector,\n                      final InternalProcessorContext processorContext,\n                      final LogContext logContext,\n                      final boolean processingThreadsEnabled\n                      ) {\n        super(\n            id,\n            topology,\n            stateDirectory,\n            stateMgr,\n            inputPartitions,\n            config,\n            \"task\",\n            StreamTask.class\n        );\n        this.mainConsumer = mainConsumer;\n\n        this.processorContext = processorContext;\n        processorContext.transitionToActive(this, recordCollector, cache);\n\n        this.time = time;\n        this.recordCollector = recordCollector;\n        this.eosEnabled = config.eosEnabled;\n\n        final String threadId = Thread.currentThread().getName();\n        this.streamsMetrics = streamsMetrics;\n        closeTaskSensor = ThreadMetrics.closeTaskSensor(threadId, streamsMetrics);\n        final String taskId = id.toString();\n        restoreSensor = TaskMetrics.restoreSensor(threadId, taskId, streamsMetrics);\n        restoreRemainingSensor = TaskMetrics.restoreRemainingRecordsSensor(threadId, taskId, streamsMetrics);\n        processRatioSensor = TaskMetrics.activeProcessRatioSensor(threadId, taskId, streamsMetrics);\n        processLatencySensor = TaskMetrics.processLatencySensor(threadId, taskId, streamsMetrics);\n        punctuateLatencySensor = TaskMetrics.punctuateSensor(threadId, taskId, streamsMetrics);\n        bufferedRecordsSensor = TaskMetrics.activeBufferedRecordsSensor(threadId, taskId, streamsMetrics);\n\n        for (final String terminalNodeName : topology.terminalNodes()) {\n            e2eLatencySensors.put(\n                terminalNodeName,\n                ProcessorNodeMetrics.e2ELatencySensor(threadId, taskId, terminalNodeName, streamsMetrics)\n            );\n        }\n\n        for (final ProcessorNode<?, ?, ?, ?> sourceNode : topology.sources()) {\n            final String sourceNodeName = sourceNode.name();\n            e2eLatencySensors.put(\n                sourceNodeName,\n                ProcessorNodeMetrics.e2ELatencySensor(threadId, taskId, sourceNodeName, streamsMetrics)\n            );\n        }\n\n        streamTimePunctuationQueue = new PunctuationQueue();\n        systemTimePunctuationQueue = new PunctuationQueue();\n        maxBufferedSize = config.maxBufferedSize;\n\n        // initialize the consumed and committed offset cache\n        consumedOffsets = new HashMap<>();\n        resetOffsetsForPartitions = new HashSet<>();\n        partitionsToResume = new HashSet<>();\n\n        recordQueueCreator = new RecordQueueCreator(this.logContext, config.timestampExtractor, config.deserializationExceptionHandler);\n\n        recordInfo = new RecordInfo();\n\n        final Sensor enforcedProcessingSensor;\n        enforcedProcessingSensor = TaskMetrics.enforcedProcessingSensor(threadId, taskId, streamsMetrics);\n        final long maxTaskIdleMs = config.maxTaskIdleMs;\n        if (processingThreadsEnabled) {\n            partitionGroup = new SynchronizedPartitionGroup(new PartitionGroup(\n                logContext,\n                createPartitionQueues(),\n                mainConsumer::currentLag,\n                TaskMetrics.recordLatenessSensor(threadId, taskId, streamsMetrics),\n                enforcedProcessingSensor,\n                maxTaskIdleMs\n            ));\n        } else {\n            partitionGroup = new PartitionGroup(\n                logContext,\n                createPartitionQueues(),\n                mainConsumer::currentLag,\n                TaskMetrics.recordLatenessSensor(threadId, taskId, streamsMetrics),\n                enforcedProcessingSensor,\n                maxTaskIdleMs\n            );\n        }\n\n        stateMgr.registerGlobalStateStores(topology.globalStateStores());\n        committedOffsets = new HashMap<>();\n        highWatermark = new HashMap<>();\n        for (final TopicPartition topicPartition: inputPartitions) {\n            committedOffsets.put(topicPartition, -1L);\n            highWatermark.put(topicPartition, -1L);\n        }\n        timeCurrentIdlingStarted = Optional.empty();\n    }\n\n    // create queues for each assigned partition and associate them\n    // to corresponding source nodes in the processor topology\n    private Map<TopicPartition, RecordQueue> createPartitionQueues() {\n        final Map<TopicPartition, RecordQueue> partitionQueues = new HashMap<>();\n        for (final TopicPartition partition : inputPartitions()) {\n            partitionQueues.put(partition, recordQueueCreator.createQueue(partition));\n        }\n        return partitionQueues;\n    }\n\n    @Override\n    public boolean isActive() {\n        return true;\n    }\n\n    @Override\n    public void recordRestoration(final Time time, final long numRecords, final boolean initRemaining) {\n        if (initRemaining) {\n            maybeRecordSensor(numRecords, time, restoreRemainingSensor);\n        } else {\n            maybeRecordSensor(numRecords, time, restoreSensor);\n            maybeRecordSensor(-1 * numRecords, time, restoreRemainingSensor);\n        }\n    }\n\n    /**\n     * @throws TaskCorruptedException if the state cannot be reused (with EOS) and needs to be reset\n     * @throws LockException    could happen when multi-threads within the single instance, could retry\n     * @throws TimeoutException if initializing record collector timed out\n     * @throws StreamsException fatal error, should close the thread\n     */\n    @Override\n    public void initializeIfNeeded() {\n        if (state() == State.CREATED) {\n            recordCollector.initialize();\n\n            StateManagerUtil.registerStateStores(log, logPrefix, topology, stateMgr, stateDirectory, processorContext);\n\n            // without EOS the checkpoint file would not be deleted after loading, and\n            // with EOS we would not checkpoint ever during running state anyways.\n            // therefore we can initialize the snapshot as empty so that we would checkpoint right after loading\n            offsetSnapshotSinceLastFlush = Collections.emptyMap();\n\n            transitionTo(State.RESTORING);\n\n            log.info(\"Initialized\");\n        }\n    }\n\n    public void addPartitionsForOffsetReset(final Set<TopicPartition> partitionsForOffsetReset) {\n        mainConsumer.pause(partitionsForOffsetReset);\n        resetOffsetsForPartitions.addAll(partitionsForOffsetReset);\n    }\n\n    /**\n     * @throws TimeoutException if fetching committed offsets timed out\n     */\n    @Override\n    public void completeRestoration(final java.util.function.Consumer<Set<TopicPartition>> offsetResetter) {\n        switch (state()) {\n            case RUNNING:\n                return;\n\n            case RESTORING:\n                resetOffsetsIfNeededAndInitializeMetadata(offsetResetter);\n                initializeTopology();\n                processorContext.initialize();\n                if (!eosEnabled) {\n                    maybeCheckpoint(true);\n                }\n\n                transitionTo(State.RUNNING);\n\n                log.info(\"Restored and ready to run\");\n\n                break;\n\n            case CREATED:\n            case SUSPENDED:\n            case CLOSED:\n                throw new IllegalStateException(\"Illegal state \" + state() + \" while completing restoration for active task \" + id);\n\n            default:\n                throw new IllegalStateException(\"Unknown state \" + state() + \" while completing restoration for active task \" + id);\n        }\n    }\n\n    @Override\n    public void suspend() {\n        switch (state()) {\n            case CREATED:\n                transitToSuspend();\n                break;\n\n            case RESTORING:\n                transitToSuspend();\n                break;\n\n            case RUNNING:\n                try {\n                    // use try-catch to ensure state transition to SUSPENDED even if user code throws in `Processor#close()`\n                    closeTopology();\n\n                    // we must clear the buffered records when suspending because upon resuming the consumer would\n                    // re-fetch those records starting from the committed position\n                    partitionGroup.clear();\n                } finally {\n                    transitToSuspend();\n                }\n\n                break;\n\n            case SUSPENDED:\n                log.info(\"Skip suspending since state is {}\", state());\n\n                break;\n\n            case CLOSED:\n                throw new IllegalStateException(\"Illegal state \" + state() + \" while suspending active task \" + id);\n\n            default:\n                throw new IllegalStateException(\"Unknown state \" + state() + \" while suspending active task \" + id);\n        }\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    private void closeTopology() {\n        log.trace(\"Closing processor topology\");\n\n        // close the processors\n        // make sure close() is called for each node even when there is a RuntimeException\n        RuntimeException exception = null;\n        for (final ProcessorNode<?, ?, ?, ?> node : topology.processors()) {\n            processorContext.setCurrentNode(node);\n            try {\n                node.close();\n            } catch (final RuntimeException e) {\n                exception = e;\n            } finally {\n                processorContext.setCurrentNode(null);\n            }\n        }\n\n        if (exception != null) {\n            throw exception;\n        }\n    }\n\n    /**\n     * <pre>\n     * - resume the task\n     * </pre>\n     */\n    @Override\n    public void resume() {\n        switch (state()) {\n            case CREATED:\n            case RUNNING:\n            case RESTORING:\n                // no need to do anything, just let them continue running / restoring / closing\n                log.trace(\"Skip resuming since state is {}\", state());\n                break;\n\n            case SUSPENDED:\n                // just transit the state without any logical changes: suspended and restoring states\n                // are not actually any different for inner modules\n\n                // Deleting checkpoint file before transition to RESTORING state (KAFKA-10362)\n                try {\n                    stateMgr.deleteCheckPointFileIfEOSEnabled();\n                    log.debug(\"Deleted check point file upon resuming with EOS enabled\");\n                } catch (final IOException ioe) {\n                    log.error(\"Encountered error while deleting the checkpoint file due to this exception\", ioe);\n                }\n\n                transitionTo(State.RESTORING);\n                log.info(\"Resumed to restoring state\");\n\n                break;\n\n            case CLOSED:\n                throw new IllegalStateException(\"Illegal state \" + state() + \" while resuming active task \" + id);\n\n            default:\n                throw new IllegalStateException(\"Unknown state \" + state() + \" while resuming active task \" + id);\n        }\n        timeCurrentIdlingStarted = Optional.empty();\n    }\n\n\n    public void flush() {\n        stateMgr.flushCache();\n        recordCollector.flush();\n    }\n\n    /**\n     * @throws StreamsException fatal error that should cause the thread to die\n     * @throws TaskMigratedException recoverable error that would cause the task to be removed\n     * @return offsets that should be committed for this task\n     */\n    @Override\n    public Map<TopicPartition, OffsetAndMetadata> prepareCommit() {\n        switch (state()) {\n            case CREATED:\n            case RESTORING:\n            case RUNNING:\n            case SUSPENDED:\n                // the commitNeeded flag just indicates whether we have reached RUNNING and processed any new data,\n                // so it only indicates whether the record collector should be flushed or not, whereas the state\n                // manager should always be flushed; either there is newly restored data or the flush will be a no-op\n                if (commitNeeded) {\n                    // we need to flush the store caches before flushing the record collector since it may cause some\n                    // cached records to be processed and hence generate more records to be sent out\n                    //\n                    // TODO: this should be removed after we decouple caching with emitting\n                    flush();\n                    hasPendingTxCommit = eosEnabled;\n\n                    log.debug(\"Prepared {} task for committing\", state());\n                    return committableOffsetsAndMetadata();\n                } else {\n                    log.debug(\"Skipped preparing {} task for commit since there is nothing to commit\", state());\n                    return Collections.emptyMap();\n                }\n\n            case CLOSED:\n                throw new IllegalStateException(\"Illegal state \" + state() + \" while preparing active task \" + id + \" for committing\");\n\n            default:\n                throw new IllegalStateException(\"Unknown state \" + state() + \" while preparing active task \" + id + \" for committing\");\n        }\n    }\n\n    private Long findOffset(final TopicPartition partition) {\n        Long offset = partitionGroup.headRecordOffset(partition);\n        if (offset == null) {\n            try {\n                offset = mainConsumer.position(partition);\n            } catch (final TimeoutException error) {\n                // the `consumer.position()` call should never block, because we know that we did process data\n                // for the requested partition and thus the consumer should have a valid local position\n                // that it can return immediately\n\n                // hence, a `TimeoutException` indicates a bug and thus we rethrow it as fatal `IllegalStateException`\n                throw new IllegalStateException(error);\n            } catch (final KafkaException fatal) {\n                throw new StreamsException(fatal);\n            }\n        }\n        return offset;\n    }\n\n    private Map<TopicPartition, OffsetAndMetadata> committableOffsetsAndMetadata() {\n        final Map<TopicPartition, OffsetAndMetadata> committableOffsets;\n\n        switch (state()) {\n            case CREATED:\n            case RESTORING:\n                committableOffsets = Collections.emptyMap();\n\n                break;\n\n            case RUNNING:\n            case SUSPENDED:\n                final Map<TopicPartition, Long> partitionTimes = extractPartitionTimes();\n\n                // If there's processor metadata to be committed. We need to commit them to all\n                // input partitions\n                final Set<TopicPartition> partitionsNeedCommit = processorContext.getProcessorMetadata().needsCommit() ?\n                    inputPartitions() : consumedOffsets.keySet();\n                committableOffsets = new HashMap<>(partitionsNeedCommit.size());\n\n                for (final TopicPartition partition : partitionsNeedCommit) {\n                    final Long offset = findOffset(partition);\n                    final long partitionTime = partitionTimes.get(partition);\n                    committableOffsets.put(partition, new OffsetAndMetadata(offset,\n                        new TopicPartitionMetadata(partitionTime, processorContext.getProcessorMetadata()).encode()));\n                }\n                break;\n\n            case CLOSED:\n                throw new IllegalStateException(\"Illegal state \" + state() + \" while getting committable offsets for active task \" + id);\n\n            default:\n                throw new IllegalStateException(\"Unknown state \" + state() + \" while post committing active task \" + id);\n        }\n\n        return committableOffsets;\n    }\n\n    @Override\n    public void postCommit(final boolean enforceCheckpoint) {\n        switch (state()) {\n            case CREATED:\n                // We should never write a checkpoint for a CREATED task as we may overwrite an existing checkpoint\n                // with empty uninitialized offsets\n                log.debug(\"Skipped writing checkpoint for {} task\", state());\n\n                break;\n\n            case RESTORING:\n            case SUSPENDED:\n                maybeCheckpoint(enforceCheckpoint);\n                log.debug(\"Finalized commit for {} task with enforce checkpoint {}\", state(), enforceCheckpoint);\n\n                break;\n\n            case RUNNING:\n                if (enforceCheckpoint || !eosEnabled) {\n                    maybeCheckpoint(enforceCheckpoint);\n                }\n                log.debug(\"Finalized commit for {} task with eos {} enforce checkpoint {}\", state(), eosEnabled, enforceCheckpoint);\n\n                break;\n\n            case CLOSED:\n                throw new IllegalStateException(\"Illegal state \" + state() + \" while post committing active task \" + id);\n\n            default:\n                throw new IllegalStateException(\"Unknown state \" + state() + \" while post committing active task \" + id);\n        }\n\n        clearCommitStatuses();\n    }\n\n    private void clearCommitStatuses() {\n        commitNeeded = false;\n        commitRequested = false;\n        hasPendingTxCommit = false;\n        processorContext.getProcessorMetadata().setNeedsCommit(false);\n    }\n\n    private Map<TopicPartition, Long> extractPartitionTimes() {\n        final Map<TopicPartition, Long> partitionTimes = new HashMap<>();\n        for (final TopicPartition partition : partitionGroup.partitions()) {\n            partitionTimes.put(partition, partitionGroup.partitionTimestamp(partition));\n        }\n        return partitionTimes;\n    }\n\n    @Override\n    public void closeClean() {\n        validateClean();\n        removeAllSensors();\n        clearCommitStatuses();\n        close(true);\n        log.info(\"Closed clean\");\n    }\n\n    @Override\n    public void closeDirty() {\n        removeAllSensors();\n        clearCommitStatuses();\n        close(false);\n        log.info(\"Closed dirty\");\n    }\n\n    @Override\n    public void updateInputPartitions(final Set<TopicPartition> topicPartitions, final Map<String, List<String>> allTopologyNodesToSourceTopics) {\n        super.updateInputPartitions(topicPartitions, allTopologyNodesToSourceTopics);\n        partitionGroup.updatePartitions(topicPartitions, recordQueueCreator::createQueue);\n        processorContext.getProcessorMetadata().setNeedsCommit(true);\n    }\n\n    @Override\n    public void prepareRecycle() {\n        validateClean();\n        removeAllSensors();\n        clearCommitStatuses();\n        switch (state()) {\n            case SUSPENDED:\n                stateMgr.recycle();\n                partitionGroup.close();\n                recordCollector.closeClean();\n\n                break;\n\n            case CREATED:\n            case RESTORING:\n            case RUNNING:\n            case CLOSED:\n                throw new IllegalStateException(\"Illegal state \" + state() + \" while recycling active task \" + id);\n            default:\n                throw new IllegalStateException(\"Unknown state \" + state() + \" while recycling active task \" + id);\n        }\n\n        closeTaskSensor.record();\n        transitionTo(State.CLOSED);\n\n        log.info(\"Closed and recycled state\");\n    }\n\n    @Override\n    public void resumePollingForPartitionsWithAvailableSpace() {\n        if (!partitionsToResume.isEmpty()) {\n            mainConsumer.resume(partitionsToResume);\n            partitionsToResume.clear();\n        }\n    }\n\n    @Override\n    public void updateLags() {\n        if (state() == State.RUNNING) {\n            partitionGroup.updateLags();\n        }\n    }\n\n    /**\n     * The following exceptions maybe thrown from the state manager flushing call\n     *\n     * @throws TaskMigratedException recoverable error sending changelog records that would cause the task to be removed\n     * @throws StreamsException fatal error when flushing the state store, for example sending changelog records failed\n     *                          or flushing state store get IO errors; such error should cause the thread to die\n     */\n    @Override\n    public void maybeCheckpoint(final boolean enforceCheckpoint) {\n        // commitNeeded indicates we may have processed some records since last commit\n        // and hence we need to refresh checkpointable offsets regardless whether we should checkpoint or not\n        if (commitNeeded || enforceCheckpoint) {\n            stateMgr.updateChangelogOffsets(checkpointableOffsets());\n        }\n\n        super.maybeCheckpoint(enforceCheckpoint);\n    }\n\n    private void validateClean() {\n        // It may be that we failed to commit a task during handleRevocation, but \"forgot\" this and tried to\n        // closeClean in handleAssignment. We should throw if we detect this to force the TaskManager to closeDirty\n        if (commitNeeded) {\n            log.debug(\"Tried to close clean but there was pending uncommitted data, this means we failed to\"\n                          + \" commit and should close as dirty instead\");\n            throw new TaskMigratedException(\"Tried to close dirty task as clean\");\n        }\n    }\n\n    private void removeAllSensors() {\n        streamsMetrics.removeAllTaskLevelSensors(Thread.currentThread().getName(), id.toString());\n        for (final String nodeName : e2eLatencySensors.keySet()) {\n            streamsMetrics.removeAllNodeLevelSensors(Thread.currentThread().getName(), id.toString(), nodeName);\n        }\n    }\n\n    /**\n     * You must commit a task and checkpoint the state manager before closing as this will release the state dir lock\n     */\n    private void close(final boolean clean) {\n        switch (state()) {\n            case SUSPENDED:\n                TaskManager.executeAndMaybeSwallow(\n                    clean,\n                    partitionGroup::close,\n                    \"partition group close\",\n                    log\n                );\n\n                // first close state manager (which is idempotent) then close the record collector\n                // if the latter throws and we re-close dirty which would close the state manager again.\n                TaskManager.executeAndMaybeSwallow(\n                    clean,\n                    () -> StateManagerUtil.closeStateManager(\n                        log,\n                        logPrefix,\n                        clean,\n                        eosEnabled,\n                        stateMgr,\n                        stateDirectory,\n                        TaskType.ACTIVE\n                    ),\n                    \"state manager close\",\n                    log);\n\n                TaskManager.executeAndMaybeSwallow(\n                    clean,\n                    clean ? recordCollector::closeClean : recordCollector::closeDirty,\n                    \"record collector close\",\n                    log\n                );\n\n                break;\n\n            case CLOSED:\n                log.trace(\"Skip closing since state is {}\", state());\n                return;\n\n            case CREATED:\n            case RESTORING:\n            case RUNNING:\n                throw new IllegalStateException(\"Illegal state \" + state() + \" while closing active task \" + id);\n\n            default:\n                throw new IllegalStateException(\"Unknown state \" + state() + \" while closing active task \" + id);\n        }\n\n        record = null;\n        closeTaskSensor.record();\n        partitionsToResume.clear();\n\n        transitionTo(State.CLOSED);\n    }\n\n    /**\n     * An active task is processable if its buffer contains data for all of its input\n     * source topic partitions, or if it is enforced to be processable.\n     */\n    public boolean isProcessable(final long wallClockTime) {\n        if (state() == State.CLOSED) {\n            // a task is only closing / closed when 1) task manager is closing, 2) a rebalance is undergoing;\n            // in either case we can just log it and move on without notifying the thread since the consumer\n            // would soon be updated to not return any records for this task anymore.\n            log.info(\"Stream task {} is already in {} state, skip processing it.\", id(), state());\n\n            return false;\n        }\n\n        if (hasPendingTxCommit) {\n            // if the task has a pending TX commit, we should just retry the commit but not process any records\n            // thus, the task is not processable, even if there is available data in the record queue\n            return false;\n        }\n        final boolean readyToProcess = partitionGroup.readyToProcess(wallClockTime);\n        if (!readyToProcess) {\n            if (!timeCurrentIdlingStarted.isPresent()) {\n                timeCurrentIdlingStarted = Optional.of(wallClockTime);\n            }\n        } else {\n            timeCurrentIdlingStarted = Optional.empty();\n        }\n        return readyToProcess;\n    }\n\n    /**\n     * Process one record.\n     *\n     * @return true if this method processes a record, false if it does not process a record.\n     * @throws TaskMigratedException if the task producer got fenced (EOS only)\n     */\n    @SuppressWarnings(\"unchecked\")\n    public boolean process(final long wallClockTime) {\n        if (record == null) {\n            if (!isProcessable(wallClockTime)) {\n                return false;\n            }\n\n            // get the next record to process\n            record = partitionGroup.nextRecord(recordInfo, wallClockTime);\n\n            // if there is no record to process, return immediately\n            if (record == null) {\n                return false;\n            }\n        }\n\n        try {\n            final TopicPartition partition = recordInfo.partition();\n\n            if (!(record instanceof CorruptedRecord)) {\n                doProcess(wallClockTime);\n            }\n\n            // update the consumed offset map after processing is done\n            consumedOffsets.put(partition, record.offset());\n            commitNeeded = true;\n\n            // after processing this record, if its partition queue's buffered size has been\n            // decreased to the threshold, we can then resume the consumption on this partition\n            if (recordInfo.queue().size() == maxBufferedSize) {\n                partitionsToResume.add(partition);\n            }\n\n            record = null;\n        } catch (final TimeoutException timeoutException) {\n            if (!eosEnabled) {\n                throw timeoutException;\n            } else {\n                record = null;\n                throw new TaskCorruptedException(Collections.singleton(id));\n            }\n        } catch (final StreamsException exception) {\n            record = null;\n            throw exception;\n        } catch (final RuntimeException e) {\n            final StreamsException error = new StreamsException(\n                String.format(\n                    \"Exception caught in process. taskId=%s, processor=%s, topic=%s, partition=%d, offset=%d, stacktrace=%s\",\n                    id(),\n                    processorContext.currentNode().name(),\n                    record.topic(),\n                    record.partition(),\n                    record.offset(),\n                    getStacktraceString(e)\n                ),\n                e\n            );\n            record = null;\n\n            throw error;\n        } finally {\n            processorContext.setCurrentNode(null);\n        }\n\n        return true;\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    private void doProcess(final long wallClockTime) {\n        // process the record by passing to the source node of the topology\n        final ProcessorNode<Object, Object, Object, Object> currNode = (ProcessorNode<Object, Object, Object, Object>) recordInfo.node();\n        log.trace(\"Start processing one record [{}]\", record);\n\n        final ProcessorRecordContext recordContext = new ProcessorRecordContext(\n            record.timestamp,\n            record.offset(),\n            record.partition(),\n            record.topic(),\n            record.headers()\n        );\n        updateProcessorContext(currNode, wallClockTime, recordContext);\n\n        maybeRecordE2ELatency(record.timestamp, wallClockTime, currNode.name());\n        final Record<Object, Object> toProcess = new Record<>(\n            record.key(),\n            record.value(),\n            processorContext.timestamp(),\n            processorContext.headers()\n        );\n        maybeMeasureLatency(() -> currNode.process(toProcess), time, processLatencySensor);\n\n        log.trace(\"Completed processing one record [{}]\", record);\n    }\n\n    @Override\n    public void recordProcessBatchTime(final long processBatchTime) {\n        processTimeMs += processBatchTime;\n    }\n\n    @Override\n    public void recordProcessTimeRatioAndBufferSize(final long allTaskProcessMs, final long now) {\n        bufferedRecordsSensor.record(partitionGroup.numBuffered());\n        processRatioSensor.record((double) processTimeMs / allTaskProcessMs, now);\n        processTimeMs = 0L;\n    }\n\n    private String getStacktraceString(final RuntimeException e) {\n        String stacktrace = null;\n        try (final StringWriter stringWriter = new StringWriter();\n             final PrintWriter printWriter = new PrintWriter(stringWriter)) {\n            e.printStackTrace(printWriter);\n            stacktrace = stringWriter.toString();\n        } catch (final IOException ioe) {\n            log.error(\"Encountered error extracting stacktrace from this exception\", ioe);\n        }\n        return stacktrace;\n    }\n\n    /**\n     * @throws IllegalStateException if the current node is not null\n     * @throws TaskMigratedException if the task producer got fenced (EOS only)\n     */\n    @SuppressWarnings(\"unchecked\")\n    @Override\n    public void punctuate(final ProcessorNode<?, ?, ?, ?> node,\n                          final long timestamp,\n                          final PunctuationType type,\n                          final Punctuator punctuator) {\n        if (processorContext.currentNode() != null) {\n            throw new IllegalStateException(String.format(\"%sCurrent node is not null\", logPrefix));\n        }\n\n        // when punctuating, we need to preserve the timestamp (this can be either system time or event time)\n        // while other record context are set as dummy: null topic, -1 partition, -1 offset and empty header\n        final ProcessorRecordContext recordContext = new ProcessorRecordContext(\n            timestamp,\n            -1L,\n            -1,\n            null,\n            new RecordHeaders()\n        );\n        updateProcessorContext(node, time.milliseconds(), recordContext);\n\n        if (log.isTraceEnabled()) {\n            log.trace(\"Punctuating processor {} with timestamp {} and punctuation type {}\", node.name(), timestamp, type);\n        }\n\n        try {\n            maybeMeasureLatency(() -> node.punctuate(timestamp, punctuator), time, punctuateLatencySensor);\n        } catch (final StreamsException e) {\n            throw e;\n        } catch (final RuntimeException e) {\n            throw new StreamsException(String.format(\"%sException caught while punctuating processor '%s'\", logPrefix, node.name()), e);\n        } finally {\n            processorContext.setCurrentNode(null);\n        }\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    private void updateProcessorContext(final ProcessorNode<?, ?, ?, ?> currNode,\n                                        final long wallClockTime,\n                                        final ProcessorRecordContext recordContext) {\n        processorContext.setRecordContext(recordContext);\n        processorContext.setCurrentNode(currNode);\n        processorContext.setSystemTimeMs(wallClockTime);\n    }\n\n    /**\n     * Return all the checkpointable offsets(written + consumed) to the state manager.\n     * Currently only changelog topic offsets need to be checkpointed.\n     */\n    private Map<TopicPartition, Long> checkpointableOffsets() {\n        final Map<TopicPartition, Long> checkpointableOffsets = new HashMap<>(recordCollector.offsets());\n        for (final Map.Entry<TopicPartition, Long> entry : consumedOffsets.entrySet()) {\n            checkpointableOffsets.putIfAbsent(entry.getKey(), entry.getValue());\n        }\n\n        log.debug(\"Checkpointable offsets {}\", checkpointableOffsets);\n\n        return checkpointableOffsets;\n    }\n\n    private void resetOffsetsIfNeededAndInitializeMetadata(final java.util.function.Consumer<Set<TopicPartition>> offsetResetter) {\n        try {\n            final Map<TopicPartition, OffsetAndMetadata> offsetsAndMetadata = mainConsumer.committed(inputPartitions());\n\n            for (final Map.Entry<TopicPartition, OffsetAndMetadata> committedEntry : offsetsAndMetadata.entrySet()) {\n                if (resetOffsetsForPartitions.contains(committedEntry.getKey())) {\n                    final OffsetAndMetadata offsetAndMetadata = committedEntry.getValue();\n                    if (offsetAndMetadata != null) {\n                        mainConsumer.seek(committedEntry.getKey(), offsetAndMetadata);\n                        resetOffsetsForPartitions.remove(committedEntry.getKey());\n                    }\n                }\n            }\n\n            offsetResetter.accept(resetOffsetsForPartitions);\n            resetOffsetsForPartitions.clear();\n\n            initializeTaskTimeAndProcessorMetadata(offsetsAndMetadata.entrySet().stream()\n                .filter(e -> e.getValue() != null)\n                .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue))\n            );\n        } catch (final TimeoutException timeoutException) {\n            log.warn(\n                \"Encountered {} while trying to fetch committed offsets, will retry initializing the metadata in the next loop.\" +\n                    \"\\nConsider overwriting consumer config {} to a larger value to avoid timeout errors\",\n                time.toString(),\n                ConsumerConfig.DEFAULT_API_TIMEOUT_MS_CONFIG);\n\n            // re-throw to trigger `task.timeout.ms`\n            throw timeoutException;\n        } catch (final KafkaException e) {\n            throw new StreamsException(String.format(\"task [%s] Failed to initialize offsets for %s\", id, inputPartitions()), e);\n        }\n    }\n\n    private void initializeTaskTimeAndProcessorMetadata(final Map<TopicPartition, OffsetAndMetadata> offsetsAndMetadata) {\n        final ProcessorMetadata finalProcessMetadata = new ProcessorMetadata();\n        for (final Map.Entry<TopicPartition, OffsetAndMetadata> entry : offsetsAndMetadata.entrySet()) {\n            final TopicPartition partition = entry.getKey();\n            final OffsetAndMetadata metadata = entry.getValue();\n\n            if (metadata != null) {\n                final TopicPartitionMetadata committedTimestampAndMeta = TopicPartitionMetadata.decode(metadata.metadata());\n                final long committedTimestamp = committedTimestampAndMeta.partitionTime();\n                partitionGroup.setPartitionTime(partition, committedTimestamp);\n                log.debug(\"A committed timestamp was detected: setting the partition time of partition {}\"\n                    + \" to {} in stream task {}\", partition, committedTimestamp, id);\n\n                final ProcessorMetadata processorMetadata = committedTimestampAndMeta.processorMetadata();\n                finalProcessMetadata.update(processorMetadata);\n            } else {\n                log.debug(\"No committed timestamp was found in metadata for partition {}\", partition);\n            }\n        }\n        processorContext.setProcessorMetadata(finalProcessMetadata);\n\n        final Set<TopicPartition> nonCommitted = new HashSet<>(inputPartitions());\n        nonCommitted.removeAll(offsetsAndMetadata.keySet());\n        for (final TopicPartition partition : nonCommitted) {\n            log.debug(\"No committed offset for partition {}, therefore no timestamp can be found for this partition\", partition);\n        }\n    }\n\n    @Override\n    public Map<TopicPartition, Long> purgeableOffsets() {\n        final Map<TopicPartition, Long> purgeableConsumedOffsets = new HashMap<>();\n        for (final Map.Entry<TopicPartition, Long> entry : consumedOffsets.entrySet()) {\n            final TopicPartition tp = entry.getKey();\n            if (topology.isRepartitionTopic(tp.topic())) {\n                purgeableConsumedOffsets.put(tp, entry.getValue() + 1);\n            }\n        }\n\n        return purgeableConsumedOffsets;\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    private void initializeTopology() {\n        // initialize the task by initializing all its processor nodes in the topology\n        log.trace(\"Initializing processor nodes of the topology\");\n        for (final ProcessorNode<?, ?, ?, ?> node : topology.processors()) {\n            processorContext.setCurrentNode(node);\n            try {\n                init(node);\n            } finally {\n                processorContext.setCurrentNode(null);\n            }\n        }\n    }\n\n    private void init(ProcessorNode<?, ?, ?, ?> node) {\n        node.init(processorContext);\n    }\n\n    /**\n     * Adds records to queues. If a record has an invalid (i.e., negative) timestamp, the record is skipped\n     * and not added to the queue for processing\n     *\n     * @param partition the partition\n     * @param records   the records\n     */\n    @Override\n    public void addRecords(final TopicPartition partition, final Iterable<ConsumerRecord<byte[], byte[]>> records) {\n        final int newQueueSize = partitionGroup.addRawRecords(partition, records);\n\n        if (log.isTraceEnabled()) {\n            log.trace(\"Added records into the buffered queue of partition {}, new queue size is {}\", partition, newQueueSize);\n        }\n\n        // if after adding these records, its partition queue's buffered size has been\n        // increased beyond the threshold, we can then pause the consumption for this partition\n        if (newQueueSize > maxBufferedSize) {\n            mainConsumer.pause(singleton(partition));\n        }\n    }\n\n    /**\n     * Schedules a punctuation for the processor\n     *\n     * @param interval the interval in milliseconds\n     * @param type     the punctuation type\n     * @throws IllegalStateException if the current node is not null\n     */\n    public Cancellable schedule(final long interval, final PunctuationType type, final Punctuator punctuator) {\n        switch (type) {\n            case STREAM_TIME:\n                // align punctuation to 0L, punctuate as soon as we have data\n                return schedule(0L, interval, type, punctuator);\n            case WALL_CLOCK_TIME:\n                // align punctuation to now, punctuate after interval has elapsed\n                return schedule(time.milliseconds() + interval, interval, type, punctuator);\n            default:\n                throw new IllegalArgumentException(\"Unrecognized PunctuationType: \" + type);\n        }\n    }\n\n    /**\n     * Schedules a punctuation for the processor\n     *\n     * @param startTime time of the first punctuation\n     * @param interval  the interval in milliseconds\n     * @param type      the punctuation type\n     * @throws IllegalStateException if the current node is not null\n     */\n    private Cancellable schedule(final long startTime, final long interval, final PunctuationType type, final Punctuator punctuator) {\n        if (processorContext.currentNode() == null) {\n            throw new IllegalStateException(String.format(\"%sCurrent node is null\", logPrefix));\n        }\n\n        final PunctuationSchedule schedule = new PunctuationSchedule(processorContext.currentNode(), startTime, interval, punctuator);\n\n        switch (type) {\n            case STREAM_TIME:\n                // STREAM_TIME punctuation is data driven, will first punctuate as soon as stream-time is known and >= time,\n                // stream-time is known when we have received at least one record from each input topic\n                return streamTimePunctuationQueue.schedule(schedule);\n            case WALL_CLOCK_TIME:\n                // WALL_CLOCK_TIME is driven by the wall clock time, will first punctuate when now >= time\n                return systemTimePunctuationQueue.schedule(schedule);\n            default:\n                throw new IllegalArgumentException(\"Unrecognized PunctuationType: \" + type);\n        }\n    }\n\n    /**\n     * Possibly trigger registered stream-time punctuation functions if\n     * current partition group timestamp has reached the defined stamp\n     * Note, this is only called in the presence of new records\n     *\n     * @throws TaskMigratedException if the task producer got fenced (EOS only)\n     */\n    public boolean maybePunctuateStreamTime() {\n        final long streamTime = partitionGroup.streamTime();\n\n        // if the timestamp is not known yet, meaning there is not enough data accumulated\n        // to reason stream partition time, then skip.\n        if (streamTime == RecordQueue.UNKNOWN) {\n            return false;\n        } else {\n            final boolean punctuated = streamTimePunctuationQueue.maybePunctuate(streamTime, PunctuationType.STREAM_TIME, this);\n\n            if (punctuated) {\n                commitNeeded = true;\n            }\n\n            return punctuated;\n        }\n    }\n\n    public boolean canPunctuateStreamTime() {\n        final long streamTime = partitionGroup.streamTime();\n        return streamTimePunctuationQueue.canPunctuate(streamTime);\n    }\n\n    /**\n     * Possibly trigger registered system-time punctuation functions if\n     * current system timestamp has reached the defined stamp\n     * Note, this is called irrespective of the presence of new records\n     *\n     * @throws TaskMigratedException if the task producer got fenced (EOS only)\n     */\n    public boolean maybePunctuateSystemTime() {\n        final long systemTime = time.milliseconds();\n\n        final boolean punctuated = systemTimePunctuationQueue.maybePunctuate(systemTime, PunctuationType.WALL_CLOCK_TIME, this);\n\n        if (punctuated) {\n            commitNeeded = true;\n        }\n\n        return punctuated;\n    }\n\n    public boolean canPunctuateSystemTime() {\n        final long systemTime = time.milliseconds();\n        return systemTimePunctuationQueue.canPunctuate(systemTime);\n    }\n\n    void maybeRecordE2ELatency(final long recordTimestamp, final long now, final String nodeName) {\n        final Sensor e2eLatencySensor = e2eLatencySensors.get(nodeName);\n        if (e2eLatencySensor == null) {\n            throw new IllegalStateException(\"Requested to record e2e latency but could not find sensor for node \" + nodeName);\n        } else if (e2eLatencySensor.shouldRecord() && e2eLatencySensor.hasMetrics()) {\n            e2eLatencySensor.record(now - recordTimestamp, now);\n        }\n    }\n\n    /**\n     * Request committing the current task's state\n     */\n    void requestCommit() {\n        commitRequested = true;\n    }\n\n    /**\n     * Whether or not a request has been made to commit the current state\n     */\n    @Override\n    public boolean commitRequested() {\n        return commitRequested;\n    }\n\n    @SuppressWarnings(\"rawtypes\")\n    public InternalProcessorContext processorContext() {\n        return processorContext;\n    }\n\n    /**\n     * Produces a string representation containing useful information about a Task.\n     * This is useful in debugging scenarios.\n     *\n     * @return A string representation of the StreamTask instance.\n     */\n    @Override\n    public String toString() {\n        return toString(\"\");\n    }\n\n    /**\n     * Produces a string representation containing useful information about a Task starting with the given indent.\n     * This is useful in debugging scenarios.\n     *\n     * @return A string representation of the Task instance.\n     */\n    public String toString(final String indent) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(indent);\n        sb.append(\"TaskId: \");\n        sb.append(id);\n        sb.append(\"\\n\");\n\n        // print topology\n        if (topology != null) {\n            sb.append(indent).append(topology.toString(indent + \"\\t\"));\n        }\n\n        // print assigned partitions\n        final Set<TopicPartition> partitions = inputPartitions();\n        if (partitions != null && !partitions.isEmpty()) {\n            sb.append(indent).append(\"Partitions [\");\n            for (final TopicPartition topicPartition : partitions) {\n                sb.append(topicPartition).append(\", \");\n            }\n            sb.setLength(sb.length() - 2);\n            sb.append(\"]\\n\");\n        }\n        return sb.toString();\n    }\n\n    @Override\n    public boolean commitNeeded() {\n        // we need to do an extra check if the flag was false, that\n        // if the consumer position has been updated; this is because\n        // there may be non data records such as control markers bypassed\n        if (commitNeeded) {\n            return true;\n        } else {\n            for (final Map.Entry<TopicPartition, Long> entry : consumedOffsets.entrySet()) {\n                final TopicPartition partition = entry.getKey();\n                try {\n                    final long offset = mainConsumer.position(partition);\n\n                    // note the position in consumer is the \"next\" record to fetch,\n                    // so it should be larger than the consumed offset by 1; if it is\n                    // more than 1 it means there are control records, which the consumer skips over silently\n                    if (offset > entry.getValue() + 1) {\n                        commitNeeded = true;\n                        entry.setValue(offset - 1);\n                    }\n                } catch (final TimeoutException swallow) {\n                    log.debug(\n                        String.format(\"Could not get consumer position for partition %s\", partition),\n                        swallow\n                    );\n                } catch (final KafkaException fatal) {\n                    throw new StreamsException(fatal);\n                }\n            }\n\n            return commitNeeded;\n        }\n    }\n\n    @Override\n    public Map<TopicPartition, Long> changelogOffsets() {\n        if (state() == State.RUNNING) {\n            // if we are in running state, just return the latest offset sentinel indicating\n            // we should be at the end of the changelog\n            return changelogPartitions().stream()\n                                        .collect(Collectors.toMap(Function.identity(), tp -> Task.LATEST_OFFSET));\n        } else {\n            return Collections.unmodifiableMap(stateMgr.changelogOffsets());\n        }\n    }\n\n    @Override\n    public Map<TopicPartition, Long> committedOffsets() {\n        return Collections.unmodifiableMap(committedOffsets);\n    }\n\n    @Override\n    public Map<TopicPartition, Long> highWaterMark() {\n        return Collections.unmodifiableMap(highWatermark);\n    }\n\n    private void transitToSuspend() {\n        log.info(\"Suspended from {}\", state());\n        transitionTo(State.SUSPENDED);\n        timeCurrentIdlingStarted = Optional.of(System.currentTimeMillis());\n    }\n\n    @Override\n    public Optional<Long> timeCurrentIdlingStarted() {\n        return timeCurrentIdlingStarted;\n    }\n\n    public void updateCommittedOffsets(final TopicPartition topicPartition, final Long offset) {\n        committedOffsets.put(topicPartition, offset);\n    }\n\n    public void updateEndOffsets(final TopicPartition topicPartition, final Long offset) {\n        highWatermark.put(topicPartition, offset);\n    }\n\n    public boolean hasRecordsQueued() {\n        return numBuffered() > 0;\n    }\n\n    RecordCollector recordCollector() {\n        return recordCollector;\n    }\n\n    // below are visible for testing only\n    int numBuffered() {\n        return partitionGroup.numBuffered();\n    }\n\n    long streamTime() {\n        return partitionGroup.streamTime();\n    }\n\n    private class RecordQueueCreator {\n        private final LogContext logContext;\n        private final TimestampExtractor defaultTimestampExtractor;\n        private final DeserializationExceptionHandler defaultDeserializationExceptionHandler;\n\n        private RecordQueueCreator(final LogContext logContext,\n                                   final TimestampExtractor defaultTimestampExtractor,\n                                   final DeserializationExceptionHandler defaultDeserializationExceptionHandler) {\n            this.logContext = logContext;\n            this.defaultTimestampExtractor = defaultTimestampExtractor;\n            this.defaultDeserializationExceptionHandler = defaultDeserializationExceptionHandler;\n        }\n\n        public RecordQueue createQueue(final TopicPartition partition) {\n            final SourceNode<?, ?> source = topology.source(partition.topic());\n            if (source == null) {\n                throw new TopologyException(\n                        \"Topic \" + partition.topic() + \" is unknown to the topology. \" +\n                                \"This may happen if different KafkaStreams instances of the same application execute different Topologies. \" +\n                                \"Note that Topologies are only identical if all operators are added in the same order.\"\n                );\n            }\n\n            final TimestampExtractor sourceTimestampExtractor = source.getTimestampExtractor();\n            final TimestampExtractor timestampExtractor = sourceTimestampExtractor != null ? sourceTimestampExtractor : defaultTimestampExtractor;\n            return new RecordQueue(\n                    partition,\n                    source,\n                    timestampExtractor,\n                    defaultDeserializationExceptionHandler,\n                    processorContext,\n                    logContext\n            );\n        }\n    }\n}",
                "methodCount": 68
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 14,
                "candidates": [
                    {
                        "lineStart": 663,
                        "lineEnd": 719,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method close to class InternalProcessorContext",
                        "description": "Move method close to org.apache.kafka.streams.processor.internals.InternalProcessorContext\nRationale: The method `close(boolean clean)` is responsible for handling state transitions, closing state managers, record collectors, and ensuring that the task transitions to a `CLOSED` state properly. This functionality is closely related to the internal state management and lifecycle operations of tasks, which align with the responsibilities of the `InternalProcessorContext` class. The `InternalProcessorContext` already deals with task types, transitions, and state-related operations, making it the most appropriate place for this method.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 646,
                        "lineEnd": 654,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method validateClean to class InternalProcessorContext",
                        "description": "Move method validateClean to org.apache.kafka.streams.processor.internals.InternalProcessorContext\nRationale: The `validateClean` method checks if there is pending uncommitted data before trying to close a task cleanly, which relates closely to the task management and state handling functionalities. `InternalProcessorContext` deals with tasks, their states, record contexts, and other task-related operations, making it an appropriate place for the `validateClean` method. Additionally, the context handles operations like registering caches and marking contexts, which aligns with the responsibilities of performing validation before closing tasks.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 656,
                        "lineEnd": 661,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method removeAllSensors to class InternalProcessorContext",
                        "description": "Move method removeAllSensors to org.apache.kafka.streams.processor.internals.InternalProcessorContext\nRationale: The method 'removeAllSensors' involves operations closely related to metrics handling, particularly 'streamsMetrics' which seems related to a processor context. The InternalProcessorContext class is responsible for providing an internal context during processing, maintaining the state and metrics for the processor node, and thus is the most appropriate class to house this method.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 824,
                        "lineEnd": 849,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method doProcess to class RecordQueueCreator",
                        "description": "Move method doProcess to org.apache.kafka.streams.processor.internals.StreamTask.RecordQueueCreator\nRationale: The method doProcess() is involved in processing records within a particular node of the topology. RecordQueueCreator already handles source nodes, queue creation, and probably has access to processor context required for processing. Moving doProcess() to RecordQueueCreator aligns with its responsibility of managing and processing records in relation to the topology, leveraging its existing access to relevant components like processorContext and node-related functionality.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 915,
                        "lineEnd": 922,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method updateProcessorContext to class RecordInfo",
                        "description": "Move method updateProcessorContext to org.apache.kafka.streams.processor.internals.AbstractPartitionGroup.RecordInfo\nRationale: The method updateProcessorContext() interacts with different components of the processor like ProcessorNode and ProcessorRecordContext. Since the RecordInfo class has a close relationship with ProcessorNode through its methods, it is suitable for placing the updateProcessorContext() method here. This enhances cohesion because RecordInfo now encapsulates the details about updating processor context based on its content, maintaining a single point of interaction for these processor elements.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1284,
                        "lineEnd": 1288,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method transitToSuspend to class InternalProcessorContext",
                        "description": "Move method transitToSuspend to org.apache.kafka.streams.processor.internals.InternalProcessorContext\nRationale: The `transitToSuspend` method contains operations that are closely related to state transitions and logging, which are key responsibilities of the `InternalProcessorContext` class. This method involves managing the state (`State.SUSPENDED`), tracking idling times, and logging state transitions. These activities align with the existing methods in `InternalProcessorContext` such as `setSystemTimeMs`, `transitionToActive`, and `transitionToStandby`, which deal with task states and context transitions. It will fit logically within `InternalProcessorContext`, centralizing state transition management within the same class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1295,
                        "lineEnd": 1297,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method updateCommittedOffsets to class InternalProcessorContext",
                        "description": "Move method updateCommittedOffsets to org.apache.kafka.streams.processor.internals.InternalProcessorContext\nRationale: The method updateCommittedOffsets() involves handling of committed offsets which is closely related to the context of processing records, state handling and task transitions managed by InternalProcessorContext. The method would fit well in this class as the context needs to keep track of offsets while processing and transitioning tasks. This ensures the offsets are consistent within the processor context, reducing the risk of inconsistencies and improving encapsulation.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1299,
                        "lineEnd": 1301,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method updateEndOffsets to class InternalProcessorContext",
                        "description": "Move method updateEndOffsets to org.apache.kafka.streams.processor.internals.InternalProcessorContext\nRationale: The `updateEndOffsets` method is dealing with `TopicPartition` and offsets, which are critical concepts within the context of processing records in Kafka Streams. The `InternalProcessorContext` interface already handles various context-related operations such as setting system time, managing the current record, and transitioning between different states of tasks. By moving `updateEndOffsets` to `InternalProcessorContext`, the method will have better contextual coherence and easier access to related functionalities like `logChange` and `processorMetadataForKey`.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 546,
                        "lineEnd": 551,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method clearCommitStatuses to class InternalProcessorContext",
                        "description": "Move method clearCommitStatuses to org.apache.kafka.streams.processor.internals.InternalProcessorContext\nRationale: The method clearCommitStatuses() is closely interacting with the processorContext object, specifically setting commit states and using the ProcessorMetadata. Since InternalProcessorContext manages context for the processor, including metadata and commit statuses, it would logically centralize the management of this state-related functionality within this class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 411,
                        "lineEnd": 414,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method flush to class InternalProcessorContext",
                        "description": "Move method flush to org.apache.kafka.streams.processor.internals.InternalProcessorContext\nRationale: The `flush` method interacts with both a `stateMgr` for cache flushing and a `recordCollector` for record flushing. `InternalProcessorContext` is a contextual interface that deals with managing state and processors, including methods for cache management and transitioning tasks that involve the `recordCollector`. This makes it a suitable location for the `flush` method, as it consolidates related responsibilities where state and processing contexts are managed.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1311,
                        "lineEnd": 1314,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method numBuffered to class AbstractPartitionGroup",
                        "description": "Move method numBuffered to org.apache.kafka.streams.processor.internals.AbstractPartitionGroup\nRationale: The method numBuffered() logically belongs to AbstractPartitionGroup, which seems to be managing partitions and buffering records. AbstractPartitionGroup already contains a declaration for numBuffered(), suggesting it is responsible for this functionality. Therefore, moving the method there will maintain cohesion and encapsulation within the class responsible for such partition-related operations.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1303,
                        "lineEnd": 1305,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method hasRecordsQueued to class PunctuationQueue",
                        "description": "Move method hasRecordsQueued to org.apache.kafka.streams.processor.internals.PunctuationQueue\nRationale: The method `hasRecordsQueued()` appears to be checking whether there are any records currently queued, which aligns with the responsibilities of the `PunctuationQueue` class that manages a queue of `PunctuationSchedule` objects. The `PunctuationQueue` class already contains similar methods (e.g., `canPunctuate`) that check the state of the queue. Thus, moving `hasRecordsQueued()` to `PunctuationQueue` would centralize the queue state management in the appropriate class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1316,
                        "lineEnd": 1318,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method streamTime to class AbstractPartitionGroup",
                        "description": "Move method streamTime to org.apache.kafka.streams.processor.internals.AbstractPartitionGroup\nRationale: The method `streamTime()` directly calls `partitionGroup.streamTime()`, which suggests that it is inherently tied to the partition group functionality. This class already contains an abstract method with the same name and purpose, indicating that the `streamTime()` method is a core part of `AbstractPartitionGroup`. Keeping this method within `AbstractPartitionGroup` aligns with the Single Responsibility Principle, as the partition-related logic should reside within the class that handles partitions.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1029,
                        "lineEnd": 1031,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method init to class RecordInfo",
                        "description": "Move method init to org.apache.kafka.streams.processor.internals.AbstractPartitionGroup.RecordInfo\nRationale: The init method is initializing a ProcessorNode which is closely related to the internals of the RecordInfo class, as it directly refers to the ProcessorNode via its node() method. The association between RecordInfo and ProcessorNode makes it a more appropriate location for the init method. On the other hand, PunctuationQueue primarily focuses on scheduling and handling punctuations and does not have a direct relationship with ProcessorNode initialization.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "createPartitionQueues",
                            "method_signature": "private createPartitionQueues()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addPartitionsForOffsetReset",
                            "method_signature": "public addPartitionsForOffsetReset(final Set<TopicPartition> partitionsForOffsetReset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "closeTopology",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private closeTopology()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "flush",
                            "method_signature": "public flush()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "findOffset",
                            "method_signature": "private findOffset(final TopicPartition partition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "committableOffsetsAndMetadata",
                            "method_signature": "private committableOffsetsAndMetadata()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "clearCommitStatuses",
                            "method_signature": "private clearCommitStatuses()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "extractPartitionTimes",
                            "method_signature": "private extractPartitionTimes()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validateClean",
                            "method_signature": "private validateClean()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "removeAllSensors",
                            "method_signature": "private removeAllSensors()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "private close(final boolean clean)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isProcessable",
                            "method_signature": "public isProcessable(final long wallClockTime)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "process",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    public process(final long wallClockTime)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "doProcess",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private doProcess(final long wallClockTime)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getStacktraceString",
                            "method_signature": "private getStacktraceString(final RuntimeException e)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateProcessorContext",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private updateProcessorContext(final ProcessorNode<?, ?, ?, ?> currNode,\n                                        final long wallClockTime,\n                                        final ProcessorRecordContext recordContext)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "checkpointableOffsets",
                            "method_signature": "private checkpointableOffsets()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resetOffsetsIfNeededAndInitializeMetadata",
                            "method_signature": "private resetOffsetsIfNeededAndInitializeMetadata(final java.util.function.Consumer<Set<TopicPartition>> offsetResetter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "initializeTaskTimeAndProcessorMetadata",
                            "method_signature": "private initializeTaskTimeAndProcessorMetadata(final Map<TopicPartition, OffsetAndMetadata> offsetsAndMetadata)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "initializeTopology",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private initializeTopology()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "init",
                            "method_signature": "private init(ProcessorNode<?, ?, ?, ?> node)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "schedule",
                            "method_signature": "public schedule(final long interval, final PunctuationType type, final Punctuator punctuator)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "schedule",
                            "method_signature": "private schedule(final long startTime, final long interval, final PunctuationType type, final Punctuator punctuator)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybePunctuateStreamTime",
                            "method_signature": "public maybePunctuateStreamTime()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "canPunctuateStreamTime",
                            "method_signature": "public canPunctuateStreamTime()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybePunctuateSystemTime",
                            "method_signature": "public maybePunctuateSystemTime()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "canPunctuateSystemTime",
                            "method_signature": "public canPunctuateSystemTime()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeRecordE2ELatency",
                            "method_signature": " maybeRecordE2ELatency(final long recordTimestamp, final long now, final String nodeName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "toString",
                            "method_signature": "public toString(final String indent)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitToSuspend",
                            "method_signature": "private transitToSuspend()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateCommittedOffsets",
                            "method_signature": "public updateCommittedOffsets(final TopicPartition topicPartition, final Long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateEndOffsets",
                            "method_signature": "public updateEndOffsets(final TopicPartition topicPartition, final Long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasRecordsQueued",
                            "method_signature": "public hasRecordsQueued()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "numBuffered",
                            "method_signature": " numBuffered()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "streamTime",
                            "method_signature": " streamTime()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createQueue",
                            "method_signature": "public createQueue(final TopicPartition partition)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "init",
                            "method_signature": "private init(ProcessorNode<?, ?, ?, ?> node)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addPartitionsForOffsetReset",
                            "method_signature": "public addPartitionsForOffsetReset(final Set<TopicPartition> partitionsForOffsetReset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "numBuffered",
                            "method_signature": " numBuffered()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateProcessorContext",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private updateProcessorContext(final ProcessorNode<?, ?, ?, ?> currNode,\n                                        final long wallClockTime,\n                                        final ProcessorRecordContext recordContext)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "clearCommitStatuses",
                            "method_signature": "private clearCommitStatuses()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasRecordsQueued",
                            "method_signature": "public hasRecordsQueued()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitToSuspend",
                            "method_signature": "private transitToSuspend()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "doProcess",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private doProcess(final long wallClockTime)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "flush",
                            "method_signature": "public flush()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateCommittedOffsets",
                            "method_signature": "public updateCommittedOffsets(final TopicPartition topicPartition, final Long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateEndOffsets",
                            "method_signature": "public updateEndOffsets(final TopicPartition topicPartition, final Long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "streamTime",
                            "method_signature": " streamTime()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validateClean",
                            "method_signature": "private validateClean()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "removeAllSensors",
                            "method_signature": "private removeAllSensors()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "private close(final boolean clean)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "init",
                            "method_signature": "private init(ProcessorNode<?, ?, ?, ?> node)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addPartitionsForOffsetReset",
                            "method_signature": "public addPartitionsForOffsetReset(final Set<TopicPartition> partitionsForOffsetReset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "numBuffered",
                            "method_signature": " numBuffered()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateProcessorContext",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private updateProcessorContext(final ProcessorNode<?, ?, ?, ?> currNode,\n                                        final long wallClockTime,\n                                        final ProcessorRecordContext recordContext)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "clearCommitStatuses",
                            "method_signature": "private clearCommitStatuses()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasRecordsQueued",
                            "method_signature": "public hasRecordsQueued()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitToSuspend",
                            "method_signature": "private transitToSuspend()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "doProcess",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private doProcess(final long wallClockTime)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "flush",
                            "method_signature": "public flush()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateCommittedOffsets",
                            "method_signature": "public updateCommittedOffsets(final TopicPartition topicPartition, final Long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateEndOffsets",
                            "method_signature": "public updateEndOffsets(final TopicPartition topicPartition, final Long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "streamTime",
                            "method_signature": " streamTime()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validateClean",
                            "method_signature": "private validateClean()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "removeAllSensors",
                            "method_signature": "private removeAllSensors()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "private close(final boolean clean)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private init(ProcessorNode<?, ?, ?, ?> node)": {
                    "first": {
                        "method_name": "init",
                        "method_signature": "private init(ProcessorNode<?, ?, ?, ?> node)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.32680825911547245
                },
                "public addPartitionsForOffsetReset(final Set<TopicPartition> partitionsForOffsetReset)": {
                    "first": {
                        "method_name": "addPartitionsForOffsetReset",
                        "method_signature": "public addPartitionsForOffsetReset(final Set<TopicPartition> partitionsForOffsetReset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.35366489308737675
                },
                " numBuffered()": {
                    "first": {
                        "method_name": "numBuffered",
                        "method_signature": " numBuffered()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.35820780113989903
                },
                "@SuppressWarnings(\"unchecked\")\n    private updateProcessorContext(final ProcessorNode<?, ?, ?, ?> currNode,\n                                        final long wallClockTime,\n                                        final ProcessorRecordContext recordContext)": {
                    "first": {
                        "method_name": "updateProcessorContext",
                        "method_signature": "@SuppressWarnings(\"unchecked\")\n    private updateProcessorContext(final ProcessorNode<?, ?, ?, ?> currNode,\n                                        final long wallClockTime,\n                                        final ProcessorRecordContext recordContext)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3599262226857492
                },
                "private clearCommitStatuses()": {
                    "first": {
                        "method_name": "clearCommitStatuses",
                        "method_signature": "private clearCommitStatuses()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3753355984949126
                },
                "public hasRecordsQueued()": {
                    "first": {
                        "method_name": "hasRecordsQueued",
                        "method_signature": "public hasRecordsQueued()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3765450000347422
                },
                "private transitToSuspend()": {
                    "first": {
                        "method_name": "transitToSuspend",
                        "method_signature": "private transitToSuspend()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3881606550139836
                },
                "@SuppressWarnings(\"unchecked\")\n    private doProcess(final long wallClockTime)": {
                    "first": {
                        "method_name": "doProcess",
                        "method_signature": "@SuppressWarnings(\"unchecked\")\n    private doProcess(final long wallClockTime)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.390441997247461
                },
                "public flush()": {
                    "first": {
                        "method_name": "flush",
                        "method_signature": "public flush()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3945313949144016
                },
                "public updateCommittedOffsets(final TopicPartition topicPartition, final Long offset)": {
                    "first": {
                        "method_name": "updateCommittedOffsets",
                        "method_signature": "public updateCommittedOffsets(final TopicPartition topicPartition, final Long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.41180573635088075
                },
                "public updateEndOffsets(final TopicPartition topicPartition, final Long offset)": {
                    "first": {
                        "method_name": "updateEndOffsets",
                        "method_signature": "public updateEndOffsets(final TopicPartition topicPartition, final Long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.41180573635088075
                },
                " streamTime()": {
                    "first": {
                        "method_name": "streamTime",
                        "method_signature": " streamTime()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.42414029064239894
                },
                "private validateClean()": {
                    "first": {
                        "method_name": "validateClean",
                        "method_signature": "private validateClean()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.43644862257662487
                },
                "private removeAllSensors()": {
                    "first": {
                        "method_name": "removeAllSensors",
                        "method_signature": "private removeAllSensors()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.46568926590869963
                },
                "private close(final boolean clean)": {
                    "first": {
                        "method_name": "close",
                        "method_signature": "private close(final boolean clean)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4733578766618938
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "private close(final boolean clean)",
                    "private validateClean()",
                    "private removeAllSensors()",
                    "@SuppressWarnings(\"unchecked\")\n    private doProcess(final long wallClockTime)",
                    "@SuppressWarnings(\"unchecked\")\n    private updateProcessorContext(final ProcessorNode<?, ?, ?, ?> currNode,\n                                        final long wallClockTime,\n                                        final ProcessorRecordContext recordContext)",
                    "private transitToSuspend()",
                    "public addPartitionsForOffsetReset(final Set<TopicPartition> partitionsForOffsetReset)",
                    "public updateCommittedOffsets(final TopicPartition topicPartition, final Long offset)",
                    "public updateEndOffsets(final TopicPartition topicPartition, final Long offset)",
                    "private clearCommitStatuses()",
                    "public flush()",
                    " numBuffered()",
                    "public hasRecordsQueued()",
                    " streamTime()",
                    "private init(ProcessorNode<?, ?, ?, ?> node)"
                ],
                "llm_response_time": 5785
            },
            "targetClassMap": {
                "close": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.2529517469265689
                        },
                        {
                            "class_name": "RecordCollector",
                            "similarity_score": 0.17234063730967686
                        },
                        {
                            "class_name": "RecordQueueCreator",
                            "similarity_score": 0.2891997518273646
                        },
                        {
                            "class_name": "InternalProcessorContext",
                            "similarity_score": 0.21701472579331305
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "InternalProcessorContext",
                        "RecordQueueCreator",
                        "Time"
                    ],
                    "llm_response_time": 3141,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                },
                "validateClean": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.26952538290590256
                        },
                        {
                            "class_name": "RecordCollector",
                            "similarity_score": 0.17880663468123595
                        },
                        {
                            "class_name": "RecordQueueCreator",
                            "similarity_score": 0.21656490257412284
                        },
                        {
                            "class_name": "InternalProcessorContext",
                            "similarity_score": 0.24814833826932564
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "InternalProcessorContext",
                        "RecordQueueCreator",
                        "Time"
                    ],
                    "llm_response_time": 4280,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "removeAllSensors": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.15477830766614897
                        },
                        {
                            "class_name": "RecordCollector",
                            "similarity_score": 0.12321832407941992
                        },
                        {
                            "class_name": "RecordQueueCreator",
                            "similarity_score": 0.25035587206627025
                        },
                        {
                            "class_name": "InternalProcessorContext",
                            "similarity_score": 0.15138452909856615
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "InternalProcessorContext",
                        "RecordQueueCreator",
                        "Time"
                    ],
                    "llm_response_time": 2631,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "doProcess": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.23168925818718217
                        },
                        {
                            "class_name": "RecordCollector",
                            "similarity_score": 0.2822594825602105
                        },
                        {
                            "class_name": "RecordQueueCreator",
                            "similarity_score": 0.34843417083628786
                        },
                        {
                            "class_name": "InternalProcessorContext",
                            "similarity_score": 0.21630866322508296
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "RecordQueueCreator",
                        "RecordCollector",
                        "Time"
                    ],
                    "llm_response_time": 3806,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "updateProcessorContext": {
                    "target_classes": [
                        {
                            "class_name": "ProcessorRecordContext",
                            "similarity_score": 0.3416943532896973
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.0952643233621425
                        },
                        {
                            "class_name": "AbstractPartitionGroup",
                            "similarity_score": 0.2042752923427804
                        },
                        {
                            "class_name": "RecordCollector",
                            "similarity_score": 0.3145935030558458
                        },
                        {
                            "class_name": "RecordInfo",
                            "similarity_score": 0.3572172541558802
                        },
                        {
                            "class_name": "PunctuationQueue",
                            "similarity_score": 0.2952049618466985
                        },
                        {
                            "class_name": "PunctuationQueue",
                            "similarity_score": 0.2952049618466985
                        },
                        {
                            "class_name": "RecordQueueCreator",
                            "similarity_score": 0.3268602252303068
                        },
                        {
                            "class_name": "InternalProcessorContext",
                            "similarity_score": 0.24329155846903872
                        },
                        {
                            "class_name": "StampedRecord",
                            "similarity_score": 0.25349836065200815
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "RecordInfo",
                        "ProcessorRecordContext",
                        "RecordQueueCreator"
                    ],
                    "llm_response_time": 4574,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                },
                "transitToSuspend": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.11985067279586492
                        },
                        {
                            "class_name": "RecordCollector",
                            "similarity_score": 0.10686209958036957
                        },
                        {
                            "class_name": "RecordQueueCreator",
                            "similarity_score": 0.28127320806674083
                        },
                        {
                            "class_name": "InternalProcessorContext",
                            "similarity_score": 0.12660056161540376
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "InternalProcessorContext",
                        "Time",
                        "RecordQueueCreator"
                    ],
                    "llm_response_time": 4478,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "addPartitionsForOffsetReset": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 4488,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "updateCommittedOffsets": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.13971387952048386
                        },
                        {
                            "class_name": "RecordCollector",
                            "similarity_score": 0.29660159910402595
                        },
                        {
                            "class_name": "InternalProcessorContext",
                            "similarity_score": 0.19765506347107742
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "InternalProcessorContext",
                        "RecordCollector",
                        "Time"
                    ],
                    "llm_response_time": 4040,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "updateEndOffsets": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.13971387952048386
                        },
                        {
                            "class_name": "RecordCollector",
                            "similarity_score": 0.29660159910402595
                        },
                        {
                            "class_name": "InternalProcessorContext",
                            "similarity_score": 0.19765506347107742
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "InternalProcessorContext",
                        "RecordCollector",
                        "Time"
                    ],
                    "llm_response_time": 3958,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "clearCommitStatuses": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.11642823293373654
                        },
                        {
                            "class_name": "RecordCollector",
                            "similarity_score": 0.06179199981333873
                        },
                        {
                            "class_name": "RecordQueueCreator",
                            "similarity_score": 0.3184390015367056
                        },
                        {
                            "class_name": "InternalProcessorContext",
                            "similarity_score": 0.1024878106887068
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "InternalProcessorContext",
                        "RecordQueueCreator",
                        "Time"
                    ],
                    "llm_response_time": 3179,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                },
                "flush": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.13719630611540523
                        },
                        {
                            "class_name": "RecordCollector",
                            "similarity_score": 0.13349278634402317
                        },
                        {
                            "class_name": "InternalProcessorContext",
                            "similarity_score": 0.1533577633604255
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "InternalProcessorContext",
                        "RecordCollector",
                        "Time"
                    ],
                    "llm_response_time": 3203,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                },
                "numBuffered": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.1390267804432033
                        },
                        {
                            "class_name": "AbstractPartitionGroup",
                            "similarity_score": 0.224179415327122
                        },
                        {
                            "class_name": "RecordCollector",
                            "similarity_score": 0.05343104979018479
                        },
                        {
                            "class_name": "InternalProcessorContext",
                            "similarity_score": 0.06330028080770188
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "AbstractPartitionGroup",
                        "InternalProcessorContext",
                        "Time"
                    ],
                    "llm_response_time": 4057,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "hasRecordsQueued": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.13251917549040004
                        },
                        {
                            "class_name": "RecordCollector",
                            "similarity_score": 0.05351344158899463
                        },
                        {
                            "class_name": "PunctuationQueue",
                            "similarity_score": 0.4579642998393853
                        },
                        {
                            "class_name": "PunctuationQueue",
                            "similarity_score": 0.4579642998393853
                        },
                        {
                            "class_name": "InternalProcessorContext",
                            "similarity_score": 0.05071831293409737
                        },
                        {
                            "class_name": "StampedRecord",
                            "similarity_score": 0.6119424801101773
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "PunctuationQueue",
                        "PunctuationQueue",
                        "StampedRecord"
                    ],
                    "llm_response_time": 3026,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "streamTime": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.20464187981497275
                        },
                        {
                            "class_name": "AbstractPartitionGroup",
                            "similarity_score": 0.29855619650098675
                        },
                        {
                            "class_name": "RecordCollector",
                            "similarity_score": 0.07864837576396146
                        },
                        {
                            "class_name": "InternalProcessorContext",
                            "similarity_score": 0.08282265820222595
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "AbstractPartitionGroup",
                        "InternalProcessorContext",
                        "Time"
                    ],
                    "llm_response_time": 3720,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "init": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.09979562822891704
                        },
                        {
                            "class_name": "AbstractPartitionGroup",
                            "similarity_score": 0.2222222222222222
                        },
                        {
                            "class_name": "RecordCollector",
                            "similarity_score": 0.0926879997200081
                        },
                        {
                            "class_name": "RecordInfo",
                            "similarity_score": 0.46909709371657093
                        },
                        {
                            "class_name": "PunctuationQueue",
                            "similarity_score": 0.31311214554257477
                        },
                        {
                            "class_name": "PunctuationQueue",
                            "similarity_score": 0.31311214554257477
                        },
                        {
                            "class_name": "RecordQueueCreator",
                            "similarity_score": 0.21571674297647803
                        },
                        {
                            "class_name": "InternalProcessorContext",
                            "similarity_score": 0.183013947658405
                        },
                        {
                            "class_name": "StampedRecord",
                            "similarity_score": 0.29615285103456107
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "RecordInfo",
                        "PunctuationQueue",
                        "PunctuationQueue"
                    ],
                    "llm_response_time": 2155,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from package SnapshottableCoordinator(logContext LogContext, snapshotRegistry SnapshotRegistry, coordinator S, tp TopicPartition) in class org.apache.kafka.coordinator.group.runtime.SnapshottableCoordinator & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/SnapshottableCoordinator.java",
                    "startLine": 65,
                    "endLine": 78,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package SnapshottableCoordinator(logContext LogContext, snapshotRegistry SnapshotRegistry, coordinator S, tp TopicPartition)"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/SnapshottableCoordinator.java",
                    "startLine": 77,
                    "endLine": 77,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/SnapshottableCoordinator.java",
                    "startLine": 65,
                    "endLine": 78,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package SnapshottableCoordinator(logContext LogContext, snapshotRegistry SnapshotRegistry, coordinator S, tp TopicPartition)"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/SnapshottableCoordinator.java",
                    "startLine": 77,
                    "endLine": 77,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(0)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 603,
        "extraction_results": {
            "success": false
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public updateLastWrittenOffset(offset Long) : void in class org.apache.kafka.coordinator.group.runtime.SnapshottableCoordinator & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/SnapshottableCoordinator.java",
                    "startLine": 134,
                    "endLine": 150,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public updateLastWrittenOffset(offset Long) : void"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/SnapshottableCoordinator.java",
                    "startLine": 148,
                    "endLine": 148,
                    "startColumn": 9,
                    "endColumn": 54,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/SnapshottableCoordinator.java",
                    "startLine": 134,
                    "endLine": 150,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public updateLastWrittenOffset(offset Long) : void"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/SnapshottableCoordinator.java",
                    "startLine": 148,
                    "endLine": 148,
                    "startColumn": 9,
                    "endColumn": 58,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(offset)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 604,
        "extraction_results": {
            "success": true,
            "newCommitHash": "c20cab42e1d9996007ecc6fa1defd949297eb8be",
            "newBranchName": "extract-idempotentCreateSnapshot-updateLastWrittenOffset-130af38"
        },
        "telemetry": {
            "id": "9db9b58e-874a-4748-b0b3-d85f8645d1e3",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 213,
                "lineStart": 28,
                "lineEnd": 240,
                "bodyLineStart": 28,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/SnapshottableCoordinator.java",
                "sourceCode": "/**\n * SnapshottableCoordinator is a wrapper on top of the coordinator state machine. This object is not accessed concurrently\n * but multiple threads access it while loading the coordinator partition and therefore requires all methods to be\n * synchronized.\n */\nclass SnapshottableCoordinator<S extends CoordinatorShard<U>, U> implements CoordinatorPlayback<U> {\n    /**\n     * The logger.\n     */\n    private final Logger log;\n\n    /**\n     * The snapshot registry backing the coordinator.\n     */\n    private final SnapshotRegistry snapshotRegistry;\n\n    /**\n     * The actual state machine.\n     */\n    private final S coordinator;\n\n    /**\n     * The topic partition.\n     */\n    private final TopicPartition tp;\n\n    /**\n     * The last offset written to the partition.\n     */\n    private long lastWrittenOffset;\n\n    /**\n     * The last offset committed. This represents the high\n     * watermark of the partition.\n     */\n    private long lastCommittedOffset;\n\n    SnapshottableCoordinator(\n        LogContext logContext,\n        SnapshotRegistry snapshotRegistry,\n        S coordinator,\n        TopicPartition tp\n    ) {\n        this.log = logContext.logger(SnapshottableCoordinator.class);\n        this.coordinator = coordinator;\n        this.snapshotRegistry = snapshotRegistry;\n        this.tp = tp;\n        this.lastWrittenOffset = 0;\n        this.lastCommittedOffset = 0;\n        snapshotRegistry.getOrCreateSnapshot(0);\n    }\n\n    /**\n     * Reverts the last written offset. This also reverts the snapshot\n     * registry to this offset. All the changes applied after the offset\n     * are lost.\n     *\n     * @param offset The offset to revert to.\n     */\n    synchronized void revertLastWrittenOffset(\n        long offset\n    ) {\n        if (offset > lastWrittenOffset) {\n            throw new IllegalStateException(\"New offset \" + offset + \" of \" + tp +\n                \" must be smaller than \" + lastWrittenOffset + \".\");\n        }\n\n        log.debug(\"Revert last written offset of {} to {}.\", tp, offset);\n        lastWrittenOffset = offset;\n        snapshotRegistry.revertToSnapshot(offset);\n    }\n\n    /**\n     * Replays the record onto the state machine.\n     *\n     * @param offset        The offset of the record in the log.\n     * @param producerId    The producer id.\n     * @param producerEpoch The producer epoch.\n     * @param record        A record.\n     */\n    @Override\n    public synchronized void replay(\n        long offset,\n        long producerId,\n        short producerEpoch,\n        U record\n    ) {\n        coordinator.replay(offset, producerId, producerEpoch, record);\n    }\n\n    /**\n     * Applies the end transaction marker.\n     *\n     * @param producerId    The producer id.\n     * @param producerEpoch The producer epoch.\n     * @param result        The result of the transaction.\n     */\n    @Override\n    public synchronized void replayEndTransactionMarker(\n        long producerId,\n        short producerEpoch,\n        TransactionResult result\n    ) {\n        coordinator.replayEndTransactionMarker(producerId, producerEpoch, result);\n    }\n\n    /**\n     * Updates the last written offset. This also create a new snapshot\n     * in the snapshot registry.\n     *\n     * @param offset The new last written offset.\n     */\n    @Override\n    public synchronized void updateLastWrittenOffset(Long offset) {\n        if (offset <= lastWrittenOffset) {\n            throw new IllegalStateException(\"New last written offset \" + offset + \" of \" + tp +\n                \" must be greater than \" + lastWrittenOffset + \".\");\n        }\n\n        lastWrittenOffset = offset;\n        idempotentCreateSnapshot(offset);\n        log.debug(\"Updated last written offset of {} to {}.\", tp, offset);\n    }\n\n    private void idempotentCreateSnapshot(Long offset) {\n        snapshotRegistry.getOrCreateSnapshot(offset);\n    }\n\n    /**\n     * Updates the last committed offset. This completes all the deferred\n     * events waiting on this offset. This also cleanups all the snapshots\n     * prior to this offset.\n     *\n     * @param offset The new last committed offset.\n     */\n    @Override\n    public synchronized void updateLastCommittedOffset(Long offset) {\n        if (offset < lastCommittedOffset) {\n            throw new IllegalStateException(\"New committed offset \" + offset + \" of \" + tp +\n                \" must be greater than or equal to \" + lastCommittedOffset + \".\");\n        }\n\n        if (offset > lastWrittenOffset) {\n            throw new IllegalStateException(\"New committed offset \" + offset + \" of \" + tp +\n                \"must be less than or equal to \" + lastWrittenOffset + \".\");\n        }\n\n        lastCommittedOffset = offset;\n        snapshotRegistry.deleteSnapshotsUpTo(offset);\n        log.debug(\"Updated committed offset of {} to {}.\", tp, offset);\n    }\n\n    /**\n     * The coordinator has been loaded. This is used to apply any\n     * post loading operations.\n     *\n     * @param newImage  The metadata image.\n     */\n    synchronized void onLoaded(MetadataImage newImage) {\n        this.coordinator.onLoaded(newImage);\n    }\n\n    /**\n     * The coordinator has been unloaded. This is used to apply\n     * any post unloading operations.\n     */\n    synchronized void onUnloaded() {\n        if (this.coordinator != null) {\n            this.coordinator.onUnloaded();\n        }\n    }\n\n    /**\n     * @return The last written offset.\n     */\n    synchronized long lastWrittenOffset() {\n        return this.lastWrittenOffset;\n    }\n\n    /**\n     * A new metadata image is available. This is only called after {@link SnapshottableCoordinator#onLoaded(MetadataImage)}\n     * is called to signal that the coordinator has been fully loaded.\n     *\n     * @param newImage  The new metadata image.\n     * @param delta     The delta image.\n     */\n    synchronized void onNewMetadataImage(MetadataImage newImage, MetadataDelta delta) {\n        this.coordinator.onNewMetadataImage(newImage, delta);\n    }\n\n    /**\n     * @return The last committed offset.\n     */\n    synchronized long lastCommittedOffset() {\n        return this.lastCommittedOffset;\n    }\n\n    /**\n     * @return The coordinator.\n     */\n    synchronized S coordinator() {\n        return this.coordinator;\n    }\n\n    /**\n     * @return The snapshot registry.\n     *\n     * Only used for testing.\n     */\n    synchronized SnapshotRegistry snapshotRegistry() {\n        return this.snapshotRegistry;\n    }\n}",
                "methodCount": 14
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 151,
                        "lineEnd": 153,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method `idempotentCreateSnapshot(Long offset)` directly uses the `getOrCreateSnapshot` method from the `SnapshotRegistry` class. This suggests that it is tightly coupled with the responsibilities and operations of the `SnapshotRegistry`. Moving this method to `SnapshotRegistry` would encapsulate all snapshot-related functionalities within the class that is already responsible for managing them, thereby improving coherence and maintainability.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "revertLastWrittenOffset",
                            "method_signature": "synchronized revertLastWrittenOffset(\n        long offset\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(Long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onLoaded",
                            "method_signature": "synchronized onLoaded(MetadataImage newImage)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onUnloaded",
                            "method_signature": "synchronized onUnloaded()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onNewMetadataImage",
                            "method_signature": "synchronized onNewMetadataImage(MetadataImage newImage, MetadataDelta delta)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(Long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onUnloaded",
                            "method_signature": "synchronized onUnloaded()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onNewMetadataImage",
                            "method_signature": "synchronized onNewMetadataImage(MetadataImage newImage, MetadataDelta delta)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onLoaded",
                            "method_signature": "synchronized onLoaded(MetadataImage newImage)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "revertLastWrittenOffset",
                            "method_signature": "synchronized revertLastWrittenOffset(\n        long offset\n    )",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(Long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onUnloaded",
                            "method_signature": "synchronized onUnloaded()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onNewMetadataImage",
                            "method_signature": "synchronized onNewMetadataImage(MetadataImage newImage, MetadataDelta delta)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onLoaded",
                            "method_signature": "synchronized onLoaded(MetadataImage newImage)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "revertLastWrittenOffset",
                            "method_signature": "synchronized revertLastWrittenOffset(\n        long offset\n    )",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(Long offset)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(Long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2044794529772991
                },
                "synchronized onUnloaded()": {
                    "first": {
                        "method_name": "onUnloaded",
                        "method_signature": "synchronized onUnloaded()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5482556581945002
                },
                "synchronized onNewMetadataImage(MetadataImage newImage, MetadataDelta delta)": {
                    "first": {
                        "method_name": "onNewMetadataImage",
                        "method_signature": "synchronized onNewMetadataImage(MetadataImage newImage, MetadataDelta delta)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5921744793018301
                },
                "synchronized onLoaded(MetadataImage newImage)": {
                    "first": {
                        "method_name": "onLoaded",
                        "method_signature": "synchronized onLoaded(MetadataImage newImage)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6700623210842714
                },
                "synchronized revertLastWrittenOffset(\n        long offset\n    )": {
                    "first": {
                        "method_name": "revertLastWrittenOffset",
                        "method_signature": "synchronized revertLastWrittenOffset(\n        long offset\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.7677929142021896
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "synchronized onNewMetadataImage(MetadataImage newImage, MetadataDelta delta)",
                    "synchronized onLoaded(MetadataImage newImage)",
                    "synchronized onUnloaded()",
                    "private idempotentCreateSnapshot(Long offset)"
                ],
                "llm_response_time": 4083
            },
            "targetClassMap": {
                "onNewMetadataImage": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 4005,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "onLoaded": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2587,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "onUnloaded": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3504,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 2081,
                    "similarity_computation_time": 4,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public GroupMetadataManagerTestContext(time MockTime, timer MockCoordinatorTimer<Void,CoordinatorRecord>, snapshotRegistry SnapshotRegistry, metrics GroupCoordinatorMetricsShard, groupMetadataManager GroupMetadataManager, classicGroupInitialRebalanceDelayMs int, classicGroupNewMemberJoinTimeoutMs int) in class org.apache.kafka.coordinator.group.GroupMetadataManagerTestContext & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 519,
                    "endLine": 536,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public GroupMetadataManagerTestContext(time MockTime, timer MockCoordinatorTimer<Void,CoordinatorRecord>, snapshotRegistry SnapshotRegistry, metrics GroupCoordinatorMetricsShard, groupMetadataManager GroupMetadataManager, classicGroupInitialRebalanceDelayMs int, classicGroupNewMemberJoinTimeoutMs int)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 535,
                    "endLine": 535,
                    "startColumn": 9,
                    "endColumn": 65,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 519,
                    "endLine": 536,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public GroupMetadataManagerTestContext(time MockTime, timer MockCoordinatorTimer<Void,CoordinatorRecord>, snapshotRegistry SnapshotRegistry, metrics GroupCoordinatorMetricsShard, groupMetadataManager GroupMetadataManager, classicGroupInitialRebalanceDelayMs int, classicGroupNewMemberJoinTimeoutMs int)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 535,
                    "endLine": 535,
                    "startColumn": 9,
                    "endColumn": 69,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(lastWrittenOffset)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 605,
        "extraction_results": {
            "success": false
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public replay(record CoordinatorRecord, groupType GroupType) : void in class org.apache.kafka.coordinator.group.GroupMetadataManagerTestContext & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 1430,
                    "endLine": 1516,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public replay(record CoordinatorRecord, groupType GroupType) : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 1515,
                    "endLine": 1515,
                    "startColumn": 9,
                    "endColumn": 65,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 1430,
                    "endLine": 1516,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public replay(record CoordinatorRecord, groupType GroupType) : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 1515,
                    "endLine": 1515,
                    "startColumn": 9,
                    "endColumn": 69,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(lastWrittenOffset)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 606,
        "extraction_results": {
            "success": true,
            "newCommitHash": "2fcf6a3cc81b951b12e3a2d1b90d4149720c9bce",
            "newBranchName": "extract-idempotentCreateSnapshot-replay-130af38"
        },
        "telemetry": {
            "id": "fb380c7b-aad7-4e47-8e0c-449932aefe9a",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1406,
                "lineStart": 120,
                "lineEnd": 1525,
                "bodyLineStart": 120,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                "sourceCode": "public class GroupMetadataManagerTestContext {\n\n    public static void assertNoOrEmptyResult(List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts) {\n        assertTrue(timeouts.size() <= 1);\n        timeouts.forEach(timeout -> assertEquals(EMPTY_RESULT, timeout.result));\n    }\n\n    public static JoinGroupRequestData.JoinGroupRequestProtocolCollection toProtocols(String... protocolNames) {\n        JoinGroupRequestData.JoinGroupRequestProtocolCollection protocols = new JoinGroupRequestData.JoinGroupRequestProtocolCollection(0);\n        List<String> topicNames = Arrays.asList(\"foo\", \"bar\", \"baz\");\n        for (int i = 0; i < protocolNames.length; i++) {\n            protocols.add(new JoinGroupRequestData.JoinGroupRequestProtocol()\n                .setName(protocolNames[i])\n                .setMetadata(ConsumerProtocol.serializeSubscription(new ConsumerPartitionAssignor.Subscription(\n                    Collections.singletonList(topicNames.get(i % topicNames.size())))).array())\n            );\n        }\n        return protocols;\n    }\n\n    public static JoinGroupRequestData.JoinGroupRequestProtocolCollection toConsumerProtocol(\n        List<String> topicNames,\n        List<TopicPartition> ownedPartitions\n    ) {\n        return toConsumerProtocol(topicNames, ownedPartitions, ConsumerProtocolSubscription.HIGHEST_SUPPORTED_VERSION);\n    }\n\n    public static JoinGroupRequestData.JoinGroupRequestProtocolCollection toConsumerProtocol(\n        List<String> topicNames,\n        List<TopicPartition> ownedPartitions,\n        short version\n    ) {\n        JoinGroupRequestData.JoinGroupRequestProtocolCollection protocols =\n            new JoinGroupRequestData.JoinGroupRequestProtocolCollection(0);\n        protocols.add(new JoinGroupRequestData.JoinGroupRequestProtocol()\n            .setName(\"range\")\n            .setMetadata(ConsumerProtocol.serializeSubscription(\n                new ConsumerPartitionAssignor.Subscription(\n                    topicNames,\n                    null,\n                    ownedPartitions\n                ),\n                version\n            ).array())\n        );\n        return protocols;\n    }\n\n    public static CoordinatorRecord newGroupMetadataRecord(\n        String groupId,\n        GroupMetadataValue value,\n        MetadataVersion metadataVersion\n    ) {\n        return new CoordinatorRecord(\n            new ApiMessageAndVersion(\n                new GroupMetadataKey()\n                    .setGroup(groupId),\n                (short) 2\n            ),\n            new ApiMessageAndVersion(\n                value,\n                metadataVersion.groupMetadataValueVersion()\n            )\n        );\n    }\n\n    public static class RebalanceResult {\n        int generationId;\n        String leaderId;\n        byte[] leaderAssignment;\n        String followerId;\n        byte[] followerAssignment;\n\n        RebalanceResult(\n            int generationId,\n            String leaderId,\n            byte[] leaderAssignment,\n            String followerId,\n            byte[] followerAssignment\n        ) {\n            this.generationId = generationId;\n            this.leaderId = leaderId;\n            this.leaderAssignment = leaderAssignment;\n            this.followerId = followerId;\n            this.followerAssignment = followerAssignment;\n        }\n    }\n\n    public static class PendingMemberGroupResult {\n        String leaderId;\n        String followerId;\n        JoinGroupResponseData pendingMemberResponse;\n\n        public PendingMemberGroupResult(\n            String leaderId,\n            String followerId,\n            JoinGroupResponseData pendingMemberResponse\n        ) {\n            this.leaderId = leaderId;\n            this.followerId = followerId;\n            this.pendingMemberResponse = pendingMemberResponse;\n        }\n    }\n\n    public static class JoinResult {\n        CompletableFuture<JoinGroupResponseData> joinFuture;\n        List<CoordinatorRecord> records;\n        CompletableFuture<Void> appendFuture;\n\n        public JoinResult(\n            CompletableFuture<JoinGroupResponseData> joinFuture,\n            CoordinatorResult<Void, CoordinatorRecord> coordinatorResult\n        ) {\n            this.joinFuture = joinFuture;\n            this.records = coordinatorResult.records();\n            this.appendFuture = coordinatorResult.appendFuture();\n        }\n    }\n\n    public static class SyncResult {\n        CompletableFuture<SyncGroupResponseData> syncFuture;\n        List<CoordinatorRecord> records;\n        CompletableFuture<Void> appendFuture;\n\n        public SyncResult(\n            CompletableFuture<SyncGroupResponseData> syncFuture,\n            CoordinatorResult<Void, CoordinatorRecord> coordinatorResult\n        ) {\n            this.syncFuture = syncFuture;\n            this.records = coordinatorResult.records();\n            this.appendFuture = coordinatorResult.appendFuture();\n        }\n    }\n\n    public static class JoinGroupRequestBuilder {\n        String groupId = null;\n        String groupInstanceId = null;\n        String memberId = null;\n        String protocolType = \"consumer\";\n        JoinGroupRequestData.JoinGroupRequestProtocolCollection protocols = new JoinGroupRequestData.JoinGroupRequestProtocolCollection(0);\n        int sessionTimeoutMs = 500;\n        int rebalanceTimeoutMs = 500;\n        String reason = null;\n\n        JoinGroupRequestBuilder withGroupId(String groupId) {\n            this.groupId = groupId;\n            return this;\n        }\n\n        JoinGroupRequestBuilder withGroupInstanceId(String groupInstanceId) {\n            this.groupInstanceId = groupInstanceId;\n            return this;\n        }\n\n        JoinGroupRequestBuilder withMemberId(String memberId) {\n            this.memberId = memberId;\n            return this;\n        }\n\n        JoinGroupRequestBuilder withDefaultProtocolTypeAndProtocols() {\n            this.protocols = toProtocols(\"range\");\n            return this;\n        }\n\n        JoinGroupRequestBuilder withProtocolSuperset() {\n            this.protocols = toProtocols(\"range\", \"roundrobin\");\n            return this;\n        }\n\n        JoinGroupRequestBuilder withProtocolType(String protocolType) {\n            this.protocolType = protocolType;\n            return this;\n        }\n\n        JoinGroupRequestBuilder withProtocols(JoinGroupRequestData.JoinGroupRequestProtocolCollection protocols) {\n            this.protocols = protocols;\n            return this;\n        }\n\n        JoinGroupRequestBuilder withRebalanceTimeoutMs(int rebalanceTimeoutMs) {\n            this.rebalanceTimeoutMs = rebalanceTimeoutMs;\n            return this;\n        }\n\n        JoinGroupRequestBuilder withSessionTimeoutMs(int sessionTimeoutMs) {\n            this.sessionTimeoutMs = sessionTimeoutMs;\n            return this;\n        }\n\n        JoinGroupRequestBuilder withReason(String reason) {\n            this.reason = reason;\n            return this;\n        }\n\n        JoinGroupRequestData build() {\n            return new JoinGroupRequestData()\n                .setGroupId(groupId)\n                .setGroupInstanceId(groupInstanceId)\n                .setMemberId(memberId)\n                .setProtocolType(protocolType)\n                .setProtocols(protocols)\n                .setRebalanceTimeoutMs(rebalanceTimeoutMs)\n                .setSessionTimeoutMs(sessionTimeoutMs)\n                .setReason(reason);\n        }\n    }\n\n    public static class SyncGroupRequestBuilder {\n        String groupId = null;\n        String groupInstanceId = null;\n        String memberId = null;\n        String protocolType = \"consumer\";\n        String protocolName = \"range\";\n        int generationId = 0;\n        List<SyncGroupRequestData.SyncGroupRequestAssignment> assignment = Collections.emptyList();\n\n        SyncGroupRequestBuilder withGroupId(String groupId) {\n            this.groupId = groupId;\n            return this;\n        }\n\n        SyncGroupRequestBuilder withGroupInstanceId(String groupInstanceId) {\n            this.groupInstanceId = groupInstanceId;\n            return this;\n        }\n\n        SyncGroupRequestBuilder withMemberId(String memberId) {\n            this.memberId = memberId;\n            return this;\n        }\n\n        SyncGroupRequestBuilder withGenerationId(int generationId) {\n            this.generationId = generationId;\n            return this;\n        }\n\n        SyncGroupRequestBuilder withProtocolType(String protocolType) {\n            this.protocolType = protocolType;\n            return this;\n        }\n\n        SyncGroupRequestBuilder withProtocolName(String protocolName) {\n            this.protocolName = protocolName;\n            return this;\n        }\n\n        SyncGroupRequestBuilder withAssignment(List<SyncGroupRequestData.SyncGroupRequestAssignment> assignment) {\n            this.assignment = assignment;\n            return this;\n        }\n\n\n        SyncGroupRequestData build() {\n            return new SyncGroupRequestData()\n                .setGroupId(groupId)\n                .setGroupInstanceId(groupInstanceId)\n                .setMemberId(memberId)\n                .setGenerationId(generationId)\n                .setProtocolType(protocolType)\n                .setProtocolName(protocolName)\n                .setAssignments(assignment);\n        }\n    }\n\n    public static class Builder {\n        private final MockTime time = new MockTime();\n        private final MockCoordinatorTimer<Void, CoordinatorRecord> timer = new MockCoordinatorTimer<>(time);\n        private final LogContext logContext = new LogContext();\n        private final SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        private MetadataImage metadataImage;\n        private List<ConsumerGroupPartitionAssignor> consumerGroupAssignors = Collections.singletonList(new MockPartitionAssignor(\"range\"));\n        private final List<ConsumerGroupBuilder> consumerGroupBuilders = new ArrayList<>();\n        private int consumerGroupMaxSize = Integer.MAX_VALUE;\n        private int consumerGroupMetadataRefreshIntervalMs = Integer.MAX_VALUE;\n        private int classicGroupMaxSize = Integer.MAX_VALUE;\n        private int classicGroupInitialRebalanceDelayMs = 3000;\n        private final int classicGroupNewMemberJoinTimeoutMs = 5 * 60 * 1000;\n        private int classicGroupMinSessionTimeoutMs = 10;\n        private int classicGroupMaxSessionTimeoutMs = 10 * 60 * 1000;\n        private final GroupCoordinatorMetricsShard metrics = mock(GroupCoordinatorMetricsShard.class);\n        private ConsumerGroupMigrationPolicy consumerGroupMigrationPolicy = ConsumerGroupMigrationPolicy.DISABLED;\n        // Share group configs\n        private ShareGroupPartitionAssignor shareGroupAssignor = new MockPartitionAssignor(\"share\");\n        private int shareGroupMaxSize = Integer.MAX_VALUE;\n\n        public Builder withMetadataImage(MetadataImage metadataImage) {\n            this.metadataImage = metadataImage;\n            return this;\n        }\n\n        public Builder withConsumerGroupAssignors(List<ConsumerGroupPartitionAssignor> assignors) {\n            this.consumerGroupAssignors = assignors;\n            return this;\n        }\n\n        public Builder withConsumerGroup(ConsumerGroupBuilder builder) {\n            this.consumerGroupBuilders.add(builder);\n            return this;\n        }\n\n        public Builder withConsumerGroupMaxSize(int consumerGroupMaxSize) {\n            this.consumerGroupMaxSize = consumerGroupMaxSize;\n            return this;\n        }\n\n        public Builder withConsumerGroupMetadataRefreshIntervalMs(int consumerGroupMetadataRefreshIntervalMs) {\n            this.consumerGroupMetadataRefreshIntervalMs = consumerGroupMetadataRefreshIntervalMs;\n            return this;\n        }\n\n        public Builder withClassicGroupMaxSize(int classicGroupMaxSize) {\n            this.classicGroupMaxSize = classicGroupMaxSize;\n            return this;\n        }\n\n        public Builder withClassicGroupInitialRebalanceDelayMs(int classicGroupInitialRebalanceDelayMs) {\n            this.classicGroupInitialRebalanceDelayMs = classicGroupInitialRebalanceDelayMs;\n            return this;\n        }\n\n        public Builder withClassicGroupMinSessionTimeoutMs(int classicGroupMinSessionTimeoutMs) {\n            this.classicGroupMinSessionTimeoutMs = classicGroupMinSessionTimeoutMs;\n            return this;\n        }\n\n        public Builder withClassicGroupMaxSessionTimeoutMs(int classicGroupMaxSessionTimeoutMs) {\n            this.classicGroupMaxSessionTimeoutMs = classicGroupMaxSessionTimeoutMs;\n            return this;\n        }\n\n        public Builder withConsumerGroupMigrationPolicy(ConsumerGroupMigrationPolicy consumerGroupMigrationPolicy) {\n            this.consumerGroupMigrationPolicy = consumerGroupMigrationPolicy;\n            return this;\n        }\n\n        public Builder withShareGroupAssignor(ShareGroupPartitionAssignor shareGroupAssignor) {\n            this.shareGroupAssignor = shareGroupAssignor;\n            return this;\n        }\n\n        public Builder withShareGroupMaxSize(int shareGroupMaxSize) {\n            this.shareGroupMaxSize = shareGroupMaxSize;\n            return this;\n        }\n\n        public GroupMetadataManagerTestContext build() {\n            if (metadataImage == null) metadataImage = MetadataImage.EMPTY;\n            if (consumerGroupAssignors == null) consumerGroupAssignors = Collections.emptyList();\n\n            GroupMetadataManagerTestContext context = new GroupMetadataManagerTestContext(\n                time,\n                timer,\n                snapshotRegistry,\n                metrics,\n                new GroupMetadataManager.Builder()\n                    .withSnapshotRegistry(snapshotRegistry)\n                    .withLogContext(logContext)\n                    .withTime(time)\n                    .withTimer(timer)\n                    .withMetadataImage(metadataImage)\n                    .withConsumerGroupHeartbeatInterval(5000)\n                    .withConsumerGroupSessionTimeout(45000)\n                    .withConsumerGroupMaxSize(consumerGroupMaxSize)\n                    .withConsumerGroupAssignors(consumerGroupAssignors)\n                    .withConsumerGroupMetadataRefreshIntervalMs(consumerGroupMetadataRefreshIntervalMs)\n                    .withClassicGroupMaxSize(classicGroupMaxSize)\n                    .withClassicGroupMinSessionTimeoutMs(classicGroupMinSessionTimeoutMs)\n                    .withClassicGroupMaxSessionTimeoutMs(classicGroupMaxSessionTimeoutMs)\n                    .withClassicGroupInitialRebalanceDelayMs(classicGroupInitialRebalanceDelayMs)\n                    .withClassicGroupNewMemberJoinTimeoutMs(classicGroupNewMemberJoinTimeoutMs)\n                    .withGroupCoordinatorMetricsShard(metrics)\n                    .withConsumerGroupMigrationPolicy(consumerGroupMigrationPolicy)\n                    .withShareGroupAssignor(shareGroupAssignor)\n                    .withShareGroupMaxSize(shareGroupMaxSize)\n                    .build(),\n                classicGroupInitialRebalanceDelayMs,\n                classicGroupNewMemberJoinTimeoutMs\n            );\n\n            consumerGroupBuilders.forEach(builder -> builder.build(metadataImage.topics()).forEach(context::replay));\n            consumerGroupBuilders.forEach(builder -> builder.build(metadataImage.topics()).forEach(context::replay));\n\n            context.commit();\n\n            return context;\n        }\n    }\n\n    final MockTime time;\n    final MockCoordinatorTimer<Void, CoordinatorRecord> timer;\n    final SnapshotRegistry snapshotRegistry;\n    final GroupCoordinatorMetricsShard metrics;\n    final GroupMetadataManager groupMetadataManager;\n    final int classicGroupInitialRebalanceDelayMs;\n    final int classicGroupNewMemberJoinTimeoutMs;\n\n    long lastCommittedOffset = 0L;\n    long lastWrittenOffset = 0L;\n\n    public GroupMetadataManagerTestContext(\n        MockTime time,\n        MockCoordinatorTimer<Void, CoordinatorRecord> timer,\n        SnapshotRegistry snapshotRegistry,\n        GroupCoordinatorMetricsShard metrics,\n        GroupMetadataManager groupMetadataManager,\n        int classicGroupInitialRebalanceDelayMs,\n        int classicGroupNewMemberJoinTimeoutMs\n    ) {\n        this.time = time;\n        this.timer = timer;\n        this.snapshotRegistry = snapshotRegistry;\n        this.metrics = metrics;\n        this.groupMetadataManager = groupMetadataManager;\n        this.classicGroupInitialRebalanceDelayMs = classicGroupInitialRebalanceDelayMs;\n        this.classicGroupNewMemberJoinTimeoutMs = classicGroupNewMemberJoinTimeoutMs;\n        snapshotRegistry.getOrCreateSnapshot(lastWrittenOffset);\n    }\n\n    public void commit() {\n        long lastCommittedOffset = this.lastCommittedOffset;\n        this.lastCommittedOffset = lastWrittenOffset;\n        snapshotRegistry.deleteSnapshotsUpTo(lastCommittedOffset);\n    }\n\n    public void rollback() {\n        lastWrittenOffset = lastCommittedOffset;\n        snapshotRegistry.revertToSnapshot(lastCommittedOffset);\n    }\n\n    public ConsumerGroup.ConsumerGroupState consumerGroupState(\n        String groupId\n    ) {\n        return groupMetadataManager\n            .consumerGroup(groupId)\n            .state();\n    }\n\n    public ShareGroup.ShareGroupState shareGroupState(\n        String groupId\n    ) {\n        return groupMetadataManager\n            .shareGroup(groupId)\n            .state();\n    }\n\n    public MemberState consumerGroupMemberState(\n        String groupId,\n        String memberId\n    ) {\n        return groupMetadataManager\n            .consumerGroup(groupId)\n            .getOrMaybeCreateMember(memberId, false)\n            .state();\n    }\n\n    public CoordinatorResult<ConsumerGroupHeartbeatResponseData, CoordinatorRecord> consumerGroupHeartbeat(\n        ConsumerGroupHeartbeatRequestData request\n    ) {\n        RequestContext context = new RequestContext(\n            new RequestHeader(\n                ApiKeys.CONSUMER_GROUP_HEARTBEAT,\n                ApiKeys.CONSUMER_GROUP_HEARTBEAT.latestVersion(),\n                \"client\",\n                0\n            ),\n            \"1\",\n            InetAddress.getLoopbackAddress(),\n            KafkaPrincipal.ANONYMOUS,\n            ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n            SecurityProtocol.PLAINTEXT,\n            ClientInformation.EMPTY,\n            false\n        );\n\n        CoordinatorResult<ConsumerGroupHeartbeatResponseData, CoordinatorRecord> result = groupMetadataManager.consumerGroupHeartbeat(\n            context,\n            request\n        );\n\n        if (result.replayRecords()) {\n            result.records().forEach(this::replay);\n        }\n        return result;\n    }\n\n    public CoordinatorResult<ShareGroupHeartbeatResponseData, CoordinatorRecord> shareGroupHeartbeat(\n        ShareGroupHeartbeatRequestData request\n    ) {\n        RequestContext context = new RequestContext(\n            new RequestHeader(\n                ApiKeys.SHARE_GROUP_HEARTBEAT,\n                ApiKeys.SHARE_GROUP_HEARTBEAT.latestVersion(),\n                \"client\",\n                0\n            ),\n            \"1\",\n            InetAddress.getLoopbackAddress(),\n            KafkaPrincipal.ANONYMOUS,\n            ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n            SecurityProtocol.PLAINTEXT,\n            ClientInformation.EMPTY,\n            false\n        );\n\n        CoordinatorResult<ShareGroupHeartbeatResponseData, CoordinatorRecord> result = groupMetadataManager.shareGroupHeartbeat(\n            context,\n            request\n        );\n\n        result.records().forEach(this::replay);\n        return result;\n    }\n\n    public List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> sleep(long ms) {\n        time.sleep(ms);\n        List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts = timer.poll();\n        timeouts.forEach(timeout -> {\n            if (timeout.result.replayRecords()) {\n                timeout.result.records().forEach(this::replay);\n            }\n        });\n        return timeouts;\n    }\n\n    public void assertSessionTimeout(\n        String groupId,\n        String memberId,\n        long delayMs\n    ) {\n        MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> timeout =\n            timer.timeout(groupSessionTimeoutKey(groupId, memberId));\n        assertNotNull(timeout);\n        assertEquals(time.milliseconds() + delayMs, timeout.deadlineMs);\n    }\n\n    public void assertNoSessionTimeout(\n        String groupId,\n        String memberId\n    ) {\n        MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> timeout =\n            timer.timeout(groupSessionTimeoutKey(groupId, memberId));\n        assertNull(timeout);\n    }\n\n    public MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> assertRebalanceTimeout(\n        String groupId,\n        String memberId,\n        long delayMs\n    ) {\n        MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> timeout =\n            timer.timeout(consumerGroupRebalanceTimeoutKey(groupId, memberId));\n        assertNotNull(timeout);\n        assertEquals(time.milliseconds() + delayMs, timeout.deadlineMs);\n        return timeout;\n    }\n\n    public void assertNoRebalanceTimeout(\n        String groupId,\n        String memberId\n    ) {\n        MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> timeout =\n            timer.timeout(consumerGroupRebalanceTimeoutKey(groupId, memberId));\n        assertNull(timeout);\n    }\n\n    public MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> assertJoinTimeout(\n        String groupId,\n        String memberId,\n        long delayMs\n    ) {\n        MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> timeout =\n            timer.timeout(consumerGroupJoinKey(groupId, memberId));\n        assertNotNull(timeout);\n        assertEquals(time.milliseconds() + delayMs, timeout.deadlineMs);\n        return timeout;\n    }\n\n    public void assertNoJoinTimeout(\n        String groupId,\n        String memberId\n    ) {\n        MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> timeout =\n            timer.timeout(consumerGroupJoinKey(groupId, memberId));\n        assertNull(timeout);\n    }\n\n    public MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> assertSyncTimeout(\n        String groupId,\n        String memberId,\n        long delayMs\n    ) {\n        MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> timeout =\n            timer.timeout(consumerGroupSyncKey(groupId, memberId));\n        assertNotNull(timeout);\n        assertEquals(time.milliseconds() + delayMs, timeout.deadlineMs);\n        return timeout;\n    }\n\n    public void assertNoSyncTimeout(\n        String groupId,\n        String memberId\n    ) {\n        MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> timeout =\n            timer.timeout(consumerGroupSyncKey(groupId, memberId));\n        assertNull(timeout);\n    }\n\n    ClassicGroup createClassicGroup(String groupId) {\n        return groupMetadataManager.getOrMaybeCreateClassicGroup(groupId, true);\n    }\n\n    public JoinResult sendClassicGroupJoin(\n        JoinGroupRequestData request\n    ) {\n        return sendClassicGroupJoin(request, false);\n    }\n\n    public JoinResult sendClassicGroupJoin(\n        JoinGroupRequestData request,\n        boolean requireKnownMemberId\n    ) {\n        return sendClassicGroupJoin(request, requireKnownMemberId, false);\n    }\n\n    public JoinResult sendClassicGroupJoin(\n        JoinGroupRequestData request,\n        boolean requireKnownMemberId,\n        boolean supportSkippingAssignment\n    ) {\n        // requireKnownMemberId is true: version >= 4 (See JoinGroupRequest#requiresKnownMemberId())\n        // supportSkippingAssignment is true: version >= 9 (See JoinGroupRequest#supportsSkippingAssignment())\n        short joinGroupVersion = 3;\n\n        if (requireKnownMemberId) {\n            joinGroupVersion = 4;\n            if (supportSkippingAssignment) {\n                joinGroupVersion = ApiKeys.JOIN_GROUP.latestVersion();\n            }\n        }\n\n        RequestContext context = new RequestContext(\n            new RequestHeader(\n                ApiKeys.JOIN_GROUP,\n                joinGroupVersion,\n                \"client\",\n                0\n            ),\n            \"1\",\n            InetAddress.getLoopbackAddress(),\n            KafkaPrincipal.ANONYMOUS,\n            ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n            SecurityProtocol.PLAINTEXT,\n            ClientInformation.EMPTY,\n            false\n        );\n\n        CompletableFuture<JoinGroupResponseData> responseFuture = new CompletableFuture<>();\n        CoordinatorResult<Void, CoordinatorRecord> coordinatorResult = groupMetadataManager.classicGroupJoin(\n            context,\n            request,\n            responseFuture\n        );\n\n        if (coordinatorResult.replayRecords()) {\n            coordinatorResult.records().forEach(this::replay);\n        }\n\n        return new JoinResult(responseFuture, coordinatorResult);\n    }\n\n    public JoinGroupResponseData joinClassicGroupAsDynamicMemberAndCompleteRebalance(\n        String groupId\n    ) throws Exception {\n        ClassicGroup group = createClassicGroup(groupId);\n\n        JoinGroupResponseData leaderJoinResponse =\n            joinClassicGroupAsDynamicMemberAndCompleteJoin(new JoinGroupRequestBuilder()\n                .withGroupId(groupId)\n                .withMemberId(UNKNOWN_MEMBER_ID)\n                .withDefaultProtocolTypeAndProtocols()\n                .withRebalanceTimeoutMs(10000)\n                .withSessionTimeoutMs(5000)\n                .build());\n\n        assertEquals(1, leaderJoinResponse.generationId());\n        assertTrue(group.isInState(COMPLETING_REBALANCE));\n\n        SyncResult syncResult = sendClassicGroupSync(new SyncGroupRequestBuilder()\n            .withGroupId(groupId)\n            .withMemberId(leaderJoinResponse.memberId())\n            .withGenerationId(leaderJoinResponse.generationId())\n            .build());\n\n        assertEquals(\n            Collections.singletonList(CoordinatorRecordHelpers.newGroupMetadataRecord(group, group.groupAssignment(), MetadataVersion.latestTesting())),\n            syncResult.records\n        );\n        // Simulate a successful write to the log.\n        syncResult.appendFuture.complete(null);\n\n        assertTrue(syncResult.syncFuture.isDone());\n        assertEquals(Errors.NONE.code(), syncResult.syncFuture.get().errorCode());\n        assertTrue(group.isInState(STABLE));\n\n        return leaderJoinResponse;\n    }\n\n    public JoinGroupResponseData joinClassicGroupAsDynamicMemberAndCompleteJoin(\n        JoinGroupRequestData request\n    ) throws ExecutionException, InterruptedException {\n        boolean requireKnownMemberId = true;\n        String newMemberId = request.memberId();\n\n        if (request.memberId().equals(UNKNOWN_MEMBER_ID)) {\n            // Since member id is required, we need another round to get the successful join group result.\n            JoinResult firstJoinResult = sendClassicGroupJoin(\n                request,\n                requireKnownMemberId\n            );\n            assertTrue(firstJoinResult.records.isEmpty());\n            assertTrue(firstJoinResult.joinFuture.isDone());\n            assertEquals(Errors.MEMBER_ID_REQUIRED.code(), firstJoinResult.joinFuture.get().errorCode());\n            newMemberId = firstJoinResult.joinFuture.get().memberId();\n        }\n\n        // Second round\n        JoinGroupRequestData secondRequest = new JoinGroupRequestData()\n            .setGroupId(request.groupId())\n            .setMemberId(newMemberId)\n            .setProtocolType(request.protocolType())\n            .setProtocols(request.protocols())\n            .setSessionTimeoutMs(request.sessionTimeoutMs())\n            .setRebalanceTimeoutMs(request.rebalanceTimeoutMs())\n            .setReason(request.reason());\n\n        JoinResult secondJoinResult = sendClassicGroupJoin(\n            secondRequest,\n            requireKnownMemberId\n        );\n\n        assertTrue(secondJoinResult.records.isEmpty());\n        List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts = sleep(classicGroupInitialRebalanceDelayMs);\n        assertEquals(1, timeouts.size());\n        assertTrue(secondJoinResult.joinFuture.isDone());\n        assertEquals(Errors.NONE.code(), secondJoinResult.joinFuture.get().errorCode());\n\n        return secondJoinResult.joinFuture.get();\n    }\n\n    public JoinGroupResponseData joinClassicGroupAndCompleteJoin(\n        JoinGroupRequestData request,\n        boolean requireKnownMemberId,\n        boolean supportSkippingAssignment\n    ) throws ExecutionException, InterruptedException {\n        return joinClassicGroupAndCompleteJoin(\n            request,\n            requireKnownMemberId,\n            supportSkippingAssignment,\n            classicGroupInitialRebalanceDelayMs\n        );\n    }\n\n    public JoinGroupResponseData joinClassicGroupAndCompleteJoin(\n        JoinGroupRequestData request,\n        boolean requireKnownMemberId,\n        boolean supportSkippingAssignment,\n        int advanceClockMs\n    ) throws ExecutionException, InterruptedException {\n        if (requireKnownMemberId && request.groupInstanceId().isEmpty()) {\n            return joinClassicGroupAsDynamicMemberAndCompleteJoin(request);\n        }\n\n        try {\n            JoinResult joinResult = sendClassicGroupJoin(\n                request,\n                requireKnownMemberId,\n                supportSkippingAssignment\n            );\n\n            sleep(advanceClockMs);\n            assertTrue(joinResult.joinFuture.isDone());\n            assertEquals(Errors.NONE.code(), joinResult.joinFuture.get().errorCode());\n            return joinResult.joinFuture.get();\n        } catch (Exception e) {\n            fail(\"Failed to due: \" + e.getMessage());\n        }\n        return null;\n    }\n\n    public SyncResult sendClassicGroupSync(SyncGroupRequestData request) {\n        RequestContext context = new RequestContext(\n            new RequestHeader(\n                ApiKeys.SYNC_GROUP,\n                ApiKeys.SYNC_GROUP.latestVersion(),\n                \"client\",\n                0\n            ),\n            \"1\",\n            InetAddress.getLoopbackAddress(),\n            KafkaPrincipal.ANONYMOUS,\n            ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n            SecurityProtocol.PLAINTEXT,\n            ClientInformation.EMPTY,\n            false\n        );\n\n        CompletableFuture<SyncGroupResponseData> responseFuture = new CompletableFuture<>();\n\n        CoordinatorResult<Void, CoordinatorRecord> coordinatorResult = groupMetadataManager.classicGroupSync(\n            context,\n            request,\n            responseFuture\n        );\n\n        if (coordinatorResult.replayRecords()) {\n            coordinatorResult.records().forEach(this::replay);\n        }\n\n        return new SyncResult(responseFuture, coordinatorResult);\n    }\n\n    public RebalanceResult staticMembersJoinAndRebalance(\n        String groupId,\n        String leaderInstanceId,\n        String followerInstanceId\n    ) throws Exception {\n        return staticMembersJoinAndRebalance(\n            groupId,\n            leaderInstanceId,\n            followerInstanceId,\n            10000,\n            5000\n        );\n    }\n\n    public RebalanceResult staticMembersJoinAndRebalance(\n        String groupId,\n        String leaderInstanceId,\n        String followerInstanceId,\n        int rebalanceTimeoutMs,\n        int sessionTimeoutMs\n    ) throws Exception {\n        ClassicGroup group = createClassicGroup(groupId);\n\n        JoinGroupRequestData joinRequest = new JoinGroupRequestBuilder()\n            .withGroupId(groupId)\n            .withGroupInstanceId(leaderInstanceId)\n            .withMemberId(UNKNOWN_MEMBER_ID)\n            .withProtocolType(\"consumer\")\n            .withProtocolSuperset()\n            .withRebalanceTimeoutMs(rebalanceTimeoutMs)\n            .withSessionTimeoutMs(sessionTimeoutMs)\n            .build();\n\n        JoinResult leaderJoinResult = sendClassicGroupJoin(joinRequest);\n        JoinResult followerJoinResult = sendClassicGroupJoin(joinRequest.setGroupInstanceId(followerInstanceId));\n\n        assertTrue(leaderJoinResult.records.isEmpty());\n        assertTrue(followerJoinResult.records.isEmpty());\n        assertFalse(leaderJoinResult.joinFuture.isDone());\n        assertFalse(followerJoinResult.joinFuture.isDone());\n\n        // The goal for two timer advance is to let first group initial join complete and set newMemberAdded flag to false. Next advance is\n        // to trigger the rebalance as needed for follower delayed join. One large time advance won't help because we could only populate one\n        // delayed join from purgatory and the new delayed op is created at that time and never be triggered.\n        assertNoOrEmptyResult(sleep(classicGroupInitialRebalanceDelayMs));\n        assertNoOrEmptyResult(sleep(classicGroupInitialRebalanceDelayMs));\n\n        assertTrue(leaderJoinResult.joinFuture.isDone());\n        assertTrue(followerJoinResult.joinFuture.isDone());\n        assertEquals(Errors.NONE.code(), leaderJoinResult.joinFuture.get().errorCode());\n        assertEquals(Errors.NONE.code(), followerJoinResult.joinFuture.get().errorCode());\n        assertEquals(1, leaderJoinResult.joinFuture.get().generationId());\n        assertEquals(1, followerJoinResult.joinFuture.get().generationId());\n        assertEquals(2, group.numMembers());\n        assertEquals(1, group.generationId());\n        assertTrue(group.isInState(COMPLETING_REBALANCE));\n\n        String leaderId = leaderJoinResult.joinFuture.get().memberId();\n        String followerId = followerJoinResult.joinFuture.get().memberId();\n        List<SyncGroupRequestData.SyncGroupRequestAssignment> assignment = new ArrayList<>();\n        assignment.add(new SyncGroupRequestData.SyncGroupRequestAssignment().setMemberId(leaderId)\n                                                                            .setAssignment(new byte[]{1}));\n        assignment.add(new SyncGroupRequestData.SyncGroupRequestAssignment().setMemberId(followerId)\n                                                                            .setAssignment(new byte[]{2}));\n\n        SyncGroupRequestData syncRequest = new SyncGroupRequestBuilder()\n            .withGroupId(groupId)\n            .withGroupInstanceId(leaderInstanceId)\n            .withMemberId(leaderId)\n            .withGenerationId(1)\n            .withAssignment(assignment)\n            .build();\n\n        SyncResult leaderSyncResult = sendClassicGroupSync(syncRequest);\n\n        // The generated record should contain the new assignment.\n        Map<String, byte[]> groupAssignment = assignment.stream().collect(Collectors.toMap(\n            SyncGroupRequestData.SyncGroupRequestAssignment::memberId, SyncGroupRequestData.SyncGroupRequestAssignment::assignment\n        ));\n        assertEquals(\n            Collections.singletonList(\n                CoordinatorRecordHelpers.newGroupMetadataRecord(group, groupAssignment, MetadataVersion.latestTesting())),\n            leaderSyncResult.records\n        );\n\n        // Simulate a successful write to the log.\n        leaderSyncResult.appendFuture.complete(null);\n\n        assertTrue(leaderSyncResult.syncFuture.isDone());\n        assertEquals(Errors.NONE.code(), leaderSyncResult.syncFuture.get().errorCode());\n        assertTrue(group.isInState(STABLE));\n\n        SyncResult followerSyncResult = sendClassicGroupSync(\n            syncRequest.setGroupInstanceId(followerInstanceId)\n                       .setMemberId(followerId)\n                       .setAssignments(Collections.emptyList())\n        );\n\n        assertTrue(followerSyncResult.records.isEmpty());\n        assertTrue(followerSyncResult.syncFuture.isDone());\n        assertEquals(Errors.NONE.code(), followerSyncResult.syncFuture.get().errorCode());\n        assertTrue(group.isInState(STABLE));\n\n        assertEquals(2, group.numMembers());\n        assertEquals(1, group.generationId());\n\n        return new RebalanceResult(\n            1,\n            leaderId,\n            leaderSyncResult.syncFuture.get().assignment(),\n            followerId,\n            followerSyncResult.syncFuture.get().assignment()\n        );\n    }\n\n    public PendingMemberGroupResult setupGroupWithPendingMember(ClassicGroup group) throws Exception {\n        // Add the first member\n        JoinGroupRequestData joinRequest = new JoinGroupRequestBuilder()\n            .withGroupId(group.groupId())\n            .withMemberId(UNKNOWN_MEMBER_ID)\n            .withDefaultProtocolTypeAndProtocols()\n            .withRebalanceTimeoutMs(10000)\n            .withSessionTimeoutMs(5000)\n            .build();\n\n        JoinGroupResponseData leaderJoinResponse =\n            joinClassicGroupAsDynamicMemberAndCompleteJoin(joinRequest);\n\n        List<SyncGroupRequestData.SyncGroupRequestAssignment> assignment = new ArrayList<>();\n        assignment.add(new SyncGroupRequestData.SyncGroupRequestAssignment().setMemberId(leaderJoinResponse.memberId()));\n        SyncGroupRequestData syncRequest = new SyncGroupRequestBuilder()\n            .withGroupId(group.groupId())\n            .withMemberId(leaderJoinResponse.memberId())\n            .withGenerationId(leaderJoinResponse.generationId())\n            .withAssignment(assignment)\n            .build();\n\n        SyncResult syncResult = sendClassicGroupSync(syncRequest);\n\n        // Now the group is stable, with the one member that joined above\n        assertEquals(\n            Collections.singletonList(CoordinatorRecordHelpers.newGroupMetadataRecord(group, group.groupAssignment(), MetadataVersion.latestTesting())),\n            syncResult.records\n        );\n        // Simulate a successful write to log.\n        syncResult.appendFuture.complete(null);\n\n        assertTrue(syncResult.syncFuture.isDone());\n        assertEquals(Errors.NONE.code(), syncResult.syncFuture.get().errorCode());\n\n        // Start the join for the second member\n        JoinResult followerJoinResult = sendClassicGroupJoin(\n            joinRequest.setMemberId(UNKNOWN_MEMBER_ID)\n        );\n\n        assertTrue(followerJoinResult.records.isEmpty());\n        assertFalse(followerJoinResult.joinFuture.isDone());\n\n        JoinResult leaderJoinResult = sendClassicGroupJoin(\n            joinRequest.setMemberId(leaderJoinResponse.memberId())\n        );\n\n        assertTrue(leaderJoinResult.records.isEmpty());\n        assertTrue(group.isInState(COMPLETING_REBALANCE));\n        assertTrue(leaderJoinResult.joinFuture.isDone());\n        assertTrue(followerJoinResult.joinFuture.isDone());\n        assertEquals(Errors.NONE.code(), leaderJoinResult.joinFuture.get().errorCode());\n        assertEquals(Errors.NONE.code(), followerJoinResult.joinFuture.get().errorCode());\n        assertEquals(leaderJoinResult.joinFuture.get().generationId(), followerJoinResult.joinFuture.get().generationId());\n        assertEquals(leaderJoinResponse.memberId(), leaderJoinResult.joinFuture.get().leader());\n        assertEquals(leaderJoinResponse.memberId(), followerJoinResult.joinFuture.get().leader());\n\n        int nextGenerationId = leaderJoinResult.joinFuture.get().generationId();\n        String followerId = followerJoinResult.joinFuture.get().memberId();\n\n        // Stabilize the group\n        syncResult = sendClassicGroupSync(syncRequest.setGenerationId(nextGenerationId));\n\n        assertEquals(\n            Collections.singletonList(CoordinatorRecordHelpers.newGroupMetadataRecord(group, group.groupAssignment(), MetadataVersion.latestTesting())),\n            syncResult.records\n        );\n        // Simulate a successful write to log.\n        syncResult.appendFuture.complete(null);\n\n        assertTrue(syncResult.syncFuture.isDone());\n        assertEquals(Errors.NONE.code(), syncResult.syncFuture.get().errorCode());\n        assertTrue(group.isInState(STABLE));\n\n        // Re-join an existing member, to transition the group to PreparingRebalance state.\n        leaderJoinResult = sendClassicGroupJoin(\n            joinRequest.setMemberId(leaderJoinResponse.memberId()));\n\n        assertTrue(leaderJoinResult.records.isEmpty());\n        assertFalse(leaderJoinResult.joinFuture.isDone());\n        assertTrue(group.isInState(PREPARING_REBALANCE));\n\n        // Create a pending member in the group\n        JoinResult pendingMemberJoinResult = sendClassicGroupJoin(\n            joinRequest\n                .setMemberId(UNKNOWN_MEMBER_ID)\n                .setSessionTimeoutMs(2500),\n            true\n        );\n\n        assertTrue(pendingMemberJoinResult.records.isEmpty());\n        assertTrue(pendingMemberJoinResult.joinFuture.isDone());\n        assertEquals(Errors.MEMBER_ID_REQUIRED.code(), pendingMemberJoinResult.joinFuture.get().errorCode());\n        assertEquals(1, group.numPendingJoinMembers());\n\n        // Re-join the second existing member\n        followerJoinResult = sendClassicGroupJoin(\n            joinRequest.setMemberId(followerId).setSessionTimeoutMs(5000)\n        );\n\n        assertTrue(followerJoinResult.records.isEmpty());\n        assertFalse(followerJoinResult.joinFuture.isDone());\n        assertTrue(group.isInState(PREPARING_REBALANCE));\n        assertEquals(2, group.numMembers());\n        assertEquals(1, group.numPendingJoinMembers());\n\n        return new PendingMemberGroupResult(\n            leaderJoinResponse.memberId(),\n            followerId,\n            pendingMemberJoinResult.joinFuture.get()\n        );\n    }\n\n    public void verifySessionExpiration(ClassicGroup group, int timeoutMs) {\n        Set<String> expectedHeartbeatKeys = group.allMembers().stream()\n                                                 .map(member -> classicGroupHeartbeatKey(group.groupId(), member.memberId())).collect(Collectors.toSet());\n\n        // Member should be removed as session expires.\n        List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts = sleep(timeoutMs);\n        List<CoordinatorRecord> expectedRecords = Collections.singletonList(newGroupMetadataRecord(\n            group.groupId(),\n            new GroupMetadataValue()\n                .setMembers(Collections.emptyList())\n                .setGeneration(group.generationId())\n                .setLeader(null)\n                .setProtocolType(\"consumer\")\n                .setProtocol(null)\n                .setCurrentStateTimestamp(time.milliseconds()),\n            MetadataVersion.latestTesting()\n        ));\n\n\n        Set<String> heartbeatKeys = timeouts.stream().map(timeout -> timeout.key).collect(Collectors.toSet());\n        assertEquals(expectedHeartbeatKeys, heartbeatKeys);\n\n        // Only the last member leaving the group should result in the empty group metadata record.\n        int timeoutsSize = timeouts.size();\n        assertEquals(expectedRecords, timeouts.get(timeoutsSize - 1).result.records());\n        assertNoOrEmptyResult(timeouts.subList(0, timeoutsSize - 1));\n        assertTrue(group.isInState(EMPTY));\n        assertEquals(0, group.numMembers());\n    }\n\n    public CoordinatorResult<HeartbeatResponseData, CoordinatorRecord> sendClassicGroupHeartbeat(\n        HeartbeatRequestData request\n    ) {\n        RequestContext context = new RequestContext(\n            new RequestHeader(\n                ApiKeys.HEARTBEAT,\n                ApiKeys.HEARTBEAT.latestVersion(),\n                \"client\",\n                0\n            ),\n            \"1\",\n            InetAddress.getLoopbackAddress(),\n            KafkaPrincipal.ANONYMOUS,\n            ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n            SecurityProtocol.PLAINTEXT,\n            ClientInformation.EMPTY,\n            false\n        );\n\n        return groupMetadataManager.classicGroupHeartbeat(\n            context,\n            request\n        );\n    }\n\n    public List<ListGroupsResponseData.ListedGroup> sendListGroups(List<String> statesFilter, List<String> typesFilter) {\n        Set<String> statesFilterSet = new HashSet<>(statesFilter);\n        Set<String> typesFilterSet = new HashSet<>(typesFilter);\n        return groupMetadataManager.listGroups(statesFilterSet, typesFilterSet, lastCommittedOffset);\n    }\n\n    public List<ConsumerGroupDescribeResponseData.DescribedGroup> sendConsumerGroupDescribe(List<String> groupIds) {\n        return groupMetadataManager.consumerGroupDescribe(groupIds, lastCommittedOffset);\n    }\n\n    public List<DescribeGroupsResponseData.DescribedGroup> describeGroups(List<String> groupIds) {\n        return groupMetadataManager.describeGroups(groupIds, lastCommittedOffset);\n    }\n\n    public List<ShareGroupDescribeResponseData.DescribedGroup> sendShareGroupDescribe(List<String> groupIds) {\n        return groupMetadataManager.shareGroupDescribe(groupIds, lastCommittedOffset);\n    }\n\n    public void verifyHeartbeat(\n        String groupId,\n        JoinGroupResponseData joinResponse,\n        Errors expectedError\n    ) {\n        HeartbeatRequestData request = new HeartbeatRequestData()\n            .setGroupId(groupId)\n            .setMemberId(joinResponse.memberId())\n            .setGenerationId(joinResponse.generationId());\n\n        if (expectedError == Errors.UNKNOWN_MEMBER_ID) {\n            assertThrows(UnknownMemberIdException.class, () -> sendClassicGroupHeartbeat(request));\n        } else {\n            HeartbeatResponseData response = sendClassicGroupHeartbeat(request).response();\n            assertEquals(expectedError.code(), response.errorCode());\n        }\n    }\n\n    public List<JoinGroupResponseData> joinWithNMembers(\n        String groupId,\n        int numMembers,\n        int rebalanceTimeoutMs,\n        int sessionTimeoutMs\n    ) {\n        ClassicGroup group = createClassicGroup(groupId);\n        boolean requireKnownMemberId = true;\n\n        // First join requests\n        JoinGroupRequestData request = new JoinGroupRequestBuilder()\n            .withGroupId(groupId)\n            .withMemberId(UNKNOWN_MEMBER_ID)\n            .withDefaultProtocolTypeAndProtocols()\n            .withRebalanceTimeoutMs(rebalanceTimeoutMs)\n            .withSessionTimeoutMs(sessionTimeoutMs)\n            .build();\n\n        List<String> memberIds = IntStream.range(0, numMembers).mapToObj(i -> {\n            JoinResult joinResult = sendClassicGroupJoin(request, requireKnownMemberId);\n\n            assertTrue(joinResult.records.isEmpty());\n            assertTrue(joinResult.joinFuture.isDone());\n\n            try {\n                return joinResult.joinFuture.get().memberId();\n            } catch (Exception e) {\n                fail(\"Unexpected exception: \" + e.getMessage());\n            }\n            return null;\n        }).collect(Collectors.toList());\n\n        // Second join requests\n        List<CompletableFuture<JoinGroupResponseData>> secondJoinFutures = IntStream.range(0, numMembers).mapToObj(i -> {\n            JoinResult joinResult = sendClassicGroupJoin(request.setMemberId(memberIds.get(i)), requireKnownMemberId);\n\n            assertTrue(joinResult.records.isEmpty());\n            assertFalse(joinResult.joinFuture.isDone());\n\n            return joinResult.joinFuture;\n        }).collect(Collectors.toList());\n\n        // Advance clock by initial rebalance delay.\n        assertNoOrEmptyResult(sleep(classicGroupInitialRebalanceDelayMs));\n        secondJoinFutures.forEach(future -> assertFalse(future.isDone()));\n        // Advance clock by rebalance timeout to complete join phase.\n        assertNoOrEmptyResult(sleep(rebalanceTimeoutMs));\n\n        List<JoinGroupResponseData> joinResponses = secondJoinFutures.stream().map(future -> {\n            assertTrue(future.isDone());\n            try {\n                assertEquals(Errors.NONE.code(), future.get().errorCode());\n                return future.get();\n            } catch (Exception e) {\n                fail(\"Unexpected exception: \" + e.getMessage());\n            }\n            return null;\n        }).collect(Collectors.toList());\n\n        assertEquals(numMembers, group.numMembers());\n        assertTrue(group.isInState(COMPLETING_REBALANCE));\n\n        return joinResponses;\n    }\n\n    public CoordinatorResult<LeaveGroupResponseData, CoordinatorRecord> sendClassicGroupLeave(\n        LeaveGroupRequestData request\n    ) {\n        RequestContext context = new RequestContext(\n            new RequestHeader(\n                ApiKeys.LEAVE_GROUP,\n                ApiKeys.LEAVE_GROUP.latestVersion(),\n                \"client\",\n                0\n            ),\n            \"1\",\n            InetAddress.getLoopbackAddress(),\n            KafkaPrincipal.ANONYMOUS,\n            ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n            SecurityProtocol.PLAINTEXT,\n            ClientInformation.EMPTY,\n            false\n        );\n\n        return groupMetadataManager.classicGroupLeave(context, request);\n    }\n\n    public void verifyDescribeGroupsReturnsDeadGroup(String groupId) {\n        List<DescribeGroupsResponseData.DescribedGroup> describedGroups =\n            describeGroups(Collections.singletonList(groupId));\n\n        assertEquals(\n            Collections.singletonList(new DescribeGroupsResponseData.DescribedGroup()\n                .setGroupId(groupId)\n                .setGroupState(DEAD.toString())\n            ),\n            describedGroups\n        );\n    }\n\n    public void verifyClassicGroupSyncToConsumerGroup(\n        String groupId,\n        String memberId,\n        int generationId,\n        String protocolName,\n        String protocolType,\n        List<TopicPartition> topicPartitionList,\n        short version\n    ) throws Exception {\n        GroupMetadataManagerTestContext.SyncResult syncResult = sendClassicGroupSync(\n            new GroupMetadataManagerTestContext.SyncGroupRequestBuilder()\n                .withGroupId(groupId)\n                .withMemberId(memberId)\n                .withGenerationId(generationId)\n                .withProtocolName(protocolName)\n                .withProtocolType(protocolType)\n                .build()\n        );\n        assertEquals(Collections.emptyList(), syncResult.records);\n        assertFalse(syncResult.syncFuture.isDone());\n\n        // Simulate a successful write to log.\n        syncResult.appendFuture.complete(null);\n        assertSyncGroupResponseEquals(\n            new SyncGroupResponseData()\n                .setProtocolType(protocolType)\n                .setProtocolName(protocolName)\n                .setAssignment(ConsumerProtocol.serializeAssignment(\n                    new ConsumerPartitionAssignor.Assignment(topicPartitionList),\n                    version\n                ).array()),\n            syncResult.syncFuture.get()\n        );\n        assertSessionTimeout(groupId, memberId, 5000);\n        assertNoSyncTimeout(groupId, memberId);\n    }\n\n    public void verifyClassicGroupSyncToConsumerGroup(\n        String groupId,\n        String memberId,\n        int generationId,\n        String protocolName,\n        String protocolType,\n        List<TopicPartition> topicPartitionList\n    ) throws Exception {\n        verifyClassicGroupSyncToConsumerGroup(\n            groupId,\n            memberId,\n            generationId,\n            protocolName,\n            protocolType,\n            topicPartitionList,\n            ConsumerProtocolAssignment.HIGHEST_SUPPORTED_VERSION\n        );\n    }\n\n    private ApiMessage messageOrNull(ApiMessageAndVersion apiMessageAndVersion) {\n        if (apiMessageAndVersion == null) {\n            return null;\n        } else {\n            return apiMessageAndVersion.message();\n        }\n    }\n\n    public void replay(\n        CoordinatorRecord record\n    ) {\n        replay(record, null);\n    }\n\n    public void replay(\n        CoordinatorRecord record,\n        GroupType groupType\n    ) {\n        ApiMessageAndVersion key = record.key();\n        ApiMessageAndVersion value = record.value();\n\n        if (key == null) {\n            throw new IllegalStateException(\"Received a null key in \" + record);\n        }\n\n        switch (key.version()) {\n            case GroupMetadataKey.HIGHEST_SUPPORTED_VERSION:\n                groupMetadataManager.replay(\n                    (GroupMetadataKey) key.message(),\n                    (GroupMetadataValue) messageOrNull(value)\n                );\n                break;\n\n            case ConsumerGroupMemberMetadataKey.HIGHEST_SUPPORTED_VERSION:\n                groupMetadataManager.replay(\n                    (ConsumerGroupMemberMetadataKey) key.message(),\n                    (ConsumerGroupMemberMetadataValue) messageOrNull(value)\n                );\n                break;\n\n            case ConsumerGroupMetadataKey.HIGHEST_SUPPORTED_VERSION:\n                groupMetadataManager.replay(\n                    (ConsumerGroupMetadataKey) key.message(),\n                    (ConsumerGroupMetadataValue) messageOrNull(value)\n                );\n                break;\n\n            case ConsumerGroupPartitionMetadataKey.HIGHEST_SUPPORTED_VERSION:\n                groupMetadataManager.replay(\n                    (ConsumerGroupPartitionMetadataKey) key.message(),\n                    (ConsumerGroupPartitionMetadataValue) messageOrNull(value),\n                    groupType != null ? groupType : GroupType.CONSUMER\n                );\n                break;\n\n            case ConsumerGroupTargetAssignmentMemberKey.HIGHEST_SUPPORTED_VERSION:\n                groupMetadataManager.replay(\n                    (ConsumerGroupTargetAssignmentMemberKey) key.message(),\n                    (ConsumerGroupTargetAssignmentMemberValue) messageOrNull(value),\n                    groupType != null ? groupType : GroupType.CONSUMER\n                );\n                break;\n\n            case ConsumerGroupTargetAssignmentMetadataKey.HIGHEST_SUPPORTED_VERSION:\n                groupMetadataManager.replay(\n                    (ConsumerGroupTargetAssignmentMetadataKey) key.message(),\n                    (ConsumerGroupTargetAssignmentMetadataValue) messageOrNull(value),\n                    groupType != null ? groupType : GroupType.CONSUMER\n                );\n                break;\n\n            case ConsumerGroupCurrentMemberAssignmentKey.HIGHEST_SUPPORTED_VERSION:\n                groupMetadataManager.replay(\n                    (ConsumerGroupCurrentMemberAssignmentKey) key.message(),\n                    (ConsumerGroupCurrentMemberAssignmentValue) messageOrNull(value),\n                    groupType != null ? groupType : GroupType.CONSUMER\n                );\n                break;\n\n            case ShareGroupMetadataKey.HIGHEST_SUPPORTED_VERSION:\n                groupMetadataManager.replay(\n                    (ShareGroupMetadataKey) key.message(),\n                    (ShareGroupMetadataValue) messageOrNull(value)\n                );\n                break;\n\n            case ShareGroupMemberMetadataKey.HIGHEST_SUPPORTED_VERSION:\n                groupMetadataManager.replay(\n                    (ShareGroupMemberMetadataKey) key.message(),\n                    (ShareGroupMemberMetadataValue) messageOrNull(value)\n                );\n                break;\n\n            default:\n                throw new IllegalStateException(\"Received an unknown record type \" + key.version()\n                    + \" in \" + record);\n        }\n\n        lastWrittenOffset++;\n        idempotentCreateSnapshot();\n    }\n\n    private void idempotentCreateSnapshot() {\n        snapshotRegistry.getOrCreateSnapshot(lastWrittenOffset);\n    }\n\n    void onUnloaded() {\n        groupMetadataManager.onUnloaded();\n    }\n}",
                "methodCount": 87
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 4,
                "candidates": [
                    {
                        "lineStart": 121,
                        "lineEnd": 124,
                        "refactoringType": "MyMoveStaticMethodRefactoring",
                        "refactoringInfo": "Move Static method assertNoOrEmptyResult to class Assertions",
                        "description": "move method assertNoOrEmptyResult to PsiClass:Assertions\nRationale: The method assertNoOrEmptyResult() is an assertion utility method designed to check the size and content of a list of ExpiredTimeout objects. The existing Assertions class already contains various assertion methods for validating data structures and responses, making it the most appropriate class to host this new method. Moving this method to Assertions improves cohesion by keeping all assertion-related operations together. On the other hand, MockPartitionAssignor and NoOpPartitionAssignor deal with partition assignment logic for consumer groups, which is unrelated to the assertion functionalities, making them unsuitable for hosting this method.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1346,
                        "lineEnd": 1357,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method verifyDescribeGroupsReturnsDeadGroup to class GroupCoordinatorMetricsShard",
                        "description": "Move method verifyDescribeGroupsReturnsDeadGroup to org.apache.kafka.coordinator.group.metrics.GroupCoordinatorMetricsShard\nRationale: The verifyDescribeGroupsReturnsDeadGroup() method deals with verifying the state of a Kafka group by checking it to be 'DEAD'. The GroupCoordinatorMetricsShard class specializes in managing metrics and states related to Kafka group coordinators, including states such as DEAD. Hence, it fits logically for verifying group states to be handled within this class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 726,
                        "lineEnd": 728,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method createClassicGroup to class GroupMetadataManager",
                        "description": "Move method createClassicGroup to org.apache.kafka.coordinator.group.GroupMetadataManager\nRationale: The method createClassicGroup(String groupId) is responsible for creating or retrieving a ClassicGroup by interacting with the groupMetadataManager. Additionally, this logic primarily deals with managing group metadata, which aligns with the responsibilities of the GroupMetadataManager class. This method should thus be moved to the GroupMetadataManager class to maintain cohesive and modular code organization. Placing the method within GroupMetadataManager will streamline access to internal methods and properties, reducing inter-class dependencies and enhancing code readability.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1521,
                        "lineEnd": 1523,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method onUnloaded to class GroupMetadataManager",
                        "description": "Move method onUnloaded to org.apache.kafka.coordinator.group.GroupMetadataManager\nRationale: The method `onUnloaded` directly calls `groupMetadataManager.onUnloaded()`, indicating that its functionality is deeply coupled with the `GroupMetadataManager` class. Moving the method to `GroupMetadataManager` will simplify the class structure and improve cohesion, as the logic for handling the 'unload' event would be encapsulated within the class that manages the group metadata. This improves maintainability and readability by keeping related functionalities within the same class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "assertNoOrEmptyResult",
                            "method_signature": "public static assertNoOrEmptyResult(List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "toProtocols",
                            "method_signature": "public static toProtocols(String... protocolNames)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "toConsumerProtocol",
                            "method_signature": "public static toConsumerProtocol(\n        List<String> topicNames,\n        List<TopicPartition> ownedPartitions\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "toConsumerProtocol",
                            "method_signature": "public static toConsumerProtocol(\n        List<String> topicNames,\n        List<TopicPartition> ownedPartitions,\n        short version\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "newGroupMetadataRecord",
                            "method_signature": "public static newGroupMetadataRecord(\n        String groupId,\n        GroupMetadataValue value,\n        MetadataVersion metadataVersion\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withGroupId",
                            "method_signature": " withGroupId(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withGroupInstanceId",
                            "method_signature": " withGroupInstanceId(String groupInstanceId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withMemberId",
                            "method_signature": " withMemberId(String memberId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withDefaultProtocolTypeAndProtocols",
                            "method_signature": " withDefaultProtocolTypeAndProtocols()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withProtocolSuperset",
                            "method_signature": " withProtocolSuperset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withProtocolType",
                            "method_signature": " withProtocolType(String protocolType)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withProtocols",
                            "method_signature": " withProtocols(JoinGroupRequestData.JoinGroupRequestProtocolCollection protocols)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withRebalanceTimeoutMs",
                            "method_signature": " withRebalanceTimeoutMs(int rebalanceTimeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withSessionTimeoutMs",
                            "method_signature": " withSessionTimeoutMs(int sessionTimeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withReason",
                            "method_signature": " withReason(String reason)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": " build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withGroupId",
                            "method_signature": " withGroupId(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withGroupInstanceId",
                            "method_signature": " withGroupInstanceId(String groupInstanceId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withMemberId",
                            "method_signature": " withMemberId(String memberId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withGenerationId",
                            "method_signature": " withGenerationId(int generationId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withProtocolType",
                            "method_signature": " withProtocolType(String protocolType)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withProtocolName",
                            "method_signature": " withProtocolName(String protocolName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withAssignment",
                            "method_signature": " withAssignment(List<SyncGroupRequestData.SyncGroupRequestAssignment> assignment)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": " build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withMetadataImage",
                            "method_signature": "public withMetadataImage(MetadataImage metadataImage)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withConsumerGroupAssignors",
                            "method_signature": "public withConsumerGroupAssignors(List<ConsumerGroupPartitionAssignor> assignors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withConsumerGroup",
                            "method_signature": "public withConsumerGroup(ConsumerGroupBuilder builder)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withConsumerGroupMaxSize",
                            "method_signature": "public withConsumerGroupMaxSize(int consumerGroupMaxSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withConsumerGroupMetadataRefreshIntervalMs",
                            "method_signature": "public withConsumerGroupMetadataRefreshIntervalMs(int consumerGroupMetadataRefreshIntervalMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withClassicGroupMaxSize",
                            "method_signature": "public withClassicGroupMaxSize(int classicGroupMaxSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withClassicGroupInitialRebalanceDelayMs",
                            "method_signature": "public withClassicGroupInitialRebalanceDelayMs(int classicGroupInitialRebalanceDelayMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withClassicGroupMinSessionTimeoutMs",
                            "method_signature": "public withClassicGroupMinSessionTimeoutMs(int classicGroupMinSessionTimeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withClassicGroupMaxSessionTimeoutMs",
                            "method_signature": "public withClassicGroupMaxSessionTimeoutMs(int classicGroupMaxSessionTimeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withConsumerGroupMigrationPolicy",
                            "method_signature": "public withConsumerGroupMigrationPolicy(ConsumerGroupMigrationPolicy consumerGroupMigrationPolicy)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withShareGroupAssignor",
                            "method_signature": "public withShareGroupAssignor(ShareGroupPartitionAssignor shareGroupAssignor)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withShareGroupMaxSize",
                            "method_signature": "public withShareGroupMaxSize(int shareGroupMaxSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commit",
                            "method_signature": "public commit()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "rollback",
                            "method_signature": "public rollback()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "consumerGroupState",
                            "method_signature": "public consumerGroupState(\n        String groupId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "shareGroupState",
                            "method_signature": "public shareGroupState(\n        String groupId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "consumerGroupMemberState",
                            "method_signature": "public consumerGroupMemberState(\n        String groupId,\n        String memberId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "consumerGroupHeartbeat",
                            "method_signature": "public consumerGroupHeartbeat(\n        ConsumerGroupHeartbeatRequestData request\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "shareGroupHeartbeat",
                            "method_signature": "public shareGroupHeartbeat(\n        ShareGroupHeartbeatRequestData request\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sleep",
                            "method_signature": "public sleep(long ms)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSessionTimeout",
                            "method_signature": "public assertSessionTimeout(\n        String groupId,\n        String memberId,\n        long delayMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertNoSessionTimeout",
                            "method_signature": "public assertNoSessionTimeout(\n        String groupId,\n        String memberId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertRebalanceTimeout",
                            "method_signature": "public assertRebalanceTimeout(\n        String groupId,\n        String memberId,\n        long delayMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertNoRebalanceTimeout",
                            "method_signature": "public assertNoRebalanceTimeout(\n        String groupId,\n        String memberId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertJoinTimeout",
                            "method_signature": "public assertJoinTimeout(\n        String groupId,\n        String memberId,\n        long delayMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertNoJoinTimeout",
                            "method_signature": "public assertNoJoinTimeout(\n        String groupId,\n        String memberId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSyncTimeout",
                            "method_signature": "public assertSyncTimeout(\n        String groupId,\n        String memberId,\n        long delayMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertNoSyncTimeout",
                            "method_signature": "public assertNoSyncTimeout(\n        String groupId,\n        String memberId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createClassicGroup",
                            "method_signature": " createClassicGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sendClassicGroupJoin",
                            "method_signature": "public sendClassicGroupJoin(\n        JoinGroupRequestData request\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sendClassicGroupJoin",
                            "method_signature": "public sendClassicGroupJoin(\n        JoinGroupRequestData request,\n        boolean requireKnownMemberId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sendClassicGroupJoin",
                            "method_signature": "public sendClassicGroupJoin(\n        JoinGroupRequestData request,\n        boolean requireKnownMemberId,\n        boolean supportSkippingAssignment\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "joinClassicGroupAsDynamicMemberAndCompleteRebalance",
                            "method_signature": "public joinClassicGroupAsDynamicMemberAndCompleteRebalance(\n        String groupId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "joinClassicGroupAsDynamicMemberAndCompleteJoin",
                            "method_signature": "public joinClassicGroupAsDynamicMemberAndCompleteJoin(\n        JoinGroupRequestData request\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "joinClassicGroupAndCompleteJoin",
                            "method_signature": "public joinClassicGroupAndCompleteJoin(\n        JoinGroupRequestData request,\n        boolean requireKnownMemberId,\n        boolean supportSkippingAssignment\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "joinClassicGroupAndCompleteJoin",
                            "method_signature": "public joinClassicGroupAndCompleteJoin(\n        JoinGroupRequestData request,\n        boolean requireKnownMemberId,\n        boolean supportSkippingAssignment,\n        int advanceClockMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sendClassicGroupSync",
                            "method_signature": "public sendClassicGroupSync(SyncGroupRequestData request)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "staticMembersJoinAndRebalance",
                            "method_signature": "public staticMembersJoinAndRebalance(\n        String groupId,\n        String leaderInstanceId,\n        String followerInstanceId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "staticMembersJoinAndRebalance",
                            "method_signature": "public staticMembersJoinAndRebalance(\n        String groupId,\n        String leaderInstanceId,\n        String followerInstanceId,\n        int rebalanceTimeoutMs,\n        int sessionTimeoutMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifySessionExpiration",
                            "method_signature": "public verifySessionExpiration(ClassicGroup group, int timeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sendClassicGroupHeartbeat",
                            "method_signature": "public sendClassicGroupHeartbeat(\n        HeartbeatRequestData request\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sendListGroups",
                            "method_signature": "public sendListGroups(List<String> statesFilter, List<String> typesFilter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sendConsumerGroupDescribe",
                            "method_signature": "public sendConsumerGroupDescribe(List<String> groupIds)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "describeGroups",
                            "method_signature": "public describeGroups(List<String> groupIds)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sendShareGroupDescribe",
                            "method_signature": "public sendShareGroupDescribe(List<String> groupIds)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyHeartbeat",
                            "method_signature": "public verifyHeartbeat(\n        String groupId,\n        JoinGroupResponseData joinResponse,\n        Errors expectedError\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "joinWithNMembers",
                            "method_signature": "public joinWithNMembers(\n        String groupId,\n        int numMembers,\n        int rebalanceTimeoutMs,\n        int sessionTimeoutMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sendClassicGroupLeave",
                            "method_signature": "public sendClassicGroupLeave(\n        LeaveGroupRequestData request\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyDescribeGroupsReturnsDeadGroup",
                            "method_signature": "public verifyDescribeGroupsReturnsDeadGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyClassicGroupSyncToConsumerGroup",
                            "method_signature": "public verifyClassicGroupSyncToConsumerGroup(\n        String groupId,\n        String memberId,\n        int generationId,\n        String protocolName,\n        String protocolType,\n        List<TopicPartition> topicPartitionList,\n        short version\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyClassicGroupSyncToConsumerGroup",
                            "method_signature": "public verifyClassicGroupSyncToConsumerGroup(\n        String groupId,\n        String memberId,\n        int generationId,\n        String protocolName,\n        String protocolType,\n        List<TopicPartition> topicPartitionList\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(\n        CoordinatorRecord record\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(\n        CoordinatorRecord record,\n        GroupType groupType\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onUnloaded",
                            "method_signature": " onUnloaded()",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(\n        CoordinatorRecord record,\n        GroupType groupType\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertNoOrEmptyResult",
                            "method_signature": "public static assertNoOrEmptyResult(List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "joinClassicGroupAndCompleteJoin",
                            "method_signature": "public joinClassicGroupAndCompleteJoin(\n        JoinGroupRequestData request,\n        boolean requireKnownMemberId,\n        boolean supportSkippingAssignment\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": " build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": " build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyClassicGroupSyncToConsumerGroup",
                            "method_signature": "public verifyClassicGroupSyncToConsumerGroup(\n        String groupId,\n        String memberId,\n        int generationId,\n        String protocolName,\n        String protocolType,\n        List<TopicPartition> topicPartitionList\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createClassicGroup",
                            "method_signature": " createClassicGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onUnloaded",
                            "method_signature": " onUnloaded()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "newGroupMetadataRecord",
                            "method_signature": "public static newGroupMetadataRecord(\n        String groupId,\n        GroupMetadataValue value,\n        MetadataVersion metadataVersion\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "toConsumerProtocol",
                            "method_signature": "public static toConsumerProtocol(\n        List<String> topicNames,\n        List<TopicPartition> ownedPartitions\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(\n        CoordinatorRecord record\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "staticMembersJoinAndRebalance",
                            "method_signature": "public staticMembersJoinAndRebalance(\n        String groupId,\n        String leaderInstanceId,\n        String followerInstanceId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyDescribeGroupsReturnsDeadGroup",
                            "method_signature": "public verifyDescribeGroupsReturnsDeadGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(\n        CoordinatorRecord record,\n        GroupType groupType\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertNoOrEmptyResult",
                            "method_signature": "public static assertNoOrEmptyResult(List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "joinClassicGroupAndCompleteJoin",
                            "method_signature": "public joinClassicGroupAndCompleteJoin(\n        JoinGroupRequestData request,\n        boolean requireKnownMemberId,\n        boolean supportSkippingAssignment\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": " build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": " build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyClassicGroupSyncToConsumerGroup",
                            "method_signature": "public verifyClassicGroupSyncToConsumerGroup(\n        String groupId,\n        String memberId,\n        int generationId,\n        String protocolName,\n        String protocolType,\n        List<TopicPartition> topicPartitionList\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createClassicGroup",
                            "method_signature": " createClassicGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onUnloaded",
                            "method_signature": " onUnloaded()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "newGroupMetadataRecord",
                            "method_signature": "public static newGroupMetadataRecord(\n        String groupId,\n        GroupMetadataValue value,\n        MetadataVersion metadataVersion\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "toConsumerProtocol",
                            "method_signature": "public static toConsumerProtocol(\n        List<String> topicNames,\n        List<TopicPartition> ownedPartitions\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(\n        CoordinatorRecord record\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "staticMembersJoinAndRebalance",
                            "method_signature": "public staticMembersJoinAndRebalance(\n        String groupId,\n        String leaderInstanceId,\n        String followerInstanceId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyDescribeGroupsReturnsDeadGroup",
                            "method_signature": "public verifyDescribeGroupsReturnsDeadGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "public replay(\n        CoordinatorRecord record,\n        GroupType groupType\n    )": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(\n        CoordinatorRecord record,\n        GroupType groupType\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.20297371972416442
                },
                "public static assertNoOrEmptyResult(List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts)": {
                    "first": {
                        "method_name": "assertNoOrEmptyResult",
                        "method_signature": "public static assertNoOrEmptyResult(List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2984217153975
                },
                "public joinClassicGroupAndCompleteJoin(\n        JoinGroupRequestData request,\n        boolean requireKnownMemberId,\n        boolean supportSkippingAssignment\n    )": {
                    "first": {
                        "method_name": "joinClassicGroupAndCompleteJoin",
                        "method_signature": "public joinClassicGroupAndCompleteJoin(\n        JoinGroupRequestData request,\n        boolean requireKnownMemberId,\n        boolean supportSkippingAssignment\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3239255425379285
                },
                " build()": {
                    "first": {
                        "method_name": "build",
                        "method_signature": " build()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.33220218425901177
                },
                "public verifyClassicGroupSyncToConsumerGroup(\n        String groupId,\n        String memberId,\n        int generationId,\n        String protocolName,\n        String protocolType,\n        List<TopicPartition> topicPartitionList\n    )": {
                    "first": {
                        "method_name": "verifyClassicGroupSyncToConsumerGroup",
                        "method_signature": "public verifyClassicGroupSyncToConsumerGroup(\n        String groupId,\n        String memberId,\n        int generationId,\n        String protocolName,\n        String protocolType,\n        List<TopicPartition> topicPartitionList\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3332175486608564
                },
                "private idempotentCreateSnapshot()": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3678733595183951
                },
                " createClassicGroup(String groupId)": {
                    "first": {
                        "method_name": "createClassicGroup",
                        "method_signature": " createClassicGroup(String groupId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.36832220710137703
                },
                " onUnloaded()": {
                    "first": {
                        "method_name": "onUnloaded",
                        "method_signature": " onUnloaded()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.37304915755370066
                },
                "public static newGroupMetadataRecord(\n        String groupId,\n        GroupMetadataValue value,\n        MetadataVersion metadataVersion\n    )": {
                    "first": {
                        "method_name": "newGroupMetadataRecord",
                        "method_signature": "public static newGroupMetadataRecord(\n        String groupId,\n        GroupMetadataValue value,\n        MetadataVersion metadataVersion\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3770535808747107
                },
                "public static toConsumerProtocol(\n        List<String> topicNames,\n        List<TopicPartition> ownedPartitions\n    )": {
                    "first": {
                        "method_name": "toConsumerProtocol",
                        "method_signature": "public static toConsumerProtocol(\n        List<String> topicNames,\n        List<TopicPartition> ownedPartitions\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3816298256633368
                },
                "public replay(\n        CoordinatorRecord record\n    )": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(\n        CoordinatorRecord record\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3835101449159958
                },
                "public staticMembersJoinAndRebalance(\n        String groupId,\n        String leaderInstanceId,\n        String followerInstanceId\n    )": {
                    "first": {
                        "method_name": "staticMembersJoinAndRebalance",
                        "method_signature": "public staticMembersJoinAndRebalance(\n        String groupId,\n        String leaderInstanceId,\n        String followerInstanceId\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3910573244065102
                },
                "public build()": {
                    "first": {
                        "method_name": "build",
                        "method_signature": "public build()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4067243118300372
                },
                "public verifyDescribeGroupsReturnsDeadGroup(String groupId)": {
                    "first": {
                        "method_name": "verifyDescribeGroupsReturnsDeadGroup",
                        "method_signature": "public verifyDescribeGroupsReturnsDeadGroup(String groupId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4155327860714273
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "public static assertNoOrEmptyResult(List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts)",
                    "public verifyDescribeGroupsReturnsDeadGroup(String groupId)",
                    " build()",
                    " build()",
                    "public build()",
                    " createClassicGroup(String groupId)",
                    " onUnloaded()",
                    "private idempotentCreateSnapshot()"
                ],
                "llm_response_time": 4622
            },
            "targetClassMap": {
                "assertNoOrEmptyResult": {
                    "target_classes": [
                        {
                            "class_name": "TestUtil",
                            "similarity_score": 0.2788866755113585
                        },
                        {
                            "class_name": "AssignmentTestUtil",
                            "similarity_score": 0.3100194575128992
                        },
                        {
                            "class_name": "Assertions",
                            "similarity_score": 0.3114222159379954
                        },
                        {
                            "class_name": "GroupCoordinatorConfigTest",
                            "similarity_score": 0.17491527034192925
                        },
                        {
                            "class_name": "CoordinatorRecordHelpersTest",
                            "similarity_score": 0.17066921534597945
                        },
                        {
                            "class_name": "OffsetMetadataManagerTest",
                            "similarity_score": 0.19312680589973524
                        },
                        {
                            "class_name": "GroupCoordinatorServiceTest",
                            "similarity_score": 0.16764108345552195
                        },
                        {
                            "class_name": "GroupMetadataManagerTest",
                            "similarity_score": 0.13325079983720237
                        },
                        {
                            "class_name": "MetadataImageBuilder",
                            "similarity_score": 0.22338875108890427
                        },
                        {
                            "class_name": "MockCoordinatorTimer",
                            "similarity_score": 0.30489226938312647
                        },
                        {
                            "class_name": "MockPartitionAssignor",
                            "similarity_score": 0.354374653931171
                        },
                        {
                            "class_name": "NoOpPartitionAssignor",
                            "similarity_score": 0.3305898024536431
                        },
                        {
                            "class_name": "OffsetAndMetadataTest",
                            "similarity_score": 0.16787229821795305
                        },
                        {
                            "class_name": "OffsetExpirationConditionImplTest",
                            "similarity_score": 0.0870542864324558
                        },
                        {
                            "class_name": "OffsetMetadataManagerTestContext",
                            "similarity_score": 0.2893800111592325
                        },
                        {
                            "class_name": "PendingMemberGroupResult",
                            "similarity_score": 0.22592402852876597
                        },
                        {
                            "class_name": "RebalanceResult",
                            "similarity_score": 0.15118578920369088
                        },
                        {
                            "class_name": "ScheduledTimeout",
                            "similarity_score": 0.24096579867074966
                        },
                        {
                            "class_name": "SyncGroupRequestBuilder",
                            "similarity_score": 0.20672455764868075
                        },
                        {
                            "class_name": "SyncResult",
                            "similarity_score": 0.2636352520041483
                        },
                        {
                            "class_name": "CoordinatorRecordSerdeTest",
                            "similarity_score": 0.1718302273894527
                        },
                        {
                            "class_name": "CoordinatorRecordTest",
                            "similarity_score": 0.23424278964210216
                        },
                        {
                            "class_name": "ExpiredTimeout",
                            "similarity_score": 0.23000322710873394
                        },
                        {
                            "class_name": "GroupCoordinatorShardTest",
                            "similarity_score": 0.10458573537752495
                        },
                        {
                            "class_name": "JoinGroupRequestBuilder",
                            "similarity_score": 0.20455960047249264
                        },
                        {
                            "class_name": "JoinResult",
                            "similarity_score": 0.2636352520041483
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Assertions",
                        "MockPartitionAssignor",
                        "NoOpPartitionAssignor"
                    ],
                    "llm_response_time": 2786,
                    "similarity_computation_time": 48,
                    "similarity_metric": "cosine"
                },
                "verifyDescribeGroupsReturnsDeadGroup": {
                    "target_classes": [
                        {
                            "class_name": "MockTime",
                            "similarity_score": 0.36817560849209524
                        },
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.24900742200520393
                        },
                        {
                            "class_name": "GroupCoordinatorMetricsShard",
                            "similarity_score": 0.26621786583356427
                        },
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.1763284455893675
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GroupCoordinatorMetricsShard",
                        "MockTime",
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 4076,
                    "similarity_computation_time": 12,
                    "similarity_metric": "cosine"
                },
                "build": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3595,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "createClassicGroup": {
                    "target_classes": [
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.2217300042423741
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GroupMetadataManager"
                    ],
                    "llm_response_time": 10804,
                    "similarity_computation_time": 10,
                    "similarity_metric": "cosine"
                },
                "onUnloaded": {
                    "target_classes": [
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.239443772869678
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GroupMetadataManager"
                    ],
                    "llm_response_time": 9997,
                    "similarity_computation_time": 11,
                    "similarity_metric": "cosine"
                },
                "idempotentCreateSnapshot": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 1717,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from private replay(producerId long, record CoordinatorRecord) : void in class org.apache.kafka.coordinator.group.OffsetMetadataManagerTest.OffsetMetadataManagerTestContext & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 480,
                    "endLine": 509,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private replay(producerId long, record CoordinatorRecord) : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 484,
                    "endLine": 484,
                    "startColumn": 13,
                    "endColumn": 69,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 480,
                    "endLine": 509,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private replay(producerId long, record CoordinatorRecord) : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 484,
                    "endLine": 484,
                    "startColumn": 13,
                    "endColumn": 73,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(lastWrittenOffset)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 607,
        "extraction_results": {
            "success": true,
            "newCommitHash": "e62eecd5e8915b6e6f83df0729e7ecd1b5103dce",
            "newBranchName": "extract-idempotentCreateSnapshot-replay-130af38"
        },
        "telemetry": {
            "id": "74b25e25-cc3c-4d23-9f59-76784d02fce4",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 3100,
                "lineStart": 96,
                "lineEnd": 3195,
                "bodyLineStart": 96,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                "sourceCode": "public class OffsetMetadataManagerTest {\n    static class OffsetMetadataManagerTestContext {\n        public static class Builder {\n            private final MockTime time = new MockTime();\n            private final MockCoordinatorTimer<Void, CoordinatorRecord> timer = new MockCoordinatorTimer<>(time);\n            private final LogContext logContext = new LogContext();\n            private final SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n            private GroupMetadataManager groupMetadataManager = null;\n            private MetadataImage metadataImage = null;\n            private GroupCoordinatorConfig config = null;\n            private GroupCoordinatorMetricsShard metrics = mock(GroupCoordinatorMetricsShard.class);\n\n            Builder withOffsetMetadataMaxSize(int offsetMetadataMaxSize) {\n                config = GroupCoordinatorConfigTest.createGroupCoordinatorConfig(offsetMetadataMaxSize, 60000L, 24 * 60);\n                return this;\n            }\n\n            Builder withOffsetsRetentionMinutes(int offsetsRetentionMinutes) {\n                config = GroupCoordinatorConfigTest.createGroupCoordinatorConfig(4096, 60000L, offsetsRetentionMinutes);\n                return this;\n            }\n\n            Builder withGroupMetadataManager(GroupMetadataManager groupMetadataManager) {\n                this.groupMetadataManager = groupMetadataManager;\n                return this;\n            }\n\n            OffsetMetadataManagerTestContext build() {\n                if (metadataImage == null) metadataImage = MetadataImage.EMPTY;\n                if (config == null) {\n                    config = GroupCoordinatorConfigTest.createGroupCoordinatorConfig(4096, 60000L, 24);\n                }\n\n                if (groupMetadataManager == null) {\n                    groupMetadataManager = new GroupMetadataManager.Builder()\n                        .withTime(time)\n                        .withTimer(timer)\n                        .withSnapshotRegistry(snapshotRegistry)\n                        .withLogContext(logContext)\n                        .withMetadataImage(metadataImage)\n                        .withConsumerGroupAssignors(Collections.singletonList(new RangeAssignor()))\n                        .withGroupCoordinatorMetricsShard(metrics)\n                        .build();\n                }\n\n                OffsetMetadataManager offsetMetadataManager = new OffsetMetadataManager.Builder()\n                    .withTime(time)\n                    .withLogContext(logContext)\n                    .withSnapshotRegistry(snapshotRegistry)\n                    .withMetadataImage(metadataImage)\n                    .withGroupMetadataManager(groupMetadataManager)\n                    .withGroupCoordinatorConfig(config)\n                    .withGroupCoordinatorMetricsShard(metrics)\n                    .build();\n\n                return new OffsetMetadataManagerTestContext(\n                    time,\n                    timer,\n                    snapshotRegistry,\n                    metrics,\n                    groupMetadataManager,\n                    offsetMetadataManager\n                );\n            }\n        }\n\n        final MockTime time;\n        final MockCoordinatorTimer<Void, CoordinatorRecord> timer;\n        final SnapshotRegistry snapshotRegistry;\n        final GroupCoordinatorMetricsShard metrics;\n        final GroupMetadataManager groupMetadataManager;\n        final OffsetMetadataManager offsetMetadataManager;\n\n        long lastCommittedOffset = 0L;\n        long lastWrittenOffset = 0L;\n\n        OffsetMetadataManagerTestContext(\n            MockTime time,\n            MockCoordinatorTimer<Void, CoordinatorRecord> timer,\n            SnapshotRegistry snapshotRegistry,\n            GroupCoordinatorMetricsShard metrics,\n            GroupMetadataManager groupMetadataManager,\n            OffsetMetadataManager offsetMetadataManager\n        ) {\n            this.time = time;\n            this.timer = timer;\n            this.snapshotRegistry = snapshotRegistry;\n            this.metrics = metrics;\n            this.groupMetadataManager = groupMetadataManager;\n            this.offsetMetadataManager = offsetMetadataManager;\n        }\n\n        public Group getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        ) {\n            switch (groupType) {\n                case CLASSIC:\n                    return groupMetadataManager.getOrMaybeCreateClassicGroup(\n                        groupId,\n                        true\n                    );\n                case CONSUMER:\n                    return groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n                        groupId,\n                        true\n                    );\n                default:\n                    throw new IllegalArgumentException(\"Invalid group type: \" + groupType);\n            }\n        }\n\n        public void commit() {\n            long lastCommittedOffset = this.lastCommittedOffset;\n            this.lastCommittedOffset = lastWrittenOffset;\n            snapshotRegistry.deleteSnapshotsUpTo(lastCommittedOffset);\n        }\n\n        public CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> commitOffset(\n            OffsetCommitRequestData request\n        ) {\n            return commitOffset(ApiKeys.OFFSET_COMMIT.latestVersion(), request);\n        }\n\n        public CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> commitOffset(\n            short version,\n            OffsetCommitRequestData request\n        ) {\n            RequestContext context = new RequestContext(\n                new RequestHeader(\n                    ApiKeys.OFFSET_COMMIT,\n                    version,\n                    \"client\",\n                    0\n                ),\n                \"1\",\n                InetAddress.getLoopbackAddress(),\n                KafkaPrincipal.ANONYMOUS,\n                ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n                SecurityProtocol.PLAINTEXT,\n                ClientInformation.EMPTY,\n                false\n            );\n\n            CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> result = offsetMetadataManager.commitOffset(\n                context,\n                request\n            );\n\n            result.records().forEach(this::replay);\n            return result;\n        }\n\n        public CoordinatorResult<TxnOffsetCommitResponseData, CoordinatorRecord> commitTransactionalOffset(\n            TxnOffsetCommitRequestData request\n        ) {\n            RequestContext context = new RequestContext(\n                new RequestHeader(\n                    ApiKeys.TXN_OFFSET_COMMIT,\n                    ApiKeys.TXN_OFFSET_COMMIT.latestVersion(),\n                    \"client\",\n                    0\n                ),\n                \"1\",\n                InetAddress.getLoopbackAddress(),\n                KafkaPrincipal.ANONYMOUS,\n                ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n                SecurityProtocol.PLAINTEXT,\n                ClientInformation.EMPTY,\n                false\n            );\n\n            CoordinatorResult<TxnOffsetCommitResponseData, CoordinatorRecord> result = offsetMetadataManager.commitTransactionalOffset(\n                context,\n                request\n            );\n\n            result.records().forEach(record -> replay(\n                request.producerId(),\n                record\n            ));\n\n            return result;\n        }\n\n        public List<CoordinatorRecord> deletePartitions(\n            List<TopicPartition> topicPartitions\n        ) {\n            List<CoordinatorRecord> records = offsetMetadataManager.onPartitionsDeleted(topicPartitions);\n            records.forEach(this::replay);\n            return records;\n        }\n\n        public CoordinatorResult<OffsetDeleteResponseData, CoordinatorRecord> deleteOffsets(\n            OffsetDeleteRequestData request\n        ) {\n            CoordinatorResult<OffsetDeleteResponseData, CoordinatorRecord> result = offsetMetadataManager.deleteOffsets(request);\n            result.records().forEach(this::replay);\n            return result;\n        }\n\n        public int deleteAllOffsets(\n            String groupId,\n            List<CoordinatorRecord> records\n        ) {\n            List<CoordinatorRecord> addedRecords = new ArrayList<>();\n            int numDeletedOffsets = offsetMetadataManager.deleteAllOffsets(groupId, addedRecords);\n            addedRecords.forEach(this::replay);\n\n            records.addAll(addedRecords);\n            return numDeletedOffsets;\n        }\n\n        public boolean cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records) {\n            List<CoordinatorRecord> addedRecords = new ArrayList<>();\n            boolean isOffsetsEmptyForGroup = offsetMetadataManager.cleanupExpiredOffsets(groupId, addedRecords);\n            addedRecords.forEach(this::replay);\n\n            records.addAll(addedRecords);\n            return isOffsetsEmptyForGroup;\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        ) {\n            return fetchOffsets(\n                groupId,\n                null,\n                -1,\n                topics,\n                committedOffset\n            );\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        ) {\n            OffsetFetchResponseData.OffsetFetchResponseGroup response = offsetMetadataManager.fetchOffsets(\n                new OffsetFetchRequestData.OffsetFetchRequestGroup()\n                    .setGroupId(groupId)\n                    .setMemberId(memberId)\n                    .setMemberEpoch(memberEpoch)\n                    .setTopics(topics),\n                committedOffset\n            );\n            assertEquals(groupId, response.groupId());\n            return response.topics();\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        ) {\n            return fetchAllOffsets(\n                groupId,\n                null,\n                -1,\n                committedOffset\n            );\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchAllOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            long committedOffset\n        ) {\n            OffsetFetchResponseData.OffsetFetchResponseGroup response = offsetMetadataManager.fetchAllOffsets(\n                new OffsetFetchRequestData.OffsetFetchRequestGroup()\n                    .setGroupId(groupId)\n                    .setMemberId(memberId)\n                    .setMemberEpoch(memberEpoch),\n                committedOffset\n            );\n            assertEquals(groupId, response.groupId());\n            return response.topics();\n        }\n\n        public List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> sleep(long ms) {\n            time.sleep(ms);\n            List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts = timer.poll();\n            timeouts.forEach(timeout -> {\n                if (timeout.result.replayRecords()) {\n                    timeout.result.records().forEach(this::replay);\n                }\n            });\n            return timeouts;\n        }\n\n        public void commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        ) {\n            commitOffset(\n                groupId,\n                topic,\n                partition,\n                offset,\n                leaderEpoch,\n                time.milliseconds()\n            );\n        }\n\n        public void commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        ) {\n            commitOffset(\n                RecordBatch.NO_PRODUCER_ID,\n                groupId,\n                topic,\n                partition,\n                offset,\n                leaderEpoch,\n                commitTimestamp\n            );\n        }\n\n        public void commitOffset(\n            long producerId,\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        ) {\n            replay(producerId, CoordinatorRecordHelpers.newOffsetCommitRecord(\n                groupId,\n                topic,\n                partition,\n                new OffsetAndMetadata(\n                    offset,\n                    OptionalInt.of(leaderEpoch),\n                    \"metadata\",\n                    commitTimestamp,\n                    OptionalLong.empty()\n                ),\n                MetadataVersion.latestTesting()\n            ));\n        }\n\n        public void deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        ) {\n            replay(CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\n                groupId,\n                topic,\n                partition\n            ));\n        }\n\n        private ApiMessage messageOrNull(ApiMessageAndVersion apiMessageAndVersion) {\n            if (apiMessageAndVersion == null) {\n                return null;\n            } else {\n                return apiMessageAndVersion.message();\n            }\n        }\n\n        private void replay(\n            CoordinatorRecord record\n        ) {\n            replay(\n                RecordBatch.NO_PRODUCER_ID,\n                record\n            );\n        }\n\n        private void replay(\n            long producerId,\n            CoordinatorRecord record\n        ) {\n            idempotentCreateSnapshot();\n\n            ApiMessageAndVersion key = record.key();\n            ApiMessageAndVersion value = record.value();\n\n            if (key == null) {\n                throw new IllegalStateException(\"Received a null key in \" + record);\n            }\n\n            switch (key.version()) {\n                case OffsetCommitKey.HIGHEST_SUPPORTED_VERSION:\n                    offsetMetadataManager.replay(\n                        lastWrittenOffset,\n                        producerId,\n                        (OffsetCommitKey) key.message(),\n                        (OffsetCommitValue) messageOrNull(value)\n                    );\n                    break;\n\n                default:\n                    throw new IllegalStateException(\"Received an unknown record type \" + key.version()\n                        + \" in \" + record);\n            }\n\n            lastWrittenOffset++;\n        }\n\n        private void idempotentCreateSnapshot() {\n            snapshotRegistry.getOrCreateSnapshot(lastWrittenOffset);\n        }\n\n        private void replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        ) {\n            idempotentCreateSnapshot();\n            offsetMetadataManager.replayEndTransactionMarker(producerId, result);\n            lastWrittenOffset++;\n        }\n\n        public void testOffsetDeleteWith(\n            String groupId,\n            String topic,\n            int partition,\n            Errors expectedError\n        ) {\n            final OffsetDeleteRequestData.OffsetDeleteRequestTopicCollection requestTopicCollection =\n                new OffsetDeleteRequestData.OffsetDeleteRequestTopicCollection(Collections.singletonList(\n                    new OffsetDeleteRequestData.OffsetDeleteRequestTopic()\n                        .setName(topic)\n                        .setPartitions(Collections.singletonList(\n                            new OffsetDeleteRequestData.OffsetDeleteRequestPartition().setPartitionIndex(partition)\n                        ))\n                ).iterator());\n\n            final OffsetDeleteResponseData.OffsetDeleteResponsePartitionCollection expectedResponsePartitionCollection =\n                new OffsetDeleteResponseData.OffsetDeleteResponsePartitionCollection();\n            expectedResponsePartitionCollection.add(\n                new OffsetDeleteResponseData.OffsetDeleteResponsePartition()\n                    .setPartitionIndex(partition)\n                    .setErrorCode(expectedError.code())\n            );\n\n            final OffsetDeleteResponseData.OffsetDeleteResponseTopicCollection expectedResponseTopicCollection =\n                new OffsetDeleteResponseData.OffsetDeleteResponseTopicCollection(Collections.singletonList(\n                    new OffsetDeleteResponseData.OffsetDeleteResponseTopic()\n                        .setName(topic)\n                        .setPartitions(expectedResponsePartitionCollection)\n                ).iterator());\n\n            List<CoordinatorRecord> expectedRecords = Collections.emptyList();\n            if (hasOffset(groupId, topic, partition) && expectedError == Errors.NONE) {\n                expectedRecords = Collections.singletonList(\n                    CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(groupId, topic, partition)\n                );\n            }\n\n            final CoordinatorResult<OffsetDeleteResponseData, CoordinatorRecord> coordinatorResult = deleteOffsets(\n                new OffsetDeleteRequestData()\n                    .setGroupId(groupId)\n                    .setTopics(requestTopicCollection)\n            );\n\n            assertEquals(new OffsetDeleteResponseData().setTopics(expectedResponseTopicCollection), coordinatorResult.response());\n            assertEquals(expectedRecords, coordinatorResult.records());\n        }\n\n        public boolean hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        ) {\n            return offsetMetadataManager.hasCommittedOffset(groupId, topic, partition) ||\n                offsetMetadataManager.hasPendingTransactionalOffsets(groupId, topic, partition);\n        }\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testOffsetCommitWithUnknownGroup(short version) {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        Class<? extends Throwable> expectedType;\n        if (version >= 9) {\n            expectedType = GroupIdNotFoundException.class;\n        } else {\n            expectedType = IllegalGenerationException.class;\n        }\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(expectedType, () -> context.commitOffset(\n            version,\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithDeadGroup() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a dead group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n        group.transitionTo(ClassicGroupState.DEAD);\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(CoordinatorNotAvailableException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithUnknownMemberId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(UnknownMemberIdException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithIllegalGeneration() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(IllegalGenerationException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithUnknownInstanceId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member without static id.\n        group.add(mkGenericMember(\"member\", Optional.empty()));\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(UnknownMemberIdException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGroupInstanceId(\"instanceid\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithFencedInstanceId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member with static id.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(UnknownMemberIdException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGroupInstanceId(\"old-instance-id\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWhileInCompletingRebalanceState() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(RebalanceInProgressException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(1)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithoutMemberIdAndGeneration() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(UnknownMemberIdException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithRetentionTime() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n        group.transitionTo(ClassicGroupState.STABLE);\n\n        CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(1)\n                .setRetentionTimeMs(1234L)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(CoordinatorRecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.empty(),\n                    \"\",\n                    context.time.milliseconds(),\n                    OptionalLong.of(context.time.milliseconds() + 1234L)\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitMaintainsSession() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        ClassicGroupMember member = mkGenericMember(\"member\", Optional.empty());\n        group.add(member);\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n        group.transitionTo(ClassicGroupState.STABLE);\n\n        // Schedule session timeout. This would be normally done when\n        // the group transitions to stable.\n        context.groupMetadataManager.rescheduleClassicGroupMemberHeartbeat(group, member);\n\n        // Advance time by half of the session timeout. No timeouts are\n        // expired.\n        assertEquals(Collections.emptyList(), context.sleep(5000 / 2));\n\n        // Commit.\n        context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(1)\n                .setRetentionTimeMs(1234L)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n        );\n\n        // Advance time by half of the session timeout. No timeouts are\n        // expired.\n        assertEquals(Collections.emptyList(), context.sleep(5000 / 2));\n\n        // Advance time by half of the session timeout again. The timeout should\n        // expire and the member is removed from the group.\n        List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts =\n            context.sleep(5000 / 2);\n        assertEquals(1, timeouts.size());\n        assertFalse(group.hasMember(member.memberId()));\n    }\n\n    @Test\n    public void testSimpleGroupOffsetCommit() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(CoordinatorRecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.empty(),\n                    \"\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n\n        // A generic should have been created.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            false\n        );\n        assertNotNull(group);\n        assertEquals(\"foo\", group.groupId());\n    }\n\n    @Test\n    public void testSimpleGroupOffsetCommitWithInstanceId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                // Instance id should be ignored.\n                .setGroupInstanceId(\"instance-id\")\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(CoordinatorRecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.empty(),\n                    \"\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testConsumerGroupOffsetCommitWithUnknownMemberId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(UnknownMemberIdException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testConsumerGroupOffsetCommitWithStaleMemberEpoch() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .build()\n        );\n\n        OffsetCommitRequestData request = new OffsetCommitRequestData()\n            .setGroupId(\"foo\")\n            .setMemberId(\"member\")\n            .setGenerationIdOrMemberEpoch(9)\n            .setTopics(Collections.singletonList(\n                new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                    .setName(\"bar\")\n                    .setPartitions(Collections.singletonList(\n                        new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                            .setPartitionIndex(0)\n                            .setCommittedOffset(100L)\n                    ))\n            ));\n\n        // Verify that a smaller epoch is rejected.\n        assertThrows(StaleMemberEpochException.class, () -> context.commitOffset(request));\n\n        // Verify that a larger epoch is rejected.\n        request.setGenerationIdOrMemberEpoch(11);\n        assertThrows(StaleMemberEpochException.class, () -> context.commitOffset(request));\n    }\n\n    @Test\n    public void testConsumerGroupOffsetCommitWithIllegalGenerationId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata())\n            .build()\n        );\n\n        OffsetCommitRequestData request = new OffsetCommitRequestData()\n            .setGroupId(\"foo\")\n            .setMemberId(\"member\")\n            .setGenerationIdOrMemberEpoch(9)\n            .setTopics(Collections.singletonList(\n                new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                    .setName(\"bar\")\n                    .setPartitions(Collections.singletonList(\n                        new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                            .setPartitionIndex(0)\n                            .setCommittedOffset(100L)\n                    ))\n            ));\n\n        // Verify that a smaller epoch is rejected.\n        assertThrows(IllegalGenerationException.class, () -> context.commitOffset(request));\n\n        // Verify that a larger epoch is rejected.\n        request.setGenerationIdOrMemberEpoch(11);\n        assertThrows(IllegalGenerationException.class, () -> context.commitOffset(request));\n    }\n\n    @Test\n    public void testConsumerGroupOffsetCommitFromAdminClient() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(CoordinatorRecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.empty(),\n                    \"\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testConsumerGroupOffsetCommit() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .build()\n        );\n\n        CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                                .setCommitTimestamp(context.time.milliseconds())\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(CoordinatorRecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.of(10),\n                    \"metadata\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testConsumerGroupOffsetCommitWithOffsetMetadataTooLarge() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .withOffsetMetadataMaxSize(5)\n            .build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .build()\n        );\n\n        CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Arrays.asList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"toolarge\")\n                                .setCommitTimestamp(context.time.milliseconds()),\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(1)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"small\")\n                                .setCommitTimestamp(context.time.milliseconds())\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Arrays.asList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.OFFSET_METADATA_TOO_LARGE.code()),\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(1)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(CoordinatorRecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                1,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.of(10),\n                    \"small\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testConsumerGroupTransactionalOffsetCommit() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .build()\n        );\n\n        CoordinatorResult<TxnOffsetCommitResponseData, CoordinatorRecord> result = context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(10)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new TxnOffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitResponseData.TxnOffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitResponseData.TxnOffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(CoordinatorRecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.of(10),\n                    \"metadata\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testConsumerGroupTransactionalOffsetCommitWithUnknownGroupId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        assertThrows(IllegalGenerationException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(10)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testConsumerGroupTransactionalOffsetCommitWithUnknownMemberId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        assertThrows(UnknownMemberIdException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(10)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testConsumerGroupTransactionalOffsetCommitWithStaleMemberEpoch() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .build()\n        );\n\n        assertThrows(IllegalGenerationException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(100)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testGenericGroupTransactionalOffsetCommit() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        ClassicGroupMember member = mkGenericMember(\"member\", Optional.empty());\n        group.add(member);\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n        group.transitionTo(ClassicGroupState.STABLE);\n\n        CoordinatorResult<TxnOffsetCommitResponseData, CoordinatorRecord> result = context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(1)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new TxnOffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitResponseData.TxnOffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitResponseData.TxnOffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(CoordinatorRecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.of(10),\n                    \"metadata\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testGenericGroupTransactionalOffsetCommitWithUnknownGroupId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        assertThrows(IllegalGenerationException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(10)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testGenericGroupTransactionalOffsetCommitWithUnknownMemberId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        assertThrows(UnknownMemberIdException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(10)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testGenericGroupTransactionalOffsetCommitWithIllegalGenerationId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        ClassicGroupMember member = mkGenericMember(\"member\", Optional.empty());\n        group.add(member);\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n        group.transitionTo(ClassicGroupState.STABLE);\n\n        assertThrows(IllegalGenerationException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(100)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testGenericGroupFetchOffsetsWithDeadGroup() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a dead group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"group\",\n            true\n        );\n        group.transitionTo(ClassicGroupState.DEAD);\n\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> request = Arrays.asList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Arrays.asList(0, 1)),\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"bar\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        List<OffsetFetchResponseData.OffsetFetchResponseTopics> expectedResponse = Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Collections.singletonList(\n                    mkInvalidOffsetPartitionResponse(0)\n                ))\n        );\n\n        assertEquals(expectedResponse, context.fetchOffsets(\"group\", request, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchOffsetsWithUnknownGroup() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> request = Arrays.asList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Arrays.asList(0, 1)),\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"bar\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        List<OffsetFetchResponseData.OffsetFetchResponseTopics> expectedResponse = Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Collections.singletonList(\n                    mkInvalidOffsetPartitionResponse(0)\n                ))\n        );\n\n        assertEquals(expectedResponse, context.fetchOffsets(\"group\", request, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchOffsetsAtDifferentCommittedOffset() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\"group\", true);\n\n        assertEquals(0, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n        assertEquals(1, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 1, 110L, 1);\n        assertEquals(2, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"bar\", 0, 200L, 1);\n        assertEquals(3, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 1, 111L, 2);\n        assertEquals(4, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"bar\", 1, 210L, 2);\n        assertEquals(5, context.lastWrittenOffset);\n\n        // Always use the same request.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> request = Arrays.asList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Arrays.asList(0, 1)),\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"bar\")\n                .setPartitionIndexes(Arrays.asList(0, 1))\n        );\n\n        // Fetching with 0 should return all invalid offsets.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, 0L));\n\n        // Fetching with 1 should return data up to offset 1.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkInvalidOffsetPartitionResponse(1)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, 1L));\n\n        // Fetching with 2 should return data up to offset 2.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, 2L));\n\n        // Fetching with 3 should return data up to offset 3.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, 3L));\n\n        // Fetching with 4 should return data up to offset 4.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 2, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, 4L));\n\n        // Fetching with 5 should return data up to offset 5.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 2, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 210L, 2, \"metadata\")\n                ))\n        ), context.fetchOffsets(\"group\", request, 5L));\n\n        // Fetching with Long.MAX_VALUE should return all offsets.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 2, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 210L, 2, \"metadata\")\n                ))\n        ), context.fetchOffsets(\"group\", request, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchOffsetsWithPendingTransactionalOffsets() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\"group\", true);\n\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n        context.commitOffset(\"group\", \"foo\", 1, 110L, 1);\n        context.commitOffset(\"group\", \"bar\", 0, 200L, 1);\n\n        context.commit();\n\n        assertEquals(3, context.lastWrittenOffset);\n        assertEquals(3, context.lastCommittedOffset);\n\n        context.commitOffset(10L, \"group\", \"foo\", 1, 111L, 1, context.time.milliseconds());\n        context.commitOffset(10L, \"group\", \"bar\", 0, 201L, 1, context.time.milliseconds());\n        // Note that bar-1 does not exist in the initial commits. UNSTABLE_OFFSET_COMMIT errors\n        // must be returned in this case too.\n        context.commitOffset(10L, \"group\", \"bar\", 1, 211L, 1, context.time.milliseconds());\n\n        // Always use the same request.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> request = Arrays.asList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Arrays.asList(0, 1)),\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"bar\")\n                .setPartitionIndexes(Arrays.asList(0, 1))\n        );\n\n        // Fetching offsets with \"require stable\" (Long.MAX_VALUE) should return the committed offset for\n        // foo-0 and the UNSTABLE_OFFSET_COMMIT error for foo-1, bar-0 and bar-1.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, Errors.UNSTABLE_OFFSET_COMMIT)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, Errors.UNSTABLE_OFFSET_COMMIT),\n                    mkOffsetPartitionResponse(1, Errors.UNSTABLE_OFFSET_COMMIT)\n                ))\n        ), context.fetchOffsets(\"group\", request, Long.MAX_VALUE));\n\n        // Fetching offsets without \"require stable\" (lastCommittedOffset) should return the committed\n        // offset for foo-0, foo-1 and bar-0 and the INVALID_OFFSET for bar-1.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, context.lastCommittedOffset));\n\n        // Commit the ongoing transaction.\n        context.replayEndTransactionMarker(10L, TransactionResult.COMMIT);\n\n        // Fetching offsets with \"require stable\" (Long.MAX_VALUE) should not return any errors now.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 201L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 211L, 1, \"metadata\")\n                ))\n        ), context.fetchOffsets(\"group\", request, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testGenericGroupFetchAllOffsetsWithDeadGroup() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a dead group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"group\",\n            true\n        );\n        group.transitionTo(ClassicGroupState.DEAD);\n\n        assertEquals(Collections.emptyList(), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchAllOffsetsWithUnknownGroup() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        assertEquals(Collections.emptyList(), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchAllOffsetsAtDifferentCommittedOffset() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\"group\", true);\n\n        assertEquals(0, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n        assertEquals(1, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 1, 110L, 1);\n        assertEquals(2, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"bar\", 0, 200L, 1);\n        assertEquals(3, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 1, 111L, 2);\n        assertEquals(4, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"bar\", 1, 210L, 2);\n        assertEquals(5, context.lastWrittenOffset);\n\n        // Fetching with 0 should no offsets.\n        assertEquals(Collections.emptyList(), context.fetchAllOffsets(\"group\", 0L));\n\n        // Fetching with 1 should return data up to offset 1.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", 1L));\n\n        // Fetching with 2 should return data up to offset 2.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", 2L));\n\n        // Fetching with 3 should return data up to offset 3.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", 3L));\n\n        // Fetching with 4 should return data up to offset 4.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 2, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", 4L));\n\n        // Fetching with Long.MAX_VALUE should return all offsets.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 210L, 2, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 2, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchAllOffsetsWithPendingTransactionalOffsets() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\"group\", true);\n\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n        context.commitOffset(\"group\", \"foo\", 1, 110L, 1);\n        context.commitOffset(\"group\", \"bar\", 0, 200L, 1);\n\n        context.commit();\n\n        assertEquals(3, context.lastWrittenOffset);\n        assertEquals(3, context.lastCommittedOffset);\n\n        context.commitOffset(10L, \"group\", \"foo\", 1, 111L, 1, context.time.milliseconds());\n        context.commitOffset(10L, \"group\", \"bar\", 0, 201L, 1, context.time.milliseconds());\n        // Note that bar-1 does not exist in the initial commits. The API does not return it at all until\n        // the transaction is committed.\n        context.commitOffset(10L, \"group\", \"bar\", 1, 211L, 1, context.time.milliseconds());\n\n        // Fetching offsets with \"require stable\" (Long.MAX_VALUE) should return the committed offset for\n        // foo-0 and the UNSTABLE_OFFSET_COMMIT error for foo-1 and bar-0.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, Errors.UNSTABLE_OFFSET_COMMIT)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, Errors.UNSTABLE_OFFSET_COMMIT)\n                ))\n        ), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n\n        // Fetching offsets without \"require stable\" (lastCommittedOffset) should the committed\n        // offset for the foo-0, foo-1 and bar-0.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", context.lastCommittedOffset));\n\n        // Commit the ongoing transaction.\n        context.replayEndTransactionMarker(10L, TransactionResult.COMMIT);\n\n        // Fetching offsets with \"require stable\" (Long.MAX_VALUE) should not return any errors now.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 201L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 211L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testConsumerGroupOffsetFetchWithMemberIdAndEpoch() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        // Create consumer group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\"group\", true);\n        // Create member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\").build());\n        // Commit offset.\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n\n        // Fetch offsets case.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics = Collections.singletonList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        assertEquals(Collections.singletonList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Collections.singletonList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\")\n                ))\n        ), context.fetchOffsets(\"group\", \"member\", 0, topics, Long.MAX_VALUE));\n\n        // Fetch all offsets case.\n        assertEquals(Collections.singletonList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Collections.singletonList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", \"member\", 0, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testConsumerGroupOffsetFetchFromAdminClient() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        // Create consumer group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\"group\", true);\n        // Create member.\n        group.getOrMaybeCreateMember(\"member\", true);\n        // Commit offset.\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n\n        // Fetch offsets case.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics = Collections.singletonList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        assertEquals(Collections.singletonList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Collections.singletonList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\")\n                ))\n        ), context.fetchOffsets(\"group\", topics, Long.MAX_VALUE));\n\n        // Fetch all offsets case.\n        assertEquals(Collections.singletonList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Collections.singletonList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testConsumerGroupOffsetFetchWithUnknownMemberId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\"group\", true);\n\n        // Fetch offsets case.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics = Collections.singletonList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        // Fetch offsets cases.\n        assertThrows(UnknownMemberIdException.class,\n            () -> context.fetchOffsets(\"group\", \"\", 0, topics, Long.MAX_VALUE));\n        assertThrows(UnknownMemberIdException.class,\n            () -> context.fetchOffsets(\"group\", \"member\", 0, topics, Long.MAX_VALUE));\n\n        // Fetch all offsets cases.\n        assertThrows(UnknownMemberIdException.class,\n            () -> context.fetchAllOffsets(\"group\", \"\", 0, Long.MAX_VALUE));\n        assertThrows(UnknownMemberIdException.class,\n            () -> context.fetchAllOffsets(\"group\", \"member\", 0, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testConsumerGroupOffsetFetchWithStaleMemberEpoch() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\"group\", true);\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\").build());\n\n        // Fetch offsets case.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics = Collections.singletonList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        // Fetch offsets case.\n        assertThrows(StaleMemberEpochException.class,\n            () -> context.fetchOffsets(\"group\", \"member\", 10, topics, Long.MAX_VALUE));\n\n        // Fetch all offsets case.\n        assertThrows(StaleMemberEpochException.class,\n            () -> context.fetchAllOffsets(\"group\", \"member\", 10, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testConsumerGroupOffsetFetchWithIllegalGenerationId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\"group\", true);\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n                .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata())\n                .build()\n        );\n\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics = Collections.singletonList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        // Fetch offsets case.\n        assertThrows(IllegalGenerationException.class,\n            () -> context.fetchOffsets(\"group\", \"member\", 10, topics, Long.MAX_VALUE));\n\n        // Fetch all offsets case.\n        assertThrows(IllegalGenerationException.class,\n            () -> context.fetchAllOffsets(\"group\", \"member\", 10, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testGenericGroupOffsetDelete() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n        context.commitOffset(\"foo\", \"bar\", 0, 100L, 0);\n        group.setSubscribedTopics(Optional.of(Collections.emptySet()));\n        context.testOffsetDeleteWith(\"foo\", \"bar\", 0, Errors.NONE);\n        assertFalse(context.hasOffset(\"foo\", \"bar\", 0));\n    }\n\n    @Test\n    public void testGenericGroupOffsetDeleteWithErrors() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n        group.setSubscribedTopics(Optional.of(Collections.singleton(\"bar\")));\n        context.commitOffset(\"foo\", \"bar\", 0, 100L, 0);\n\n        // Delete the offset whose topic partition doesn't exist.\n        context.testOffsetDeleteWith(\"foo\", \"bar1\", 0, Errors.NONE);\n        // Delete the offset from the topic that the group is subscribed to.\n        context.testOffsetDeleteWith(\"foo\", \"bar\", 0, Errors.GROUP_SUBSCRIBED_TO_TOPIC);\n    }\n\n    @Test\n    public void testGenericGroupOffsetDeleteWithPendingTransactionalOffsets() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n        context.commitOffset(10L, \"foo\", \"bar\", 0, 100L, 0, context.time.milliseconds());\n        group.setSubscribedTopics(Optional.of(Collections.emptySet()));\n        context.testOffsetDeleteWith(\"foo\", \"bar\", 0, Errors.NONE);\n        assertFalse(context.hasOffset(\"foo\", \"bar\", 0));\n    }\n\n    @Test\n    public void testConsumerGroupOffsetDelete() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n            \"foo\",\n            true\n        );\n        context.commitOffset(\"foo\", \"bar\", 0, 100L, 0);\n        assertFalse(group.isSubscribedToTopic(\"bar\"));\n        context.testOffsetDeleteWith(\"foo\", \"bar\", 0, Errors.NONE);\n    }\n\n    @Test\n    public void testConsumerGroupOffsetDeleteWithErrors() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n            \"foo\",\n            true\n        );\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(Uuid.randomUuid(), \"foo\", 1)\n            .addRacks()\n            .build();\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n        group.computeSubscriptionMetadata(\n            group.computeSubscribedTopicNames(null, member1),\n            image.topics(),\n            image.cluster()\n        );\n        group.updateMember(member1);\n        context.commitOffset(\"foo\", \"bar\", 0, 100L, 0);\n        assertTrue(group.isSubscribedToTopic(\"bar\"));\n\n        // Delete the offset whose topic partition doesn't exist.\n        context.testOffsetDeleteWith(\"foo\", \"bar1\", 0, Errors.NONE);\n        // Delete the offset from the topic that the group is subscribed to.\n        context.testOffsetDeleteWith(\"foo\", \"bar\", 0, Errors.GROUP_SUBSCRIBED_TO_TOPIC);\n    }\n\n    @Test\n    public void testConsumerGroupOffsetDeleteWithPendingTransactionalOffsets() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n            \"foo\",\n            true\n        );\n        context.commitOffset(10L, \"foo\", \"bar\", 0, 100L, 0, context.time.milliseconds());\n        assertFalse(group.isSubscribedToTopic(\"bar\"));\n        context.testOffsetDeleteWith(\"foo\", \"bar\", 0, Errors.NONE);\n        assertFalse(context.hasOffset(\"foo\", \"bar\", 0));\n    }\n\n    @ParameterizedTest\n    @EnumSource(value = Group.GroupType.class, names = {\"CLASSIC\", \"CONSUMER\"})\n    public void testDeleteGroupAllOffsets(Group.GroupType groupType) {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        context.getOrMaybeCreateGroup(groupType, \"foo\");\n\n        context.commitOffset(\"foo\", \"bar-0\", 0, 100L, 0);\n        context.commitOffset(\"foo\", \"bar-0\", 1, 100L, 0);\n        context.commitOffset(\"foo\", \"bar-1\", 0, 100L, 0);\n\n        List<CoordinatorRecord> expectedRecords = Arrays.asList(\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"foo\", \"bar-1\", 0),\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"foo\", \"bar-0\", 0),\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"foo\", \"bar-0\", 1)\n        );\n\n        List<CoordinatorRecord> records = new ArrayList<>();\n        int numDeleteOffsets = context.deleteAllOffsets(\"foo\", records);\n\n        assertEquals(expectedRecords, records);\n        assertEquals(3, numDeleteOffsets);\n    }\n\n    @ParameterizedTest\n    @EnumSource(value = Group.GroupType.class, names = {\"CLASSIC\", \"CONSUMER\"})\n    public void testDeleteGroupAllOffsetsWithPendingTransactionalOffsets(Group.GroupType groupType) {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        context.getOrMaybeCreateGroup(groupType, \"foo\");\n\n        context.commitOffset(\"foo\", \"bar-0\", 0, 100L, 0);\n        context.commitOffset(\"foo\", \"bar-0\", 1, 100L, 0);\n        context.commitOffset(\"foo\", \"bar-1\", 0, 100L, 0);\n\n        context.commitOffset(10L, \"foo\", \"bar-1\", 0, 101L, 0, context.time.milliseconds());\n        context.commitOffset(10L, \"foo\", \"bar-2\", 0, 100L, 0, context.time.milliseconds());\n\n        List<CoordinatorRecord> expectedRecords = Arrays.asList(\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"foo\", \"bar-1\", 0),\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"foo\", \"bar-0\", 0),\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"foo\", \"bar-0\", 1),\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"foo\", \"bar-2\", 0)\n        );\n\n        List<CoordinatorRecord> records = new ArrayList<>();\n        int numDeleteOffsets = context.deleteAllOffsets(\"foo\", records);\n\n        assertEquals(expectedRecords, records);\n        assertEquals(4, numDeleteOffsets);\n\n        assertFalse(context.hasOffset(\"foo\", \"bar-0\", 0));\n        assertFalse(context.hasOffset(\"foo\", \"bar-0\", 1));\n        assertFalse(context.hasOffset(\"foo\", \"bar-1\", 0));\n        assertFalse(context.hasOffset(\"foo\", \"bar-2\", 0));\n    }\n\n    @Test\n    public void testCleanupExpiredOffsetsGroupHasNoOffsets() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .build();\n\n        List<CoordinatorRecord> records = new ArrayList<>();\n        assertTrue(context.cleanupExpiredOffsets(\"unknown-group-id\", records));\n        assertEquals(Collections.emptyList(), records);\n    }\n\n    @Test\n    public void testCleanupExpiredOffsetsGroupDoesNotExist() {\n        GroupMetadataManager groupMetadataManager = mock(GroupMetadataManager.class);\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .withGroupMetadataManager(groupMetadataManager)\n            .build();\n\n        when(groupMetadataManager.group(\"unknown-group-id\")).thenThrow(GroupIdNotFoundException.class);\n        context.commitOffset(\"unknown-group-id\", \"topic\", 0, 100L, 0);\n        assertThrows(GroupIdNotFoundException.class, () -> context.cleanupExpiredOffsets(\"unknown-group-id\", new ArrayList<>()));\n    }\n\n    @Test\n    public void testCleanupExpiredOffsetsEmptyOffsetExpirationCondition() {\n        GroupMetadataManager groupMetadataManager = mock(GroupMetadataManager.class);\n        Group group = mock(Group.class);\n\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .withGroupMetadataManager(groupMetadataManager)\n            .build();\n\n        context.commitOffset(\"group-id\", \"topic\", 0, 100L, 0);\n\n        when(groupMetadataManager.group(\"group-id\")).thenReturn(group);\n        when(group.offsetExpirationCondition()).thenReturn(Optional.empty());\n\n        List<CoordinatorRecord> records = new ArrayList<>();\n        assertFalse(context.cleanupExpiredOffsets(\"group-id\", records));\n        assertEquals(Collections.emptyList(), records);\n    }\n\n    @Test\n    public void testCleanupExpiredOffsets() {\n        GroupMetadataManager groupMetadataManager = mock(GroupMetadataManager.class);\n        Group group = mock(Group.class);\n\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .withGroupMetadataManager(groupMetadataManager)\n            .withOffsetsRetentionMinutes(1)\n            .build();\n\n        long commitTimestamp = context.time.milliseconds();\n\n        context.commitOffset(\"group-id\", \"firstTopic\", 0, 100L, 0, commitTimestamp);\n        context.commitOffset(\"group-id\", \"secondTopic\", 0, 100L, 0, commitTimestamp);\n        context.commitOffset(\"group-id\", \"secondTopic\", 1, 100L, 0, commitTimestamp + 500);\n\n        context.time.sleep(Duration.ofMinutes(1).toMillis());\n\n        // firstTopic-0: group is still subscribed to firstTopic. Do not expire.\n        // secondTopic-0: should expire as offset retention has passed.\n        // secondTopic-1: has not passed offset retention. Do not expire.\n        List<CoordinatorRecord> expectedRecords = Collections.singletonList(\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"secondTopic\", 0)\n        );\n\n        when(groupMetadataManager.group(\"group-id\")).thenReturn(group);\n        when(group.offsetExpirationCondition()).thenReturn(Optional.of(\n            new OffsetExpirationConditionImpl(offsetAndMetadata -> offsetAndMetadata.commitTimestampMs)));\n        when(group.isSubscribedToTopic(\"firstTopic\")).thenReturn(true);\n        when(group.isSubscribedToTopic(\"secondTopic\")).thenReturn(false);\n\n        List<CoordinatorRecord> records = new ArrayList<>();\n        assertFalse(context.cleanupExpiredOffsets(\"group-id\", records));\n        assertEquals(expectedRecords, records);\n\n        // Expire secondTopic-1.\n        context.time.sleep(500);\n        expectedRecords = Collections.singletonList(\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"secondTopic\", 1)\n        );\n\n        records = new ArrayList<>();\n        assertFalse(context.cleanupExpiredOffsets(\"group-id\", records));\n        assertEquals(expectedRecords, records);\n\n        // Add 2 more commits, then expire all.\n        when(group.isSubscribedToTopic(\"firstTopic\")).thenReturn(false);\n        context.commitOffset(\"group-id\", \"firstTopic\", 1, 100L, 0, commitTimestamp + 500);\n        context.commitOffset(\"group-id\", \"secondTopic\", 0, 101L, 0, commitTimestamp + 500);\n\n        expectedRecords = Arrays.asList(\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"firstTopic\", 0),\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"firstTopic\", 1),\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"secondTopic\", 0)\n        );\n\n        records = new ArrayList<>();\n        assertTrue(context.cleanupExpiredOffsets(\"group-id\", records));\n        assertEquals(expectedRecords, records);\n    }\n\n    @Test\n    public void testCleanupExpiredOffsetsWithPendingTransactionalOffsets() {\n        GroupMetadataManager groupMetadataManager = mock(GroupMetadataManager.class);\n        Group group = mock(Group.class);\n\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .withGroupMetadataManager(groupMetadataManager)\n            .withOffsetsRetentionMinutes(1)\n            .build();\n\n        long commitTimestamp = context.time.milliseconds();\n\n        context.commitOffset(\"group-id\", \"foo\", 0, 100L, 0, commitTimestamp);\n        context.commitOffset(10L, \"group-id\", \"foo\", 0, 101L, 0, commitTimestamp + 500);\n\n        context.time.sleep(Duration.ofMinutes(1).toMillis());\n\n        when(groupMetadataManager.group(\"group-id\")).thenReturn(group);\n        when(group.offsetExpirationCondition()).thenReturn(Optional.of(\n            new OffsetExpirationConditionImpl(offsetAndMetadata -> offsetAndMetadata.commitTimestampMs)));\n        when(group.isSubscribedToTopic(\"foo\")).thenReturn(false);\n\n        // foo-0 should not be expired because it has a pending transactional offset commit.\n        List<CoordinatorRecord> records = new ArrayList<>();\n        assertFalse(context.cleanupExpiredOffsets(\"group-id\", records));\n        assertEquals(Collections.emptyList(), records);\n    }\n\n    private static OffsetFetchResponseData.OffsetFetchResponsePartitions mkOffsetPartitionResponse(\n        int partition,\n        long offset,\n        int leaderEpoch,\n        String metadata\n    ) {\n        return new OffsetFetchResponseData.OffsetFetchResponsePartitions()\n            .setPartitionIndex(partition)\n            .setCommittedOffset(offset)\n            .setCommittedLeaderEpoch(leaderEpoch)\n            .setMetadata(metadata);\n    }\n\n    private static OffsetFetchResponseData.OffsetFetchResponsePartitions mkInvalidOffsetPartitionResponse(int partition) {\n        return new OffsetFetchResponseData.OffsetFetchResponsePartitions()\n            .setPartitionIndex(partition)\n            .setCommittedOffset(INVALID_OFFSET)\n            .setCommittedLeaderEpoch(-1)\n            .setMetadata(\"\");\n    }\n\n    private static OffsetFetchResponseData.OffsetFetchResponsePartitions mkOffsetPartitionResponse(int partition, Errors error) {\n        return new OffsetFetchResponseData.OffsetFetchResponsePartitions()\n            .setPartitionIndex(partition)\n            .setErrorCode(error.code())\n            .setCommittedOffset(INVALID_OFFSET)\n            .setCommittedLeaderEpoch(-1)\n            .setMetadata(\"\");\n    }\n\n    @Test\n    public void testReplay() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        verifyReplay(context, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            0L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyReplay(context, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            1L,\n            200L,\n            OptionalInt.of(10),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyReplay(context, \"foo\", \"bar\", 1, new OffsetAndMetadata(\n            2L,\n            200L,\n            OptionalInt.of(10),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyReplay(context, \"foo\", \"bar\", 1, new OffsetAndMetadata(\n            3L,\n            300L,\n            OptionalInt.of(10),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.of(12345L)\n        ));\n    }\n\n    @Test\n    public void testTransactionalReplay() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        verifyTransactionalReplay(context, 5, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            0L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyTransactionalReplay(context, 5, \"foo\", \"bar\", 1, new OffsetAndMetadata(\n            1L,\n            101L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyTransactionalReplay(context, 5, \"bar\", \"zar\", 0, new OffsetAndMetadata(\n            2L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyTransactionalReplay(context, 5, \"bar\", \"zar\", 1, new OffsetAndMetadata(\n            3L,\n            101L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyTransactionalReplay(context, 6, \"foo\", \"bar\", 2, new OffsetAndMetadata(\n            4L,\n            102L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyTransactionalReplay(context, 6, \"foo\", \"bar\", 3, new OffsetAndMetadata(\n            5L,\n            102L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n    }\n\n    @Test\n    public void testReplayWithTombstoneAndPendingTransactionalOffsets() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Add the offsets.\n        verifyReplay(context, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            0L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyTransactionalReplay(context, 10L, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            1L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyTransactionalReplay(context, 10L, \"foo\", \"bar\", 1, new OffsetAndMetadata(\n            2L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Delete the offsets.\n        context.replay(CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\n            \"foo\",\n            \"bar\",\n            0\n        ));\n\n        context.replay(CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\n            \"foo\",\n            \"bar\",\n            1\n        ));\n\n        // Verify that the offset is gone.\n        assertFalse(context.hasOffset(\"foo\", \"bar\", 0));\n        assertFalse(context.hasOffset(\"foo\", \"bar\", 1));\n    }\n\n    @Test\n    public void testReplayTransactionEndMarkerWithCommit() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Add regular offset commit.\n        verifyReplay(context, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            0L,\n            99L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Add pending transactional commit for producer id 5.\n        verifyTransactionalReplay(context, 5L, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            1L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Add pending transactional commit for producer id 6.\n        verifyTransactionalReplay(context, 6L, \"foo\", \"bar\", 1, new OffsetAndMetadata(\n            2L,\n            200L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Replaying an end marker with an unknown producer id should not fail.\n        context.replayEndTransactionMarker(1L, TransactionResult.COMMIT);\n\n        // Replaying an end marker to commit transaction of producer id 5.\n        context.replayEndTransactionMarker(5L, TransactionResult.COMMIT);\n\n        // The pending offset is removed...\n        assertNull(context.offsetMetadataManager.pendingTransactionalOffset(\n            5L,\n            \"foo\",\n            \"bar\",\n            0\n        ));\n\n        // ... and added to the main offset storage.\n        assertEquals(new OffsetAndMetadata(\n            1L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ), context.offsetMetadataManager.offset(\n            \"foo\",\n            \"bar\",\n            0\n        ));\n\n        // Replaying an end marker to abort transaction of producer id 6.\n        context.replayEndTransactionMarker(6L, TransactionResult.ABORT);\n\n        // The pending offset is removed from the pending offsets and\n        // it is not added to the main offset storage.\n        assertNull(context.offsetMetadataManager.pendingTransactionalOffset(\n            6L,\n            \"foo\",\n            \"bar\",\n            1\n        ));\n        assertNull(context.offsetMetadataManager.offset(\n            \"foo\",\n            \"bar\",\n            1\n        ));\n    }\n\n    @Test\n    public void testReplayTransactionEndMarkerKeepsTheMostRecentCommittedOffset() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Add pending transactional offset commit for producer id 5.\n        verifyTransactionalReplay(context, 5L, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            0L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Add regular offset commit.\n        verifyReplay(context, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            1L,\n            101L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Replaying an end marker to commit transaction of producer id 5.\n        context.replayEndTransactionMarker(5L, TransactionResult.COMMIT);\n\n        // The pending offset is removed...\n        assertNull(context.offsetMetadataManager.pendingTransactionalOffset(\n            5L,\n            \"foo\",\n            \"bar\",\n            0\n        ));\n\n        // ... but it is not added to the main storage because the regular\n        // committed offset is more recent.\n        assertEquals(new OffsetAndMetadata(\n            1L,\n            101L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ), context.offsetMetadataManager.offset(\n            \"foo\",\n            \"bar\",\n            0\n        ));\n    }\n\n    @Test\n    public void testOffsetCommitsNumberMetricWithTransactionalOffsets() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Add pending transactional commit for producer id 4.\n        verifyTransactionalReplay(context, 4L, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            0L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Add pending transactional commit for producer id 5.\n        verifyTransactionalReplay(context, 5L, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            1L,\n            101L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Add pending transactional commit for producer id 6.\n        verifyTransactionalReplay(context, 6L, \"foo\", \"bar\", 1, new OffsetAndMetadata(\n            2L,\n            200L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Commit all the transactions.\n        context.replayEndTransactionMarker(4L, TransactionResult.COMMIT);\n        context.replayEndTransactionMarker(5L, TransactionResult.COMMIT);\n        context.replayEndTransactionMarker(6L, TransactionResult.COMMIT);\n\n        // Verify the sensor is called twice as we have only\n        // two partitions.\n        verify(context.metrics, times(2)).incrementNumOffsets();\n    }\n\n    @Test\n    public void testOffsetCommitsSensor() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n        group.transitionTo(ClassicGroupState.STABLE);\n\n        CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(1)\n                .setRetentionTimeMs(1234L)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Arrays.asList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L),\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(1)\n                                .setCommittedOffset(150L)\n                        ))\n                ))\n        );\n\n        verify(context.metrics).record(OFFSET_COMMITS_SENSOR_NAME, 2);\n    }\n\n    @Test\n    public void testOffsetsExpiredSensor() {\n        GroupMetadataManager groupMetadataManager = mock(GroupMetadataManager.class);\n        Group group = mock(Group.class);\n\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .withGroupMetadataManager(groupMetadataManager)\n            .withOffsetsRetentionMinutes(1)\n            .build();\n\n        long commitTimestamp = context.time.milliseconds();\n\n        context.commitOffset(\"group-id\", \"firstTopic\", 0, 100L, 0, commitTimestamp);\n        context.commitOffset(\"group-id\", \"secondTopic\", 0, 100L, 0, commitTimestamp);\n        context.commitOffset(\"group-id\", \"secondTopic\", 1, 100L, 0, commitTimestamp + 500);\n\n        context.time.sleep(Duration.ofMinutes(1).toMillis());\n\n        // firstTopic-0: group is still subscribed to firstTopic. Do not expire.\n        // secondTopic-0: should expire as offset retention has passed.\n        // secondTopic-1: has not passed offset retention. Do not expire.\n        List<CoordinatorRecord> expectedRecords = Collections.singletonList(\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"secondTopic\", 0)\n        );\n\n        when(groupMetadataManager.group(\"group-id\")).thenReturn(group);\n        when(group.offsetExpirationCondition()).thenReturn(Optional.of(\n            new OffsetExpirationConditionImpl(offsetAndMetadata -> offsetAndMetadata.commitTimestampMs)));\n        when(group.isSubscribedToTopic(\"firstTopic\")).thenReturn(true);\n        when(group.isSubscribedToTopic(\"secondTopic\")).thenReturn(false);\n\n        List<CoordinatorRecord> records = new ArrayList<>();\n        assertFalse(context.cleanupExpiredOffsets(\"group-id\", records));\n        assertEquals(expectedRecords, records);\n\n        // Expire secondTopic-1.\n        context.time.sleep(500);\n\n        records = new ArrayList<>();\n        assertFalse(context.cleanupExpiredOffsets(\"group-id\", records));\n        verify(context.metrics, times(2)).record(OFFSET_EXPIRED_SENSOR_NAME, 1);\n\n        // Add 2 more commits, then expire all.\n        when(group.isSubscribedToTopic(\"firstTopic\")).thenReturn(false);\n        context.commitOffset(\"group-id\", \"firstTopic\", 1, 100L, 0, commitTimestamp + 500);\n        context.commitOffset(\"group-id\", \"secondTopic\", 0, 101L, 0, commitTimestamp + 500);\n\n        records = new ArrayList<>();\n        assertTrue(context.cleanupExpiredOffsets(\"group-id\", records));\n        verify(context.metrics).record(OFFSET_EXPIRED_SENSOR_NAME, 3);\n    }\n\n    @Test\n    public void testOffsetDeletionsSensor() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\"foo\", true);\n\n        context.commitOffset(\"foo\", \"bar\", 0, 100L, 0);\n        context.commitOffset(\"foo\", \"bar\", 1, 150L, 0);\n        group.setSubscribedTopics(Optional.of(Collections.emptySet()));\n\n        OffsetDeleteRequestData.OffsetDeleteRequestTopicCollection requestTopicCollection =\n            new OffsetDeleteRequestData.OffsetDeleteRequestTopicCollection(Collections.singletonList(\n                new OffsetDeleteRequestData.OffsetDeleteRequestTopic()\n                    .setName(\"bar\")\n                    .setPartitions(Arrays.asList(\n                        new OffsetDeleteRequestData.OffsetDeleteRequestPartition().setPartitionIndex(0),\n                        new OffsetDeleteRequestData.OffsetDeleteRequestPartition().setPartitionIndex(1)\n                    ))\n            ).iterator());\n\n        context.deleteOffsets(\n            new OffsetDeleteRequestData()\n                .setGroupId(\"foo\")\n                .setTopics(requestTopicCollection)\n        );\n\n        verify(context.metrics).record(OFFSET_DELETIONS_SENSOR_NAME, 2);\n    }\n\n    @Test\n    public void testOnPartitionsDeleted() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Commit offsets.\n        context.commitOffset(\"grp-0\", \"foo\", 1, 100, 1, context.time.milliseconds());\n        context.commitOffset(\"grp-0\", \"foo\", 2, 200, 1, context.time.milliseconds());\n        context.commitOffset(\"grp-0\", \"foo\", 3, 300, 1, context.time.milliseconds());\n\n        context.commitOffset(\"grp-1\", \"bar\", 1, 100, 1, context.time.milliseconds());\n        context.commitOffset(\"grp-1\", \"bar\", 2, 200, 1, context.time.milliseconds());\n        context.commitOffset(\"grp-1\", \"bar\", 3, 300, 1, context.time.milliseconds());\n\n        context.commitOffset(100L, \"grp-2\", \"foo\", 1, 100, 1, context.time.milliseconds());\n        context.commitOffset(100L, \"grp-2\", \"foo\", 2, 200, 1, context.time.milliseconds());\n        context.commitOffset(100L, \"grp-2\", \"foo\", 3, 300, 1, context.time.milliseconds());\n\n        // Delete partitions.\n        List<CoordinatorRecord> records = context.deletePartitions(Arrays.asList(\n            new TopicPartition(\"foo\", 1),\n            new TopicPartition(\"foo\", 2),\n            new TopicPartition(\"foo\", 3),\n            new TopicPartition(\"bar\", 1)\n        ));\n\n        // Verify.\n        List<CoordinatorRecord> expectedRecords = Arrays.asList(\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"grp-0\", \"foo\", 1),\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"grp-0\", \"foo\", 2),\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"grp-0\", \"foo\", 3),\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"grp-1\", \"bar\", 1),\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"grp-2\", \"foo\", 1),\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"grp-2\", \"foo\", 2),\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"grp-2\", \"foo\", 3)\n        );\n\n        assertEquals(new HashSet<>(expectedRecords), new HashSet<>(records));\n\n        assertFalse(context.hasOffset(\"grp-0\", \"foo\", 1));\n        assertFalse(context.hasOffset(\"grp-0\", \"foo\", 2));\n        assertFalse(context.hasOffset(\"grp-0\", \"foo\", 3));\n        assertFalse(context.hasOffset(\"grp-1\", \"bar\", 1));\n        assertFalse(context.hasOffset(\"grp-2\", \"foo\", 1));\n        assertFalse(context.hasOffset(\"grp-2\", \"foo\", 2));\n        assertFalse(context.hasOffset(\"grp-2\", \"foo\", 3));\n    }\n\n    private void verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    ) {\n        context.replay(CoordinatorRecordHelpers.newOffsetCommitRecord(\n            groupId,\n            topic,\n            partition,\n            offsetAndMetadata,\n            MetadataImage.EMPTY.features().metadataVersion()\n        ));\n\n        assertEquals(offsetAndMetadata, context.offsetMetadataManager.offset(\n            groupId,\n            topic,\n            partition\n        ));\n    }\n\n    private void verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    ) {\n        context.replay(producerId, CoordinatorRecordHelpers.newOffsetCommitRecord(\n            groupId,\n            topic,\n            partition,\n            offsetAndMetadata,\n            MetadataImage.EMPTY.features().metadataVersion()\n        ));\n\n        assertEquals(offsetAndMetadata, context.offsetMetadataManager.pendingTransactionalOffset(\n            producerId,\n            groupId,\n            topic,\n            partition\n        ));\n    }\n\n    private ClassicGroupMember mkGenericMember(\n        String memberId,\n        Optional<String> groupInstanceId\n    ) {\n        return new ClassicGroupMember(\n            memberId,\n            groupInstanceId,\n            \"client-id\",\n            \"host\",\n            5000,\n            5000,\n            \"consumer\",\n            new JoinGroupRequestData.JoinGroupRequestProtocolCollection(\n                Collections.singletonList(new JoinGroupRequestData.JoinGroupRequestProtocol()\n                    .setName(\"range\")\n                    .setMetadata(new byte[0])\n                ).iterator()\n            )\n        );\n    }\n}",
                "methodCount": 98
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 11,
                "candidates": [
                    {
                        "lineStart": 407,
                        "lineEnd": 424,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method commitOffset to class OffsetMetadataManager",
                        "description": "Move method commitOffset to org.apache.kafka.coordinator.group.OffsetMetadataManager\nRationale: The commitOffset() method is concerned with managing offsets, which is directly related to the responsibilities of OffsetMetadataManager. The OffsetMetadataManager class already contains methods that handle offset commits and manages the state of offsets for different groups, topics, and partitions. This aligns perfectly with the functionality of the commitOffset() method, which simplifies offset management tasks such as committing offsets and updating the state of these offsets in the system.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 213,
                        "lineEnd": 217,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method commitOffset to class GroupCoordinatorMetricsShard",
                        "description": "Move method commitOffset to org.apache.kafka.coordinator.group.metrics.GroupCoordinatorMetricsShard\nRationale: The commitOffset method is closely related to committing offsets and dealing with coordinator record structures, which fall into the domain of the GroupCoordinatorMetricsShard class. The GroupCoordinatorMetricsShard already handles operations related to group and offset metadata, making it a more appropriate place for this method. Additionally, placing this method here would provide better encapsulation of functionality related to coordinator metrics and offset management.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 317,
                        "lineEnd": 329,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method fetchOffsets to class OffsetMetadataManager",
                        "description": "Move method fetchOffsets to org.apache.kafka.coordinator.group.OffsetMetadataManager\nRationale: The `fetchOffsets` method is directly involved with the offset fetching logic related to groups, which fits within the responsibility of the `OffsetMetadataManager`. This class already contains other methods related to offset management such as validation of offset fetch requests and offset commits, making it the natural home for the `fetchOffsets` method. In contrast, `MockTime` handles time management and `GroupCoordinatorMetricsShard` is focused on metrics collection, neither of which align with the functionality of fetching offsets for groups.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 350,
                        "lineEnd": 360,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method fetchAllOffsets to class OffsetMetadataManager",
                        "description": "Move method fetchAllOffsets to org.apache.kafka.coordinator.group.OffsetMetadataManager\nRationale: The `fetchAllOffsets` method deals with fetching offsets based on group IDs and committed offsets, which directly correlates with the responsibility of the `OffsetMetadataManager` class. This class already contains methods that manage offsets, such as `fetchOffsets`, `deleteOffsets`, `commitOffset`, and validation methods. Also, `OffsetMetadataManager` is tightly coupled with `GroupMetadataManager`, `SnapshotRegistry`, and the `Offsets` class, which are essential components for managing offset metadata.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 390,
                        "lineEnd": 405,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method commitOffset to class MockTime",
                        "description": "Move method commitOffset to org.apache.kafka.common.utils.MockTime\nRationale: The commitOffset() method relies on the time.milliseconds() method, which is part of the MockTime class. This strong dependency suggests that the commitOffset() method is closely related to the functionality provided by MockTime. Moving commitOffset() to MockTime would ensure encapsulation of time-related operations in one place, making the codebase easier to maintain and understand. Additionally, MockTime is designed to handle operations that involve manual advancement of time, which aligns well with the purpose of the commitOffset() method, as it likely involves operations dependent on precise timing.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 570,
                        "lineEnd": 577,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method hasOffset to class GroupMetadataManager",
                        "description": "Move method hasOffset to org.apache.kafka.coordinator.group.GroupMetadataManager\nRationale: The method hasOffset() is responsible for checking if there are committed or pending offsets for a given group, topic, and partition. This functionality is directly related to the overall group metadata management and the operation of consumer groups. The GroupMetadataManager class handles various aspects of group metadata, including offset validation and management. Therefore, moving hasOffset() to this class would align with its responsibilities and centralize the group-related operations in one place.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 462,
                        "lineEnd": 468,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method messageOrNull to class ApiMessageAndVersion",
                        "description": "Move method messageOrNull to org.apache.kafka.server.common.ApiMessageAndVersion\nRationale: The method messageOrNull(ApiMessageAndVersion apiMessageAndVersion) deals directly with the ApiMessageAndVersion class by accessing its message method. This implies a strong relation where the method is presumably providing a utility function specific to ApiMessageAndVersion objects. Moving it to ApiMessageAndVersion consolidates all related functionalities within one class, improving encapsulation and cohesion.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 3128,
                        "lineEnd": 3148,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method verifyReplay to class OffsetMetadataManagerTestContext",
                        "description": "Move method verifyReplay to org.apache.kafka.coordinator.group.OffsetMetadataManagerTest.OffsetMetadataManagerTestContext\nRationale: The method `verifyReplay` is critically tied to verifying the state managed by `OffsetMetadataManagerTestContext`. It calls `context.replay` and uses `context.offsetMetadataManager.offset`, both of which are fields handled by `OffsetMetadataManagerTestContext`. This indicates that `verifyReplay` heavily relies on the context's methods and fields, making this class the most appropriate place to move the method to ensure cohesion and maintainability.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 3150,
                        "lineEnd": 3172,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method verifyTransactionalReplay to class OffsetMetadataManagerTestContext",
                        "description": "Move method verifyTransactionalReplay to org.apache.kafka.coordinator.group.OffsetMetadataManagerTest.OffsetMetadataManagerTestContext\nRationale: The `verifyTransactionalReplay` method extensively uses the `OffsetMetadataManagerTestContext` class by invoking its methods such as `replay` and accessing its `offsetMetadataManager` field. Since the method's core operations revolve around the testing context and its verification of transactional replay operations, it is more appropriate for this method to reside within the `OffsetMetadataManagerTestContext` class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 107,
                        "lineEnd": 110,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method withOffsetMetadataMaxSize to class GroupCoordinatorConfig",
                        "description": "Move method withOffsetMetadataMaxSize to org.apache.kafka.coordinator.group.GroupCoordinatorConfig\nRationale: The method withOffsetMetadataMaxSize() relies heavily on the creation and configuration of GroupCoordinatorConfig objects. Moving this method to GroupCoordinatorConfig centralizes configuration handling within the same class, improving cohesion and ensuring that configuration logic is encapsulated where it is most relevant. This move also keeps the Builder pattern aligned with the configuration setting and enhances maintainability.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 112,
                        "lineEnd": 115,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method withOffsetsRetentionMinutes to class GroupCoordinatorConfig",
                        "description": "Move method withOffsetsRetentionMinutes to org.apache.kafka.coordinator.group.GroupCoordinatorConfig\nRationale: The method withOffsetsRetentionMinutes(int offsetsRetentionMinutes) is creating a GroupCoordinatorConfig object and setting its offset retention time. Moving this method to the GroupCoordinatorConfig class is more coherent since it concerns configuration specifically related to the GroupCoordinator. By placing it in GroupCoordinatorConfig, it will be part of the same class that directly deals with the parameters and settings of the Group Coordinator, ensuring better encapsulation and cohesion.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "withOffsetMetadataMaxSize",
                            "method_signature": " withOffsetMetadataMaxSize(int offsetMetadataMaxSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withOffsetsRetentionMinutes",
                            "method_signature": " withOffsetsRetentionMinutes(int offsetsRetentionMinutes)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withGroupMetadataManager",
                            "method_signature": " withGroupMetadataManager(GroupMetadataManager groupMetadataManager)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": " build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getOrMaybeCreateGroup",
                            "method_signature": "public getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commit",
                            "method_signature": "public commit()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            short version,\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitTransactionalOffset",
                            "method_signature": "public commitTransactionalOffset(\n            TxnOffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deletePartitions",
                            "method_signature": "public deletePartitions(\n            List<TopicPartition> topicPartitions\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteOffsets",
                            "method_signature": "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteAllOffsets",
                            "method_signature": "public deleteAllOffsets(\n            String groupId,\n            List<CoordinatorRecord> records\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "cleanupExpiredOffsets",
                            "method_signature": "public cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public fetchOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public fetchAllOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sleep",
                            "method_signature": "public sleep(long ms)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            long producerId,\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteOffset",
                            "method_signature": "public deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            CoordinatorRecord record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            long producerId,\n            CoordinatorRecord record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testOffsetDeleteWith",
                            "method_signature": "public testOffsetDeleteWith(\n            String groupId,\n            String topic,\n            int partition,\n            Errors expectedError\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasOffset",
                            "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testOffsetCommitWithUnknownGroup",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testOffsetCommitWithUnknownGroup(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testDeleteGroupAllOffsets",
                            "method_signature": "@ParameterizedTest\n    @EnumSource(value = Group.GroupType.class, names = {\"CLASSIC\", \"CONSUMER\"})\n    public testDeleteGroupAllOffsets(Group.GroupType groupType)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testDeleteGroupAllOffsetsWithPendingTransactionalOffsets",
                            "method_signature": "@ParameterizedTest\n    @EnumSource(value = Group.GroupType.class, names = {\"CLASSIC\", \"CONSUMER\"})\n    public testDeleteGroupAllOffsetsWithPendingTransactionalOffsets(Group.GroupType groupType)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "mkOffsetPartitionResponse",
                            "method_signature": "private static mkOffsetPartitionResponse(\n        int partition,\n        long offset,\n        int leaderEpoch,\n        String metadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "mkInvalidOffsetPartitionResponse",
                            "method_signature": "private static mkInvalidOffsetPartitionResponse(int partition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "mkOffsetPartitionResponse",
                            "method_signature": "private static mkOffsetPartitionResponse(int partition, Errors error)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyReplay",
                            "method_signature": "private verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyTransactionalReplay",
                            "method_signature": "private verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "mkGenericMember",
                            "method_signature": "private mkGenericMember(\n        String memberId,\n        Optional<String> groupInstanceId\n    )",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "hasOffset",
                            "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyTransactionalReplay",
                            "method_signature": "private verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyReplay",
                            "method_signature": "private verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            CoordinatorRecord record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteOffset",
                            "method_signature": "public deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withOffsetMetadataMaxSize",
                            "method_signature": " withOffsetMetadataMaxSize(int offsetMetadataMaxSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withOffsetsRetentionMinutes",
                            "method_signature": " withOffsetsRetentionMinutes(int offsetsRetentionMinutes)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "hasOffset",
                            "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyTransactionalReplay",
                            "method_signature": "private verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyReplay",
                            "method_signature": "private verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            CoordinatorRecord record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteOffset",
                            "method_signature": "public deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withOffsetMetadataMaxSize",
                            "method_signature": " withOffsetMetadataMaxSize(int offsetMetadataMaxSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withOffsetsRetentionMinutes",
                            "method_signature": " withOffsetsRetentionMinutes(int offsetsRetentionMinutes)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )": {
                    "first": {
                        "method_name": "hasOffset",
                        "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.1723816869324546
                },
                "private verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )": {
                    "first": {
                        "method_name": "verifyTransactionalReplay",
                        "method_signature": "private verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.17461310385029913
                },
                "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )": {
                    "first": {
                        "method_name": "replayEndTransactionMarker",
                        "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.1758096837297937
                },
                "private verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )": {
                    "first": {
                        "method_name": "verifyReplay",
                        "method_signature": "private verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.17864286572149796
                },
                "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        )": {
                    "first": {
                        "method_name": "commitOffset",
                        "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.1983667055674084
                },
                "private replay(\n            CoordinatorRecord record\n        )": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "private replay(\n            CoordinatorRecord record\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2007192020373833
                },
                "public fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )": {
                    "first": {
                        "method_name": "fetchOffsets",
                        "method_signature": "public fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2030475999869505
                },
                "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )": {
                    "first": {
                        "method_name": "commitOffset",
                        "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.20895643609398898
                },
                "public deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        )": {
                    "first": {
                        "method_name": "deleteOffset",
                        "method_signature": "public deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.21044236383116743
                },
                "public fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        )": {
                    "first": {
                        "method_name": "fetchAllOffsets",
                        "method_signature": "public fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.21548227985458038
                },
                "public commitOffset(\n            OffsetCommitRequestData request\n        )": {
                    "first": {
                        "method_name": "commitOffset",
                        "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.21600261122184558
                },
                "private idempotentCreateSnapshot()": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.21841260061183598
                },
                " withOffsetMetadataMaxSize(int offsetMetadataMaxSize)": {
                    "first": {
                        "method_name": "withOffsetMetadataMaxSize",
                        "method_signature": " withOffsetMetadataMaxSize(int offsetMetadataMaxSize)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.21959560616745527
                },
                "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)": {
                    "first": {
                        "method_name": "messageOrNull",
                        "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.23077528690684884
                },
                " withOffsetsRetentionMinutes(int offsetsRetentionMinutes)": {
                    "first": {
                        "method_name": "withOffsetsRetentionMinutes",
                        "method_signature": " withOffsetsRetentionMinutes(int offsetsRetentionMinutes)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.23768245987036973
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        )",
                    "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                    "public fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                    "public fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        )",
                    "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                    "public deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                    "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                    "private replay(\n            CoordinatorRecord record\n        )",
                    "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                    "private idempotentCreateSnapshot()",
                    "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                    "private verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                    "private verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                    " withOffsetMetadataMaxSize(int offsetMetadataMaxSize)",
                    " withOffsetsRetentionMinutes(int offsetsRetentionMinutes)"
                ],
                "llm_response_time": 12503
            },
            "targetClassMap": {
                "commitOffset": {
                    "target_classes": [
                        {
                            "class_name": "MockTime",
                            "similarity_score": 0.27134238459003635
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "MockTime"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "fetchOffsets": {
                    "target_classes": [
                        {
                            "class_name": "MockTime",
                            "similarity_score": 0.29140866251201486
                        },
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.19588008878590737
                        },
                        {
                            "class_name": "GroupCoordinatorMetricsShard",
                            "similarity_score": 0.20744234793771407
                        },
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.17469566370073444
                        },
                        {
                            "class_name": "OffsetMetadataManager",
                            "similarity_score": 0.2195147841070221
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "OffsetMetadataManager",
                        "GroupCoordinatorMetricsShard",
                        "MockTime"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 12,
                    "similarity_metric": "cosine"
                },
                "fetchAllOffsets": {
                    "target_classes": [
                        {
                            "class_name": "MockTime",
                            "similarity_score": 0.3215270791515118
                        },
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.21612519088609763
                        },
                        {
                            "class_name": "GroupCoordinatorMetricsShard",
                            "similarity_score": 0.22888246234613863
                        },
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.19275125868235324
                        },
                        {
                            "class_name": "OffsetMetadataManager",
                            "similarity_score": 0.2422026399493032
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "OffsetMetadataManager",
                        "GroupCoordinatorMetricsShard",
                        "MockTime"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 20,
                    "similarity_metric": "cosine"
                },
                "deleteOffset": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3651,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "hasOffset": {
                    "target_classes": [
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.18169682579561122
                        },
                        {
                            "class_name": "OffsetMetadataManager",
                            "similarity_score": 0.265533546076446
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GroupMetadataManager",
                        "OffsetMetadataManager"
                    ],
                    "llm_response_time": 16596,
                    "similarity_computation_time": 11,
                    "similarity_metric": "cosine"
                },
                "replay": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3352,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "replayEndTransactionMarker": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2703,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "idempotentCreateSnapshot": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3316,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "messageOrNull": {
                    "target_classes": [
                        {
                            "class_name": "ApiMessageAndVersion",
                            "similarity_score": 0.6106401198187058
                        },
                        {
                            "class_name": "MockTime",
                            "similarity_score": 0.5914470146353319
                        },
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.4220009252215372
                        },
                        {
                            "class_name": "GroupCoordinatorMetricsShard",
                            "similarity_score": 0.4615361902486966
                        },
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.35945481197585616
                        },
                        {
                            "class_name": "OffsetMetadataManager",
                            "similarity_score": 0.48967032494126206
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "ApiMessageAndVersion",
                        "OffsetMetadataManager",
                        "MockTime"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 25,
                    "similarity_metric": "cosine"
                },
                "verifyReplay": {
                    "target_classes": [
                        {
                            "class_name": "OffsetMetadataManagerTestContext",
                            "similarity_score": 0.41535297146126127
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "OffsetMetadataManagerTestContext"
                    ],
                    "llm_response_time": 1954,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "verifyTransactionalReplay": {
                    "target_classes": [
                        {
                            "class_name": "OffsetMetadataManagerTestContext",
                            "similarity_score": 0.4224932464329102
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "OffsetMetadataManagerTestContext"
                    ],
                    "llm_response_time": 2230,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                },
                "withOffsetMetadataMaxSize": {
                    "target_classes": [
                        {
                            "class_name": "GroupCoordinatorConfig",
                            "similarity_score": 0.2360171186932256
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GroupCoordinatorConfig"
                    ],
                    "llm_response_time": 3156,
                    "similarity_computation_time": 4,
                    "similarity_metric": "cosine"
                },
                "withOffsetsRetentionMinutes": {
                    "target_classes": [
                        {
                            "class_name": "GroupCoordinatorConfig",
                            "similarity_score": 0.20082722825608562
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GroupCoordinatorConfig"
                    ],
                    "llm_response_time": 3106,
                    "similarity_computation_time": 4,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from private replayEndTransactionMarker(producerId long, result TransactionResult) : void in class org.apache.kafka.coordinator.group.OffsetMetadataManagerTest.OffsetMetadataManagerTestContext & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 511,
                    "endLine": 518,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private replayEndTransactionMarker(producerId long, result TransactionResult) : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 515,
                    "endLine": 515,
                    "startColumn": 13,
                    "endColumn": 69,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 511,
                    "endLine": 518,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private replayEndTransactionMarker(producerId long, result TransactionResult) : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 515,
                    "endLine": 515,
                    "startColumn": 13,
                    "endColumn": 73,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(lastWrittenOffset)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 608,
        "extraction_results": {
            "success": true,
            "newCommitHash": "2d5adc292dd053243e3ad49fb9e5ee75e20fd598",
            "newBranchName": "extract-idempotentCreateSnapshot-replayEndTransactionMarker-130af38"
        },
        "telemetry": {
            "id": "01f3f13f-eabf-48e7-ada2-107578789a74",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 3100,
                "lineStart": 96,
                "lineEnd": 3195,
                "bodyLineStart": 96,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                "sourceCode": "public class OffsetMetadataManagerTest {\n    static class OffsetMetadataManagerTestContext {\n        public static class Builder {\n            private final MockTime time = new MockTime();\n            private final MockCoordinatorTimer<Void, CoordinatorRecord> timer = new MockCoordinatorTimer<>(time);\n            private final LogContext logContext = new LogContext();\n            private final SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n            private GroupMetadataManager groupMetadataManager = null;\n            private MetadataImage metadataImage = null;\n            private GroupCoordinatorConfig config = null;\n            private GroupCoordinatorMetricsShard metrics = mock(GroupCoordinatorMetricsShard.class);\n\n            Builder withOffsetMetadataMaxSize(int offsetMetadataMaxSize) {\n                config = GroupCoordinatorConfigTest.createGroupCoordinatorConfig(offsetMetadataMaxSize, 60000L, 24 * 60);\n                return this;\n            }\n\n            Builder withOffsetsRetentionMinutes(int offsetsRetentionMinutes) {\n                config = GroupCoordinatorConfigTest.createGroupCoordinatorConfig(4096, 60000L, offsetsRetentionMinutes);\n                return this;\n            }\n\n            Builder withGroupMetadataManager(GroupMetadataManager groupMetadataManager) {\n                this.groupMetadataManager = groupMetadataManager;\n                return this;\n            }\n\n            OffsetMetadataManagerTestContext build() {\n                if (metadataImage == null) metadataImage = MetadataImage.EMPTY;\n                if (config == null) {\n                    config = GroupCoordinatorConfigTest.createGroupCoordinatorConfig(4096, 60000L, 24);\n                }\n\n                if (groupMetadataManager == null) {\n                    groupMetadataManager = new GroupMetadataManager.Builder()\n                        .withTime(time)\n                        .withTimer(timer)\n                        .withSnapshotRegistry(snapshotRegistry)\n                        .withLogContext(logContext)\n                        .withMetadataImage(metadataImage)\n                        .withConsumerGroupAssignors(Collections.singletonList(new RangeAssignor()))\n                        .withGroupCoordinatorMetricsShard(metrics)\n                        .build();\n                }\n\n                OffsetMetadataManager offsetMetadataManager = new OffsetMetadataManager.Builder()\n                    .withTime(time)\n                    .withLogContext(logContext)\n                    .withSnapshotRegistry(snapshotRegistry)\n                    .withMetadataImage(metadataImage)\n                    .withGroupMetadataManager(groupMetadataManager)\n                    .withGroupCoordinatorConfig(config)\n                    .withGroupCoordinatorMetricsShard(metrics)\n                    .build();\n\n                return new OffsetMetadataManagerTestContext(\n                    time,\n                    timer,\n                    snapshotRegistry,\n                    metrics,\n                    groupMetadataManager,\n                    offsetMetadataManager\n                );\n            }\n        }\n\n        final MockTime time;\n        final MockCoordinatorTimer<Void, CoordinatorRecord> timer;\n        final SnapshotRegistry snapshotRegistry;\n        final GroupCoordinatorMetricsShard metrics;\n        final GroupMetadataManager groupMetadataManager;\n        final OffsetMetadataManager offsetMetadataManager;\n\n        long lastCommittedOffset = 0L;\n        long lastWrittenOffset = 0L;\n\n        OffsetMetadataManagerTestContext(\n            MockTime time,\n            MockCoordinatorTimer<Void, CoordinatorRecord> timer,\n            SnapshotRegistry snapshotRegistry,\n            GroupCoordinatorMetricsShard metrics,\n            GroupMetadataManager groupMetadataManager,\n            OffsetMetadataManager offsetMetadataManager\n        ) {\n            this.time = time;\n            this.timer = timer;\n            this.snapshotRegistry = snapshotRegistry;\n            this.metrics = metrics;\n            this.groupMetadataManager = groupMetadataManager;\n            this.offsetMetadataManager = offsetMetadataManager;\n        }\n\n        public Group getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        ) {\n            switch (groupType) {\n                case CLASSIC:\n                    return groupMetadataManager.getOrMaybeCreateClassicGroup(\n                        groupId,\n                        true\n                    );\n                case CONSUMER:\n                    return groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n                        groupId,\n                        true\n                    );\n                default:\n                    throw new IllegalArgumentException(\"Invalid group type: \" + groupType);\n            }\n        }\n\n        public void commit() {\n            long lastCommittedOffset = this.lastCommittedOffset;\n            this.lastCommittedOffset = lastWrittenOffset;\n            snapshotRegistry.deleteSnapshotsUpTo(lastCommittedOffset);\n        }\n\n        public CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> commitOffset(\n            OffsetCommitRequestData request\n        ) {\n            return commitOffset(ApiKeys.OFFSET_COMMIT.latestVersion(), request);\n        }\n\n        public CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> commitOffset(\n            short version,\n            OffsetCommitRequestData request\n        ) {\n            RequestContext context = new RequestContext(\n                new RequestHeader(\n                    ApiKeys.OFFSET_COMMIT,\n                    version,\n                    \"client\",\n                    0\n                ),\n                \"1\",\n                InetAddress.getLoopbackAddress(),\n                KafkaPrincipal.ANONYMOUS,\n                ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n                SecurityProtocol.PLAINTEXT,\n                ClientInformation.EMPTY,\n                false\n            );\n\n            CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> result = offsetMetadataManager.commitOffset(\n                context,\n                request\n            );\n\n            result.records().forEach(this::replay);\n            return result;\n        }\n\n        public CoordinatorResult<TxnOffsetCommitResponseData, CoordinatorRecord> commitTransactionalOffset(\n            TxnOffsetCommitRequestData request\n        ) {\n            RequestContext context = new RequestContext(\n                new RequestHeader(\n                    ApiKeys.TXN_OFFSET_COMMIT,\n                    ApiKeys.TXN_OFFSET_COMMIT.latestVersion(),\n                    \"client\",\n                    0\n                ),\n                \"1\",\n                InetAddress.getLoopbackAddress(),\n                KafkaPrincipal.ANONYMOUS,\n                ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n                SecurityProtocol.PLAINTEXT,\n                ClientInformation.EMPTY,\n                false\n            );\n\n            CoordinatorResult<TxnOffsetCommitResponseData, CoordinatorRecord> result = offsetMetadataManager.commitTransactionalOffset(\n                context,\n                request\n            );\n\n            result.records().forEach(record -> replay(\n                request.producerId(),\n                record\n            ));\n\n            return result;\n        }\n\n        public List<CoordinatorRecord> deletePartitions(\n            List<TopicPartition> topicPartitions\n        ) {\n            List<CoordinatorRecord> records = offsetMetadataManager.onPartitionsDeleted(topicPartitions);\n            records.forEach(this::replay);\n            return records;\n        }\n\n        public CoordinatorResult<OffsetDeleteResponseData, CoordinatorRecord> deleteOffsets(\n            OffsetDeleteRequestData request\n        ) {\n            CoordinatorResult<OffsetDeleteResponseData, CoordinatorRecord> result = offsetMetadataManager.deleteOffsets(request);\n            result.records().forEach(this::replay);\n            return result;\n        }\n\n        public int deleteAllOffsets(\n            String groupId,\n            List<CoordinatorRecord> records\n        ) {\n            List<CoordinatorRecord> addedRecords = new ArrayList<>();\n            int numDeletedOffsets = offsetMetadataManager.deleteAllOffsets(groupId, addedRecords);\n            addedRecords.forEach(this::replay);\n\n            records.addAll(addedRecords);\n            return numDeletedOffsets;\n        }\n\n        public boolean cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records) {\n            List<CoordinatorRecord> addedRecords = new ArrayList<>();\n            boolean isOffsetsEmptyForGroup = offsetMetadataManager.cleanupExpiredOffsets(groupId, addedRecords);\n            addedRecords.forEach(this::replay);\n\n            records.addAll(addedRecords);\n            return isOffsetsEmptyForGroup;\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        ) {\n            return fetchOffsets(\n                groupId,\n                null,\n                -1,\n                topics,\n                committedOffset\n            );\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        ) {\n            OffsetFetchResponseData.OffsetFetchResponseGroup response = offsetMetadataManager.fetchOffsets(\n                new OffsetFetchRequestData.OffsetFetchRequestGroup()\n                    .setGroupId(groupId)\n                    .setMemberId(memberId)\n                    .setMemberEpoch(memberEpoch)\n                    .setTopics(topics),\n                committedOffset\n            );\n            assertEquals(groupId, response.groupId());\n            return response.topics();\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        ) {\n            return fetchAllOffsets(\n                groupId,\n                null,\n                -1,\n                committedOffset\n            );\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchAllOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            long committedOffset\n        ) {\n            OffsetFetchResponseData.OffsetFetchResponseGroup response = offsetMetadataManager.fetchAllOffsets(\n                new OffsetFetchRequestData.OffsetFetchRequestGroup()\n                    .setGroupId(groupId)\n                    .setMemberId(memberId)\n                    .setMemberEpoch(memberEpoch),\n                committedOffset\n            );\n            assertEquals(groupId, response.groupId());\n            return response.topics();\n        }\n\n        public List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> sleep(long ms) {\n            time.sleep(ms);\n            List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts = timer.poll();\n            timeouts.forEach(timeout -> {\n                if (timeout.result.replayRecords()) {\n                    timeout.result.records().forEach(this::replay);\n                }\n            });\n            return timeouts;\n        }\n\n        public void commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        ) {\n            commitOffset(\n                groupId,\n                topic,\n                partition,\n                offset,\n                leaderEpoch,\n                time.milliseconds()\n            );\n        }\n\n        public void commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        ) {\n            commitOffset(\n                RecordBatch.NO_PRODUCER_ID,\n                groupId,\n                topic,\n                partition,\n                offset,\n                leaderEpoch,\n                commitTimestamp\n            );\n        }\n\n        public void commitOffset(\n            long producerId,\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        ) {\n            replay(producerId, CoordinatorRecordHelpers.newOffsetCommitRecord(\n                groupId,\n                topic,\n                partition,\n                new OffsetAndMetadata(\n                    offset,\n                    OptionalInt.of(leaderEpoch),\n                    \"metadata\",\n                    commitTimestamp,\n                    OptionalLong.empty()\n                ),\n                MetadataVersion.latestTesting()\n            ));\n        }\n\n        public void deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        ) {\n            replay(CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\n                groupId,\n                topic,\n                partition\n            ));\n        }\n\n        private ApiMessage messageOrNull(ApiMessageAndVersion apiMessageAndVersion) {\n            if (apiMessageAndVersion == null) {\n                return null;\n            } else {\n                return apiMessageAndVersion.message();\n            }\n        }\n\n        private void replay(\n            CoordinatorRecord record\n        ) {\n            replay(\n                RecordBatch.NO_PRODUCER_ID,\n                record\n            );\n        }\n\n        private void replay(\n            long producerId,\n            CoordinatorRecord record\n        ) {\n            idempotentCreateSnapshot();\n\n            ApiMessageAndVersion key = record.key();\n            ApiMessageAndVersion value = record.value();\n\n            if (key == null) {\n                throw new IllegalStateException(\"Received a null key in \" + record);\n            }\n\n            switch (key.version()) {\n                case OffsetCommitKey.HIGHEST_SUPPORTED_VERSION:\n                    offsetMetadataManager.replay(\n                        lastWrittenOffset,\n                        producerId,\n                        (OffsetCommitKey) key.message(),\n                        (OffsetCommitValue) messageOrNull(value)\n                    );\n                    break;\n\n                default:\n                    throw new IllegalStateException(\"Received an unknown record type \" + key.version()\n                        + \" in \" + record);\n            }\n\n            lastWrittenOffset++;\n        }\n\n        private void replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        ) {\n            idempotentCreateSnapshot();\n            offsetMetadataManager.replayEndTransactionMarker(producerId, result);\n            lastWrittenOffset++;\n        }\n\n        private void idempotentCreateSnapshot() {\n            snapshotRegistry.getOrCreateSnapshot(lastWrittenOffset);\n        }\n\n        public void testOffsetDeleteWith(\n            String groupId,\n            String topic,\n            int partition,\n            Errors expectedError\n        ) {\n            final OffsetDeleteRequestData.OffsetDeleteRequestTopicCollection requestTopicCollection =\n                new OffsetDeleteRequestData.OffsetDeleteRequestTopicCollection(Collections.singletonList(\n                    new OffsetDeleteRequestData.OffsetDeleteRequestTopic()\n                        .setName(topic)\n                        .setPartitions(Collections.singletonList(\n                            new OffsetDeleteRequestData.OffsetDeleteRequestPartition().setPartitionIndex(partition)\n                        ))\n                ).iterator());\n\n            final OffsetDeleteResponseData.OffsetDeleteResponsePartitionCollection expectedResponsePartitionCollection =\n                new OffsetDeleteResponseData.OffsetDeleteResponsePartitionCollection();\n            expectedResponsePartitionCollection.add(\n                new OffsetDeleteResponseData.OffsetDeleteResponsePartition()\n                    .setPartitionIndex(partition)\n                    .setErrorCode(expectedError.code())\n            );\n\n            final OffsetDeleteResponseData.OffsetDeleteResponseTopicCollection expectedResponseTopicCollection =\n                new OffsetDeleteResponseData.OffsetDeleteResponseTopicCollection(Collections.singletonList(\n                    new OffsetDeleteResponseData.OffsetDeleteResponseTopic()\n                        .setName(topic)\n                        .setPartitions(expectedResponsePartitionCollection)\n                ).iterator());\n\n            List<CoordinatorRecord> expectedRecords = Collections.emptyList();\n            if (hasOffset(groupId, topic, partition) && expectedError == Errors.NONE) {\n                expectedRecords = Collections.singletonList(\n                    CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(groupId, topic, partition)\n                );\n            }\n\n            final CoordinatorResult<OffsetDeleteResponseData, CoordinatorRecord> coordinatorResult = deleteOffsets(\n                new OffsetDeleteRequestData()\n                    .setGroupId(groupId)\n                    .setTopics(requestTopicCollection)\n            );\n\n            assertEquals(new OffsetDeleteResponseData().setTopics(expectedResponseTopicCollection), coordinatorResult.response());\n            assertEquals(expectedRecords, coordinatorResult.records());\n        }\n\n        public boolean hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        ) {\n            return offsetMetadataManager.hasCommittedOffset(groupId, topic, partition) ||\n                offsetMetadataManager.hasPendingTransactionalOffsets(groupId, topic, partition);\n        }\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testOffsetCommitWithUnknownGroup(short version) {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        Class<? extends Throwable> expectedType;\n        if (version >= 9) {\n            expectedType = GroupIdNotFoundException.class;\n        } else {\n            expectedType = IllegalGenerationException.class;\n        }\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(expectedType, () -> context.commitOffset(\n            version,\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithDeadGroup() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a dead group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n        group.transitionTo(ClassicGroupState.DEAD);\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(CoordinatorNotAvailableException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithUnknownMemberId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(UnknownMemberIdException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithIllegalGeneration() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(IllegalGenerationException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithUnknownInstanceId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member without static id.\n        group.add(mkGenericMember(\"member\", Optional.empty()));\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(UnknownMemberIdException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGroupInstanceId(\"instanceid\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithFencedInstanceId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member with static id.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(UnknownMemberIdException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGroupInstanceId(\"old-instance-id\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWhileInCompletingRebalanceState() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(RebalanceInProgressException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(1)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithoutMemberIdAndGeneration() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(UnknownMemberIdException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithRetentionTime() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n        group.transitionTo(ClassicGroupState.STABLE);\n\n        CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(1)\n                .setRetentionTimeMs(1234L)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(CoordinatorRecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.empty(),\n                    \"\",\n                    context.time.milliseconds(),\n                    OptionalLong.of(context.time.milliseconds() + 1234L)\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitMaintainsSession() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        ClassicGroupMember member = mkGenericMember(\"member\", Optional.empty());\n        group.add(member);\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n        group.transitionTo(ClassicGroupState.STABLE);\n\n        // Schedule session timeout. This would be normally done when\n        // the group transitions to stable.\n        context.groupMetadataManager.rescheduleClassicGroupMemberHeartbeat(group, member);\n\n        // Advance time by half of the session timeout. No timeouts are\n        // expired.\n        assertEquals(Collections.emptyList(), context.sleep(5000 / 2));\n\n        // Commit.\n        context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(1)\n                .setRetentionTimeMs(1234L)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n        );\n\n        // Advance time by half of the session timeout. No timeouts are\n        // expired.\n        assertEquals(Collections.emptyList(), context.sleep(5000 / 2));\n\n        // Advance time by half of the session timeout again. The timeout should\n        // expire and the member is removed from the group.\n        List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts =\n            context.sleep(5000 / 2);\n        assertEquals(1, timeouts.size());\n        assertFalse(group.hasMember(member.memberId()));\n    }\n\n    @Test\n    public void testSimpleGroupOffsetCommit() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(CoordinatorRecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.empty(),\n                    \"\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n\n        // A generic should have been created.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            false\n        );\n        assertNotNull(group);\n        assertEquals(\"foo\", group.groupId());\n    }\n\n    @Test\n    public void testSimpleGroupOffsetCommitWithInstanceId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                // Instance id should be ignored.\n                .setGroupInstanceId(\"instance-id\")\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(CoordinatorRecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.empty(),\n                    \"\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testConsumerGroupOffsetCommitWithUnknownMemberId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(UnknownMemberIdException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testConsumerGroupOffsetCommitWithStaleMemberEpoch() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .build()\n        );\n\n        OffsetCommitRequestData request = new OffsetCommitRequestData()\n            .setGroupId(\"foo\")\n            .setMemberId(\"member\")\n            .setGenerationIdOrMemberEpoch(9)\n            .setTopics(Collections.singletonList(\n                new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                    .setName(\"bar\")\n                    .setPartitions(Collections.singletonList(\n                        new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                            .setPartitionIndex(0)\n                            .setCommittedOffset(100L)\n                    ))\n            ));\n\n        // Verify that a smaller epoch is rejected.\n        assertThrows(StaleMemberEpochException.class, () -> context.commitOffset(request));\n\n        // Verify that a larger epoch is rejected.\n        request.setGenerationIdOrMemberEpoch(11);\n        assertThrows(StaleMemberEpochException.class, () -> context.commitOffset(request));\n    }\n\n    @Test\n    public void testConsumerGroupOffsetCommitWithIllegalGenerationId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata())\n            .build()\n        );\n\n        OffsetCommitRequestData request = new OffsetCommitRequestData()\n            .setGroupId(\"foo\")\n            .setMemberId(\"member\")\n            .setGenerationIdOrMemberEpoch(9)\n            .setTopics(Collections.singletonList(\n                new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                    .setName(\"bar\")\n                    .setPartitions(Collections.singletonList(\n                        new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                            .setPartitionIndex(0)\n                            .setCommittedOffset(100L)\n                    ))\n            ));\n\n        // Verify that a smaller epoch is rejected.\n        assertThrows(IllegalGenerationException.class, () -> context.commitOffset(request));\n\n        // Verify that a larger epoch is rejected.\n        request.setGenerationIdOrMemberEpoch(11);\n        assertThrows(IllegalGenerationException.class, () -> context.commitOffset(request));\n    }\n\n    @Test\n    public void testConsumerGroupOffsetCommitFromAdminClient() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(CoordinatorRecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.empty(),\n                    \"\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testConsumerGroupOffsetCommit() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .build()\n        );\n\n        CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                                .setCommitTimestamp(context.time.milliseconds())\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(CoordinatorRecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.of(10),\n                    \"metadata\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testConsumerGroupOffsetCommitWithOffsetMetadataTooLarge() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .withOffsetMetadataMaxSize(5)\n            .build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .build()\n        );\n\n        CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Arrays.asList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"toolarge\")\n                                .setCommitTimestamp(context.time.milliseconds()),\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(1)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"small\")\n                                .setCommitTimestamp(context.time.milliseconds())\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Arrays.asList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.OFFSET_METADATA_TOO_LARGE.code()),\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(1)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(CoordinatorRecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                1,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.of(10),\n                    \"small\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testConsumerGroupTransactionalOffsetCommit() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .build()\n        );\n\n        CoordinatorResult<TxnOffsetCommitResponseData, CoordinatorRecord> result = context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(10)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new TxnOffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitResponseData.TxnOffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitResponseData.TxnOffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(CoordinatorRecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.of(10),\n                    \"metadata\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testConsumerGroupTransactionalOffsetCommitWithUnknownGroupId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        assertThrows(IllegalGenerationException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(10)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testConsumerGroupTransactionalOffsetCommitWithUnknownMemberId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        assertThrows(UnknownMemberIdException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(10)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testConsumerGroupTransactionalOffsetCommitWithStaleMemberEpoch() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .build()\n        );\n\n        assertThrows(IllegalGenerationException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(100)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testGenericGroupTransactionalOffsetCommit() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        ClassicGroupMember member = mkGenericMember(\"member\", Optional.empty());\n        group.add(member);\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n        group.transitionTo(ClassicGroupState.STABLE);\n\n        CoordinatorResult<TxnOffsetCommitResponseData, CoordinatorRecord> result = context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(1)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new TxnOffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitResponseData.TxnOffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitResponseData.TxnOffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(CoordinatorRecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.of(10),\n                    \"metadata\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testGenericGroupTransactionalOffsetCommitWithUnknownGroupId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        assertThrows(IllegalGenerationException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(10)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testGenericGroupTransactionalOffsetCommitWithUnknownMemberId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        assertThrows(UnknownMemberIdException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(10)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testGenericGroupTransactionalOffsetCommitWithIllegalGenerationId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        ClassicGroupMember member = mkGenericMember(\"member\", Optional.empty());\n        group.add(member);\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n        group.transitionTo(ClassicGroupState.STABLE);\n\n        assertThrows(IllegalGenerationException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(100)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testGenericGroupFetchOffsetsWithDeadGroup() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a dead group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"group\",\n            true\n        );\n        group.transitionTo(ClassicGroupState.DEAD);\n\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> request = Arrays.asList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Arrays.asList(0, 1)),\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"bar\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        List<OffsetFetchResponseData.OffsetFetchResponseTopics> expectedResponse = Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Collections.singletonList(\n                    mkInvalidOffsetPartitionResponse(0)\n                ))\n        );\n\n        assertEquals(expectedResponse, context.fetchOffsets(\"group\", request, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchOffsetsWithUnknownGroup() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> request = Arrays.asList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Arrays.asList(0, 1)),\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"bar\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        List<OffsetFetchResponseData.OffsetFetchResponseTopics> expectedResponse = Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Collections.singletonList(\n                    mkInvalidOffsetPartitionResponse(0)\n                ))\n        );\n\n        assertEquals(expectedResponse, context.fetchOffsets(\"group\", request, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchOffsetsAtDifferentCommittedOffset() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\"group\", true);\n\n        assertEquals(0, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n        assertEquals(1, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 1, 110L, 1);\n        assertEquals(2, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"bar\", 0, 200L, 1);\n        assertEquals(3, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 1, 111L, 2);\n        assertEquals(4, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"bar\", 1, 210L, 2);\n        assertEquals(5, context.lastWrittenOffset);\n\n        // Always use the same request.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> request = Arrays.asList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Arrays.asList(0, 1)),\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"bar\")\n                .setPartitionIndexes(Arrays.asList(0, 1))\n        );\n\n        // Fetching with 0 should return all invalid offsets.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, 0L));\n\n        // Fetching with 1 should return data up to offset 1.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkInvalidOffsetPartitionResponse(1)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, 1L));\n\n        // Fetching with 2 should return data up to offset 2.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, 2L));\n\n        // Fetching with 3 should return data up to offset 3.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, 3L));\n\n        // Fetching with 4 should return data up to offset 4.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 2, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, 4L));\n\n        // Fetching with 5 should return data up to offset 5.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 2, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 210L, 2, \"metadata\")\n                ))\n        ), context.fetchOffsets(\"group\", request, 5L));\n\n        // Fetching with Long.MAX_VALUE should return all offsets.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 2, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 210L, 2, \"metadata\")\n                ))\n        ), context.fetchOffsets(\"group\", request, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchOffsetsWithPendingTransactionalOffsets() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\"group\", true);\n\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n        context.commitOffset(\"group\", \"foo\", 1, 110L, 1);\n        context.commitOffset(\"group\", \"bar\", 0, 200L, 1);\n\n        context.commit();\n\n        assertEquals(3, context.lastWrittenOffset);\n        assertEquals(3, context.lastCommittedOffset);\n\n        context.commitOffset(10L, \"group\", \"foo\", 1, 111L, 1, context.time.milliseconds());\n        context.commitOffset(10L, \"group\", \"bar\", 0, 201L, 1, context.time.milliseconds());\n        // Note that bar-1 does not exist in the initial commits. UNSTABLE_OFFSET_COMMIT errors\n        // must be returned in this case too.\n        context.commitOffset(10L, \"group\", \"bar\", 1, 211L, 1, context.time.milliseconds());\n\n        // Always use the same request.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> request = Arrays.asList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Arrays.asList(0, 1)),\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"bar\")\n                .setPartitionIndexes(Arrays.asList(0, 1))\n        );\n\n        // Fetching offsets with \"require stable\" (Long.MAX_VALUE) should return the committed offset for\n        // foo-0 and the UNSTABLE_OFFSET_COMMIT error for foo-1, bar-0 and bar-1.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, Errors.UNSTABLE_OFFSET_COMMIT)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, Errors.UNSTABLE_OFFSET_COMMIT),\n                    mkOffsetPartitionResponse(1, Errors.UNSTABLE_OFFSET_COMMIT)\n                ))\n        ), context.fetchOffsets(\"group\", request, Long.MAX_VALUE));\n\n        // Fetching offsets without \"require stable\" (lastCommittedOffset) should return the committed\n        // offset for foo-0, foo-1 and bar-0 and the INVALID_OFFSET for bar-1.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, context.lastCommittedOffset));\n\n        // Commit the ongoing transaction.\n        context.replayEndTransactionMarker(10L, TransactionResult.COMMIT);\n\n        // Fetching offsets with \"require stable\" (Long.MAX_VALUE) should not return any errors now.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 201L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 211L, 1, \"metadata\")\n                ))\n        ), context.fetchOffsets(\"group\", request, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testGenericGroupFetchAllOffsetsWithDeadGroup() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a dead group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"group\",\n            true\n        );\n        group.transitionTo(ClassicGroupState.DEAD);\n\n        assertEquals(Collections.emptyList(), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchAllOffsetsWithUnknownGroup() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        assertEquals(Collections.emptyList(), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchAllOffsetsAtDifferentCommittedOffset() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\"group\", true);\n\n        assertEquals(0, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n        assertEquals(1, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 1, 110L, 1);\n        assertEquals(2, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"bar\", 0, 200L, 1);\n        assertEquals(3, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 1, 111L, 2);\n        assertEquals(4, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"bar\", 1, 210L, 2);\n        assertEquals(5, context.lastWrittenOffset);\n\n        // Fetching with 0 should no offsets.\n        assertEquals(Collections.emptyList(), context.fetchAllOffsets(\"group\", 0L));\n\n        // Fetching with 1 should return data up to offset 1.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", 1L));\n\n        // Fetching with 2 should return data up to offset 2.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", 2L));\n\n        // Fetching with 3 should return data up to offset 3.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", 3L));\n\n        // Fetching with 4 should return data up to offset 4.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 2, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", 4L));\n\n        // Fetching with Long.MAX_VALUE should return all offsets.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 210L, 2, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 2, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchAllOffsetsWithPendingTransactionalOffsets() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\"group\", true);\n\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n        context.commitOffset(\"group\", \"foo\", 1, 110L, 1);\n        context.commitOffset(\"group\", \"bar\", 0, 200L, 1);\n\n        context.commit();\n\n        assertEquals(3, context.lastWrittenOffset);\n        assertEquals(3, context.lastCommittedOffset);\n\n        context.commitOffset(10L, \"group\", \"foo\", 1, 111L, 1, context.time.milliseconds());\n        context.commitOffset(10L, \"group\", \"bar\", 0, 201L, 1, context.time.milliseconds());\n        // Note that bar-1 does not exist in the initial commits. The API does not return it at all until\n        // the transaction is committed.\n        context.commitOffset(10L, \"group\", \"bar\", 1, 211L, 1, context.time.milliseconds());\n\n        // Fetching offsets with \"require stable\" (Long.MAX_VALUE) should return the committed offset for\n        // foo-0 and the UNSTABLE_OFFSET_COMMIT error for foo-1 and bar-0.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, Errors.UNSTABLE_OFFSET_COMMIT)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, Errors.UNSTABLE_OFFSET_COMMIT)\n                ))\n        ), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n\n        // Fetching offsets without \"require stable\" (lastCommittedOffset) should the committed\n        // offset for the foo-0, foo-1 and bar-0.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", context.lastCommittedOffset));\n\n        // Commit the ongoing transaction.\n        context.replayEndTransactionMarker(10L, TransactionResult.COMMIT);\n\n        // Fetching offsets with \"require stable\" (Long.MAX_VALUE) should not return any errors now.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 201L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 211L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testConsumerGroupOffsetFetchWithMemberIdAndEpoch() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        // Create consumer group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\"group\", true);\n        // Create member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\").build());\n        // Commit offset.\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n\n        // Fetch offsets case.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics = Collections.singletonList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        assertEquals(Collections.singletonList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Collections.singletonList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\")\n                ))\n        ), context.fetchOffsets(\"group\", \"member\", 0, topics, Long.MAX_VALUE));\n\n        // Fetch all offsets case.\n        assertEquals(Collections.singletonList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Collections.singletonList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", \"member\", 0, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testConsumerGroupOffsetFetchFromAdminClient() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        // Create consumer group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\"group\", true);\n        // Create member.\n        group.getOrMaybeCreateMember(\"member\", true);\n        // Commit offset.\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n\n        // Fetch offsets case.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics = Collections.singletonList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        assertEquals(Collections.singletonList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Collections.singletonList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\")\n                ))\n        ), context.fetchOffsets(\"group\", topics, Long.MAX_VALUE));\n\n        // Fetch all offsets case.\n        assertEquals(Collections.singletonList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Collections.singletonList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testConsumerGroupOffsetFetchWithUnknownMemberId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\"group\", true);\n\n        // Fetch offsets case.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics = Collections.singletonList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        // Fetch offsets cases.\n        assertThrows(UnknownMemberIdException.class,\n            () -> context.fetchOffsets(\"group\", \"\", 0, topics, Long.MAX_VALUE));\n        assertThrows(UnknownMemberIdException.class,\n            () -> context.fetchOffsets(\"group\", \"member\", 0, topics, Long.MAX_VALUE));\n\n        // Fetch all offsets cases.\n        assertThrows(UnknownMemberIdException.class,\n            () -> context.fetchAllOffsets(\"group\", \"\", 0, Long.MAX_VALUE));\n        assertThrows(UnknownMemberIdException.class,\n            () -> context.fetchAllOffsets(\"group\", \"member\", 0, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testConsumerGroupOffsetFetchWithStaleMemberEpoch() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\"group\", true);\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\").build());\n\n        // Fetch offsets case.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics = Collections.singletonList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        // Fetch offsets case.\n        assertThrows(StaleMemberEpochException.class,\n            () -> context.fetchOffsets(\"group\", \"member\", 10, topics, Long.MAX_VALUE));\n\n        // Fetch all offsets case.\n        assertThrows(StaleMemberEpochException.class,\n            () -> context.fetchAllOffsets(\"group\", \"member\", 10, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testConsumerGroupOffsetFetchWithIllegalGenerationId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\"group\", true);\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n                .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata())\n                .build()\n        );\n\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics = Collections.singletonList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        // Fetch offsets case.\n        assertThrows(IllegalGenerationException.class,\n            () -> context.fetchOffsets(\"group\", \"member\", 10, topics, Long.MAX_VALUE));\n\n        // Fetch all offsets case.\n        assertThrows(IllegalGenerationException.class,\n            () -> context.fetchAllOffsets(\"group\", \"member\", 10, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testGenericGroupOffsetDelete() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n        context.commitOffset(\"foo\", \"bar\", 0, 100L, 0);\n        group.setSubscribedTopics(Optional.of(Collections.emptySet()));\n        context.testOffsetDeleteWith(\"foo\", \"bar\", 0, Errors.NONE);\n        assertFalse(context.hasOffset(\"foo\", \"bar\", 0));\n    }\n\n    @Test\n    public void testGenericGroupOffsetDeleteWithErrors() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n        group.setSubscribedTopics(Optional.of(Collections.singleton(\"bar\")));\n        context.commitOffset(\"foo\", \"bar\", 0, 100L, 0);\n\n        // Delete the offset whose topic partition doesn't exist.\n        context.testOffsetDeleteWith(\"foo\", \"bar1\", 0, Errors.NONE);\n        // Delete the offset from the topic that the group is subscribed to.\n        context.testOffsetDeleteWith(\"foo\", \"bar\", 0, Errors.GROUP_SUBSCRIBED_TO_TOPIC);\n    }\n\n    @Test\n    public void testGenericGroupOffsetDeleteWithPendingTransactionalOffsets() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n        context.commitOffset(10L, \"foo\", \"bar\", 0, 100L, 0, context.time.milliseconds());\n        group.setSubscribedTopics(Optional.of(Collections.emptySet()));\n        context.testOffsetDeleteWith(\"foo\", \"bar\", 0, Errors.NONE);\n        assertFalse(context.hasOffset(\"foo\", \"bar\", 0));\n    }\n\n    @Test\n    public void testConsumerGroupOffsetDelete() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n            \"foo\",\n            true\n        );\n        context.commitOffset(\"foo\", \"bar\", 0, 100L, 0);\n        assertFalse(group.isSubscribedToTopic(\"bar\"));\n        context.testOffsetDeleteWith(\"foo\", \"bar\", 0, Errors.NONE);\n    }\n\n    @Test\n    public void testConsumerGroupOffsetDeleteWithErrors() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n            \"foo\",\n            true\n        );\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(Uuid.randomUuid(), \"foo\", 1)\n            .addRacks()\n            .build();\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n        group.computeSubscriptionMetadata(\n            group.computeSubscribedTopicNames(null, member1),\n            image.topics(),\n            image.cluster()\n        );\n        group.updateMember(member1);\n        context.commitOffset(\"foo\", \"bar\", 0, 100L, 0);\n        assertTrue(group.isSubscribedToTopic(\"bar\"));\n\n        // Delete the offset whose topic partition doesn't exist.\n        context.testOffsetDeleteWith(\"foo\", \"bar1\", 0, Errors.NONE);\n        // Delete the offset from the topic that the group is subscribed to.\n        context.testOffsetDeleteWith(\"foo\", \"bar\", 0, Errors.GROUP_SUBSCRIBED_TO_TOPIC);\n    }\n\n    @Test\n    public void testConsumerGroupOffsetDeleteWithPendingTransactionalOffsets() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n            \"foo\",\n            true\n        );\n        context.commitOffset(10L, \"foo\", \"bar\", 0, 100L, 0, context.time.milliseconds());\n        assertFalse(group.isSubscribedToTopic(\"bar\"));\n        context.testOffsetDeleteWith(\"foo\", \"bar\", 0, Errors.NONE);\n        assertFalse(context.hasOffset(\"foo\", \"bar\", 0));\n    }\n\n    @ParameterizedTest\n    @EnumSource(value = Group.GroupType.class, names = {\"CLASSIC\", \"CONSUMER\"})\n    public void testDeleteGroupAllOffsets(Group.GroupType groupType) {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        context.getOrMaybeCreateGroup(groupType, \"foo\");\n\n        context.commitOffset(\"foo\", \"bar-0\", 0, 100L, 0);\n        context.commitOffset(\"foo\", \"bar-0\", 1, 100L, 0);\n        context.commitOffset(\"foo\", \"bar-1\", 0, 100L, 0);\n\n        List<CoordinatorRecord> expectedRecords = Arrays.asList(\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"foo\", \"bar-1\", 0),\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"foo\", \"bar-0\", 0),\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"foo\", \"bar-0\", 1)\n        );\n\n        List<CoordinatorRecord> records = new ArrayList<>();\n        int numDeleteOffsets = context.deleteAllOffsets(\"foo\", records);\n\n        assertEquals(expectedRecords, records);\n        assertEquals(3, numDeleteOffsets);\n    }\n\n    @ParameterizedTest\n    @EnumSource(value = Group.GroupType.class, names = {\"CLASSIC\", \"CONSUMER\"})\n    public void testDeleteGroupAllOffsetsWithPendingTransactionalOffsets(Group.GroupType groupType) {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        context.getOrMaybeCreateGroup(groupType, \"foo\");\n\n        context.commitOffset(\"foo\", \"bar-0\", 0, 100L, 0);\n        context.commitOffset(\"foo\", \"bar-0\", 1, 100L, 0);\n        context.commitOffset(\"foo\", \"bar-1\", 0, 100L, 0);\n\n        context.commitOffset(10L, \"foo\", \"bar-1\", 0, 101L, 0, context.time.milliseconds());\n        context.commitOffset(10L, \"foo\", \"bar-2\", 0, 100L, 0, context.time.milliseconds());\n\n        List<CoordinatorRecord> expectedRecords = Arrays.asList(\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"foo\", \"bar-1\", 0),\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"foo\", \"bar-0\", 0),\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"foo\", \"bar-0\", 1),\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"foo\", \"bar-2\", 0)\n        );\n\n        List<CoordinatorRecord> records = new ArrayList<>();\n        int numDeleteOffsets = context.deleteAllOffsets(\"foo\", records);\n\n        assertEquals(expectedRecords, records);\n        assertEquals(4, numDeleteOffsets);\n\n        assertFalse(context.hasOffset(\"foo\", \"bar-0\", 0));\n        assertFalse(context.hasOffset(\"foo\", \"bar-0\", 1));\n        assertFalse(context.hasOffset(\"foo\", \"bar-1\", 0));\n        assertFalse(context.hasOffset(\"foo\", \"bar-2\", 0));\n    }\n\n    @Test\n    public void testCleanupExpiredOffsetsGroupHasNoOffsets() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .build();\n\n        List<CoordinatorRecord> records = new ArrayList<>();\n        assertTrue(context.cleanupExpiredOffsets(\"unknown-group-id\", records));\n        assertEquals(Collections.emptyList(), records);\n    }\n\n    @Test\n    public void testCleanupExpiredOffsetsGroupDoesNotExist() {\n        GroupMetadataManager groupMetadataManager = mock(GroupMetadataManager.class);\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .withGroupMetadataManager(groupMetadataManager)\n            .build();\n\n        when(groupMetadataManager.group(\"unknown-group-id\")).thenThrow(GroupIdNotFoundException.class);\n        context.commitOffset(\"unknown-group-id\", \"topic\", 0, 100L, 0);\n        assertThrows(GroupIdNotFoundException.class, () -> context.cleanupExpiredOffsets(\"unknown-group-id\", new ArrayList<>()));\n    }\n\n    @Test\n    public void testCleanupExpiredOffsetsEmptyOffsetExpirationCondition() {\n        GroupMetadataManager groupMetadataManager = mock(GroupMetadataManager.class);\n        Group group = mock(Group.class);\n\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .withGroupMetadataManager(groupMetadataManager)\n            .build();\n\n        context.commitOffset(\"group-id\", \"topic\", 0, 100L, 0);\n\n        when(groupMetadataManager.group(\"group-id\")).thenReturn(group);\n        when(group.offsetExpirationCondition()).thenReturn(Optional.empty());\n\n        List<CoordinatorRecord> records = new ArrayList<>();\n        assertFalse(context.cleanupExpiredOffsets(\"group-id\", records));\n        assertEquals(Collections.emptyList(), records);\n    }\n\n    @Test\n    public void testCleanupExpiredOffsets() {\n        GroupMetadataManager groupMetadataManager = mock(GroupMetadataManager.class);\n        Group group = mock(Group.class);\n\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .withGroupMetadataManager(groupMetadataManager)\n            .withOffsetsRetentionMinutes(1)\n            .build();\n\n        long commitTimestamp = context.time.milliseconds();\n\n        context.commitOffset(\"group-id\", \"firstTopic\", 0, 100L, 0, commitTimestamp);\n        context.commitOffset(\"group-id\", \"secondTopic\", 0, 100L, 0, commitTimestamp);\n        context.commitOffset(\"group-id\", \"secondTopic\", 1, 100L, 0, commitTimestamp + 500);\n\n        context.time.sleep(Duration.ofMinutes(1).toMillis());\n\n        // firstTopic-0: group is still subscribed to firstTopic. Do not expire.\n        // secondTopic-0: should expire as offset retention has passed.\n        // secondTopic-1: has not passed offset retention. Do not expire.\n        List<CoordinatorRecord> expectedRecords = Collections.singletonList(\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"secondTopic\", 0)\n        );\n\n        when(groupMetadataManager.group(\"group-id\")).thenReturn(group);\n        when(group.offsetExpirationCondition()).thenReturn(Optional.of(\n            new OffsetExpirationConditionImpl(offsetAndMetadata -> offsetAndMetadata.commitTimestampMs)));\n        when(group.isSubscribedToTopic(\"firstTopic\")).thenReturn(true);\n        when(group.isSubscribedToTopic(\"secondTopic\")).thenReturn(false);\n\n        List<CoordinatorRecord> records = new ArrayList<>();\n        assertFalse(context.cleanupExpiredOffsets(\"group-id\", records));\n        assertEquals(expectedRecords, records);\n\n        // Expire secondTopic-1.\n        context.time.sleep(500);\n        expectedRecords = Collections.singletonList(\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"secondTopic\", 1)\n        );\n\n        records = new ArrayList<>();\n        assertFalse(context.cleanupExpiredOffsets(\"group-id\", records));\n        assertEquals(expectedRecords, records);\n\n        // Add 2 more commits, then expire all.\n        when(group.isSubscribedToTopic(\"firstTopic\")).thenReturn(false);\n        context.commitOffset(\"group-id\", \"firstTopic\", 1, 100L, 0, commitTimestamp + 500);\n        context.commitOffset(\"group-id\", \"secondTopic\", 0, 101L, 0, commitTimestamp + 500);\n\n        expectedRecords = Arrays.asList(\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"firstTopic\", 0),\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"firstTopic\", 1),\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"secondTopic\", 0)\n        );\n\n        records = new ArrayList<>();\n        assertTrue(context.cleanupExpiredOffsets(\"group-id\", records));\n        assertEquals(expectedRecords, records);\n    }\n\n    @Test\n    public void testCleanupExpiredOffsetsWithPendingTransactionalOffsets() {\n        GroupMetadataManager groupMetadataManager = mock(GroupMetadataManager.class);\n        Group group = mock(Group.class);\n\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .withGroupMetadataManager(groupMetadataManager)\n            .withOffsetsRetentionMinutes(1)\n            .build();\n\n        long commitTimestamp = context.time.milliseconds();\n\n        context.commitOffset(\"group-id\", \"foo\", 0, 100L, 0, commitTimestamp);\n        context.commitOffset(10L, \"group-id\", \"foo\", 0, 101L, 0, commitTimestamp + 500);\n\n        context.time.sleep(Duration.ofMinutes(1).toMillis());\n\n        when(groupMetadataManager.group(\"group-id\")).thenReturn(group);\n        when(group.offsetExpirationCondition()).thenReturn(Optional.of(\n            new OffsetExpirationConditionImpl(offsetAndMetadata -> offsetAndMetadata.commitTimestampMs)));\n        when(group.isSubscribedToTopic(\"foo\")).thenReturn(false);\n\n        // foo-0 should not be expired because it has a pending transactional offset commit.\n        List<CoordinatorRecord> records = new ArrayList<>();\n        assertFalse(context.cleanupExpiredOffsets(\"group-id\", records));\n        assertEquals(Collections.emptyList(), records);\n    }\n\n    private static OffsetFetchResponseData.OffsetFetchResponsePartitions mkOffsetPartitionResponse(\n        int partition,\n        long offset,\n        int leaderEpoch,\n        String metadata\n    ) {\n        return new OffsetFetchResponseData.OffsetFetchResponsePartitions()\n            .setPartitionIndex(partition)\n            .setCommittedOffset(offset)\n            .setCommittedLeaderEpoch(leaderEpoch)\n            .setMetadata(metadata);\n    }\n\n    private static OffsetFetchResponseData.OffsetFetchResponsePartitions mkInvalidOffsetPartitionResponse(int partition) {\n        return new OffsetFetchResponseData.OffsetFetchResponsePartitions()\n            .setPartitionIndex(partition)\n            .setCommittedOffset(INVALID_OFFSET)\n            .setCommittedLeaderEpoch(-1)\n            .setMetadata(\"\");\n    }\n\n    private static OffsetFetchResponseData.OffsetFetchResponsePartitions mkOffsetPartitionResponse(int partition, Errors error) {\n        return new OffsetFetchResponseData.OffsetFetchResponsePartitions()\n            .setPartitionIndex(partition)\n            .setErrorCode(error.code())\n            .setCommittedOffset(INVALID_OFFSET)\n            .setCommittedLeaderEpoch(-1)\n            .setMetadata(\"\");\n    }\n\n    @Test\n    public void testReplay() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        verifyReplay(context, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            0L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyReplay(context, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            1L,\n            200L,\n            OptionalInt.of(10),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyReplay(context, \"foo\", \"bar\", 1, new OffsetAndMetadata(\n            2L,\n            200L,\n            OptionalInt.of(10),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyReplay(context, \"foo\", \"bar\", 1, new OffsetAndMetadata(\n            3L,\n            300L,\n            OptionalInt.of(10),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.of(12345L)\n        ));\n    }\n\n    @Test\n    public void testTransactionalReplay() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        verifyTransactionalReplay(context, 5, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            0L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyTransactionalReplay(context, 5, \"foo\", \"bar\", 1, new OffsetAndMetadata(\n            1L,\n            101L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyTransactionalReplay(context, 5, \"bar\", \"zar\", 0, new OffsetAndMetadata(\n            2L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyTransactionalReplay(context, 5, \"bar\", \"zar\", 1, new OffsetAndMetadata(\n            3L,\n            101L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyTransactionalReplay(context, 6, \"foo\", \"bar\", 2, new OffsetAndMetadata(\n            4L,\n            102L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyTransactionalReplay(context, 6, \"foo\", \"bar\", 3, new OffsetAndMetadata(\n            5L,\n            102L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n    }\n\n    @Test\n    public void testReplayWithTombstoneAndPendingTransactionalOffsets() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Add the offsets.\n        verifyReplay(context, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            0L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyTransactionalReplay(context, 10L, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            1L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyTransactionalReplay(context, 10L, \"foo\", \"bar\", 1, new OffsetAndMetadata(\n            2L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Delete the offsets.\n        context.replay(CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\n            \"foo\",\n            \"bar\",\n            0\n        ));\n\n        context.replay(CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\n            \"foo\",\n            \"bar\",\n            1\n        ));\n\n        // Verify that the offset is gone.\n        assertFalse(context.hasOffset(\"foo\", \"bar\", 0));\n        assertFalse(context.hasOffset(\"foo\", \"bar\", 1));\n    }\n\n    @Test\n    public void testReplayTransactionEndMarkerWithCommit() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Add regular offset commit.\n        verifyReplay(context, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            0L,\n            99L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Add pending transactional commit for producer id 5.\n        verifyTransactionalReplay(context, 5L, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            1L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Add pending transactional commit for producer id 6.\n        verifyTransactionalReplay(context, 6L, \"foo\", \"bar\", 1, new OffsetAndMetadata(\n            2L,\n            200L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Replaying an end marker with an unknown producer id should not fail.\n        context.replayEndTransactionMarker(1L, TransactionResult.COMMIT);\n\n        // Replaying an end marker to commit transaction of producer id 5.\n        context.replayEndTransactionMarker(5L, TransactionResult.COMMIT);\n\n        // The pending offset is removed...\n        assertNull(context.offsetMetadataManager.pendingTransactionalOffset(\n            5L,\n            \"foo\",\n            \"bar\",\n            0\n        ));\n\n        // ... and added to the main offset storage.\n        assertEquals(new OffsetAndMetadata(\n            1L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ), context.offsetMetadataManager.offset(\n            \"foo\",\n            \"bar\",\n            0\n        ));\n\n        // Replaying an end marker to abort transaction of producer id 6.\n        context.replayEndTransactionMarker(6L, TransactionResult.ABORT);\n\n        // The pending offset is removed from the pending offsets and\n        // it is not added to the main offset storage.\n        assertNull(context.offsetMetadataManager.pendingTransactionalOffset(\n            6L,\n            \"foo\",\n            \"bar\",\n            1\n        ));\n        assertNull(context.offsetMetadataManager.offset(\n            \"foo\",\n            \"bar\",\n            1\n        ));\n    }\n\n    @Test\n    public void testReplayTransactionEndMarkerKeepsTheMostRecentCommittedOffset() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Add pending transactional offset commit for producer id 5.\n        verifyTransactionalReplay(context, 5L, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            0L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Add regular offset commit.\n        verifyReplay(context, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            1L,\n            101L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Replaying an end marker to commit transaction of producer id 5.\n        context.replayEndTransactionMarker(5L, TransactionResult.COMMIT);\n\n        // The pending offset is removed...\n        assertNull(context.offsetMetadataManager.pendingTransactionalOffset(\n            5L,\n            \"foo\",\n            \"bar\",\n            0\n        ));\n\n        // ... but it is not added to the main storage because the regular\n        // committed offset is more recent.\n        assertEquals(new OffsetAndMetadata(\n            1L,\n            101L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ), context.offsetMetadataManager.offset(\n            \"foo\",\n            \"bar\",\n            0\n        ));\n    }\n\n    @Test\n    public void testOffsetCommitsNumberMetricWithTransactionalOffsets() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Add pending transactional commit for producer id 4.\n        verifyTransactionalReplay(context, 4L, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            0L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Add pending transactional commit for producer id 5.\n        verifyTransactionalReplay(context, 5L, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            1L,\n            101L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Add pending transactional commit for producer id 6.\n        verifyTransactionalReplay(context, 6L, \"foo\", \"bar\", 1, new OffsetAndMetadata(\n            2L,\n            200L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Commit all the transactions.\n        context.replayEndTransactionMarker(4L, TransactionResult.COMMIT);\n        context.replayEndTransactionMarker(5L, TransactionResult.COMMIT);\n        context.replayEndTransactionMarker(6L, TransactionResult.COMMIT);\n\n        // Verify the sensor is called twice as we have only\n        // two partitions.\n        verify(context.metrics, times(2)).incrementNumOffsets();\n    }\n\n    @Test\n    public void testOffsetCommitsSensor() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n        group.transitionTo(ClassicGroupState.STABLE);\n\n        CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(1)\n                .setRetentionTimeMs(1234L)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Arrays.asList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L),\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(1)\n                                .setCommittedOffset(150L)\n                        ))\n                ))\n        );\n\n        verify(context.metrics).record(OFFSET_COMMITS_SENSOR_NAME, 2);\n    }\n\n    @Test\n    public void testOffsetsExpiredSensor() {\n        GroupMetadataManager groupMetadataManager = mock(GroupMetadataManager.class);\n        Group group = mock(Group.class);\n\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .withGroupMetadataManager(groupMetadataManager)\n            .withOffsetsRetentionMinutes(1)\n            .build();\n\n        long commitTimestamp = context.time.milliseconds();\n\n        context.commitOffset(\"group-id\", \"firstTopic\", 0, 100L, 0, commitTimestamp);\n        context.commitOffset(\"group-id\", \"secondTopic\", 0, 100L, 0, commitTimestamp);\n        context.commitOffset(\"group-id\", \"secondTopic\", 1, 100L, 0, commitTimestamp + 500);\n\n        context.time.sleep(Duration.ofMinutes(1).toMillis());\n\n        // firstTopic-0: group is still subscribed to firstTopic. Do not expire.\n        // secondTopic-0: should expire as offset retention has passed.\n        // secondTopic-1: has not passed offset retention. Do not expire.\n        List<CoordinatorRecord> expectedRecords = Collections.singletonList(\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"secondTopic\", 0)\n        );\n\n        when(groupMetadataManager.group(\"group-id\")).thenReturn(group);\n        when(group.offsetExpirationCondition()).thenReturn(Optional.of(\n            new OffsetExpirationConditionImpl(offsetAndMetadata -> offsetAndMetadata.commitTimestampMs)));\n        when(group.isSubscribedToTopic(\"firstTopic\")).thenReturn(true);\n        when(group.isSubscribedToTopic(\"secondTopic\")).thenReturn(false);\n\n        List<CoordinatorRecord> records = new ArrayList<>();\n        assertFalse(context.cleanupExpiredOffsets(\"group-id\", records));\n        assertEquals(expectedRecords, records);\n\n        // Expire secondTopic-1.\n        context.time.sleep(500);\n\n        records = new ArrayList<>();\n        assertFalse(context.cleanupExpiredOffsets(\"group-id\", records));\n        verify(context.metrics, times(2)).record(OFFSET_EXPIRED_SENSOR_NAME, 1);\n\n        // Add 2 more commits, then expire all.\n        when(group.isSubscribedToTopic(\"firstTopic\")).thenReturn(false);\n        context.commitOffset(\"group-id\", \"firstTopic\", 1, 100L, 0, commitTimestamp + 500);\n        context.commitOffset(\"group-id\", \"secondTopic\", 0, 101L, 0, commitTimestamp + 500);\n\n        records = new ArrayList<>();\n        assertTrue(context.cleanupExpiredOffsets(\"group-id\", records));\n        verify(context.metrics).record(OFFSET_EXPIRED_SENSOR_NAME, 3);\n    }\n\n    @Test\n    public void testOffsetDeletionsSensor() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\"foo\", true);\n\n        context.commitOffset(\"foo\", \"bar\", 0, 100L, 0);\n        context.commitOffset(\"foo\", \"bar\", 1, 150L, 0);\n        group.setSubscribedTopics(Optional.of(Collections.emptySet()));\n\n        OffsetDeleteRequestData.OffsetDeleteRequestTopicCollection requestTopicCollection =\n            new OffsetDeleteRequestData.OffsetDeleteRequestTopicCollection(Collections.singletonList(\n                new OffsetDeleteRequestData.OffsetDeleteRequestTopic()\n                    .setName(\"bar\")\n                    .setPartitions(Arrays.asList(\n                        new OffsetDeleteRequestData.OffsetDeleteRequestPartition().setPartitionIndex(0),\n                        new OffsetDeleteRequestData.OffsetDeleteRequestPartition().setPartitionIndex(1)\n                    ))\n            ).iterator());\n\n        context.deleteOffsets(\n            new OffsetDeleteRequestData()\n                .setGroupId(\"foo\")\n                .setTopics(requestTopicCollection)\n        );\n\n        verify(context.metrics).record(OFFSET_DELETIONS_SENSOR_NAME, 2);\n    }\n\n    @Test\n    public void testOnPartitionsDeleted() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Commit offsets.\n        context.commitOffset(\"grp-0\", \"foo\", 1, 100, 1, context.time.milliseconds());\n        context.commitOffset(\"grp-0\", \"foo\", 2, 200, 1, context.time.milliseconds());\n        context.commitOffset(\"grp-0\", \"foo\", 3, 300, 1, context.time.milliseconds());\n\n        context.commitOffset(\"grp-1\", \"bar\", 1, 100, 1, context.time.milliseconds());\n        context.commitOffset(\"grp-1\", \"bar\", 2, 200, 1, context.time.milliseconds());\n        context.commitOffset(\"grp-1\", \"bar\", 3, 300, 1, context.time.milliseconds());\n\n        context.commitOffset(100L, \"grp-2\", \"foo\", 1, 100, 1, context.time.milliseconds());\n        context.commitOffset(100L, \"grp-2\", \"foo\", 2, 200, 1, context.time.milliseconds());\n        context.commitOffset(100L, \"grp-2\", \"foo\", 3, 300, 1, context.time.milliseconds());\n\n        // Delete partitions.\n        List<CoordinatorRecord> records = context.deletePartitions(Arrays.asList(\n            new TopicPartition(\"foo\", 1),\n            new TopicPartition(\"foo\", 2),\n            new TopicPartition(\"foo\", 3),\n            new TopicPartition(\"bar\", 1)\n        ));\n\n        // Verify.\n        List<CoordinatorRecord> expectedRecords = Arrays.asList(\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"grp-0\", \"foo\", 1),\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"grp-0\", \"foo\", 2),\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"grp-0\", \"foo\", 3),\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"grp-1\", \"bar\", 1),\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"grp-2\", \"foo\", 1),\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"grp-2\", \"foo\", 2),\n            CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\"grp-2\", \"foo\", 3)\n        );\n\n        assertEquals(new HashSet<>(expectedRecords), new HashSet<>(records));\n\n        assertFalse(context.hasOffset(\"grp-0\", \"foo\", 1));\n        assertFalse(context.hasOffset(\"grp-0\", \"foo\", 2));\n        assertFalse(context.hasOffset(\"grp-0\", \"foo\", 3));\n        assertFalse(context.hasOffset(\"grp-1\", \"bar\", 1));\n        assertFalse(context.hasOffset(\"grp-2\", \"foo\", 1));\n        assertFalse(context.hasOffset(\"grp-2\", \"foo\", 2));\n        assertFalse(context.hasOffset(\"grp-2\", \"foo\", 3));\n    }\n\n    private void verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    ) {\n        context.replay(CoordinatorRecordHelpers.newOffsetCommitRecord(\n            groupId,\n            topic,\n            partition,\n            offsetAndMetadata,\n            MetadataImage.EMPTY.features().metadataVersion()\n        ));\n\n        assertEquals(offsetAndMetadata, context.offsetMetadataManager.offset(\n            groupId,\n            topic,\n            partition\n        ));\n    }\n\n    private void verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    ) {\n        context.replay(producerId, CoordinatorRecordHelpers.newOffsetCommitRecord(\n            groupId,\n            topic,\n            partition,\n            offsetAndMetadata,\n            MetadataImage.EMPTY.features().metadataVersion()\n        ));\n\n        assertEquals(offsetAndMetadata, context.offsetMetadataManager.pendingTransactionalOffset(\n            producerId,\n            groupId,\n            topic,\n            partition\n        ));\n    }\n\n    private ClassicGroupMember mkGenericMember(\n        String memberId,\n        Optional<String> groupInstanceId\n    ) {\n        return new ClassicGroupMember(\n            memberId,\n            groupInstanceId,\n            \"client-id\",\n            \"host\",\n            5000,\n            5000,\n            \"consumer\",\n            new JoinGroupRequestData.JoinGroupRequestProtocolCollection(\n                Collections.singletonList(new JoinGroupRequestData.JoinGroupRequestProtocol()\n                    .setName(\"range\")\n                    .setMetadata(new byte[0])\n                ).iterator()\n            )\n        );\n    }\n}",
                "methodCount": 98
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 11,
                "candidates": [
                    {
                        "lineStart": 317,
                        "lineEnd": 329,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method fetchOffsets to class OffsetMetadataManager",
                        "description": "Move method fetchOffsets to org.apache.kafka.coordinator.group.OffsetMetadataManager\nRationale: The `fetchOffsets` method is directly involved with the offset fetching logic related to groups, which fits within the responsibility of the `OffsetMetadataManager`. This class already contains other methods related to offset management such as validation of offset fetch requests and offset commits, making it the natural home for the `fetchOffsets` method. In contrast, `MockTime` handles time management and `GroupCoordinatorMetricsShard` is focused on metrics collection, neither of which align with the functionality of fetching offsets for groups.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 350,
                        "lineEnd": 360,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method fetchAllOffsets to class OffsetMetadataManager",
                        "description": "Move method fetchAllOffsets to org.apache.kafka.coordinator.group.OffsetMetadataManager\nRationale: The `fetchAllOffsets` method deals with fetching offsets based on group IDs and committed offsets, which directly correlates with the responsibility of the `OffsetMetadataManager` class. This class already contains methods that manage offsets, such as `fetchOffsets`, `deleteOffsets`, `commitOffset`, and validation methods. Also, `OffsetMetadataManager` is tightly coupled with `GroupMetadataManager`, `SnapshotRegistry`, and the `Offsets` class, which are essential components for managing offset metadata.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 213,
                        "lineEnd": 217,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method commitOffset to class GroupCoordinatorMetricsShard",
                        "description": "Move method commitOffset to org.apache.kafka.coordinator.group.metrics.GroupCoordinatorMetricsShard\nRationale: The commitOffset method is closely related to committing offsets and dealing with coordinator record structures, which fall into the domain of the GroupCoordinatorMetricsShard class. The GroupCoordinatorMetricsShard already handles operations related to group and offset metadata, making it a more appropriate place for this method. Additionally, placing this method here would provide better encapsulation of functionality related to coordinator metrics and offset management.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 390,
                        "lineEnd": 405,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method commitOffset to class MockTime",
                        "description": "Move method commitOffset to org.apache.kafka.common.utils.MockTime\nRationale: The commitOffset() method relies on the time.milliseconds() method, which is part of the MockTime class. This strong dependency suggests that the commitOffset() method is closely related to the functionality provided by MockTime. Moving commitOffset() to MockTime would ensure encapsulation of time-related operations in one place, making the codebase easier to maintain and understand. Additionally, MockTime is designed to handle operations that involve manual advancement of time, which aligns well with the purpose of the commitOffset() method, as it likely involves operations dependent on precise timing.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 570,
                        "lineEnd": 577,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method hasOffset to class GroupMetadataManager",
                        "description": "Move method hasOffset to org.apache.kafka.coordinator.group.GroupMetadataManager\nRationale: The method hasOffset() is responsible for checking if there are committed or pending offsets for a given group, topic, and partition. This functionality is directly related to the overall group metadata management and the operation of consumer groups. The GroupMetadataManager class handles various aspects of group metadata, including offset validation and management. Therefore, moving hasOffset() to this class would align with its responsibilities and centralize the group-related operations in one place.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 407,
                        "lineEnd": 424,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method commitOffset to class OffsetMetadataManager",
                        "description": "Move method commitOffset to org.apache.kafka.coordinator.group.OffsetMetadataManager\nRationale: The commitOffset() method is concerned with managing offsets, which is directly related to the responsibilities of OffsetMetadataManager. The OffsetMetadataManager class already contains methods that handle offset commits and manages the state of offsets for different groups, topics, and partitions. This aligns perfectly with the functionality of the commitOffset() method, which simplifies offset management tasks such as committing offsets and updating the state of these offsets in the system.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 462,
                        "lineEnd": 468,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method messageOrNull to class ApiMessageAndVersion",
                        "description": "Move method messageOrNull to org.apache.kafka.server.common.ApiMessageAndVersion\nRationale: The method messageOrNull(ApiMessageAndVersion apiMessageAndVersion) deals directly with the ApiMessageAndVersion class by accessing its message method. This implies a strong relation where the method is presumably providing a utility function specific to ApiMessageAndVersion objects. Moving it to ApiMessageAndVersion consolidates all related functionalities within one class, improving encapsulation and cohesion.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 107,
                        "lineEnd": 110,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method withOffsetMetadataMaxSize to class GroupCoordinatorConfig",
                        "description": "Move method withOffsetMetadataMaxSize to org.apache.kafka.coordinator.group.GroupCoordinatorConfig\nRationale: The method withOffsetMetadataMaxSize() relies heavily on the creation and configuration of GroupCoordinatorConfig objects. Moving this method to GroupCoordinatorConfig centralizes configuration handling within the same class, improving cohesion and ensuring that configuration logic is encapsulated where it is most relevant. This move also keeps the Builder pattern aligned with the configuration setting and enhances maintainability.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 112,
                        "lineEnd": 115,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method withOffsetsRetentionMinutes to class GroupCoordinatorConfig",
                        "description": "Move method withOffsetsRetentionMinutes to org.apache.kafka.coordinator.group.GroupCoordinatorConfig\nRationale: The method withOffsetsRetentionMinutes(int offsetsRetentionMinutes) is creating a GroupCoordinatorConfig object and setting its offset retention time. Moving this method to the GroupCoordinatorConfig class is more coherent since it concerns configuration specifically related to the GroupCoordinator. By placing it in GroupCoordinatorConfig, it will be part of the same class that directly deals with the parameters and settings of the Group Coordinator, ensuring better encapsulation and cohesion.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 3128,
                        "lineEnd": 3148,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method verifyReplay to class OffsetMetadataManagerTestContext",
                        "description": "Move method verifyReplay to org.apache.kafka.coordinator.group.OffsetMetadataManagerTest.OffsetMetadataManagerTestContext\nRationale: The method `verifyReplay` is critically tied to verifying the state managed by `OffsetMetadataManagerTestContext`. It calls `context.replay` and uses `context.offsetMetadataManager.offset`, both of which are fields handled by `OffsetMetadataManagerTestContext`. This indicates that `verifyReplay` heavily relies on the context's methods and fields, making this class the most appropriate place to move the method to ensure cohesion and maintainability.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 3150,
                        "lineEnd": 3172,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method verifyTransactionalReplay to class OffsetMetadataManagerTestContext",
                        "description": "Move method verifyTransactionalReplay to org.apache.kafka.coordinator.group.OffsetMetadataManagerTest.OffsetMetadataManagerTestContext\nRationale: The `verifyTransactionalReplay` method extensively uses the `OffsetMetadataManagerTestContext` class by invoking its methods such as `replay` and accessing its `offsetMetadataManager` field. Since the method's core operations revolve around the testing context and its verification of transactional replay operations, it is more appropriate for this method to reside within the `OffsetMetadataManagerTestContext` class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "withOffsetMetadataMaxSize",
                            "method_signature": " withOffsetMetadataMaxSize(int offsetMetadataMaxSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withOffsetsRetentionMinutes",
                            "method_signature": " withOffsetsRetentionMinutes(int offsetsRetentionMinutes)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withGroupMetadataManager",
                            "method_signature": " withGroupMetadataManager(GroupMetadataManager groupMetadataManager)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": " build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getOrMaybeCreateGroup",
                            "method_signature": "public getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commit",
                            "method_signature": "public commit()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            short version,\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitTransactionalOffset",
                            "method_signature": "public commitTransactionalOffset(\n            TxnOffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deletePartitions",
                            "method_signature": "public deletePartitions(\n            List<TopicPartition> topicPartitions\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteOffsets",
                            "method_signature": "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteAllOffsets",
                            "method_signature": "public deleteAllOffsets(\n            String groupId,\n            List<CoordinatorRecord> records\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "cleanupExpiredOffsets",
                            "method_signature": "public cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public fetchOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public fetchAllOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sleep",
                            "method_signature": "public sleep(long ms)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            long producerId,\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteOffset",
                            "method_signature": "public deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            CoordinatorRecord record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            long producerId,\n            CoordinatorRecord record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testOffsetDeleteWith",
                            "method_signature": "public testOffsetDeleteWith(\n            String groupId,\n            String topic,\n            int partition,\n            Errors expectedError\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasOffset",
                            "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testOffsetCommitWithUnknownGroup",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testOffsetCommitWithUnknownGroup(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testDeleteGroupAllOffsets",
                            "method_signature": "@ParameterizedTest\n    @EnumSource(value = Group.GroupType.class, names = {\"CLASSIC\", \"CONSUMER\"})\n    public testDeleteGroupAllOffsets(Group.GroupType groupType)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testDeleteGroupAllOffsetsWithPendingTransactionalOffsets",
                            "method_signature": "@ParameterizedTest\n    @EnumSource(value = Group.GroupType.class, names = {\"CLASSIC\", \"CONSUMER\"})\n    public testDeleteGroupAllOffsetsWithPendingTransactionalOffsets(Group.GroupType groupType)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "mkOffsetPartitionResponse",
                            "method_signature": "private static mkOffsetPartitionResponse(\n        int partition,\n        long offset,\n        int leaderEpoch,\n        String metadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "mkInvalidOffsetPartitionResponse",
                            "method_signature": "private static mkInvalidOffsetPartitionResponse(int partition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "mkOffsetPartitionResponse",
                            "method_signature": "private static mkOffsetPartitionResponse(int partition, Errors error)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyReplay",
                            "method_signature": "private verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyTransactionalReplay",
                            "method_signature": "private verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "mkGenericMember",
                            "method_signature": "private mkGenericMember(\n        String memberId,\n        Optional<String> groupInstanceId\n    )",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "hasOffset",
                            "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyTransactionalReplay",
                            "method_signature": "private verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyReplay",
                            "method_signature": "private verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            CoordinatorRecord record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteOffset",
                            "method_signature": "public deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withOffsetMetadataMaxSize",
                            "method_signature": " withOffsetMetadataMaxSize(int offsetMetadataMaxSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withOffsetsRetentionMinutes",
                            "method_signature": " withOffsetsRetentionMinutes(int offsetsRetentionMinutes)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "hasOffset",
                            "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyTransactionalReplay",
                            "method_signature": "private verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyReplay",
                            "method_signature": "private verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            CoordinatorRecord record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteOffset",
                            "method_signature": "public deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withOffsetMetadataMaxSize",
                            "method_signature": " withOffsetMetadataMaxSize(int offsetMetadataMaxSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withOffsetsRetentionMinutes",
                            "method_signature": " withOffsetsRetentionMinutes(int offsetsRetentionMinutes)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )": {
                    "first": {
                        "method_name": "hasOffset",
                        "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.1723816869324546
                },
                "private verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )": {
                    "first": {
                        "method_name": "verifyTransactionalReplay",
                        "method_signature": "private verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.17461310385029913
                },
                "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )": {
                    "first": {
                        "method_name": "replayEndTransactionMarker",
                        "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.1758096837297937
                },
                "private verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )": {
                    "first": {
                        "method_name": "verifyReplay",
                        "method_signature": "private verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.17864286572149796
                },
                "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        )": {
                    "first": {
                        "method_name": "commitOffset",
                        "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.1983667055674084
                },
                "private replay(\n            CoordinatorRecord record\n        )": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "private replay(\n            CoordinatorRecord record\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2007192020373833
                },
                "public fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )": {
                    "first": {
                        "method_name": "fetchOffsets",
                        "method_signature": "public fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2030475999869505
                },
                "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )": {
                    "first": {
                        "method_name": "commitOffset",
                        "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.20895643609398898
                },
                "public deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        )": {
                    "first": {
                        "method_name": "deleteOffset",
                        "method_signature": "public deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.21044236383116743
                },
                "public fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        )": {
                    "first": {
                        "method_name": "fetchAllOffsets",
                        "method_signature": "public fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.21548227985458038
                },
                "public commitOffset(\n            OffsetCommitRequestData request\n        )": {
                    "first": {
                        "method_name": "commitOffset",
                        "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.21600261122184558
                },
                "private idempotentCreateSnapshot()": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.21841260061183598
                },
                " withOffsetMetadataMaxSize(int offsetMetadataMaxSize)": {
                    "first": {
                        "method_name": "withOffsetMetadataMaxSize",
                        "method_signature": " withOffsetMetadataMaxSize(int offsetMetadataMaxSize)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.21959560616745527
                },
                "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)": {
                    "first": {
                        "method_name": "messageOrNull",
                        "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.23077528690684884
                },
                " withOffsetsRetentionMinutes(int offsetsRetentionMinutes)": {
                    "first": {
                        "method_name": "withOffsetsRetentionMinutes",
                        "method_signature": " withOffsetsRetentionMinutes(int offsetsRetentionMinutes)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.23768245987036973
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "public fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                    "public fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        )",
                    "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                    "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                    "private replay(\n            CoordinatorRecord record\n        )",
                    "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                    "public deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                    "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        )",
                    "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                    " withOffsetMetadataMaxSize(int offsetMetadataMaxSize)",
                    " withOffsetsRetentionMinutes(int offsetsRetentionMinutes)",
                    "private verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                    "private verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                    "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                    "private idempotentCreateSnapshot()"
                ],
                "llm_response_time": 8809
            },
            "targetClassMap": {
                "fetchOffsets": {
                    "target_classes": [
                        {
                            "class_name": "MockTime",
                            "similarity_score": 0.29140866251201486
                        },
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.19588008878590737
                        },
                        {
                            "class_name": "GroupCoordinatorMetricsShard",
                            "similarity_score": 0.20744234793771407
                        },
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.17469566370073444
                        },
                        {
                            "class_name": "OffsetMetadataManager",
                            "similarity_score": 0.2195147841070221
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "OffsetMetadataManager",
                        "GroupCoordinatorMetricsShard",
                        "MockTime"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 15,
                    "similarity_metric": "cosine"
                },
                "fetchAllOffsets": {
                    "target_classes": [
                        {
                            "class_name": "MockTime",
                            "similarity_score": 0.3215270791515118
                        },
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.21612519088609763
                        },
                        {
                            "class_name": "GroupCoordinatorMetricsShard",
                            "similarity_score": 0.22888246234613863
                        },
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.19275125868235324
                        },
                        {
                            "class_name": "OffsetMetadataManager",
                            "similarity_score": 0.2422026399493032
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "OffsetMetadataManager",
                        "GroupCoordinatorMetricsShard",
                        "MockTime"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 11,
                    "similarity_metric": "cosine"
                },
                "commitOffset": {
                    "target_classes": [
                        {
                            "class_name": "MockTime",
                            "similarity_score": 0.27066922621488604
                        },
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.1424425894006403
                        },
                        {
                            "class_name": "GroupCoordinatorMetricsShard",
                            "similarity_score": 0.17366138743160878
                        },
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.1473182040609786
                        },
                        {
                            "class_name": "OffsetMetadataManager",
                            "similarity_score": 0.2025011254961084
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "OffsetMetadataManager",
                        "GroupCoordinatorMetricsShard",
                        "MockTime"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 11,
                    "similarity_metric": "cosine"
                },
                "replay": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "hasOffset": {
                    "target_classes": [
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.18169682579561122
                        },
                        {
                            "class_name": "OffsetMetadataManager",
                            "similarity_score": 0.265533546076446
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GroupMetadataManager",
                        "OffsetMetadataManager"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 10,
                    "similarity_metric": "cosine"
                },
                "deleteOffset": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "messageOrNull": {
                    "target_classes": [
                        {
                            "class_name": "ApiMessageAndVersion",
                            "similarity_score": 0.6106401198187058
                        },
                        {
                            "class_name": "MockTime",
                            "similarity_score": 0.5914470146353319
                        },
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.4220009252215372
                        },
                        {
                            "class_name": "GroupCoordinatorMetricsShard",
                            "similarity_score": 0.4615361902486966
                        },
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.35945481197585616
                        },
                        {
                            "class_name": "OffsetMetadataManager",
                            "similarity_score": 0.48967032494126206
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "ApiMessageAndVersion",
                        "OffsetMetadataManager",
                        "MockTime"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 11,
                    "similarity_metric": "cosine"
                },
                "withOffsetMetadataMaxSize": {
                    "target_classes": [
                        {
                            "class_name": "GroupCoordinatorConfig",
                            "similarity_score": 0.2360171186932256
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GroupCoordinatorConfig"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "withOffsetsRetentionMinutes": {
                    "target_classes": [
                        {
                            "class_name": "GroupCoordinatorConfig",
                            "similarity_score": 0.20082722825608562
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GroupCoordinatorConfig"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "verifyReplay": {
                    "target_classes": [
                        {
                            "class_name": "OffsetMetadataManagerTestContext",
                            "similarity_score": 0.41535297146126127
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "OffsetMetadataManagerTestContext"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "verifyTransactionalReplay": {
                    "target_classes": [
                        {
                            "class_name": "OffsetMetadataManagerTestContext",
                            "similarity_score": 0.4224932464329102
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "OffsetMetadataManagerTestContext"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "replayEndTransactionMarker": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "idempotentCreateSnapshot": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testTimelineGaugeCounters() : void in class org.apache.kafka.coordinator.group.metrics.GroupCoordinatorMetricsShardTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 48,
                    "endLine": 99,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testTimelineGaugeCounters() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 64,
                    "endLine": 64,
                    "startColumn": 9,
                    "endColumn": 52,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 48,
                    "endLine": 99,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testTimelineGaugeCounters() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 64,
                    "endLine": 64,
                    "startColumn": 9,
                    "endColumn": 56,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(1000)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 90,
                    "endLine": 90,
                    "startColumn": 9,
                    "endColumn": 56,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(2000)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 609,
        "extraction_results": {
            "success": true,
            "newCommitHash": "a90c2d6088f885dc16d184ed13771ee9623b805d",
            "newBranchName": "extract-idempotentCreateSnapshot-testTimelineGaugeCounters-130af38"
        },
        "telemetry": {
            "id": "71a31caa-6d40-4ba3-a048-d6fbf98ca25f",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 201,
                "lineStart": 46,
                "lineEnd": 246,
                "bodyLineStart": 46,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                "sourceCode": "public class GroupCoordinatorMetricsShardTest {\n\n    @Test\n    public void testTimelineGaugeCounters() {\n        MetricsRegistry registry = new MetricsRegistry();\n        Metrics metrics = new Metrics();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        TopicPartition tp = new TopicPartition(\"__consumer_offsets\", 0);\n        GroupCoordinatorMetrics coordinatorMetrics = new GroupCoordinatorMetrics(registry, metrics);\n        GroupCoordinatorMetricsShard shard = coordinatorMetrics.newMetricsShard(snapshotRegistry, tp);\n\n        shard.incrementNumOffsets();\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY);\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING);\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING);\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE);\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD);\n\n        idempotentCreateSnapshot(snapshotRegistry);\n        // The value should not be updated until the offset has been committed.\n        assertEquals(0, shard.numOffsets());\n        assertEquals(0, shard.numConsumerGroups());\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD));\n\n        shard.commitUpTo(1000);\n        assertEquals(1, shard.numOffsets());\n        assertEquals(5, shard.numConsumerGroups());\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD));\n\n        shard.decrementNumOffsets();\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY);\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING);\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING);\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE);\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD);\n\n        snapshotRegistry.getOrCreateSnapshot(2000);\n        shard.commitUpTo(2000);\n        assertEquals(0, shard.numOffsets());\n        assertEquals(0, shard.numConsumerGroups());\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD));\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(1000);\n    }\n\n    @Test\n    public void testGenericGroupStateTransitionMetrics() {\n        MetricsRegistry registry = new MetricsRegistry();\n        Metrics metrics = new Metrics();\n        TopicPartition tp = new TopicPartition(\"__consumer_offsets\", 0);\n        GroupCoordinatorMetrics coordinatorMetrics = new GroupCoordinatorMetrics(registry, metrics);\n        GroupCoordinatorMetricsShard shard = coordinatorMetrics.newMetricsShard(new SnapshotRegistry(new LogContext()), tp);\n        coordinatorMetrics.activateMetricsShard(shard);\n\n        LogContext logContext = new LogContext();\n        ClassicGroup group0 = new ClassicGroup(logContext, \"groupId0\", EMPTY, Time.SYSTEM, shard);\n        ClassicGroup group1 = new ClassicGroup(logContext, \"groupId1\", EMPTY, Time.SYSTEM, shard);\n        ClassicGroup group2 = new ClassicGroup(logContext, \"groupId2\", EMPTY, Time.SYSTEM, shard);\n        ClassicGroup group3 = new ClassicGroup(logContext, \"groupId3\", EMPTY, Time.SYSTEM, shard);\n\n        IntStream.range(0, 4).forEach(__ -> shard.incrementNumClassicGroups(EMPTY));\n\n        assertEquals(4, shard.numClassicGroups());\n\n        group0.transitionTo(PREPARING_REBALANCE);\n        group0.transitionTo(COMPLETING_REBALANCE);\n        group1.transitionTo(PREPARING_REBALANCE);\n        group2.transitionTo(DEAD);\n\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.EMPTY));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.PREPARING_REBALANCE));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.COMPLETING_REBALANCE));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.DEAD));\n        assertEquals(0, shard.numClassicGroups(ClassicGroupState.STABLE));\n\n        group0.transitionTo(STABLE);\n        group1.transitionTo(COMPLETING_REBALANCE);\n        group3.transitionTo(DEAD);\n\n        assertEquals(0, shard.numClassicGroups(ClassicGroupState.EMPTY));\n        assertEquals(0, shard.numClassicGroups(ClassicGroupState.PREPARING_REBALANCE));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.COMPLETING_REBALANCE));\n        assertEquals(2, shard.numClassicGroups(ClassicGroupState.DEAD));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.STABLE));\n\n        assertGaugeValue(\n            metrics,\n            metrics.metricName(\"group-count\", \"group-coordinator-metrics\", Collections.singletonMap(\"protocol\", \"classic\")),\n            4\n        );\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroups\"), 4);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsEmpty\"), 0);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsPreparingRebalance\"), 0);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsCompletingRebalance\"), 1);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsDead\"), 2);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsStable\"), 1);\n    }\n\n    @Test\n    public void testConsumerGroupStateTransitionMetrics() {\n        MetricsRegistry registry = new MetricsRegistry();\n        Metrics metrics = new Metrics();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        TopicPartition tp = new TopicPartition(\"__consumer_offsets\", 0);\n        GroupCoordinatorMetrics coordinatorMetrics = new GroupCoordinatorMetrics(registry, metrics);\n        GroupCoordinatorMetricsShard shard = coordinatorMetrics.newMetricsShard(snapshotRegistry, tp);\n        coordinatorMetrics.activateMetricsShard(shard);\n\n        ConsumerGroup group0 = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-0\",\n            shard\n        );\n        ConsumerGroup group1 = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-1\",\n            shard\n        );\n        ConsumerGroup group2 = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-2\",\n            shard\n        );\n        ConsumerGroup group3 = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-3\",\n            shard\n        );\n\n        IntStream.range(0, 4).forEach(__ -> shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n\n        idempotentCreateSnapshot(snapshotRegistry);\n        shard.commitUpTo(1000);\n        assertEquals(4, shard.numConsumerGroups());\n        assertEquals(4, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n\n        ConsumerGroupMember member0 = group0.getOrMaybeCreateMember(\"member-id\", true);\n        ConsumerGroupMember member1 = group1.getOrMaybeCreateMember(\"member-id\", true);\n        ConsumerGroupMember member2 = group2.getOrMaybeCreateMember(\"member-id\", true);\n        ConsumerGroupMember member3 = group3.getOrMaybeCreateMember(\"member-id\", true);\n        group0.updateMember(member0);\n        group1.updateMember(member1);\n        group2.updateMember(member2);\n        group3.updateMember(member3);\n\n        snapshotRegistry.getOrCreateSnapshot(2000);\n        shard.commitUpTo(2000);\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(4, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n\n        group2.setGroupEpoch(1);\n        group3.setGroupEpoch(1);\n\n        snapshotRegistry.getOrCreateSnapshot(3000);\n        shard.commitUpTo(3000);\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(2, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(2, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n\n        group2.setTargetAssignmentEpoch(1);\n\n        // Set member2 to ASSIGNING state.\n        new ConsumerGroupMember.Builder(member2)\n            .setPartitionsPendingRevocation(Collections.singletonMap(Uuid.ZERO_UUID, Collections.singleton(0)))\n            .build();\n\n        snapshotRegistry.getOrCreateSnapshot(4000);\n        shard.commitUpTo(4000);\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING));\n        assertEquals(2, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n\n        assertGaugeValue(metrics, metrics.metricName(\"group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"protocol\", \"consumer\")), 4);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.EMPTY.toString())), 0);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.ASSIGNING.toString())), 1);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.RECONCILING.toString())), 1);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.STABLE.toString())), 2);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.DEAD.toString())), 0);\n    }\n}",
                "methodCount": 4
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 100,
                        "lineEnd": 102,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method `idempotentCreateSnapshot` directly interacts with the `SnapshotRegistry` class by calling its `getOrCreateSnapshot` method. This functionality is highly specific to managing snapshots which is the primary responsibility of the `SnapshotRegistry` class. Moving it to `SnapshotRegistry` ensures that all snapshot-related functionality is encapsulated within a single class, enhancing cohesion and maintainability. Additionally, the method name `idempotentCreateSnapshot` suggests it should be part of the snapshot management system present within `SnapshotRegistry`.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.07334800440146717
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                ],
                "llm_response_time": 1365
            },
            "targetClassMap": {
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 2529,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testConsumerGroupStateTransitionMetrics() : void in class org.apache.kafka.coordinator.group.metrics.GroupCoordinatorMetricsShardTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 154,
                    "endLine": 241,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testConsumerGroupStateTransitionMetrics() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 187,
                    "endLine": 187,
                    "startColumn": 9,
                    "endColumn": 52,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 154,
                    "endLine": 241,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testConsumerGroupStateTransitionMetrics() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 187,
                    "endLine": 187,
                    "startColumn": 9,
                    "endColumn": 56,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(1000)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 201,
                    "endLine": 201,
                    "startColumn": 9,
                    "endColumn": 56,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(2000)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 209,
                    "endLine": 209,
                    "startColumn": 9,
                    "endColumn": 56,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(3000)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 222,
                    "endLine": 222,
                    "startColumn": 9,
                    "endColumn": 56,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(4000)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 610,
        "extraction_results": {
            "success": true,
            "newCommitHash": "e118294a728c0c6eae034ce5820762d2b6315112",
            "newBranchName": "extract-idempotentCreateSnapshot-testConsumerGroupStateTransitionMetrics-130af38"
        },
        "telemetry": {
            "id": "1356c53d-29b8-4801-8e34-9057054a6b66",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 201,
                "lineStart": 46,
                "lineEnd": 246,
                "bodyLineStart": 46,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                "sourceCode": "public class GroupCoordinatorMetricsShardTest {\n\n    @Test\n    public void testTimelineGaugeCounters() {\n        MetricsRegistry registry = new MetricsRegistry();\n        Metrics metrics = new Metrics();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        TopicPartition tp = new TopicPartition(\"__consumer_offsets\", 0);\n        GroupCoordinatorMetrics coordinatorMetrics = new GroupCoordinatorMetrics(registry, metrics);\n        GroupCoordinatorMetricsShard shard = coordinatorMetrics.newMetricsShard(snapshotRegistry, tp);\n\n        shard.incrementNumOffsets();\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY);\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING);\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING);\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE);\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD);\n\n        idempotentCreateSnapshot(snapshotRegistry);\n        // The value should not be updated until the offset has been committed.\n        assertEquals(0, shard.numOffsets());\n        assertEquals(0, shard.numConsumerGroups());\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD));\n\n        shard.commitUpTo(1000);\n        assertEquals(1, shard.numOffsets());\n        assertEquals(5, shard.numConsumerGroups());\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD));\n\n        shard.decrementNumOffsets();\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY);\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING);\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING);\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE);\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD);\n\n        snapshotRegistry.getOrCreateSnapshot(2000);\n        shard.commitUpTo(2000);\n        assertEquals(0, shard.numOffsets());\n        assertEquals(0, shard.numConsumerGroups());\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD));\n    }\n\n    @Test\n    public void testGenericGroupStateTransitionMetrics() {\n        MetricsRegistry registry = new MetricsRegistry();\n        Metrics metrics = new Metrics();\n        TopicPartition tp = new TopicPartition(\"__consumer_offsets\", 0);\n        GroupCoordinatorMetrics coordinatorMetrics = new GroupCoordinatorMetrics(registry, metrics);\n        GroupCoordinatorMetricsShard shard = coordinatorMetrics.newMetricsShard(new SnapshotRegistry(new LogContext()), tp);\n        coordinatorMetrics.activateMetricsShard(shard);\n\n        LogContext logContext = new LogContext();\n        ClassicGroup group0 = new ClassicGroup(logContext, \"groupId0\", EMPTY, Time.SYSTEM, shard);\n        ClassicGroup group1 = new ClassicGroup(logContext, \"groupId1\", EMPTY, Time.SYSTEM, shard);\n        ClassicGroup group2 = new ClassicGroup(logContext, \"groupId2\", EMPTY, Time.SYSTEM, shard);\n        ClassicGroup group3 = new ClassicGroup(logContext, \"groupId3\", EMPTY, Time.SYSTEM, shard);\n\n        IntStream.range(0, 4).forEach(__ -> shard.incrementNumClassicGroups(EMPTY));\n\n        assertEquals(4, shard.numClassicGroups());\n\n        group0.transitionTo(PREPARING_REBALANCE);\n        group0.transitionTo(COMPLETING_REBALANCE);\n        group1.transitionTo(PREPARING_REBALANCE);\n        group2.transitionTo(DEAD);\n\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.EMPTY));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.PREPARING_REBALANCE));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.COMPLETING_REBALANCE));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.DEAD));\n        assertEquals(0, shard.numClassicGroups(ClassicGroupState.STABLE));\n\n        group0.transitionTo(STABLE);\n        group1.transitionTo(COMPLETING_REBALANCE);\n        group3.transitionTo(DEAD);\n\n        assertEquals(0, shard.numClassicGroups(ClassicGroupState.EMPTY));\n        assertEquals(0, shard.numClassicGroups(ClassicGroupState.PREPARING_REBALANCE));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.COMPLETING_REBALANCE));\n        assertEquals(2, shard.numClassicGroups(ClassicGroupState.DEAD));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.STABLE));\n\n        assertGaugeValue(\n            metrics,\n            metrics.metricName(\"group-count\", \"group-coordinator-metrics\", Collections.singletonMap(\"protocol\", \"classic\")),\n            4\n        );\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroups\"), 4);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsEmpty\"), 0);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsPreparingRebalance\"), 0);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsCompletingRebalance\"), 1);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsDead\"), 2);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsStable\"), 1);\n    }\n\n    @Test\n    public void testConsumerGroupStateTransitionMetrics() {\n        MetricsRegistry registry = new MetricsRegistry();\n        Metrics metrics = new Metrics();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        TopicPartition tp = new TopicPartition(\"__consumer_offsets\", 0);\n        GroupCoordinatorMetrics coordinatorMetrics = new GroupCoordinatorMetrics(registry, metrics);\n        GroupCoordinatorMetricsShard shard = coordinatorMetrics.newMetricsShard(snapshotRegistry, tp);\n        coordinatorMetrics.activateMetricsShard(shard);\n\n        ConsumerGroup group0 = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-0\",\n            shard\n        );\n        ConsumerGroup group1 = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-1\",\n            shard\n        );\n        ConsumerGroup group2 = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-2\",\n            shard\n        );\n        ConsumerGroup group3 = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-3\",\n            shard\n        );\n\n        IntStream.range(0, 4).forEach(__ -> shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n\n        idempotentCreateSnapshot(snapshotRegistry);\n        shard.commitUpTo(1000);\n        assertEquals(4, shard.numConsumerGroups());\n        assertEquals(4, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n\n        ConsumerGroupMember member0 = group0.getOrMaybeCreateMember(\"member-id\", true);\n        ConsumerGroupMember member1 = group1.getOrMaybeCreateMember(\"member-id\", true);\n        ConsumerGroupMember member2 = group2.getOrMaybeCreateMember(\"member-id\", true);\n        ConsumerGroupMember member3 = group3.getOrMaybeCreateMember(\"member-id\", true);\n        group0.updateMember(member0);\n        group1.updateMember(member1);\n        group2.updateMember(member2);\n        group3.updateMember(member3);\n\n        snapshotRegistry.getOrCreateSnapshot(2000);\n        shard.commitUpTo(2000);\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(4, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n\n        group2.setGroupEpoch(1);\n        group3.setGroupEpoch(1);\n\n        snapshotRegistry.getOrCreateSnapshot(3000);\n        shard.commitUpTo(3000);\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(2, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(2, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n\n        group2.setTargetAssignmentEpoch(1);\n\n        // Set member2 to ASSIGNING state.\n        new ConsumerGroupMember.Builder(member2)\n            .setPartitionsPendingRevocation(Collections.singletonMap(Uuid.ZERO_UUID, Collections.singleton(0)))\n            .build();\n\n        snapshotRegistry.getOrCreateSnapshot(4000);\n        shard.commitUpTo(4000);\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING));\n        assertEquals(2, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n\n        assertGaugeValue(metrics, metrics.metricName(\"group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"protocol\", \"consumer\")), 4);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.EMPTY.toString())), 0);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.ASSIGNING.toString())), 1);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.RECONCILING.toString())), 1);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.STABLE.toString())), 2);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.DEAD.toString())), 0);\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(1000);\n    }\n}",
                "methodCount": 4
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 242,
                        "lineEnd": 244,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method `idempotentCreateSnapshot` directly interacts with the `SnapshotRegistry` class by calling its `getOrCreateSnapshot` method. This functionality is highly specific to managing snapshots which is the primary responsibility of the `SnapshotRegistry` class. Moving it to `SnapshotRegistry` ensures that all snapshot-related functionality is encapsulated within a single class, enhancing cohesion and maintainability. Additionally, the method name `idempotentCreateSnapshot` suggests it should be part of the snapshot management system present within `SnapshotRegistry`.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.07334800440146717
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                ],
                "llm_response_time": 1782
            },
            "targetClassMap": {
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public aggregateShards() : void in class org.apache.kafka.coordinator.group.metrics.GroupCoordinatorMetricsTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsTest.java",
                    "startLine": 120,
                    "endLine": 172,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public aggregateShards() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsTest.java",
                    "startLine": 157,
                    "endLine": 157,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsTest.java",
                    "startLine": 120,
                    "endLine": 172,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public aggregateShards() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsTest.java",
                    "startLine": 157,
                    "endLine": 157,
                    "startColumn": 9,
                    "endColumn": 57,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry0.idempotentCreateSnapshot(1000)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsTest.java",
                    "startLine": 158,
                    "endLine": 158,
                    "startColumn": 9,
                    "endColumn": 57,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry1.idempotentCreateSnapshot(1500)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 611,
        "extraction_results": {
            "success": true,
            "newCommitHash": "f76c771f07e85852f1fca4bd5e7c2e881fe59b87",
            "newBranchName": "extract-idempotentCreateSnapshot-aggregateShards-130af38"
        },
        "telemetry": {
            "id": "4d1abae8-953a-45de-9e4a-0d04e2d24db5",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 159,
                "lineStart": 50,
                "lineEnd": 208,
                "bodyLineStart": 50,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsTest.java",
                "sourceCode": "public class GroupCoordinatorMetricsTest {\n\n    @Test\n    public void testMetricNames() {\n        MetricsRegistry registry = new MetricsRegistry();\n        Metrics metrics = new Metrics();\n\n        HashSet<org.apache.kafka.common.MetricName> expectedMetrics = new HashSet<>(Arrays.asList(\n            metrics.metricName(\"offset-commit-rate\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\"offset-commit-count\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\"offset-expiration-rate\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\"offset-expiration-count\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\"offset-deletion-rate\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\"offset-deletion-count\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\"group-completed-rebalance-rate\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\"group-completed-rebalance-count\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\"consumer-group-rebalance-rate\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\"consumer-group-rebalance-count\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\n                \"group-count\",\n                GroupCoordinatorMetrics.METRICS_GROUP,\n                Collections.singletonMap(\"protocol\", \"classic\")),\n            metrics.metricName(\n                \"group-count\",\n                GroupCoordinatorMetrics.METRICS_GROUP,\n                Collections.singletonMap(\"protocol\", \"consumer\")),\n            metrics.metricName(\n                \"consumer-group-count\",\n                GroupCoordinatorMetrics.METRICS_GROUP,\n                Collections.singletonMap(\"state\", ConsumerGroupState.EMPTY.toString())),\n            metrics.metricName(\n                \"consumer-group-count\",\n                GroupCoordinatorMetrics.METRICS_GROUP,\n                Collections.singletonMap(\"state\", ConsumerGroupState.ASSIGNING.toString())),\n            metrics.metricName(\n                \"consumer-group-count\",\n                GroupCoordinatorMetrics.METRICS_GROUP,\n                Collections.singletonMap(\"state\", ConsumerGroupState.RECONCILING.toString())),\n            metrics.metricName(\n                \"consumer-group-count\",\n                GroupCoordinatorMetrics.METRICS_GROUP,\n                Collections.singletonMap(\"state\", ConsumerGroupState.STABLE.toString())),\n            metrics.metricName(\n                \"consumer-group-count\",\n                GroupCoordinatorMetrics.METRICS_GROUP,\n                Collections.singletonMap(\"state\", ConsumerGroupState.DEAD.toString()))\n        ));\n\n        try {\n            try (GroupCoordinatorMetrics ignored = new GroupCoordinatorMetrics(registry, metrics)) {\n                HashSet<String> expectedRegistry = new HashSet<>(Arrays.asList(\n                    \"kafka.coordinator.group:type=GroupMetadataManager,name=NumOffsets\",\n                    \"kafka.coordinator.group:type=GroupMetadataManager,name=NumGroups\",\n                    \"kafka.coordinator.group:type=GroupMetadataManager,name=NumGroupsPreparingRebalance\",\n                    \"kafka.coordinator.group:type=GroupMetadataManager,name=NumGroupsCompletingRebalance\",\n                    \"kafka.coordinator.group:type=GroupMetadataManager,name=NumGroupsStable\",\n                    \"kafka.coordinator.group:type=GroupMetadataManager,name=NumGroupsDead\",\n                    \"kafka.coordinator.group:type=GroupMetadataManager,name=NumGroupsEmpty\"\n                ));\n\n                assertMetricsForTypeEqual(registry, \"kafka.coordinator.group\", expectedRegistry);\n                expectedMetrics.forEach(metricName -> assertTrue(metrics.metrics().containsKey(metricName)));\n            }\n            assertMetricsForTypeEqual(registry, \"kafka.coordinator.group\", Collections.emptySet());\n            expectedMetrics.forEach(metricName -> assertFalse(metrics.metrics().containsKey(metricName)));\n        } finally {\n            registry.shutdown();\n        }\n    }\n\n    @Test\n    public void aggregateShards() {\n        MetricsRegistry registry = new MetricsRegistry();\n        Metrics metrics = new Metrics();\n        GroupCoordinatorMetrics coordinatorMetrics = new GroupCoordinatorMetrics(registry, metrics);\n        SnapshotRegistry snapshotRegistry0 = new SnapshotRegistry(new LogContext());\n        SnapshotRegistry snapshotRegistry1 = new SnapshotRegistry(new LogContext());\n        TopicPartition tp0 = new TopicPartition(\"__consumer_offsets\", 0);\n        TopicPartition tp1 = new TopicPartition(\"__consumer_offsets\", 1);\n        GroupCoordinatorMetricsShard shard0 = coordinatorMetrics.newMetricsShard(snapshotRegistry0, tp0);\n        GroupCoordinatorMetricsShard shard1 = coordinatorMetrics.newMetricsShard(snapshotRegistry1, tp1);\n        coordinatorMetrics.activateMetricsShard(shard0);\n        coordinatorMetrics.activateMetricsShard(shard1);\n\n        IntStream.range(0, 5).forEach(__ -> shard0.incrementNumClassicGroups(ClassicGroupState.PREPARING_REBALANCE));\n        IntStream.range(0, 1).forEach(__ -> shard0.decrementNumClassicGroups(ClassicGroupState.COMPLETING_REBALANCE));\n        IntStream.range(0, 5).forEach(__ -> shard1.incrementNumClassicGroups(ClassicGroupState.STABLE));\n        IntStream.range(0, 4).forEach(__ -> shard1.incrementNumClassicGroups(ClassicGroupState.DEAD));\n        IntStream.range(0, 4).forEach(__ -> shard1.decrementNumClassicGroups(ClassicGroupState.EMPTY));\n\n        IntStream.range(0, 5).forEach(__ -> shard0.incrementNumConsumerGroups(ConsumerGroupState.ASSIGNING));\n        IntStream.range(0, 5).forEach(__ -> shard1.incrementNumConsumerGroups(ConsumerGroupState.RECONCILING));\n        IntStream.range(0, 3).forEach(__ -> shard1.decrementNumConsumerGroups(ConsumerGroupState.DEAD));\n\n        IntStream.range(0, 6).forEach(__ -> shard0.incrementNumOffsets());\n        IntStream.range(0, 2).forEach(__ -> shard1.incrementNumOffsets());\n        IntStream.range(0, 1).forEach(__ -> shard1.decrementNumOffsets());\n\n        assertEquals(4, shard0.numClassicGroups());\n        assertEquals(5, shard1.numClassicGroups());\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroups\"), 9);\n        assertGaugeValue(\n            metrics,\n            metrics.metricName(\"group-count\", METRICS_GROUP, Collections.singletonMap(\"protocol\", \"classic\")),\n            9\n        );\n\n        idempotentCreateSnapshot(snapshotRegistry0);\n        snapshotRegistry1.getOrCreateSnapshot(1500);\n        shard0.commitUpTo(1000);\n        shard1.commitUpTo(1500);\n\n        assertEquals(5, shard0.numConsumerGroups());\n        assertEquals(2, shard1.numConsumerGroups());\n        assertEquals(6, shard0.numOffsets());\n        assertEquals(1, shard1.numOffsets());\n        assertGaugeValue(\n            metrics,\n            metrics.metricName(\"group-count\", METRICS_GROUP, Collections.singletonMap(\"protocol\", \"consumer\")),\n            7\n        );\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumOffsets\"), 7);\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry0) {\n        snapshotRegistry0.getOrCreateSnapshot(1000);\n    }\n\n    @Test\n    public void testGlobalSensors() {\n        MetricsRegistry registry = new MetricsRegistry();\n        Time time = new MockTime();\n        Metrics metrics = new Metrics(time);\n        GroupCoordinatorMetrics coordinatorMetrics = new GroupCoordinatorMetrics(registry, metrics);\n        GroupCoordinatorMetricsShard shard = coordinatorMetrics.newMetricsShard(\n            new SnapshotRegistry(new LogContext()), new TopicPartition(\"__consumer_offsets\", 0)\n        );\n\n        shard.record(CLASSIC_GROUP_COMPLETED_REBALANCES_SENSOR_NAME, 10);\n        assertMetricValue(metrics, metrics.metricName(\"group-completed-rebalance-rate\", GroupCoordinatorMetrics.METRICS_GROUP), 1.0 / 3.0);\n        assertMetricValue(metrics, metrics.metricName(\"group-completed-rebalance-count\", GroupCoordinatorMetrics.METRICS_GROUP), 10);\n\n        shard.record(OFFSET_COMMITS_SENSOR_NAME, 20);\n        assertMetricValue(metrics, metrics.metricName(\"offset-commit-rate\", GroupCoordinatorMetrics.METRICS_GROUP), 2.0 / 3.0);\n        assertMetricValue(metrics, metrics.metricName(\"offset-commit-count\", GroupCoordinatorMetrics.METRICS_GROUP), 20);\n\n        shard.record(OFFSET_EXPIRED_SENSOR_NAME, 30);\n        assertMetricValue(metrics, metrics.metricName(\"offset-expiration-rate\", GroupCoordinatorMetrics.METRICS_GROUP), 1.0);\n        assertMetricValue(metrics, metrics.metricName(\"offset-expiration-count\", GroupCoordinatorMetrics.METRICS_GROUP), 30);\n\n        shard.record(CONSUMER_GROUP_REBALANCES_SENSOR_NAME, 50);\n        assertMetricValue(metrics, metrics.metricName(\"consumer-group-rebalance-rate\", GroupCoordinatorMetrics.METRICS_GROUP), 5.0 / 3.0);\n        assertMetricValue(metrics, metrics.metricName(\"consumer-group-rebalance-count\", GroupCoordinatorMetrics.METRICS_GROUP), 50);\n    }\n\n    private void assertMetricValue(Metrics metrics, MetricName metricName, double val) {\n        assertEquals(val, metrics.metric(metricName).metricValue());\n    }\n}",
                "methodCount": 5
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 2,
                "candidates": [
                    {
                        "lineStart": 204,
                        "lineEnd": 206,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method assertMetricValue to class Metrics",
                        "description": "Move method assertMetricValue to org.apache.kafka.common.metrics.Metrics\nRationale: The `assertMetricValue` method directly interacts with the `metrics.metric(metricName).metricValue()` method from the Metrics class. Therefore, it is more appropriate for the method to reside in the `Metrics` class to maintain its functionality and to keep related code together. This would also make it easier to manage and understand metrics-related assertions within the Metrics class context.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 173,
                        "lineEnd": 175,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The `idempotentCreateSnapshot` method is closely related to the `SnapshotRegistry` class, which manages the creation and retrieval of snapshots. The method internally calls the `getOrCreateSnapshot` method of `SnapshotRegistry`. Therefore, it logically belongs in the `SnapshotRegistry` class, where other snapshot-related operations are encapsulated. Keeping this method in `SnapshotRegistry` will improve cohesion and maintainability by consolidating related functionality in one place, making the code easier to understand and manage.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry0)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertMetricValue",
                            "method_signature": "private assertMetricValue(Metrics metrics, MetricName metricName, double val)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "assertMetricValue",
                            "method_signature": "private assertMetricValue(Metrics metrics, MetricName metricName, double val)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry0)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "assertMetricValue",
                            "method_signature": "private assertMetricValue(Metrics metrics, MetricName metricName, double val)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry0)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private assertMetricValue(Metrics metrics, MetricName metricName, double val)": {
                    "first": {
                        "method_name": "assertMetricValue",
                        "method_signature": "private assertMetricValue(Metrics metrics, MetricName metricName, double val)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.14530212501928982
                },
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry0)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry0)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.17332893138462377
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "private assertMetricValue(Metrics metrics, MetricName metricName, double val)",
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry0)"
                ],
                "llm_response_time": 3149
            },
            "targetClassMap": {
                "assertMetricValue": {
                    "target_classes": [
                        {
                            "class_name": "Metrics",
                            "similarity_score": 0.1657536369617933
                        },
                        {
                            "class_name": "MetricName",
                            "similarity_score": 0.13141220939221307
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Metrics",
                        "MetricName"
                    ],
                    "llm_response_time": 5547,
                    "similarity_computation_time": 8,
                    "similarity_metric": "cosine"
                },
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 6413,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testAsListedGroup() : void in class org.apache.kafka.coordinator.group.modern.consumer.ConsumerGroupTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1104,
                    "endLine": 1121,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testAsListedGroup() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1113,
                    "endLine": 1113,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1104,
                    "endLine": 1121,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testAsListedGroup() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1113,
                    "endLine": 1113,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(0)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1118,
                    "endLine": 1118,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(1)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 612,
        "extraction_results": {
            "success": true,
            "newCommitHash": "8703298f378f5767ab7c5b8e0740c9b6b94fce07",
            "newBranchName": "extract-idempotentCreateSnapshot-testAsListedGroup-130af38"
        },
        "telemetry": {
            "id": "42299399-462c-4f46-bc8b-7d9f31fda041",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1370,
                "lineStart": 74,
                "lineEnd": 1443,
                "bodyLineStart": 74,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                "sourceCode": "public class ConsumerGroupTest {\n\n    private ConsumerGroup createConsumerGroup(String groupId) {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        return new ConsumerGroup(\n            snapshotRegistry,\n            groupId,\n            mock(GroupCoordinatorMetricsShard.class)\n        );\n    }\n\n    @Test\n    public void testGetOrCreateMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        // Create a member.\n        member = consumerGroup.getOrMaybeCreateMember(\"member-id\", true);\n        assertEquals(\"member-id\", member.memberId());\n\n        // Add member to the group.\n        consumerGroup.updateMember(member);\n\n        // Get that member back.\n        member = consumerGroup.getOrMaybeCreateMember(\"member-id\", false);\n        assertEquals(\"member-id\", member.memberId());\n\n        assertThrows(UnknownMemberIdException.class, () ->\n            consumerGroup.getOrMaybeCreateMember(\"does-not-exist\", false));\n    }\n\n    @Test\n    public void testUpdateMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(member, consumerGroup.getOrMaybeCreateMember(\"member\", false));\n    }\n\n    @Test\n    public void testNoStaticMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        // Create a new member which is not static\n        consumerGroup.getOrMaybeCreateMember(\"member\", true);\n        assertNull(consumerGroup.staticMember(\"instance-id\"));\n    }\n\n    @Test\n    public void testGetStaticMemberByInstanceId() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .setInstanceId(\"instance\")\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(member, consumerGroup.staticMember(\"instance\"));\n        assertEquals(member, consumerGroup.getOrMaybeCreateMember(\"member\", false));\n        assertEquals(member.memberId(), consumerGroup.staticMemberId(\"instance\"));\n    }\n\n    @Test\n    public void testRemoveMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n        consumerGroup.updateMember(member);\n        assertTrue(consumerGroup.hasMember(\"member\"));\n\n        consumerGroup.removeMember(\"member\");\n        assertFalse(consumerGroup.hasMember(\"member\"));\n\n    }\n\n    @Test\n    public void testRemoveStaticMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member = new ConsumerGroupMember.Builder(\"member\")\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .setInstanceId(\"instance\")\n            .build();\n\n        consumerGroup.updateMember(member);\n        assertTrue(consumerGroup.hasMember(\"member\"));\n\n        consumerGroup.removeMember(\"member\");\n        assertFalse(consumerGroup.hasMember(\"member\"));\n        assertNull(consumerGroup.staticMember(\"instance\"));\n        assertNull(consumerGroup.staticMemberId(\"instance\"));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 9));\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(barTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(zarTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 4));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 5));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 9));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpochWhenPartitionIsReassignedBeforeBeingRevoked() {\n        Uuid fooTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(Collections.emptyMap())\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .setPartitionsPendingRevocation(Collections.emptyMap())\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(11, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpochWhenPartitionIsNotReleased() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember m1 = new ConsumerGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(m1);\n\n        ConsumerGroupMember m2 = new ConsumerGroupMember.Builder(\"m2\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        // m2 should not be able to acquire foo-1 because the partition is\n        // still owned by another member.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.updateMember(m2));\n    }\n\n    @Test\n    public void testRemovePartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        // Removing should fail because there is no epoch set.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        ));\n\n        ConsumerGroupMember m1 = new ConsumerGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(m1);\n\n        // Removing should fail because the expected epoch is incorrect.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testAddPartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        consumerGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        );\n\n        // Changing the epoch should fail because the owner of the partition\n        // should remove it first.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testDeletingMemberRemovesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 9));\n\n        consumerGroup.removeMember(member.memberId());\n\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 4));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 5));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 9));\n    }\n\n    @Test\n    public void testWaitingOnUnreleasedPartition() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n        String memberId1 = Uuid.randomUuid().toString();\n        String memberId2 = Uuid.randomUuid().toString();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        consumerGroup.updateTargetAssignment(memberId1, new Assignment(mkAssignment(\n            mkTopicAssignment(fooTopicId, 1, 2, 3),\n            mkTopicAssignment(zarTopicId, 7, 8, 9)\n        )));\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(memberId1)\n            .setMemberEpoch(10)\n            .setState(MemberState.UNRELEASED_PARTITIONS)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n        consumerGroup.updateMember(member1);\n\n        assertFalse(consumerGroup.waitingOnUnreleasedPartition(member1));\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(memberId2)\n            .setMemberEpoch(10)\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(zarTopicId, 7)))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertTrue(consumerGroup.waitingOnUnreleasedPartition(member1));\n    }\n\n    @Test\n    public void testGroupState() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member1);\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member2);\n        consumerGroup.setGroupEpoch(2);\n\n        assertEquals(MemberState.STABLE, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n\n        consumerGroup.setTargetAssignmentEpoch(2);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        member1 = new ConsumerGroupMember.Builder(member1)\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        // Member 2 is not stable so the group stays in reconciling state.\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setState(MemberState.UNREVOKED_PARTITIONS)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(MemberState.UNREVOKED_PARTITIONS, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(MemberState.STABLE, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n\n        consumerGroup.removeMember(\"member1\");\n        consumerGroup.removeMember(\"member2\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n    }\n\n    @Test\n    public void testGroupTypeFromString() {\n\n        assertEquals(Group.GroupType.parse(\"classic\"), Group.GroupType.CLASSIC);\n\n        // Test case insensitivity.\n        assertEquals(Group.GroupType.parse(\"Consumer\"), Group.GroupType.CONSUMER);\n\n        // Test with invalid group type.\n        assertEquals(Group.GroupType.parse(\"Invalid\"), Group.GroupType.UNKNOWN);\n    }\n\n    @Test\n    public void testPreferredServerAssignor() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setServerAssignorName(\"range\")\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setServerAssignorName(\"range\")\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setServerAssignorName(\"uniform\")\n            .build();\n\n        // The group is empty so the preferred assignor should be empty.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 1 has got an updated assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.computePreferredServerAssignor(null, member1)\n        );\n\n        // Update the group with member 1.\n        consumerGroup.updateMember(member1);\n\n        // Member 1 is in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 1 has been removed but this is not reflected in the group yet so\n        // we pass the removed member. The assignor should be range.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.computePreferredServerAssignor(member1, null)\n        );\n\n        // Member 2 has got an updated assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.computePreferredServerAssignor(null, member2)\n        );\n\n        // Update the group with member 2.\n        consumerGroup.updateMember(member2);\n\n        // Member 1 and 2 are in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Update the group with member 3.\n        consumerGroup.updateMember(member3);\n\n        // Member 1, 2 and 3 are in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Members without assignors\n        ConsumerGroupMember updatedMember1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setServerAssignorName(null)\n            .build();\n        ConsumerGroupMember updatedMember2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setServerAssignorName(null)\n            .build();\n        ConsumerGroupMember updatedMember3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setServerAssignorName(null)\n            .build();\n\n        // Member 1 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range or uniform.\n        Optional<String> assignor = consumerGroup.computePreferredServerAssignor(member1, updatedMember1);\n        assertTrue(assignor.equals(Optional.of(\"range\")) || assignor.equals(Optional.of(\"uniform\")));\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember1);\n\n        // Member 2 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range or uniform.\n        assertEquals(\n            Optional.of(\"uniform\"),\n            consumerGroup.computePreferredServerAssignor(member2, updatedMember2)\n        );\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember2);\n\n        // Only member 3 is left in the group so the assignor should be uniform.\n        assertEquals(\n            Optional.of(\"uniform\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 3 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be empty.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.computePreferredServerAssignor(member3, updatedMember3)\n        );\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember3);\n\n        // The group is empty so the assignor should be empty as well.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.preferredServerAssignor()\n        );\n    }\n\n    @Test\n    public void testUpdateSubscriptionMetadata() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addTopic(zarTopicId, \"zar\", 3)\n            .addRacks()\n            .build();\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Collections.singletonList(\"zar\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member1),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member1.\n        consumerGroup.updateMember(member1);\n\n        // It should return foo now.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member2),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member2.\n        consumerGroup.updateMember(member2);\n\n        // It should return foo and bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member2, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Removing member1 results in returning bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member3),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating group with member3.\n        consumerGroup.updateMember(member3);\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1, member 2 and member 3\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member1, member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2 and member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(Collections.singleton(member1)),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(Collections.emptySet()),\n                image.topics(),\n                image.cluster()\n            )\n        );\n    }\n\n    @Test\n    public void testUpdateSubscribedTopicNamesAndSubscriptionType() {\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.subscribedTopicNames()\n        );\n\n        // It should be Homogeneous by default.\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member1);\n\n        // It should be Homogeneous since there is just 1 member\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member3);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.removeMember(member1.memberId());\n\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        ConsumerGroupMember member4 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\", \"zar\"))\n            .build();\n\n        consumerGroup.updateMember(member4);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n    }\n\n    @Test\n    public void testUpdateInvertedAssignment() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = mock(GroupCoordinatorMetricsShard.class);\n        ConsumerGroup consumerGroup = new ConsumerGroup(snapshotRegistry, \"test-group\", metricsShard);\n        Uuid topicId = Uuid.randomUuid();\n        String memberId1 = \"member1\";\n        String memberId2 = \"member2\";\n\n        // Initial assignment for member1\n        Assignment initialAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, initialAssignment);\n\n        // Verify that partition 0 is assigned to member1.\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(0, memberId1)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1\n        Assignment newAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, newAssignment);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId1)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member2 to add partition 1\n        Assignment newAssignment2 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        consumerGroup.updateTargetAssignment(memberId2, newAssignment2);\n\n        // Verify that partition 1 is assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1 to revoke partition 1 and assign partition 0\n        Assignment newAssignment1 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, newAssignment1);\n\n        // Verify that partition 1 is still assigned to member2 and partition 0 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(\n                    mkEntry(0, memberId1),\n                    mkEntry(1, memberId2)\n                ))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // Test remove target assignment for member1\n        consumerGroup.removeTargetAssignment(memberId1);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is still assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n    }\n\n    @Test\n    public void testMetadataRefreshDeadline() {\n        MockTime time = new MockTime();\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Group epoch starts at 0.\n        assertEquals(0, group.groupEpoch());\n\n        // The refresh time deadline should be empty when the group is created or loaded.\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, group.metadataRefreshDeadline().epoch);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch());\n        assertFalse(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch(), group.metadataRefreshDeadline().epoch);\n\n        // Advance past the deadline. The metadata should have expired.\n        time.sleep(1001L);\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n\n        // Set the refresh time deadline with a higher group epoch. The metadata is considered\n        // as expired because the group epoch attached to the deadline is higher than the\n        // current group epoch.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch() + 1);\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch() + 1, group.metadataRefreshDeadline().epoch);\n\n        // Advance the group epoch.\n        group.setGroupEpoch(group.groupEpoch() + 1);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch());\n        assertFalse(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch(), group.metadataRefreshDeadline().epoch);\n\n        // Request metadata refresh. The metadata expires immediately.\n        group.requestMetadataRefresh();\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, group.metadataRefreshDeadline().epoch);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateTransactionalOffsetCommit(short version) {\n        boolean isTransactional = true;\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Simulate a call from the admin client without member id and member epoch.\n        // This should pass only if the group is empty.\n        group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"member-id\", null, 0, isTransactional, version));\n\n        // Create a member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member-id\").build());\n\n        // A call from the admin client should fail as the group is not empty.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version));\n\n        // The member epoch is stale.\n        assertThrows(StaleMemberEpochException.class, () ->\n            group.validateOffsetCommit(\"member-id\", \"\", 10, isTransactional, version));\n\n        // This should succeed.\n        group.validateOffsetCommit(\"member-id\", \"\", 0, isTransactional, version);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateOffsetCommit(short version) {\n        boolean isTransactional = false;\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Simulate a call from the admin client without member id and member epoch.\n        // This should pass only if the group is empty.\n        group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"member-id\", null, 0, isTransactional, version));\n\n        // Create members.\n        group.updateMember(\n            new ConsumerGroupMember\n                .Builder(\"new-protocol-member-id\").build()\n        );\n        group.updateMember(\n            new ConsumerGroupMember.Builder(\"old-protocol-member-id\")\n                .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata())\n                .build()\n        );\n\n        // A call from the admin client should fail as the group is not empty.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version));\n\n        // The member epoch is stale.\n        if (version >= 9) {\n            assertThrows(StaleMemberEpochException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 10, isTransactional, version));\n        } else {\n            assertThrows(UnsupportedVersionException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 10, isTransactional, version));\n        }\n        assertThrows(IllegalGenerationException.class, () ->\n            group.validateOffsetCommit(\"old-protocol-member-id\", \"\", 10, isTransactional, version));\n\n        // This should succeed.\n        if (version >= 9) {\n            group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 0, isTransactional, version);\n        } else {\n            assertThrows(UnsupportedVersionException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 0, isTransactional, version));\n        }\n    }\n\n    @Test\n    public void testAsListedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = new GroupCoordinatorMetricsShard(\n            snapshotRegistry,\n            Collections.emptyMap(),\n            new TopicPartition(\"__consumer_offsets\", 0)\n        );\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-foo\", metricsShard);\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE.toString(), group.stateAsString(1));\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(0);\n    }\n\n    @Test\n    public void testValidateOffsetFetch() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ConsumerGroup group = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-foo\",\n            mock(GroupCoordinatorMetricsShard.class)\n        );\n\n        // Simulate a call from the admin client without member id and member epoch.\n        group.validateOffsetFetch(null, -1, Long.MAX_VALUE);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 0, Long.MAX_VALUE));\n\n        // Create a member.\n        idempotentCreateSnapshot(snapshotRegistry);\n        group.updateMember(new ConsumerGroupMember.Builder(\"member-id\").build());\n\n        // The member does not exist at last committed offset 0.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 0, 0));\n\n        // The member exists but the epoch is stale when the last committed offset is not considered.\n        assertThrows(StaleMemberEpochException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 10, Long.MAX_VALUE));\n\n        // This should succeed.\n        group.validateOffsetFetch(\"member-id\", 0, Long.MAX_VALUE);\n    }\n\n    @Test\n    public void testValidateDeleteGroup() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        assertDoesNotThrow(consumerGroup::validateDeleteGroup);\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n        consumerGroup.updateMember(member1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n\n        consumerGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n    }\n\n    @Test\n    public void testOffsetExpirationCondition() {\n        long currentTimestamp = 30000L;\n        long commitTimestamp = 20000L;\n        long offsetsRetentionMs = 10000L;\n        OffsetAndMetadata offsetAndMetadata = new OffsetAndMetadata(15000L, OptionalInt.empty(), \"\", commitTimestamp, OptionalLong.empty());\n        ConsumerGroup group = new ConsumerGroup(new SnapshotRegistry(new LogContext()), \"group-id\", mock(GroupCoordinatorMetricsShard.class));\n\n        Optional<OffsetExpirationCondition> offsetExpirationCondition = group.offsetExpirationCondition();\n        assertTrue(offsetExpirationCondition.isPresent());\n\n        OffsetExpirationConditionImpl condition = (OffsetExpirationConditionImpl) offsetExpirationCondition.get();\n        assertEquals(commitTimestamp, condition.baseTimestamp().apply(offsetAndMetadata));\n        assertTrue(condition.isOffsetExpired(offsetAndMetadata, currentTimestamp, offsetsRetentionMs));\n    }\n\n    @Test\n    public void testIsSubscribedToTopic() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addRacks()\n            .build();\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        consumerGroup.updateMember(member1);\n        consumerGroup.updateMember(member2);\n\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        assertTrue(consumerGroup.isSubscribedToTopic(\"foo\"));\n        assertTrue(consumerGroup.isSubscribedToTopic(\"bar\"));\n\n        consumerGroup.removeMember(\"member1\");\n        assertFalse(consumerGroup.isSubscribedToTopic(\"foo\"));\n\n        consumerGroup.removeMember(\"member2\");\n        assertFalse(consumerGroup.isSubscribedToTopic(\"bar\"));\n    }\n\n    @Test\n    public void testAsDescribedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-id-1\", mock(GroupCoordinatorMetricsShard.class));\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n                .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                .setServerAssignorName(\"assignorName\")\n                .build());\n        group.updateMember(new ConsumerGroupMember.Builder(\"member2\")\n                .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n\n        ConsumerGroupDescribeResponseData.DescribedGroup expected = new ConsumerGroupDescribeResponseData.DescribedGroup()\n            .setGroupId(\"group-id-1\")\n            .setGroupState(ConsumerGroup.ConsumerGroupState.STABLE.toString())\n            .setGroupEpoch(0)\n            .setAssignmentEpoch(0)\n            .setAssignorName(\"assignorName\")\n            .setMembers(Arrays.asList(\n                new ConsumerGroupDescribeResponseData.Member()\n                    .setMemberId(\"member1\")\n                    .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                    .setSubscribedTopicRegex(\"\"),\n                new ConsumerGroupDescribeResponseData.Member().setMemberId(\"member2\")\n                    .setSubscribedTopicRegex(\"\")\n            ));\n        ConsumerGroupDescribeResponseData.DescribedGroup actual = group.asDescribedGroup(1, \"\",\n            new MetadataImageBuilder().build().topics());\n\n        assertEquals(expected, actual);\n    }\n\n    @Test\n    public void testStateTransitionMetrics() {\n        // Confirm metrics is not updated when a new ConsumerGroup is created but only when the group transitions\n        // its state.\n        GroupCoordinatorMetricsShard metrics = mock(GroupCoordinatorMetricsShard.class);\n        ConsumerGroup consumerGroup = new ConsumerGroup(\n            new SnapshotRegistry(new LogContext()),\n            \"group-id\",\n            metrics\n        );\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        verify(metrics, times(0)).onConsumerGroupStateTransition(null, ConsumerGroup.ConsumerGroupState.EMPTY);\n\n        ConsumerGroupMember member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.EMPTY, ConsumerGroup.ConsumerGroupState.RECONCILING);\n\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.RECONCILING, ConsumerGroup.ConsumerGroupState.ASSIGNING);\n\n        consumerGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.ASSIGNING, ConsumerGroup.ConsumerGroupState.STABLE);\n\n        consumerGroup.removeMember(\"member\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.STABLE, ConsumerGroup.ConsumerGroupState.EMPTY);\n    }\n\n    @Test\n    public void testIsInStatesCaseInsensitive() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = new GroupCoordinatorMetricsShard(\n            snapshotRegistry,\n            Collections.emptyMap(),\n            new TopicPartition(\"__consumer_offsets\", 0)\n        );\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-foo\", metricsShard);\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertTrue(group.isInStates(Collections.singleton(\"empty\"), 0));\n        assertFalse(group.isInStates(Collections.singleton(\"Empty\"), 0));\n\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertTrue(group.isInStates(Collections.singleton(\"empty\"), 0));\n        assertTrue(group.isInStates(Collections.singleton(\"stable\"), 1));\n        assertFalse(group.isInStates(Collections.singleton(\"empty\"), 1));\n    }\n\n    @Test\n    public void testClassicMembersSupportedProtocols() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> rangeProtocol = new ArrayList<>();\n        rangeProtocol.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> roundRobinAndRangeProtocols = new ArrayList<>();\n        roundRobinAndRangeProtocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"roundrobin\")\n            .setMetadata(new byte[0]));\n        roundRobinAndRangeProtocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member-1\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(rangeProtocol))\n            .build();\n        consumerGroup.updateMember(member1);\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertEquals(1, consumerGroup.classicMembersSupportedProtocols().get(\"roundrobin\"));\n        assertTrue(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"range\", \"sticky\"))));\n        assertFalse(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"sticky\", \"roundrobin\"))));\n\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(rangeProtocol))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertFalse(consumerGroup.classicMembersSupportedProtocols().containsKey(\"roundrobin\"));\n\n        member1 = new ConsumerGroupMember.Builder(member1)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member1);\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"roundrobin\"));\n        assertTrue(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"sticky\", \"roundrobin\"))));\n    }\n\n    @Test\n    public void testNumClassicProtocolMembers() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> protocols = new ArrayList<>();\n        protocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        // The group has member 1 (using the classic protocol).\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member-1\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(protocols))\n            .build();\n        consumerGroup.updateMember(member1);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n\n        // The group has member 1 (using the classic protocol) and member 2 (using the consumer protocol).\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .build();\n        consumerGroup.updateMember(member2);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n        assertFalse(consumerGroup.allMembersUseClassicProtocolExcept(\"member-1\"));\n        assertTrue(consumerGroup.allMembersUseClassicProtocolExcept(\"member-2\"));\n\n        // The group has member 2 (using the consumer protocol) and member 3 (using the consumer protocol).\n        consumerGroup.removeMember(member1.memberId());\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member-3\")\n            .build();\n        consumerGroup.updateMember(member3);\n        assertEquals(0, consumerGroup.numClassicProtocolMembers());\n        assertFalse(consumerGroup.allMembersUseClassicProtocolExcept(\"member-2\"));\n\n        // The group has member 2 (using the classic protocol).\n        consumerGroup.removeMember(member2.memberId());\n        member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(protocols))\n            .build();\n        consumerGroup.updateMember(member2);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n    }\n}",
                "methodCount": 34
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 1122,
                        "lineEnd": 1124,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method idempotentCreateSnapshot() directly interacts with the getOrCreateSnapshot() method from the SnapshotRegistry class to create a snapshot if it does not exist already. This tight coupling indicates that the functionality is directly relevant to the SnapshotRegistry, aligning with the responsibilities of managing and creating snapshots within this class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private createConsumerGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateTransactionalOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private createConsumerGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateTransactionalOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private createConsumerGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateTransactionalOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.13520961404876064
                },
                "private createConsumerGroup(String groupId)": {
                    "first": {
                        "method_name": "createConsumerGroup",
                        "method_signature": "private createConsumerGroup(String groupId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3996679820747262
                },
                "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)": {
                    "first": {
                        "method_name": "testValidateOffsetCommit",
                        "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.495556589130992
                },
                "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)": {
                    "first": {
                        "method_name": "testValidateTransactionalOffsetCommit",
                        "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5410938634102399
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "private createConsumerGroup(String groupId)",
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                    "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                    "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)"
                ],
                "llm_response_time": 3553
            },
            "targetClassMap": {
                "createConsumerGroup": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 1857,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 2049,
                    "similarity_computation_time": 29,
                    "similarity_metric": "cosine"
                },
                "testValidateOffsetCommit": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2721,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "testValidateTransactionalOffsetCommit": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 1906,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testValidateOffsetFetch() : void in class org.apache.kafka.coordinator.group.modern.consumer.ConsumerGroupTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1123,
                    "endLine": 1153,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testValidateOffsetFetch() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1140,
                    "endLine": 1140,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1123,
                    "endLine": 1153,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testValidateOffsetFetch() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1140,
                    "endLine": 1140,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(0)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 613,
        "extraction_results": {
            "success": true,
            "newCommitHash": "34bebf3af2ff8ca4a4ac8a279765fc681c4b95ea",
            "newBranchName": "extract-idempotentCreateSnapshot-testValidateOffsetFetch-130af38"
        },
        "telemetry": {
            "id": "4c0ed84b-cd2f-41d3-b604-b75465fe9379",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1370,
                "lineStart": 74,
                "lineEnd": 1443,
                "bodyLineStart": 74,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                "sourceCode": "public class ConsumerGroupTest {\n\n    private ConsumerGroup createConsumerGroup(String groupId) {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        return new ConsumerGroup(\n            snapshotRegistry,\n            groupId,\n            mock(GroupCoordinatorMetricsShard.class)\n        );\n    }\n\n    @Test\n    public void testGetOrCreateMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        // Create a member.\n        member = consumerGroup.getOrMaybeCreateMember(\"member-id\", true);\n        assertEquals(\"member-id\", member.memberId());\n\n        // Add member to the group.\n        consumerGroup.updateMember(member);\n\n        // Get that member back.\n        member = consumerGroup.getOrMaybeCreateMember(\"member-id\", false);\n        assertEquals(\"member-id\", member.memberId());\n\n        assertThrows(UnknownMemberIdException.class, () ->\n            consumerGroup.getOrMaybeCreateMember(\"does-not-exist\", false));\n    }\n\n    @Test\n    public void testUpdateMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(member, consumerGroup.getOrMaybeCreateMember(\"member\", false));\n    }\n\n    @Test\n    public void testNoStaticMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        // Create a new member which is not static\n        consumerGroup.getOrMaybeCreateMember(\"member\", true);\n        assertNull(consumerGroup.staticMember(\"instance-id\"));\n    }\n\n    @Test\n    public void testGetStaticMemberByInstanceId() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .setInstanceId(\"instance\")\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(member, consumerGroup.staticMember(\"instance\"));\n        assertEquals(member, consumerGroup.getOrMaybeCreateMember(\"member\", false));\n        assertEquals(member.memberId(), consumerGroup.staticMemberId(\"instance\"));\n    }\n\n    @Test\n    public void testRemoveMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n        consumerGroup.updateMember(member);\n        assertTrue(consumerGroup.hasMember(\"member\"));\n\n        consumerGroup.removeMember(\"member\");\n        assertFalse(consumerGroup.hasMember(\"member\"));\n\n    }\n\n    @Test\n    public void testRemoveStaticMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member = new ConsumerGroupMember.Builder(\"member\")\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .setInstanceId(\"instance\")\n            .build();\n\n        consumerGroup.updateMember(member);\n        assertTrue(consumerGroup.hasMember(\"member\"));\n\n        consumerGroup.removeMember(\"member\");\n        assertFalse(consumerGroup.hasMember(\"member\"));\n        assertNull(consumerGroup.staticMember(\"instance\"));\n        assertNull(consumerGroup.staticMemberId(\"instance\"));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 9));\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(barTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(zarTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 4));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 5));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 9));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpochWhenPartitionIsReassignedBeforeBeingRevoked() {\n        Uuid fooTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(Collections.emptyMap())\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .setPartitionsPendingRevocation(Collections.emptyMap())\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(11, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpochWhenPartitionIsNotReleased() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember m1 = new ConsumerGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(m1);\n\n        ConsumerGroupMember m2 = new ConsumerGroupMember.Builder(\"m2\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        // m2 should not be able to acquire foo-1 because the partition is\n        // still owned by another member.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.updateMember(m2));\n    }\n\n    @Test\n    public void testRemovePartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        // Removing should fail because there is no epoch set.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        ));\n\n        ConsumerGroupMember m1 = new ConsumerGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(m1);\n\n        // Removing should fail because the expected epoch is incorrect.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testAddPartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        consumerGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        );\n\n        // Changing the epoch should fail because the owner of the partition\n        // should remove it first.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testDeletingMemberRemovesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 9));\n\n        consumerGroup.removeMember(member.memberId());\n\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 4));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 5));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 9));\n    }\n\n    @Test\n    public void testWaitingOnUnreleasedPartition() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n        String memberId1 = Uuid.randomUuid().toString();\n        String memberId2 = Uuid.randomUuid().toString();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        consumerGroup.updateTargetAssignment(memberId1, new Assignment(mkAssignment(\n            mkTopicAssignment(fooTopicId, 1, 2, 3),\n            mkTopicAssignment(zarTopicId, 7, 8, 9)\n        )));\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(memberId1)\n            .setMemberEpoch(10)\n            .setState(MemberState.UNRELEASED_PARTITIONS)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n        consumerGroup.updateMember(member1);\n\n        assertFalse(consumerGroup.waitingOnUnreleasedPartition(member1));\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(memberId2)\n            .setMemberEpoch(10)\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(zarTopicId, 7)))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertTrue(consumerGroup.waitingOnUnreleasedPartition(member1));\n    }\n\n    @Test\n    public void testGroupState() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member1);\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member2);\n        consumerGroup.setGroupEpoch(2);\n\n        assertEquals(MemberState.STABLE, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n\n        consumerGroup.setTargetAssignmentEpoch(2);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        member1 = new ConsumerGroupMember.Builder(member1)\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        // Member 2 is not stable so the group stays in reconciling state.\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setState(MemberState.UNREVOKED_PARTITIONS)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(MemberState.UNREVOKED_PARTITIONS, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(MemberState.STABLE, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n\n        consumerGroup.removeMember(\"member1\");\n        consumerGroup.removeMember(\"member2\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n    }\n\n    @Test\n    public void testGroupTypeFromString() {\n\n        assertEquals(Group.GroupType.parse(\"classic\"), Group.GroupType.CLASSIC);\n\n        // Test case insensitivity.\n        assertEquals(Group.GroupType.parse(\"Consumer\"), Group.GroupType.CONSUMER);\n\n        // Test with invalid group type.\n        assertEquals(Group.GroupType.parse(\"Invalid\"), Group.GroupType.UNKNOWN);\n    }\n\n    @Test\n    public void testPreferredServerAssignor() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setServerAssignorName(\"range\")\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setServerAssignorName(\"range\")\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setServerAssignorName(\"uniform\")\n            .build();\n\n        // The group is empty so the preferred assignor should be empty.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 1 has got an updated assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.computePreferredServerAssignor(null, member1)\n        );\n\n        // Update the group with member 1.\n        consumerGroup.updateMember(member1);\n\n        // Member 1 is in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 1 has been removed but this is not reflected in the group yet so\n        // we pass the removed member. The assignor should be range.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.computePreferredServerAssignor(member1, null)\n        );\n\n        // Member 2 has got an updated assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.computePreferredServerAssignor(null, member2)\n        );\n\n        // Update the group with member 2.\n        consumerGroup.updateMember(member2);\n\n        // Member 1 and 2 are in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Update the group with member 3.\n        consumerGroup.updateMember(member3);\n\n        // Member 1, 2 and 3 are in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Members without assignors\n        ConsumerGroupMember updatedMember1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setServerAssignorName(null)\n            .build();\n        ConsumerGroupMember updatedMember2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setServerAssignorName(null)\n            .build();\n        ConsumerGroupMember updatedMember3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setServerAssignorName(null)\n            .build();\n\n        // Member 1 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range or uniform.\n        Optional<String> assignor = consumerGroup.computePreferredServerAssignor(member1, updatedMember1);\n        assertTrue(assignor.equals(Optional.of(\"range\")) || assignor.equals(Optional.of(\"uniform\")));\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember1);\n\n        // Member 2 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range or uniform.\n        assertEquals(\n            Optional.of(\"uniform\"),\n            consumerGroup.computePreferredServerAssignor(member2, updatedMember2)\n        );\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember2);\n\n        // Only member 3 is left in the group so the assignor should be uniform.\n        assertEquals(\n            Optional.of(\"uniform\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 3 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be empty.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.computePreferredServerAssignor(member3, updatedMember3)\n        );\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember3);\n\n        // The group is empty so the assignor should be empty as well.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.preferredServerAssignor()\n        );\n    }\n\n    @Test\n    public void testUpdateSubscriptionMetadata() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addTopic(zarTopicId, \"zar\", 3)\n            .addRacks()\n            .build();\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Collections.singletonList(\"zar\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member1),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member1.\n        consumerGroup.updateMember(member1);\n\n        // It should return foo now.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member2),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member2.\n        consumerGroup.updateMember(member2);\n\n        // It should return foo and bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member2, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Removing member1 results in returning bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member3),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating group with member3.\n        consumerGroup.updateMember(member3);\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1, member 2 and member 3\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member1, member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2 and member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(Collections.singleton(member1)),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(Collections.emptySet()),\n                image.topics(),\n                image.cluster()\n            )\n        );\n    }\n\n    @Test\n    public void testUpdateSubscribedTopicNamesAndSubscriptionType() {\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.subscribedTopicNames()\n        );\n\n        // It should be Homogeneous by default.\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member1);\n\n        // It should be Homogeneous since there is just 1 member\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member3);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.removeMember(member1.memberId());\n\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        ConsumerGroupMember member4 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\", \"zar\"))\n            .build();\n\n        consumerGroup.updateMember(member4);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n    }\n\n    @Test\n    public void testUpdateInvertedAssignment() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = mock(GroupCoordinatorMetricsShard.class);\n        ConsumerGroup consumerGroup = new ConsumerGroup(snapshotRegistry, \"test-group\", metricsShard);\n        Uuid topicId = Uuid.randomUuid();\n        String memberId1 = \"member1\";\n        String memberId2 = \"member2\";\n\n        // Initial assignment for member1\n        Assignment initialAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, initialAssignment);\n\n        // Verify that partition 0 is assigned to member1.\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(0, memberId1)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1\n        Assignment newAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, newAssignment);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId1)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member2 to add partition 1\n        Assignment newAssignment2 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        consumerGroup.updateTargetAssignment(memberId2, newAssignment2);\n\n        // Verify that partition 1 is assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1 to revoke partition 1 and assign partition 0\n        Assignment newAssignment1 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, newAssignment1);\n\n        // Verify that partition 1 is still assigned to member2 and partition 0 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(\n                    mkEntry(0, memberId1),\n                    mkEntry(1, memberId2)\n                ))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // Test remove target assignment for member1\n        consumerGroup.removeTargetAssignment(memberId1);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is still assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n    }\n\n    @Test\n    public void testMetadataRefreshDeadline() {\n        MockTime time = new MockTime();\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Group epoch starts at 0.\n        assertEquals(0, group.groupEpoch());\n\n        // The refresh time deadline should be empty when the group is created or loaded.\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, group.metadataRefreshDeadline().epoch);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch());\n        assertFalse(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch(), group.metadataRefreshDeadline().epoch);\n\n        // Advance past the deadline. The metadata should have expired.\n        time.sleep(1001L);\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n\n        // Set the refresh time deadline with a higher group epoch. The metadata is considered\n        // as expired because the group epoch attached to the deadline is higher than the\n        // current group epoch.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch() + 1);\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch() + 1, group.metadataRefreshDeadline().epoch);\n\n        // Advance the group epoch.\n        group.setGroupEpoch(group.groupEpoch() + 1);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch());\n        assertFalse(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch(), group.metadataRefreshDeadline().epoch);\n\n        // Request metadata refresh. The metadata expires immediately.\n        group.requestMetadataRefresh();\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, group.metadataRefreshDeadline().epoch);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateTransactionalOffsetCommit(short version) {\n        boolean isTransactional = true;\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Simulate a call from the admin client without member id and member epoch.\n        // This should pass only if the group is empty.\n        group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"member-id\", null, 0, isTransactional, version));\n\n        // Create a member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member-id\").build());\n\n        // A call from the admin client should fail as the group is not empty.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version));\n\n        // The member epoch is stale.\n        assertThrows(StaleMemberEpochException.class, () ->\n            group.validateOffsetCommit(\"member-id\", \"\", 10, isTransactional, version));\n\n        // This should succeed.\n        group.validateOffsetCommit(\"member-id\", \"\", 0, isTransactional, version);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateOffsetCommit(short version) {\n        boolean isTransactional = false;\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Simulate a call from the admin client without member id and member epoch.\n        // This should pass only if the group is empty.\n        group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"member-id\", null, 0, isTransactional, version));\n\n        // Create members.\n        group.updateMember(\n            new ConsumerGroupMember\n                .Builder(\"new-protocol-member-id\").build()\n        );\n        group.updateMember(\n            new ConsumerGroupMember.Builder(\"old-protocol-member-id\")\n                .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata())\n                .build()\n        );\n\n        // A call from the admin client should fail as the group is not empty.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version));\n\n        // The member epoch is stale.\n        if (version >= 9) {\n            assertThrows(StaleMemberEpochException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 10, isTransactional, version));\n        } else {\n            assertThrows(UnsupportedVersionException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 10, isTransactional, version));\n        }\n        assertThrows(IllegalGenerationException.class, () ->\n            group.validateOffsetCommit(\"old-protocol-member-id\", \"\", 10, isTransactional, version));\n\n        // This should succeed.\n        if (version >= 9) {\n            group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 0, isTransactional, version);\n        } else {\n            assertThrows(UnsupportedVersionException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 0, isTransactional, version));\n        }\n    }\n\n    @Test\n    public void testAsListedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = new GroupCoordinatorMetricsShard(\n            snapshotRegistry,\n            Collections.emptyMap(),\n            new TopicPartition(\"__consumer_offsets\", 0)\n        );\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-foo\", metricsShard);\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE.toString(), group.stateAsString(1));\n    }\n\n    @Test\n    public void testValidateOffsetFetch() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ConsumerGroup group = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-foo\",\n            mock(GroupCoordinatorMetricsShard.class)\n        );\n\n        // Simulate a call from the admin client without member id and member epoch.\n        group.validateOffsetFetch(null, -1, Long.MAX_VALUE);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 0, Long.MAX_VALUE));\n\n        // Create a member.\n        idempotentCreateSnapshot(snapshotRegistry);\n        group.updateMember(new ConsumerGroupMember.Builder(\"member-id\").build());\n\n        // The member does not exist at last committed offset 0.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 0, 0));\n\n        // The member exists but the epoch is stale when the last committed offset is not considered.\n        assertThrows(StaleMemberEpochException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 10, Long.MAX_VALUE));\n\n        // This should succeed.\n        group.validateOffsetFetch(\"member-id\", 0, Long.MAX_VALUE);\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(0);\n    }\n\n    @Test\n    public void testValidateDeleteGroup() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        assertDoesNotThrow(consumerGroup::validateDeleteGroup);\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n        consumerGroup.updateMember(member1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n\n        consumerGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n    }\n\n    @Test\n    public void testOffsetExpirationCondition() {\n        long currentTimestamp = 30000L;\n        long commitTimestamp = 20000L;\n        long offsetsRetentionMs = 10000L;\n        OffsetAndMetadata offsetAndMetadata = new OffsetAndMetadata(15000L, OptionalInt.empty(), \"\", commitTimestamp, OptionalLong.empty());\n        ConsumerGroup group = new ConsumerGroup(new SnapshotRegistry(new LogContext()), \"group-id\", mock(GroupCoordinatorMetricsShard.class));\n\n        Optional<OffsetExpirationCondition> offsetExpirationCondition = group.offsetExpirationCondition();\n        assertTrue(offsetExpirationCondition.isPresent());\n\n        OffsetExpirationConditionImpl condition = (OffsetExpirationConditionImpl) offsetExpirationCondition.get();\n        assertEquals(commitTimestamp, condition.baseTimestamp().apply(offsetAndMetadata));\n        assertTrue(condition.isOffsetExpired(offsetAndMetadata, currentTimestamp, offsetsRetentionMs));\n    }\n\n    @Test\n    public void testIsSubscribedToTopic() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addRacks()\n            .build();\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        consumerGroup.updateMember(member1);\n        consumerGroup.updateMember(member2);\n\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        assertTrue(consumerGroup.isSubscribedToTopic(\"foo\"));\n        assertTrue(consumerGroup.isSubscribedToTopic(\"bar\"));\n\n        consumerGroup.removeMember(\"member1\");\n        assertFalse(consumerGroup.isSubscribedToTopic(\"foo\"));\n\n        consumerGroup.removeMember(\"member2\");\n        assertFalse(consumerGroup.isSubscribedToTopic(\"bar\"));\n    }\n\n    @Test\n    public void testAsDescribedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-id-1\", mock(GroupCoordinatorMetricsShard.class));\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n                .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                .setServerAssignorName(\"assignorName\")\n                .build());\n        group.updateMember(new ConsumerGroupMember.Builder(\"member2\")\n                .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n\n        ConsumerGroupDescribeResponseData.DescribedGroup expected = new ConsumerGroupDescribeResponseData.DescribedGroup()\n            .setGroupId(\"group-id-1\")\n            .setGroupState(ConsumerGroup.ConsumerGroupState.STABLE.toString())\n            .setGroupEpoch(0)\n            .setAssignmentEpoch(0)\n            .setAssignorName(\"assignorName\")\n            .setMembers(Arrays.asList(\n                new ConsumerGroupDescribeResponseData.Member()\n                    .setMemberId(\"member1\")\n                    .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                    .setSubscribedTopicRegex(\"\"),\n                new ConsumerGroupDescribeResponseData.Member().setMemberId(\"member2\")\n                    .setSubscribedTopicRegex(\"\")\n            ));\n        ConsumerGroupDescribeResponseData.DescribedGroup actual = group.asDescribedGroup(1, \"\",\n            new MetadataImageBuilder().build().topics());\n\n        assertEquals(expected, actual);\n    }\n\n    @Test\n    public void testStateTransitionMetrics() {\n        // Confirm metrics is not updated when a new ConsumerGroup is created but only when the group transitions\n        // its state.\n        GroupCoordinatorMetricsShard metrics = mock(GroupCoordinatorMetricsShard.class);\n        ConsumerGroup consumerGroup = new ConsumerGroup(\n            new SnapshotRegistry(new LogContext()),\n            \"group-id\",\n            metrics\n        );\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        verify(metrics, times(0)).onConsumerGroupStateTransition(null, ConsumerGroup.ConsumerGroupState.EMPTY);\n\n        ConsumerGroupMember member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.EMPTY, ConsumerGroup.ConsumerGroupState.RECONCILING);\n\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.RECONCILING, ConsumerGroup.ConsumerGroupState.ASSIGNING);\n\n        consumerGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.ASSIGNING, ConsumerGroup.ConsumerGroupState.STABLE);\n\n        consumerGroup.removeMember(\"member\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.STABLE, ConsumerGroup.ConsumerGroupState.EMPTY);\n    }\n\n    @Test\n    public void testIsInStatesCaseInsensitive() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = new GroupCoordinatorMetricsShard(\n            snapshotRegistry,\n            Collections.emptyMap(),\n            new TopicPartition(\"__consumer_offsets\", 0)\n        );\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-foo\", metricsShard);\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertTrue(group.isInStates(Collections.singleton(\"empty\"), 0));\n        assertFalse(group.isInStates(Collections.singleton(\"Empty\"), 0));\n\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertTrue(group.isInStates(Collections.singleton(\"empty\"), 0));\n        assertTrue(group.isInStates(Collections.singleton(\"stable\"), 1));\n        assertFalse(group.isInStates(Collections.singleton(\"empty\"), 1));\n    }\n\n    @Test\n    public void testClassicMembersSupportedProtocols() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> rangeProtocol = new ArrayList<>();\n        rangeProtocol.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> roundRobinAndRangeProtocols = new ArrayList<>();\n        roundRobinAndRangeProtocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"roundrobin\")\n            .setMetadata(new byte[0]));\n        roundRobinAndRangeProtocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member-1\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(rangeProtocol))\n            .build();\n        consumerGroup.updateMember(member1);\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertEquals(1, consumerGroup.classicMembersSupportedProtocols().get(\"roundrobin\"));\n        assertTrue(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"range\", \"sticky\"))));\n        assertFalse(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"sticky\", \"roundrobin\"))));\n\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(rangeProtocol))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertFalse(consumerGroup.classicMembersSupportedProtocols().containsKey(\"roundrobin\"));\n\n        member1 = new ConsumerGroupMember.Builder(member1)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member1);\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"roundrobin\"));\n        assertTrue(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"sticky\", \"roundrobin\"))));\n    }\n\n    @Test\n    public void testNumClassicProtocolMembers() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> protocols = new ArrayList<>();\n        protocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        // The group has member 1 (using the classic protocol).\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member-1\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(protocols))\n            .build();\n        consumerGroup.updateMember(member1);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n\n        // The group has member 1 (using the classic protocol) and member 2 (using the consumer protocol).\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .build();\n        consumerGroup.updateMember(member2);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n        assertFalse(consumerGroup.allMembersUseClassicProtocolExcept(\"member-1\"));\n        assertTrue(consumerGroup.allMembersUseClassicProtocolExcept(\"member-2\"));\n\n        // The group has member 2 (using the consumer protocol) and member 3 (using the consumer protocol).\n        consumerGroup.removeMember(member1.memberId());\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member-3\")\n            .build();\n        consumerGroup.updateMember(member3);\n        assertEquals(0, consumerGroup.numClassicProtocolMembers());\n        assertFalse(consumerGroup.allMembersUseClassicProtocolExcept(\"member-2\"));\n\n        // The group has member 2 (using the classic protocol).\n        consumerGroup.removeMember(member2.memberId());\n        member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(protocols))\n            .build();\n        consumerGroup.updateMember(member2);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n    }\n}",
                "methodCount": 34
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 1154,
                        "lineEnd": 1156,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method idempotentCreateSnapshot() directly interacts with the getOrCreateSnapshot() method from the SnapshotRegistry class to create a snapshot if it does not exist already. This tight coupling indicates that the functionality is directly relevant to the SnapshotRegistry, aligning with the responsibilities of managing and creating snapshots within this class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private createConsumerGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateTransactionalOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private createConsumerGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateTransactionalOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private createConsumerGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateTransactionalOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.13520961404876064
                },
                "private createConsumerGroup(String groupId)": {
                    "first": {
                        "method_name": "createConsumerGroup",
                        "method_signature": "private createConsumerGroup(String groupId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3996679820747262
                },
                "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)": {
                    "first": {
                        "method_name": "testValidateOffsetCommit",
                        "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.495556589130992
                },
                "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)": {
                    "first": {
                        "method_name": "testValidateTransactionalOffsetCommit",
                        "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5410938634102399
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "private createConsumerGroup(String groupId)",
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                    "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                    "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)"
                ],
                "llm_response_time": 2843
            },
            "targetClassMap": {
                "createConsumerGroup": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "testValidateTransactionalOffsetCommit": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "testValidateOffsetCommit": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testAsDescribedGroup() : void in class org.apache.kafka.coordinator.group.modern.consumer.ConsumerGroupTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1243,
                    "endLine": 1276,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testAsDescribedGroup() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1247,
                    "endLine": 1247,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1243,
                    "endLine": 1276,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testAsDescribedGroup() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1247,
                    "endLine": 1247,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(0)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1256,
                    "endLine": 1256,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(1)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 614,
        "extraction_results": {
            "success": true,
            "newCommitHash": "7226b58c84508faac4518b8bf061266c17e4bdf7",
            "newBranchName": "extract-idempotentCreateSnapshot-testAsDescribedGroup-130af38"
        },
        "telemetry": {
            "id": "0f2c5a07-058e-4a92-8ff3-255553c9be60",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1370,
                "lineStart": 74,
                "lineEnd": 1443,
                "bodyLineStart": 74,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                "sourceCode": "public class ConsumerGroupTest {\n\n    private ConsumerGroup createConsumerGroup(String groupId) {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        return new ConsumerGroup(\n            snapshotRegistry,\n            groupId,\n            mock(GroupCoordinatorMetricsShard.class)\n        );\n    }\n\n    @Test\n    public void testGetOrCreateMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        // Create a member.\n        member = consumerGroup.getOrMaybeCreateMember(\"member-id\", true);\n        assertEquals(\"member-id\", member.memberId());\n\n        // Add member to the group.\n        consumerGroup.updateMember(member);\n\n        // Get that member back.\n        member = consumerGroup.getOrMaybeCreateMember(\"member-id\", false);\n        assertEquals(\"member-id\", member.memberId());\n\n        assertThrows(UnknownMemberIdException.class, () ->\n            consumerGroup.getOrMaybeCreateMember(\"does-not-exist\", false));\n    }\n\n    @Test\n    public void testUpdateMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(member, consumerGroup.getOrMaybeCreateMember(\"member\", false));\n    }\n\n    @Test\n    public void testNoStaticMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        // Create a new member which is not static\n        consumerGroup.getOrMaybeCreateMember(\"member\", true);\n        assertNull(consumerGroup.staticMember(\"instance-id\"));\n    }\n\n    @Test\n    public void testGetStaticMemberByInstanceId() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .setInstanceId(\"instance\")\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(member, consumerGroup.staticMember(\"instance\"));\n        assertEquals(member, consumerGroup.getOrMaybeCreateMember(\"member\", false));\n        assertEquals(member.memberId(), consumerGroup.staticMemberId(\"instance\"));\n    }\n\n    @Test\n    public void testRemoveMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n        consumerGroup.updateMember(member);\n        assertTrue(consumerGroup.hasMember(\"member\"));\n\n        consumerGroup.removeMember(\"member\");\n        assertFalse(consumerGroup.hasMember(\"member\"));\n\n    }\n\n    @Test\n    public void testRemoveStaticMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member = new ConsumerGroupMember.Builder(\"member\")\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .setInstanceId(\"instance\")\n            .build();\n\n        consumerGroup.updateMember(member);\n        assertTrue(consumerGroup.hasMember(\"member\"));\n\n        consumerGroup.removeMember(\"member\");\n        assertFalse(consumerGroup.hasMember(\"member\"));\n        assertNull(consumerGroup.staticMember(\"instance\"));\n        assertNull(consumerGroup.staticMemberId(\"instance\"));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 9));\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(barTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(zarTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 4));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 5));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 9));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpochWhenPartitionIsReassignedBeforeBeingRevoked() {\n        Uuid fooTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(Collections.emptyMap())\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .setPartitionsPendingRevocation(Collections.emptyMap())\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(11, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpochWhenPartitionIsNotReleased() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember m1 = new ConsumerGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(m1);\n\n        ConsumerGroupMember m2 = new ConsumerGroupMember.Builder(\"m2\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        // m2 should not be able to acquire foo-1 because the partition is\n        // still owned by another member.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.updateMember(m2));\n    }\n\n    @Test\n    public void testRemovePartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        // Removing should fail because there is no epoch set.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        ));\n\n        ConsumerGroupMember m1 = new ConsumerGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(m1);\n\n        // Removing should fail because the expected epoch is incorrect.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testAddPartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        consumerGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        );\n\n        // Changing the epoch should fail because the owner of the partition\n        // should remove it first.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testDeletingMemberRemovesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 9));\n\n        consumerGroup.removeMember(member.memberId());\n\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 4));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 5));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 9));\n    }\n\n    @Test\n    public void testWaitingOnUnreleasedPartition() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n        String memberId1 = Uuid.randomUuid().toString();\n        String memberId2 = Uuid.randomUuid().toString();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        consumerGroup.updateTargetAssignment(memberId1, new Assignment(mkAssignment(\n            mkTopicAssignment(fooTopicId, 1, 2, 3),\n            mkTopicAssignment(zarTopicId, 7, 8, 9)\n        )));\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(memberId1)\n            .setMemberEpoch(10)\n            .setState(MemberState.UNRELEASED_PARTITIONS)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n        consumerGroup.updateMember(member1);\n\n        assertFalse(consumerGroup.waitingOnUnreleasedPartition(member1));\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(memberId2)\n            .setMemberEpoch(10)\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(zarTopicId, 7)))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertTrue(consumerGroup.waitingOnUnreleasedPartition(member1));\n    }\n\n    @Test\n    public void testGroupState() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member1);\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member2);\n        consumerGroup.setGroupEpoch(2);\n\n        assertEquals(MemberState.STABLE, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n\n        consumerGroup.setTargetAssignmentEpoch(2);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        member1 = new ConsumerGroupMember.Builder(member1)\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        // Member 2 is not stable so the group stays in reconciling state.\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setState(MemberState.UNREVOKED_PARTITIONS)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(MemberState.UNREVOKED_PARTITIONS, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(MemberState.STABLE, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n\n        consumerGroup.removeMember(\"member1\");\n        consumerGroup.removeMember(\"member2\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n    }\n\n    @Test\n    public void testGroupTypeFromString() {\n\n        assertEquals(Group.GroupType.parse(\"classic\"), Group.GroupType.CLASSIC);\n\n        // Test case insensitivity.\n        assertEquals(Group.GroupType.parse(\"Consumer\"), Group.GroupType.CONSUMER);\n\n        // Test with invalid group type.\n        assertEquals(Group.GroupType.parse(\"Invalid\"), Group.GroupType.UNKNOWN);\n    }\n\n    @Test\n    public void testPreferredServerAssignor() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setServerAssignorName(\"range\")\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setServerAssignorName(\"range\")\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setServerAssignorName(\"uniform\")\n            .build();\n\n        // The group is empty so the preferred assignor should be empty.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 1 has got an updated assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.computePreferredServerAssignor(null, member1)\n        );\n\n        // Update the group with member 1.\n        consumerGroup.updateMember(member1);\n\n        // Member 1 is in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 1 has been removed but this is not reflected in the group yet so\n        // we pass the removed member. The assignor should be range.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.computePreferredServerAssignor(member1, null)\n        );\n\n        // Member 2 has got an updated assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.computePreferredServerAssignor(null, member2)\n        );\n\n        // Update the group with member 2.\n        consumerGroup.updateMember(member2);\n\n        // Member 1 and 2 are in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Update the group with member 3.\n        consumerGroup.updateMember(member3);\n\n        // Member 1, 2 and 3 are in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Members without assignors\n        ConsumerGroupMember updatedMember1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setServerAssignorName(null)\n            .build();\n        ConsumerGroupMember updatedMember2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setServerAssignorName(null)\n            .build();\n        ConsumerGroupMember updatedMember3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setServerAssignorName(null)\n            .build();\n\n        // Member 1 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range or uniform.\n        Optional<String> assignor = consumerGroup.computePreferredServerAssignor(member1, updatedMember1);\n        assertTrue(assignor.equals(Optional.of(\"range\")) || assignor.equals(Optional.of(\"uniform\")));\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember1);\n\n        // Member 2 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range or uniform.\n        assertEquals(\n            Optional.of(\"uniform\"),\n            consumerGroup.computePreferredServerAssignor(member2, updatedMember2)\n        );\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember2);\n\n        // Only member 3 is left in the group so the assignor should be uniform.\n        assertEquals(\n            Optional.of(\"uniform\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 3 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be empty.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.computePreferredServerAssignor(member3, updatedMember3)\n        );\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember3);\n\n        // The group is empty so the assignor should be empty as well.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.preferredServerAssignor()\n        );\n    }\n\n    @Test\n    public void testUpdateSubscriptionMetadata() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addTopic(zarTopicId, \"zar\", 3)\n            .addRacks()\n            .build();\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Collections.singletonList(\"zar\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member1),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member1.\n        consumerGroup.updateMember(member1);\n\n        // It should return foo now.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member2),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member2.\n        consumerGroup.updateMember(member2);\n\n        // It should return foo and bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member2, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Removing member1 results in returning bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member3),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating group with member3.\n        consumerGroup.updateMember(member3);\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1, member 2 and member 3\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member1, member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2 and member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(Collections.singleton(member1)),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(Collections.emptySet()),\n                image.topics(),\n                image.cluster()\n            )\n        );\n    }\n\n    @Test\n    public void testUpdateSubscribedTopicNamesAndSubscriptionType() {\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.subscribedTopicNames()\n        );\n\n        // It should be Homogeneous by default.\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member1);\n\n        // It should be Homogeneous since there is just 1 member\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member3);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.removeMember(member1.memberId());\n\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        ConsumerGroupMember member4 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\", \"zar\"))\n            .build();\n\n        consumerGroup.updateMember(member4);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n    }\n\n    @Test\n    public void testUpdateInvertedAssignment() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = mock(GroupCoordinatorMetricsShard.class);\n        ConsumerGroup consumerGroup = new ConsumerGroup(snapshotRegistry, \"test-group\", metricsShard);\n        Uuid topicId = Uuid.randomUuid();\n        String memberId1 = \"member1\";\n        String memberId2 = \"member2\";\n\n        // Initial assignment for member1\n        Assignment initialAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, initialAssignment);\n\n        // Verify that partition 0 is assigned to member1.\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(0, memberId1)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1\n        Assignment newAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, newAssignment);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId1)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member2 to add partition 1\n        Assignment newAssignment2 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        consumerGroup.updateTargetAssignment(memberId2, newAssignment2);\n\n        // Verify that partition 1 is assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1 to revoke partition 1 and assign partition 0\n        Assignment newAssignment1 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, newAssignment1);\n\n        // Verify that partition 1 is still assigned to member2 and partition 0 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(\n                    mkEntry(0, memberId1),\n                    mkEntry(1, memberId2)\n                ))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // Test remove target assignment for member1\n        consumerGroup.removeTargetAssignment(memberId1);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is still assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n    }\n\n    @Test\n    public void testMetadataRefreshDeadline() {\n        MockTime time = new MockTime();\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Group epoch starts at 0.\n        assertEquals(0, group.groupEpoch());\n\n        // The refresh time deadline should be empty when the group is created or loaded.\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, group.metadataRefreshDeadline().epoch);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch());\n        assertFalse(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch(), group.metadataRefreshDeadline().epoch);\n\n        // Advance past the deadline. The metadata should have expired.\n        time.sleep(1001L);\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n\n        // Set the refresh time deadline with a higher group epoch. The metadata is considered\n        // as expired because the group epoch attached to the deadline is higher than the\n        // current group epoch.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch() + 1);\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch() + 1, group.metadataRefreshDeadline().epoch);\n\n        // Advance the group epoch.\n        group.setGroupEpoch(group.groupEpoch() + 1);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch());\n        assertFalse(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch(), group.metadataRefreshDeadline().epoch);\n\n        // Request metadata refresh. The metadata expires immediately.\n        group.requestMetadataRefresh();\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, group.metadataRefreshDeadline().epoch);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateTransactionalOffsetCommit(short version) {\n        boolean isTransactional = true;\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Simulate a call from the admin client without member id and member epoch.\n        // This should pass only if the group is empty.\n        group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"member-id\", null, 0, isTransactional, version));\n\n        // Create a member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member-id\").build());\n\n        // A call from the admin client should fail as the group is not empty.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version));\n\n        // The member epoch is stale.\n        assertThrows(StaleMemberEpochException.class, () ->\n            group.validateOffsetCommit(\"member-id\", \"\", 10, isTransactional, version));\n\n        // This should succeed.\n        group.validateOffsetCommit(\"member-id\", \"\", 0, isTransactional, version);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateOffsetCommit(short version) {\n        boolean isTransactional = false;\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Simulate a call from the admin client without member id and member epoch.\n        // This should pass only if the group is empty.\n        group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"member-id\", null, 0, isTransactional, version));\n\n        // Create members.\n        group.updateMember(\n            new ConsumerGroupMember\n                .Builder(\"new-protocol-member-id\").build()\n        );\n        group.updateMember(\n            new ConsumerGroupMember.Builder(\"old-protocol-member-id\")\n                .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata())\n                .build()\n        );\n\n        // A call from the admin client should fail as the group is not empty.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version));\n\n        // The member epoch is stale.\n        if (version >= 9) {\n            assertThrows(StaleMemberEpochException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 10, isTransactional, version));\n        } else {\n            assertThrows(UnsupportedVersionException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 10, isTransactional, version));\n        }\n        assertThrows(IllegalGenerationException.class, () ->\n            group.validateOffsetCommit(\"old-protocol-member-id\", \"\", 10, isTransactional, version));\n\n        // This should succeed.\n        if (version >= 9) {\n            group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 0, isTransactional, version);\n        } else {\n            assertThrows(UnsupportedVersionException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 0, isTransactional, version));\n        }\n    }\n\n    @Test\n    public void testAsListedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = new GroupCoordinatorMetricsShard(\n            snapshotRegistry,\n            Collections.emptyMap(),\n            new TopicPartition(\"__consumer_offsets\", 0)\n        );\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-foo\", metricsShard);\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE.toString(), group.stateAsString(1));\n    }\n\n    @Test\n    public void testValidateOffsetFetch() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ConsumerGroup group = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-foo\",\n            mock(GroupCoordinatorMetricsShard.class)\n        );\n\n        // Simulate a call from the admin client without member id and member epoch.\n        group.validateOffsetFetch(null, -1, Long.MAX_VALUE);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 0, Long.MAX_VALUE));\n\n        // Create a member.\n        idempotentCreateSnapshot(snapshotRegistry);\n        group.updateMember(new ConsumerGroupMember.Builder(\"member-id\").build());\n\n        // The member does not exist at last committed offset 0.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 0, 0));\n\n        // The member exists but the epoch is stale when the last committed offset is not considered.\n        assertThrows(StaleMemberEpochException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 10, Long.MAX_VALUE));\n\n        // This should succeed.\n        group.validateOffsetFetch(\"member-id\", 0, Long.MAX_VALUE);\n    }\n\n    @Test\n    public void testValidateDeleteGroup() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        assertDoesNotThrow(consumerGroup::validateDeleteGroup);\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n        consumerGroup.updateMember(member1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n\n        consumerGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n    }\n\n    @Test\n    public void testOffsetExpirationCondition() {\n        long currentTimestamp = 30000L;\n        long commitTimestamp = 20000L;\n        long offsetsRetentionMs = 10000L;\n        OffsetAndMetadata offsetAndMetadata = new OffsetAndMetadata(15000L, OptionalInt.empty(), \"\", commitTimestamp, OptionalLong.empty());\n        ConsumerGroup group = new ConsumerGroup(new SnapshotRegistry(new LogContext()), \"group-id\", mock(GroupCoordinatorMetricsShard.class));\n\n        Optional<OffsetExpirationCondition> offsetExpirationCondition = group.offsetExpirationCondition();\n        assertTrue(offsetExpirationCondition.isPresent());\n\n        OffsetExpirationConditionImpl condition = (OffsetExpirationConditionImpl) offsetExpirationCondition.get();\n        assertEquals(commitTimestamp, condition.baseTimestamp().apply(offsetAndMetadata));\n        assertTrue(condition.isOffsetExpired(offsetAndMetadata, currentTimestamp, offsetsRetentionMs));\n    }\n\n    @Test\n    public void testIsSubscribedToTopic() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addRacks()\n            .build();\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        consumerGroup.updateMember(member1);\n        consumerGroup.updateMember(member2);\n\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        assertTrue(consumerGroup.isSubscribedToTopic(\"foo\"));\n        assertTrue(consumerGroup.isSubscribedToTopic(\"bar\"));\n\n        consumerGroup.removeMember(\"member1\");\n        assertFalse(consumerGroup.isSubscribedToTopic(\"foo\"));\n\n        consumerGroup.removeMember(\"member2\");\n        assertFalse(consumerGroup.isSubscribedToTopic(\"bar\"));\n    }\n\n    @Test\n    public void testAsDescribedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-id-1\", mock(GroupCoordinatorMetricsShard.class));\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n                .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                .setServerAssignorName(\"assignorName\")\n                .build());\n        group.updateMember(new ConsumerGroupMember.Builder(\"member2\")\n                .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n\n        ConsumerGroupDescribeResponseData.DescribedGroup expected = new ConsumerGroupDescribeResponseData.DescribedGroup()\n            .setGroupId(\"group-id-1\")\n            .setGroupState(ConsumerGroup.ConsumerGroupState.STABLE.toString())\n            .setGroupEpoch(0)\n            .setAssignmentEpoch(0)\n            .setAssignorName(\"assignorName\")\n            .setMembers(Arrays.asList(\n                new ConsumerGroupDescribeResponseData.Member()\n                    .setMemberId(\"member1\")\n                    .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                    .setSubscribedTopicRegex(\"\"),\n                new ConsumerGroupDescribeResponseData.Member().setMemberId(\"member2\")\n                    .setSubscribedTopicRegex(\"\")\n            ));\n        ConsumerGroupDescribeResponseData.DescribedGroup actual = group.asDescribedGroup(1, \"\",\n            new MetadataImageBuilder().build().topics());\n\n        assertEquals(expected, actual);\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(0);\n    }\n\n    @Test\n    public void testStateTransitionMetrics() {\n        // Confirm metrics is not updated when a new ConsumerGroup is created but only when the group transitions\n        // its state.\n        GroupCoordinatorMetricsShard metrics = mock(GroupCoordinatorMetricsShard.class);\n        ConsumerGroup consumerGroup = new ConsumerGroup(\n            new SnapshotRegistry(new LogContext()),\n            \"group-id\",\n            metrics\n        );\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        verify(metrics, times(0)).onConsumerGroupStateTransition(null, ConsumerGroup.ConsumerGroupState.EMPTY);\n\n        ConsumerGroupMember member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.EMPTY, ConsumerGroup.ConsumerGroupState.RECONCILING);\n\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.RECONCILING, ConsumerGroup.ConsumerGroupState.ASSIGNING);\n\n        consumerGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.ASSIGNING, ConsumerGroup.ConsumerGroupState.STABLE);\n\n        consumerGroup.removeMember(\"member\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.STABLE, ConsumerGroup.ConsumerGroupState.EMPTY);\n    }\n\n    @Test\n    public void testIsInStatesCaseInsensitive() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = new GroupCoordinatorMetricsShard(\n            snapshotRegistry,\n            Collections.emptyMap(),\n            new TopicPartition(\"__consumer_offsets\", 0)\n        );\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-foo\", metricsShard);\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertTrue(group.isInStates(Collections.singleton(\"empty\"), 0));\n        assertFalse(group.isInStates(Collections.singleton(\"Empty\"), 0));\n\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertTrue(group.isInStates(Collections.singleton(\"empty\"), 0));\n        assertTrue(group.isInStates(Collections.singleton(\"stable\"), 1));\n        assertFalse(group.isInStates(Collections.singleton(\"empty\"), 1));\n    }\n\n    @Test\n    public void testClassicMembersSupportedProtocols() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> rangeProtocol = new ArrayList<>();\n        rangeProtocol.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> roundRobinAndRangeProtocols = new ArrayList<>();\n        roundRobinAndRangeProtocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"roundrobin\")\n            .setMetadata(new byte[0]));\n        roundRobinAndRangeProtocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member-1\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(rangeProtocol))\n            .build();\n        consumerGroup.updateMember(member1);\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertEquals(1, consumerGroup.classicMembersSupportedProtocols().get(\"roundrobin\"));\n        assertTrue(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"range\", \"sticky\"))));\n        assertFalse(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"sticky\", \"roundrobin\"))));\n\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(rangeProtocol))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertFalse(consumerGroup.classicMembersSupportedProtocols().containsKey(\"roundrobin\"));\n\n        member1 = new ConsumerGroupMember.Builder(member1)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member1);\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"roundrobin\"));\n        assertTrue(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"sticky\", \"roundrobin\"))));\n    }\n\n    @Test\n    public void testNumClassicProtocolMembers() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> protocols = new ArrayList<>();\n        protocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        // The group has member 1 (using the classic protocol).\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member-1\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(protocols))\n            .build();\n        consumerGroup.updateMember(member1);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n\n        // The group has member 1 (using the classic protocol) and member 2 (using the consumer protocol).\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .build();\n        consumerGroup.updateMember(member2);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n        assertFalse(consumerGroup.allMembersUseClassicProtocolExcept(\"member-1\"));\n        assertTrue(consumerGroup.allMembersUseClassicProtocolExcept(\"member-2\"));\n\n        // The group has member 2 (using the consumer protocol) and member 3 (using the consumer protocol).\n        consumerGroup.removeMember(member1.memberId());\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member-3\")\n            .build();\n        consumerGroup.updateMember(member3);\n        assertEquals(0, consumerGroup.numClassicProtocolMembers());\n        assertFalse(consumerGroup.allMembersUseClassicProtocolExcept(\"member-2\"));\n\n        // The group has member 2 (using the classic protocol).\n        consumerGroup.removeMember(member2.memberId());\n        member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(protocols))\n            .build();\n        consumerGroup.updateMember(member2);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n    }\n}",
                "methodCount": 34
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 1277,
                        "lineEnd": 1279,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method idempotentCreateSnapshot() directly interacts with the getOrCreateSnapshot() method from the SnapshotRegistry class to create a snapshot if it does not exist already. This tight coupling indicates that the functionality is directly relevant to the SnapshotRegistry, aligning with the responsibilities of managing and creating snapshots within this class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private createConsumerGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateTransactionalOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private createConsumerGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateTransactionalOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private createConsumerGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateTransactionalOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.13520961404876064
                },
                "private createConsumerGroup(String groupId)": {
                    "first": {
                        "method_name": "createConsumerGroup",
                        "method_signature": "private createConsumerGroup(String groupId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3996679820747262
                },
                "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)": {
                    "first": {
                        "method_name": "testValidateOffsetCommit",
                        "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.495556589130992
                },
                "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)": {
                    "first": {
                        "method_name": "testValidateTransactionalOffsetCommit",
                        "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5410938634102399
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "private createConsumerGroup(String groupId)",
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                    "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                    "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)"
                ],
                "llm_response_time": 6978
            },
            "targetClassMap": {
                "createConsumerGroup": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "testValidateTransactionalOffsetCommit": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "testValidateOffsetCommit": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testIsInStatesCaseInsensitive() : void in class org.apache.kafka.coordinator.group.modern.consumer.ConsumerGroupTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1318,
                    "endLine": 1338,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testIsInStatesCaseInsensitive() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1327,
                    "endLine": 1327,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1318,
                    "endLine": 1338,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testIsInStatesCaseInsensitive() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1327,
                    "endLine": 1327,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(0)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1334,
                    "endLine": 1334,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(1)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 615,
        "extraction_results": {
            "success": true,
            "newCommitHash": "2afeb4dfa4330fb206a36754f2ee15817ba410df",
            "newBranchName": "extract-idempotentCreateSnapshot-testIsInStatesCaseInsensitive-130af38"
        },
        "telemetry": {
            "id": "6bdfc6b9-5970-4224-a385-ceada1011e32",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1370,
                "lineStart": 74,
                "lineEnd": 1443,
                "bodyLineStart": 74,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                "sourceCode": "public class ConsumerGroupTest {\n\n    private ConsumerGroup createConsumerGroup(String groupId) {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        return new ConsumerGroup(\n            snapshotRegistry,\n            groupId,\n            mock(GroupCoordinatorMetricsShard.class)\n        );\n    }\n\n    @Test\n    public void testGetOrCreateMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        // Create a member.\n        member = consumerGroup.getOrMaybeCreateMember(\"member-id\", true);\n        assertEquals(\"member-id\", member.memberId());\n\n        // Add member to the group.\n        consumerGroup.updateMember(member);\n\n        // Get that member back.\n        member = consumerGroup.getOrMaybeCreateMember(\"member-id\", false);\n        assertEquals(\"member-id\", member.memberId());\n\n        assertThrows(UnknownMemberIdException.class, () ->\n            consumerGroup.getOrMaybeCreateMember(\"does-not-exist\", false));\n    }\n\n    @Test\n    public void testUpdateMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(member, consumerGroup.getOrMaybeCreateMember(\"member\", false));\n    }\n\n    @Test\n    public void testNoStaticMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        // Create a new member which is not static\n        consumerGroup.getOrMaybeCreateMember(\"member\", true);\n        assertNull(consumerGroup.staticMember(\"instance-id\"));\n    }\n\n    @Test\n    public void testGetStaticMemberByInstanceId() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .setInstanceId(\"instance\")\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(member, consumerGroup.staticMember(\"instance\"));\n        assertEquals(member, consumerGroup.getOrMaybeCreateMember(\"member\", false));\n        assertEquals(member.memberId(), consumerGroup.staticMemberId(\"instance\"));\n    }\n\n    @Test\n    public void testRemoveMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n        consumerGroup.updateMember(member);\n        assertTrue(consumerGroup.hasMember(\"member\"));\n\n        consumerGroup.removeMember(\"member\");\n        assertFalse(consumerGroup.hasMember(\"member\"));\n\n    }\n\n    @Test\n    public void testRemoveStaticMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member = new ConsumerGroupMember.Builder(\"member\")\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .setInstanceId(\"instance\")\n            .build();\n\n        consumerGroup.updateMember(member);\n        assertTrue(consumerGroup.hasMember(\"member\"));\n\n        consumerGroup.removeMember(\"member\");\n        assertFalse(consumerGroup.hasMember(\"member\"));\n        assertNull(consumerGroup.staticMember(\"instance\"));\n        assertNull(consumerGroup.staticMemberId(\"instance\"));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 9));\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(barTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(zarTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 4));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 5));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 9));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpochWhenPartitionIsReassignedBeforeBeingRevoked() {\n        Uuid fooTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(Collections.emptyMap())\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .setPartitionsPendingRevocation(Collections.emptyMap())\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(11, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpochWhenPartitionIsNotReleased() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember m1 = new ConsumerGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(m1);\n\n        ConsumerGroupMember m2 = new ConsumerGroupMember.Builder(\"m2\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        // m2 should not be able to acquire foo-1 because the partition is\n        // still owned by another member.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.updateMember(m2));\n    }\n\n    @Test\n    public void testRemovePartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        // Removing should fail because there is no epoch set.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        ));\n\n        ConsumerGroupMember m1 = new ConsumerGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(m1);\n\n        // Removing should fail because the expected epoch is incorrect.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testAddPartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        consumerGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        );\n\n        // Changing the epoch should fail because the owner of the partition\n        // should remove it first.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testDeletingMemberRemovesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 9));\n\n        consumerGroup.removeMember(member.memberId());\n\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 4));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 5));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 9));\n    }\n\n    @Test\n    public void testWaitingOnUnreleasedPartition() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n        String memberId1 = Uuid.randomUuid().toString();\n        String memberId2 = Uuid.randomUuid().toString();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        consumerGroup.updateTargetAssignment(memberId1, new Assignment(mkAssignment(\n            mkTopicAssignment(fooTopicId, 1, 2, 3),\n            mkTopicAssignment(zarTopicId, 7, 8, 9)\n        )));\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(memberId1)\n            .setMemberEpoch(10)\n            .setState(MemberState.UNRELEASED_PARTITIONS)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n        consumerGroup.updateMember(member1);\n\n        assertFalse(consumerGroup.waitingOnUnreleasedPartition(member1));\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(memberId2)\n            .setMemberEpoch(10)\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(zarTopicId, 7)))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertTrue(consumerGroup.waitingOnUnreleasedPartition(member1));\n    }\n\n    @Test\n    public void testGroupState() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member1);\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member2);\n        consumerGroup.setGroupEpoch(2);\n\n        assertEquals(MemberState.STABLE, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n\n        consumerGroup.setTargetAssignmentEpoch(2);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        member1 = new ConsumerGroupMember.Builder(member1)\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        // Member 2 is not stable so the group stays in reconciling state.\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setState(MemberState.UNREVOKED_PARTITIONS)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(MemberState.UNREVOKED_PARTITIONS, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(MemberState.STABLE, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n\n        consumerGroup.removeMember(\"member1\");\n        consumerGroup.removeMember(\"member2\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n    }\n\n    @Test\n    public void testGroupTypeFromString() {\n\n        assertEquals(Group.GroupType.parse(\"classic\"), Group.GroupType.CLASSIC);\n\n        // Test case insensitivity.\n        assertEquals(Group.GroupType.parse(\"Consumer\"), Group.GroupType.CONSUMER);\n\n        // Test with invalid group type.\n        assertEquals(Group.GroupType.parse(\"Invalid\"), Group.GroupType.UNKNOWN);\n    }\n\n    @Test\n    public void testPreferredServerAssignor() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setServerAssignorName(\"range\")\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setServerAssignorName(\"range\")\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setServerAssignorName(\"uniform\")\n            .build();\n\n        // The group is empty so the preferred assignor should be empty.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 1 has got an updated assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.computePreferredServerAssignor(null, member1)\n        );\n\n        // Update the group with member 1.\n        consumerGroup.updateMember(member1);\n\n        // Member 1 is in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 1 has been removed but this is not reflected in the group yet so\n        // we pass the removed member. The assignor should be range.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.computePreferredServerAssignor(member1, null)\n        );\n\n        // Member 2 has got an updated assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.computePreferredServerAssignor(null, member2)\n        );\n\n        // Update the group with member 2.\n        consumerGroup.updateMember(member2);\n\n        // Member 1 and 2 are in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Update the group with member 3.\n        consumerGroup.updateMember(member3);\n\n        // Member 1, 2 and 3 are in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Members without assignors\n        ConsumerGroupMember updatedMember1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setServerAssignorName(null)\n            .build();\n        ConsumerGroupMember updatedMember2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setServerAssignorName(null)\n            .build();\n        ConsumerGroupMember updatedMember3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setServerAssignorName(null)\n            .build();\n\n        // Member 1 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range or uniform.\n        Optional<String> assignor = consumerGroup.computePreferredServerAssignor(member1, updatedMember1);\n        assertTrue(assignor.equals(Optional.of(\"range\")) || assignor.equals(Optional.of(\"uniform\")));\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember1);\n\n        // Member 2 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range or uniform.\n        assertEquals(\n            Optional.of(\"uniform\"),\n            consumerGroup.computePreferredServerAssignor(member2, updatedMember2)\n        );\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember2);\n\n        // Only member 3 is left in the group so the assignor should be uniform.\n        assertEquals(\n            Optional.of(\"uniform\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 3 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be empty.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.computePreferredServerAssignor(member3, updatedMember3)\n        );\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember3);\n\n        // The group is empty so the assignor should be empty as well.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.preferredServerAssignor()\n        );\n    }\n\n    @Test\n    public void testUpdateSubscriptionMetadata() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addTopic(zarTopicId, \"zar\", 3)\n            .addRacks()\n            .build();\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Collections.singletonList(\"zar\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member1),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member1.\n        consumerGroup.updateMember(member1);\n\n        // It should return foo now.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member2),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member2.\n        consumerGroup.updateMember(member2);\n\n        // It should return foo and bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member2, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Removing member1 results in returning bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member3),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating group with member3.\n        consumerGroup.updateMember(member3);\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1, member 2 and member 3\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member1, member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2 and member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(Collections.singleton(member1)),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(Collections.emptySet()),\n                image.topics(),\n                image.cluster()\n            )\n        );\n    }\n\n    @Test\n    public void testUpdateSubscribedTopicNamesAndSubscriptionType() {\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.subscribedTopicNames()\n        );\n\n        // It should be Homogeneous by default.\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member1);\n\n        // It should be Homogeneous since there is just 1 member\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member3);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.removeMember(member1.memberId());\n\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        ConsumerGroupMember member4 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\", \"zar\"))\n            .build();\n\n        consumerGroup.updateMember(member4);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n    }\n\n    @Test\n    public void testUpdateInvertedAssignment() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = mock(GroupCoordinatorMetricsShard.class);\n        ConsumerGroup consumerGroup = new ConsumerGroup(snapshotRegistry, \"test-group\", metricsShard);\n        Uuid topicId = Uuid.randomUuid();\n        String memberId1 = \"member1\";\n        String memberId2 = \"member2\";\n\n        // Initial assignment for member1\n        Assignment initialAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, initialAssignment);\n\n        // Verify that partition 0 is assigned to member1.\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(0, memberId1)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1\n        Assignment newAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, newAssignment);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId1)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member2 to add partition 1\n        Assignment newAssignment2 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        consumerGroup.updateTargetAssignment(memberId2, newAssignment2);\n\n        // Verify that partition 1 is assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1 to revoke partition 1 and assign partition 0\n        Assignment newAssignment1 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, newAssignment1);\n\n        // Verify that partition 1 is still assigned to member2 and partition 0 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(\n                    mkEntry(0, memberId1),\n                    mkEntry(1, memberId2)\n                ))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // Test remove target assignment for member1\n        consumerGroup.removeTargetAssignment(memberId1);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is still assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n    }\n\n    @Test\n    public void testMetadataRefreshDeadline() {\n        MockTime time = new MockTime();\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Group epoch starts at 0.\n        assertEquals(0, group.groupEpoch());\n\n        // The refresh time deadline should be empty when the group is created or loaded.\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, group.metadataRefreshDeadline().epoch);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch());\n        assertFalse(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch(), group.metadataRefreshDeadline().epoch);\n\n        // Advance past the deadline. The metadata should have expired.\n        time.sleep(1001L);\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n\n        // Set the refresh time deadline with a higher group epoch. The metadata is considered\n        // as expired because the group epoch attached to the deadline is higher than the\n        // current group epoch.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch() + 1);\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch() + 1, group.metadataRefreshDeadline().epoch);\n\n        // Advance the group epoch.\n        group.setGroupEpoch(group.groupEpoch() + 1);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch());\n        assertFalse(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch(), group.metadataRefreshDeadline().epoch);\n\n        // Request metadata refresh. The metadata expires immediately.\n        group.requestMetadataRefresh();\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, group.metadataRefreshDeadline().epoch);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateTransactionalOffsetCommit(short version) {\n        boolean isTransactional = true;\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Simulate a call from the admin client without member id and member epoch.\n        // This should pass only if the group is empty.\n        group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"member-id\", null, 0, isTransactional, version));\n\n        // Create a member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member-id\").build());\n\n        // A call from the admin client should fail as the group is not empty.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version));\n\n        // The member epoch is stale.\n        assertThrows(StaleMemberEpochException.class, () ->\n            group.validateOffsetCommit(\"member-id\", \"\", 10, isTransactional, version));\n\n        // This should succeed.\n        group.validateOffsetCommit(\"member-id\", \"\", 0, isTransactional, version);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateOffsetCommit(short version) {\n        boolean isTransactional = false;\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Simulate a call from the admin client without member id and member epoch.\n        // This should pass only if the group is empty.\n        group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"member-id\", null, 0, isTransactional, version));\n\n        // Create members.\n        group.updateMember(\n            new ConsumerGroupMember\n                .Builder(\"new-protocol-member-id\").build()\n        );\n        group.updateMember(\n            new ConsumerGroupMember.Builder(\"old-protocol-member-id\")\n                .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata())\n                .build()\n        );\n\n        // A call from the admin client should fail as the group is not empty.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version));\n\n        // The member epoch is stale.\n        if (version >= 9) {\n            assertThrows(StaleMemberEpochException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 10, isTransactional, version));\n        } else {\n            assertThrows(UnsupportedVersionException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 10, isTransactional, version));\n        }\n        assertThrows(IllegalGenerationException.class, () ->\n            group.validateOffsetCommit(\"old-protocol-member-id\", \"\", 10, isTransactional, version));\n\n        // This should succeed.\n        if (version >= 9) {\n            group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 0, isTransactional, version);\n        } else {\n            assertThrows(UnsupportedVersionException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 0, isTransactional, version));\n        }\n    }\n\n    @Test\n    public void testAsListedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = new GroupCoordinatorMetricsShard(\n            snapshotRegistry,\n            Collections.emptyMap(),\n            new TopicPartition(\"__consumer_offsets\", 0)\n        );\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-foo\", metricsShard);\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE.toString(), group.stateAsString(1));\n    }\n\n    @Test\n    public void testValidateOffsetFetch() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ConsumerGroup group = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-foo\",\n            mock(GroupCoordinatorMetricsShard.class)\n        );\n\n        // Simulate a call from the admin client without member id and member epoch.\n        group.validateOffsetFetch(null, -1, Long.MAX_VALUE);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 0, Long.MAX_VALUE));\n\n        // Create a member.\n        idempotentCreateSnapshot(snapshotRegistry);\n        group.updateMember(new ConsumerGroupMember.Builder(\"member-id\").build());\n\n        // The member does not exist at last committed offset 0.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 0, 0));\n\n        // The member exists but the epoch is stale when the last committed offset is not considered.\n        assertThrows(StaleMemberEpochException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 10, Long.MAX_VALUE));\n\n        // This should succeed.\n        group.validateOffsetFetch(\"member-id\", 0, Long.MAX_VALUE);\n    }\n\n    @Test\n    public void testValidateDeleteGroup() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        assertDoesNotThrow(consumerGroup::validateDeleteGroup);\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n        consumerGroup.updateMember(member1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n\n        consumerGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n    }\n\n    @Test\n    public void testOffsetExpirationCondition() {\n        long currentTimestamp = 30000L;\n        long commitTimestamp = 20000L;\n        long offsetsRetentionMs = 10000L;\n        OffsetAndMetadata offsetAndMetadata = new OffsetAndMetadata(15000L, OptionalInt.empty(), \"\", commitTimestamp, OptionalLong.empty());\n        ConsumerGroup group = new ConsumerGroup(new SnapshotRegistry(new LogContext()), \"group-id\", mock(GroupCoordinatorMetricsShard.class));\n\n        Optional<OffsetExpirationCondition> offsetExpirationCondition = group.offsetExpirationCondition();\n        assertTrue(offsetExpirationCondition.isPresent());\n\n        OffsetExpirationConditionImpl condition = (OffsetExpirationConditionImpl) offsetExpirationCondition.get();\n        assertEquals(commitTimestamp, condition.baseTimestamp().apply(offsetAndMetadata));\n        assertTrue(condition.isOffsetExpired(offsetAndMetadata, currentTimestamp, offsetsRetentionMs));\n    }\n\n    @Test\n    public void testIsSubscribedToTopic() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addRacks()\n            .build();\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        consumerGroup.updateMember(member1);\n        consumerGroup.updateMember(member2);\n\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        assertTrue(consumerGroup.isSubscribedToTopic(\"foo\"));\n        assertTrue(consumerGroup.isSubscribedToTopic(\"bar\"));\n\n        consumerGroup.removeMember(\"member1\");\n        assertFalse(consumerGroup.isSubscribedToTopic(\"foo\"));\n\n        consumerGroup.removeMember(\"member2\");\n        assertFalse(consumerGroup.isSubscribedToTopic(\"bar\"));\n    }\n\n    @Test\n    public void testAsDescribedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-id-1\", mock(GroupCoordinatorMetricsShard.class));\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n                .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                .setServerAssignorName(\"assignorName\")\n                .build());\n        group.updateMember(new ConsumerGroupMember.Builder(\"member2\")\n                .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n\n        ConsumerGroupDescribeResponseData.DescribedGroup expected = new ConsumerGroupDescribeResponseData.DescribedGroup()\n            .setGroupId(\"group-id-1\")\n            .setGroupState(ConsumerGroup.ConsumerGroupState.STABLE.toString())\n            .setGroupEpoch(0)\n            .setAssignmentEpoch(0)\n            .setAssignorName(\"assignorName\")\n            .setMembers(Arrays.asList(\n                new ConsumerGroupDescribeResponseData.Member()\n                    .setMemberId(\"member1\")\n                    .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                    .setSubscribedTopicRegex(\"\"),\n                new ConsumerGroupDescribeResponseData.Member().setMemberId(\"member2\")\n                    .setSubscribedTopicRegex(\"\")\n            ));\n        ConsumerGroupDescribeResponseData.DescribedGroup actual = group.asDescribedGroup(1, \"\",\n            new MetadataImageBuilder().build().topics());\n\n        assertEquals(expected, actual);\n    }\n\n    @Test\n    public void testStateTransitionMetrics() {\n        // Confirm metrics is not updated when a new ConsumerGroup is created but only when the group transitions\n        // its state.\n        GroupCoordinatorMetricsShard metrics = mock(GroupCoordinatorMetricsShard.class);\n        ConsumerGroup consumerGroup = new ConsumerGroup(\n            new SnapshotRegistry(new LogContext()),\n            \"group-id\",\n            metrics\n        );\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        verify(metrics, times(0)).onConsumerGroupStateTransition(null, ConsumerGroup.ConsumerGroupState.EMPTY);\n\n        ConsumerGroupMember member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.EMPTY, ConsumerGroup.ConsumerGroupState.RECONCILING);\n\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.RECONCILING, ConsumerGroup.ConsumerGroupState.ASSIGNING);\n\n        consumerGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.ASSIGNING, ConsumerGroup.ConsumerGroupState.STABLE);\n\n        consumerGroup.removeMember(\"member\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.STABLE, ConsumerGroup.ConsumerGroupState.EMPTY);\n    }\n\n    @Test\n    public void testIsInStatesCaseInsensitive() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = new GroupCoordinatorMetricsShard(\n            snapshotRegistry,\n            Collections.emptyMap(),\n            new TopicPartition(\"__consumer_offsets\", 0)\n        );\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-foo\", metricsShard);\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertTrue(group.isInStates(Collections.singleton(\"empty\"), 0));\n        assertFalse(group.isInStates(Collections.singleton(\"Empty\"), 0));\n\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertTrue(group.isInStates(Collections.singleton(\"empty\"), 0));\n        assertTrue(group.isInStates(Collections.singleton(\"stable\"), 1));\n        assertFalse(group.isInStates(Collections.singleton(\"empty\"), 1));\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(0);\n    }\n\n    @Test\n    public void testClassicMembersSupportedProtocols() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> rangeProtocol = new ArrayList<>();\n        rangeProtocol.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> roundRobinAndRangeProtocols = new ArrayList<>();\n        roundRobinAndRangeProtocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"roundrobin\")\n            .setMetadata(new byte[0]));\n        roundRobinAndRangeProtocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member-1\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(rangeProtocol))\n            .build();\n        consumerGroup.updateMember(member1);\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertEquals(1, consumerGroup.classicMembersSupportedProtocols().get(\"roundrobin\"));\n        assertTrue(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"range\", \"sticky\"))));\n        assertFalse(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"sticky\", \"roundrobin\"))));\n\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(rangeProtocol))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertFalse(consumerGroup.classicMembersSupportedProtocols().containsKey(\"roundrobin\"));\n\n        member1 = new ConsumerGroupMember.Builder(member1)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member1);\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"roundrobin\"));\n        assertTrue(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"sticky\", \"roundrobin\"))));\n    }\n\n    @Test\n    public void testNumClassicProtocolMembers() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> protocols = new ArrayList<>();\n        protocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        // The group has member 1 (using the classic protocol).\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member-1\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(protocols))\n            .build();\n        consumerGroup.updateMember(member1);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n\n        // The group has member 1 (using the classic protocol) and member 2 (using the consumer protocol).\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .build();\n        consumerGroup.updateMember(member2);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n        assertFalse(consumerGroup.allMembersUseClassicProtocolExcept(\"member-1\"));\n        assertTrue(consumerGroup.allMembersUseClassicProtocolExcept(\"member-2\"));\n\n        // The group has member 2 (using the consumer protocol) and member 3 (using the consumer protocol).\n        consumerGroup.removeMember(member1.memberId());\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member-3\")\n            .build();\n        consumerGroup.updateMember(member3);\n        assertEquals(0, consumerGroup.numClassicProtocolMembers());\n        assertFalse(consumerGroup.allMembersUseClassicProtocolExcept(\"member-2\"));\n\n        // The group has member 2 (using the classic protocol).\n        consumerGroup.removeMember(member2.memberId());\n        member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(protocols))\n            .build();\n        consumerGroup.updateMember(member2);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n    }\n}",
                "methodCount": 34
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 1339,
                        "lineEnd": 1341,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method idempotentCreateSnapshot() directly interacts with the getOrCreateSnapshot() method from the SnapshotRegistry class to create a snapshot if it does not exist already. This tight coupling indicates that the functionality is directly relevant to the SnapshotRegistry, aligning with the responsibilities of managing and creating snapshots within this class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private createConsumerGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateTransactionalOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private createConsumerGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateTransactionalOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private createConsumerGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateTransactionalOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.13520961404876064
                },
                "private createConsumerGroup(String groupId)": {
                    "first": {
                        "method_name": "createConsumerGroup",
                        "method_signature": "private createConsumerGroup(String groupId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3996679820747262
                },
                "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)": {
                    "first": {
                        "method_name": "testValidateOffsetCommit",
                        "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.495556589130992
                },
                "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)": {
                    "first": {
                        "method_name": "testValidateTransactionalOffsetCommit",
                        "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5410938634102399
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "private createConsumerGroup(String groupId)",
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                    "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                    "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)"
                ],
                "llm_response_time": 6074
            },
            "targetClassMap": {
                "createConsumerGroup": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "testValidateOffsetCommit": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "testValidateTransactionalOffsetCommit": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testAsListedGroup() : void in class org.apache.kafka.coordinator.group.modern.share.ShareGroupTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 677,
                    "endLine": 692,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testAsListedGroup() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 681,
                    "endLine": 681,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 677,
                    "endLine": 692,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testAsListedGroup() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 681,
                    "endLine": 681,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(0)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 687,
                    "endLine": 687,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(1)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 616,
        "extraction_results": {
            "success": true,
            "newCommitHash": "615b67dc17c192240f1a481c9167bb8eb032c9f8",
            "newBranchName": "extract-idempotentCreateSnapshot-testAsListedGroup-130af38"
        },
        "telemetry": {
            "id": "6d53498c-5576-4b4d-b42e-bf327bcfc25f",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 790,
                "lineStart": 56,
                "lineEnd": 845,
                "bodyLineStart": 56,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                "sourceCode": "public class ShareGroupTest {\n\n    @Test\n    public void testType() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        assertEquals(Group.GroupType.SHARE, shareGroup.type());\n    }\n\n    @Test\n    public void testProtocolType() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        assertEquals(\"share\", shareGroup.protocolType());\n    }\n\n    @Test\n    public void testGetOrCreateMember() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        // Create a member.\n        member = shareGroup.getOrMaybeCreateMember(\"member-id\", true);\n        assertEquals(\"member-id\", member.memberId());\n\n        // Add member to the group.\n        shareGroup.updateMember(member);\n\n        // Get that member back.\n        member = shareGroup.getOrMaybeCreateMember(\"member-id\", false);\n        assertEquals(\"member-id\", member.memberId());\n\n        assertThrows(UnknownMemberIdException.class, () ->\n            shareGroup.getOrMaybeCreateMember(\"does-not-exist\", false));\n    }\n\n    @Test\n    public void testUpdateMember() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        member = shareGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ShareGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(member, shareGroup.getOrMaybeCreateMember(\"member\", false));\n    }\n\n    @Test\n    public void testRemoveMember() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        ShareGroupMember member = shareGroup.getOrMaybeCreateMember(\"member\", true);\n        shareGroup.updateMember(member);\n        assertTrue(shareGroup.hasMember(\"member\"));\n\n        shareGroup.removeMember(\"member\");\n        assertFalse(shareGroup.hasMember(\"member\"));\n\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        member = new ShareGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(barTopicId, 6));\n\n        member = new ShareGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(barTopicId, 1, 2, 3)))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(11, shareGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(11, shareGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(11, shareGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n    }\n\n    @Test\n    public void testRemovePartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        // Removing should fail because there is no epoch set.\n        assertThrows(IllegalStateException.class, () -> shareGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        ));\n\n        ShareGroupMember m1 = new ShareGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        shareGroup.updateMember(m1);\n\n        // Removing should fail because the expected epoch is incorrect.\n        assertThrows(IllegalStateException.class, () -> shareGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testAddPartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        shareGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        );\n\n        // Changing the epoch should fail because the owner of the partition\n        // should remove it first.\n        assertThrows(IllegalStateException.class, () -> shareGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testDeletingMemberRemovesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        member = new ShareGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n\n        shareGroup.removeMember(member.memberId());\n\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n    }\n\n    @Test\n    public void testGroupState() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        assertEquals(ShareGroup.ShareGroupState.EMPTY, shareGroup.state());\n        assertEquals(\"Empty\", shareGroup.stateAsString());\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        shareGroup.updateMember(member1);\n        shareGroup.setGroupEpoch(1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertEquals(\"Stable\", shareGroup.stateAsString());\n    }\n\n    @Test\n    public void testGroupTypeFromString() {\n        assertEquals(Group.GroupType.parse(\"share\"), Group.GroupType.SHARE);\n        // Test case insensitivity.\n        assertEquals(Group.GroupType.parse(\"Share\"), Group.GroupType.SHARE);\n        assertEquals(Group.GroupType.parse(\"SHare\"), Group.GroupType.SHARE);\n    }\n\n    @Test\n    public void testUpdateSubscriptionMetadata() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addTopic(zarTopicId, \"zar\", 3)\n            .addRacks()\n            .build();\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ShareGroupMember member2 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n        ShareGroupMember member3 = new ShareGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Collections.singletonList(\"zar\"))\n            .build();\n\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, member1),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member1.\n        shareGroup.updateMember(member1);\n\n        // It should return foo now.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, member2),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member2.\n        shareGroup.updateMember(member2);\n\n        // It should return foo and bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(member2, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Removing member1 results in returning bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, member3),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating group with member3.\n        shareGroup.updateMember(member3);\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1, member 2 and member 3\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member1, member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2 and member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(Collections.singleton(member1)),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(Collections.emptySet()),\n                image.topics(),\n                image.cluster()\n            )\n        );\n    }\n\n    @Test\n    public void testUpdateSubscribedTopicNamesAndSubscriptionType() {\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ShareGroupMember member2 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n        ShareGroupMember member3 = new ShareGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.subscribedTopicNames()\n        );\n\n        // It should be Homogeneous by default.\n        assertEquals(\n            HOMOGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.updateMember(member1);\n\n        // It should be Homogeneous since there is just 1 member\n        assertEquals(\n            HOMOGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.updateMember(member2);\n\n        assertEquals(\n            HETEROGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.updateMember(member3);\n\n        assertEquals(\n            HETEROGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.removeMember(member1.memberId());\n\n        assertEquals(\n            HOMOGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        ShareGroupMember member4 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\", \"zar\"))\n            .build();\n\n        shareGroup.updateMember(member4);\n\n        assertEquals(\n            HETEROGENEOUS,\n            shareGroup.subscriptionType()\n        );\n    }\n\n    @Test\n    public void testUpdateInvertedAssignment() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"test-group\");\n        Uuid topicId = Uuid.randomUuid();\n        String memberId1 = \"member1\";\n        String memberId2 = \"member2\";\n\n        // Initial assignment for member1\n        Assignment initialAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        shareGroup.updateTargetAssignment(memberId1, initialAssignment);\n\n        // Verify that partition 0 is assigned to member1.\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(0, memberId1)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1\n        Assignment newAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        shareGroup.updateTargetAssignment(memberId1, newAssignment);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId1)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member2 to add partition 1\n        Assignment newAssignment2 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        shareGroup.updateTargetAssignment(memberId2, newAssignment2);\n\n        // Verify that partition 1 is assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1 to revoke partition 1 and assign partition 0\n        Assignment newAssignment1 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        shareGroup.updateTargetAssignment(memberId1, newAssignment1);\n\n        // Verify that partition 1 is still assigned to member2 and partition 0 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(\n                    mkEntry(0, memberId1),\n                    mkEntry(1, memberId2)\n                ))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // Test remove target assignment for member1\n        shareGroup.removeTargetAssignment(memberId1);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is still assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n    }\n\n    @Test\n    public void testMetadataRefreshDeadline() {\n        MockTime time = new MockTime();\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        // Group epoch starts at 0.\n        assertEquals(0, shareGroup.groupEpoch());\n\n        // The refresh time deadline should be empty when the group is created or loaded.\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, shareGroup.metadataRefreshDeadline().epoch);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        shareGroup.setMetadataRefreshDeadline(time.milliseconds() + 1000, shareGroup.groupEpoch());\n        assertFalse(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(shareGroup.groupEpoch(), shareGroup.metadataRefreshDeadline().epoch);\n\n        // Advance past the deadline. The metadata should have expired.\n        time.sleep(1001L);\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n\n        // Set the refresh time deadline with a higher group epoch. The metadata is considered\n        // as expired because the group epoch attached to the deadline is higher than the\n        // current group epoch.\n        shareGroup.setMetadataRefreshDeadline(time.milliseconds() + 1000, shareGroup.groupEpoch() + 1);\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(shareGroup.groupEpoch() + 1, shareGroup.metadataRefreshDeadline().epoch);\n\n        // Advance the group epoch.\n        shareGroup.setGroupEpoch(shareGroup.groupEpoch() + 1);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        shareGroup.setMetadataRefreshDeadline(time.milliseconds() + 1000, shareGroup.groupEpoch());\n        assertFalse(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(shareGroup.groupEpoch(), shareGroup.metadataRefreshDeadline().epoch);\n\n        // Request metadata refresh. The metadata expires immediately.\n        shareGroup.requestMetadataRefresh();\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, shareGroup.metadataRefreshDeadline().epoch);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateOffsetCommit(short version) {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, () ->\n            shareGroup.validateOffsetCommit(null, null, -1, false, version));\n    }\n\n    @Test\n    public void testAsListedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"group-foo\");\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ShareGroupState.EMPTY, shareGroup.state(0));\n        assertEquals(\"Empty\", shareGroup.stateAsString(0));\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertEquals(ShareGroupState.EMPTY, shareGroup.state(0));\n        assertEquals(\"Empty\", shareGroup.stateAsString(0));\n        assertEquals(ShareGroupState.STABLE, shareGroup.state(1));\n        assertEquals(\"Stable\", shareGroup.stateAsString(1));\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(0);\n    }\n\n    @Test\n    public void testOffsetExpirationCondition() {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, shareGroup::offsetExpirationCondition);\n    }\n\n    @Test\n    public void testValidateOffsetFetch() {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, () ->\n            shareGroup.validateOffsetFetch(null, -1, -1));\n    }\n\n    @Test\n    public void testValidateOffsetDelete() {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, shareGroup::validateOffsetDelete);\n    }\n\n    @Test\n    public void testValidateDeleteGroup() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        assertEquals(ShareGroupState.EMPTY, shareGroup.state());\n        assertDoesNotThrow(shareGroup::validateDeleteGroup);\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n        shareGroup.updateMember(member1);\n\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertThrows(GroupNotEmptyException.class, shareGroup::validateDeleteGroup);\n\n        shareGroup.setGroupEpoch(1);\n\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertThrows(GroupNotEmptyException.class, shareGroup::validateDeleteGroup);\n\n        shareGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertThrows(GroupNotEmptyException.class, shareGroup::validateDeleteGroup);\n    }\n\n    @Test\n    public void testIsSubscribedToTopic() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addRacks()\n            .build();\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ShareGroupMember member2 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        shareGroup.updateMember(member1);\n        shareGroup.updateMember(member2);\n\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        assertTrue(shareGroup.isSubscribedToTopic(\"foo\"));\n        assertTrue(shareGroup.isSubscribedToTopic(\"bar\"));\n\n        shareGroup.removeMember(\"member1\");\n        assertFalse(shareGroup.isSubscribedToTopic(\"foo\"));\n\n        shareGroup.removeMember(\"member2\");\n        assertFalse(shareGroup.isSubscribedToTopic(\"bar\"));\n    }\n\n    @Test\n    public void testAsDescribedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"group-id-1\");\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ShareGroupState.EMPTY.toString(), shareGroup.stateAsString(0));\n\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member1\")\n                .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                .build());\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member2\")\n                .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n\n        ShareGroupDescribeResponseData.DescribedGroup expected = new ShareGroupDescribeResponseData.DescribedGroup()\n            .setGroupId(\"group-id-1\")\n            .setGroupState(ShareGroupState.STABLE.toString())\n            .setGroupEpoch(0)\n            .setAssignmentEpoch(0)\n            .setAssignorName(\"assignorName\")\n            .setMembers(Arrays.asList(\n                new ShareGroupDescribeResponseData.Member()\n                    .setMemberId(\"member1\")\n                    .setSubscribedTopicNames(Collections.singletonList(\"foo\")),\n                new ShareGroupDescribeResponseData.Member().setMemberId(\"member2\")\n            ));\n        ShareGroupDescribeResponseData.DescribedGroup actual = shareGroup.asDescribedGroup(1, \"assignorName\",\n            new MetadataImageBuilder().build().topics());\n\n        assertEquals(expected, actual);\n    }\n\n    @Test\n    public void testIsInStatesCaseInsensitive() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"group-foo\");\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertTrue(shareGroup.isInStates(Collections.singleton(\"empty\"), 0));\n        assertFalse(shareGroup.isInStates(Collections.singleton(\"Empty\"), 0));\n\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertTrue(shareGroup.isInStates(Collections.singleton(\"empty\"), 0));\n        assertTrue(shareGroup.isInStates(Collections.singleton(\"stable\"), 1));\n        assertFalse(shareGroup.isInStates(Collections.singleton(\"empty\"), 1));\n    }\n\n    private ShareGroup createShareGroup(String groupId) {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        return new ShareGroup(\n            snapshotRegistry,\n            groupId\n        );\n    }\n}",
                "methodCount": 26
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 693,
                        "lineEnd": 695,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method idempotentCreateSnapshot() directly interacts with the getOrCreateSnapshot() method from the SnapshotRegistry class to create a snapshot if it does not exist already. This tight coupling indicates that the functionality is directly relevant to the SnapshotRegistry, aligning with the responsibilities of managing and creating snapshots within this class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createShareGroup",
                            "method_signature": "private createShareGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createShareGroup",
                            "method_signature": "private createShareGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createShareGroup",
                            "method_signature": "private createShareGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.17298872485207484
                },
                "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)": {
                    "first": {
                        "method_name": "testValidateOffsetCommit",
                        "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4109393189139836
                },
                "private createShareGroup(String groupId)": {
                    "first": {
                        "method_name": "createShareGroup",
                        "method_signature": "private createShareGroup(String groupId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.45189092700050687
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "private createShareGroup(String groupId)",
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                    "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)"
                ],
                "llm_response_time": 2492
            },
            "targetClassMap": {
                "createShareGroup": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3154,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "testValidateOffsetCommit": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2565,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testAsDescribedGroup() : void in class org.apache.kafka.coordinator.group.modern.share.ShareGroupTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 785,
                    "endLine": 815,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testAsDescribedGroup() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 789,
                    "endLine": 789,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 785,
                    "endLine": 815,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testAsDescribedGroup() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 789,
                    "endLine": 789,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(0)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 797,
                    "endLine": 797,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(1)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 617,
        "extraction_results": {
            "success": true,
            "newCommitHash": "6763e67eb5b4151e73ba5a9ed1ee0dd90c71759f",
            "newBranchName": "extract-idempotentCreateSnapshot-testAsDescribedGroup-130af38"
        },
        "telemetry": {
            "id": "d3c25319-8937-4749-b864-7690ea49622c",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 790,
                "lineStart": 56,
                "lineEnd": 845,
                "bodyLineStart": 56,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                "sourceCode": "public class ShareGroupTest {\n\n    @Test\n    public void testType() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        assertEquals(Group.GroupType.SHARE, shareGroup.type());\n    }\n\n    @Test\n    public void testProtocolType() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        assertEquals(\"share\", shareGroup.protocolType());\n    }\n\n    @Test\n    public void testGetOrCreateMember() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        // Create a member.\n        member = shareGroup.getOrMaybeCreateMember(\"member-id\", true);\n        assertEquals(\"member-id\", member.memberId());\n\n        // Add member to the group.\n        shareGroup.updateMember(member);\n\n        // Get that member back.\n        member = shareGroup.getOrMaybeCreateMember(\"member-id\", false);\n        assertEquals(\"member-id\", member.memberId());\n\n        assertThrows(UnknownMemberIdException.class, () ->\n            shareGroup.getOrMaybeCreateMember(\"does-not-exist\", false));\n    }\n\n    @Test\n    public void testUpdateMember() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        member = shareGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ShareGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(member, shareGroup.getOrMaybeCreateMember(\"member\", false));\n    }\n\n    @Test\n    public void testRemoveMember() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        ShareGroupMember member = shareGroup.getOrMaybeCreateMember(\"member\", true);\n        shareGroup.updateMember(member);\n        assertTrue(shareGroup.hasMember(\"member\"));\n\n        shareGroup.removeMember(\"member\");\n        assertFalse(shareGroup.hasMember(\"member\"));\n\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        member = new ShareGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(barTopicId, 6));\n\n        member = new ShareGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(barTopicId, 1, 2, 3)))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(11, shareGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(11, shareGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(11, shareGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n    }\n\n    @Test\n    public void testRemovePartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        // Removing should fail because there is no epoch set.\n        assertThrows(IllegalStateException.class, () -> shareGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        ));\n\n        ShareGroupMember m1 = new ShareGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        shareGroup.updateMember(m1);\n\n        // Removing should fail because the expected epoch is incorrect.\n        assertThrows(IllegalStateException.class, () -> shareGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testAddPartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        shareGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        );\n\n        // Changing the epoch should fail because the owner of the partition\n        // should remove it first.\n        assertThrows(IllegalStateException.class, () -> shareGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testDeletingMemberRemovesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        member = new ShareGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n\n        shareGroup.removeMember(member.memberId());\n\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n    }\n\n    @Test\n    public void testGroupState() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        assertEquals(ShareGroup.ShareGroupState.EMPTY, shareGroup.state());\n        assertEquals(\"Empty\", shareGroup.stateAsString());\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        shareGroup.updateMember(member1);\n        shareGroup.setGroupEpoch(1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertEquals(\"Stable\", shareGroup.stateAsString());\n    }\n\n    @Test\n    public void testGroupTypeFromString() {\n        assertEquals(Group.GroupType.parse(\"share\"), Group.GroupType.SHARE);\n        // Test case insensitivity.\n        assertEquals(Group.GroupType.parse(\"Share\"), Group.GroupType.SHARE);\n        assertEquals(Group.GroupType.parse(\"SHare\"), Group.GroupType.SHARE);\n    }\n\n    @Test\n    public void testUpdateSubscriptionMetadata() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addTopic(zarTopicId, \"zar\", 3)\n            .addRacks()\n            .build();\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ShareGroupMember member2 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n        ShareGroupMember member3 = new ShareGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Collections.singletonList(\"zar\"))\n            .build();\n\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, member1),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member1.\n        shareGroup.updateMember(member1);\n\n        // It should return foo now.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, member2),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member2.\n        shareGroup.updateMember(member2);\n\n        // It should return foo and bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(member2, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Removing member1 results in returning bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, member3),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating group with member3.\n        shareGroup.updateMember(member3);\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1, member 2 and member 3\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member1, member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2 and member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(Collections.singleton(member1)),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(Collections.emptySet()),\n                image.topics(),\n                image.cluster()\n            )\n        );\n    }\n\n    @Test\n    public void testUpdateSubscribedTopicNamesAndSubscriptionType() {\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ShareGroupMember member2 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n        ShareGroupMember member3 = new ShareGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.subscribedTopicNames()\n        );\n\n        // It should be Homogeneous by default.\n        assertEquals(\n            HOMOGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.updateMember(member1);\n\n        // It should be Homogeneous since there is just 1 member\n        assertEquals(\n            HOMOGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.updateMember(member2);\n\n        assertEquals(\n            HETEROGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.updateMember(member3);\n\n        assertEquals(\n            HETEROGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.removeMember(member1.memberId());\n\n        assertEquals(\n            HOMOGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        ShareGroupMember member4 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\", \"zar\"))\n            .build();\n\n        shareGroup.updateMember(member4);\n\n        assertEquals(\n            HETEROGENEOUS,\n            shareGroup.subscriptionType()\n        );\n    }\n\n    @Test\n    public void testUpdateInvertedAssignment() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"test-group\");\n        Uuid topicId = Uuid.randomUuid();\n        String memberId1 = \"member1\";\n        String memberId2 = \"member2\";\n\n        // Initial assignment for member1\n        Assignment initialAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        shareGroup.updateTargetAssignment(memberId1, initialAssignment);\n\n        // Verify that partition 0 is assigned to member1.\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(0, memberId1)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1\n        Assignment newAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        shareGroup.updateTargetAssignment(memberId1, newAssignment);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId1)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member2 to add partition 1\n        Assignment newAssignment2 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        shareGroup.updateTargetAssignment(memberId2, newAssignment2);\n\n        // Verify that partition 1 is assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1 to revoke partition 1 and assign partition 0\n        Assignment newAssignment1 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        shareGroup.updateTargetAssignment(memberId1, newAssignment1);\n\n        // Verify that partition 1 is still assigned to member2 and partition 0 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(\n                    mkEntry(0, memberId1),\n                    mkEntry(1, memberId2)\n                ))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // Test remove target assignment for member1\n        shareGroup.removeTargetAssignment(memberId1);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is still assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n    }\n\n    @Test\n    public void testMetadataRefreshDeadline() {\n        MockTime time = new MockTime();\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        // Group epoch starts at 0.\n        assertEquals(0, shareGroup.groupEpoch());\n\n        // The refresh time deadline should be empty when the group is created or loaded.\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, shareGroup.metadataRefreshDeadline().epoch);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        shareGroup.setMetadataRefreshDeadline(time.milliseconds() + 1000, shareGroup.groupEpoch());\n        assertFalse(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(shareGroup.groupEpoch(), shareGroup.metadataRefreshDeadline().epoch);\n\n        // Advance past the deadline. The metadata should have expired.\n        time.sleep(1001L);\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n\n        // Set the refresh time deadline with a higher group epoch. The metadata is considered\n        // as expired because the group epoch attached to the deadline is higher than the\n        // current group epoch.\n        shareGroup.setMetadataRefreshDeadline(time.milliseconds() + 1000, shareGroup.groupEpoch() + 1);\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(shareGroup.groupEpoch() + 1, shareGroup.metadataRefreshDeadline().epoch);\n\n        // Advance the group epoch.\n        shareGroup.setGroupEpoch(shareGroup.groupEpoch() + 1);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        shareGroup.setMetadataRefreshDeadline(time.milliseconds() + 1000, shareGroup.groupEpoch());\n        assertFalse(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(shareGroup.groupEpoch(), shareGroup.metadataRefreshDeadline().epoch);\n\n        // Request metadata refresh. The metadata expires immediately.\n        shareGroup.requestMetadataRefresh();\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, shareGroup.metadataRefreshDeadline().epoch);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateOffsetCommit(short version) {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, () ->\n            shareGroup.validateOffsetCommit(null, null, -1, false, version));\n    }\n\n    @Test\n    public void testAsListedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"group-foo\");\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ShareGroupState.EMPTY, shareGroup.state(0));\n        assertEquals(\"Empty\", shareGroup.stateAsString(0));\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertEquals(ShareGroupState.EMPTY, shareGroup.state(0));\n        assertEquals(\"Empty\", shareGroup.stateAsString(0));\n        assertEquals(ShareGroupState.STABLE, shareGroup.state(1));\n        assertEquals(\"Stable\", shareGroup.stateAsString(1));\n    }\n\n    @Test\n    public void testOffsetExpirationCondition() {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, shareGroup::offsetExpirationCondition);\n    }\n\n    @Test\n    public void testValidateOffsetFetch() {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, () ->\n            shareGroup.validateOffsetFetch(null, -1, -1));\n    }\n\n    @Test\n    public void testValidateOffsetDelete() {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, shareGroup::validateOffsetDelete);\n    }\n\n    @Test\n    public void testValidateDeleteGroup() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        assertEquals(ShareGroupState.EMPTY, shareGroup.state());\n        assertDoesNotThrow(shareGroup::validateDeleteGroup);\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n        shareGroup.updateMember(member1);\n\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertThrows(GroupNotEmptyException.class, shareGroup::validateDeleteGroup);\n\n        shareGroup.setGroupEpoch(1);\n\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertThrows(GroupNotEmptyException.class, shareGroup::validateDeleteGroup);\n\n        shareGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertThrows(GroupNotEmptyException.class, shareGroup::validateDeleteGroup);\n    }\n\n    @Test\n    public void testIsSubscribedToTopic() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addRacks()\n            .build();\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ShareGroupMember member2 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        shareGroup.updateMember(member1);\n        shareGroup.updateMember(member2);\n\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        assertTrue(shareGroup.isSubscribedToTopic(\"foo\"));\n        assertTrue(shareGroup.isSubscribedToTopic(\"bar\"));\n\n        shareGroup.removeMember(\"member1\");\n        assertFalse(shareGroup.isSubscribedToTopic(\"foo\"));\n\n        shareGroup.removeMember(\"member2\");\n        assertFalse(shareGroup.isSubscribedToTopic(\"bar\"));\n    }\n\n    @Test\n    public void testAsDescribedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"group-id-1\");\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ShareGroupState.EMPTY.toString(), shareGroup.stateAsString(0));\n\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member1\")\n                .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                .build());\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member2\")\n                .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n\n        ShareGroupDescribeResponseData.DescribedGroup expected = new ShareGroupDescribeResponseData.DescribedGroup()\n            .setGroupId(\"group-id-1\")\n            .setGroupState(ShareGroupState.STABLE.toString())\n            .setGroupEpoch(0)\n            .setAssignmentEpoch(0)\n            .setAssignorName(\"assignorName\")\n            .setMembers(Arrays.asList(\n                new ShareGroupDescribeResponseData.Member()\n                    .setMemberId(\"member1\")\n                    .setSubscribedTopicNames(Collections.singletonList(\"foo\")),\n                new ShareGroupDescribeResponseData.Member().setMemberId(\"member2\")\n            ));\n        ShareGroupDescribeResponseData.DescribedGroup actual = shareGroup.asDescribedGroup(1, \"assignorName\",\n            new MetadataImageBuilder().build().topics());\n\n        assertEquals(expected, actual);\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(0);\n    }\n\n    @Test\n    public void testIsInStatesCaseInsensitive() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"group-foo\");\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertTrue(shareGroup.isInStates(Collections.singleton(\"empty\"), 0));\n        assertFalse(shareGroup.isInStates(Collections.singleton(\"Empty\"), 0));\n\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertTrue(shareGroup.isInStates(Collections.singleton(\"empty\"), 0));\n        assertTrue(shareGroup.isInStates(Collections.singleton(\"stable\"), 1));\n        assertFalse(shareGroup.isInStates(Collections.singleton(\"empty\"), 1));\n    }\n\n    private ShareGroup createShareGroup(String groupId) {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        return new ShareGroup(\n            snapshotRegistry,\n            groupId\n        );\n    }\n}",
                "methodCount": 26
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 816,
                        "lineEnd": 818,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method idempotentCreateSnapshot() directly interacts with the getOrCreateSnapshot() method from the SnapshotRegistry class to create a snapshot if it does not exist already. This tight coupling indicates that the functionality is directly relevant to the SnapshotRegistry, aligning with the responsibilities of managing and creating snapshots within this class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createShareGroup",
                            "method_signature": "private createShareGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createShareGroup",
                            "method_signature": "private createShareGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createShareGroup",
                            "method_signature": "private createShareGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.17298872485207484
                },
                "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)": {
                    "first": {
                        "method_name": "testValidateOffsetCommit",
                        "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4109393189139836
                },
                "private createShareGroup(String groupId)": {
                    "first": {
                        "method_name": "createShareGroup",
                        "method_signature": "private createShareGroup(String groupId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.45189092700050687
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "private createShareGroup(String groupId)",
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                    "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)"
                ],
                "llm_response_time": 5126
            },
            "targetClassMap": {
                "createShareGroup": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "testValidateOffsetCommit": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testIsInStatesCaseInsensitive() : void in class org.apache.kafka.coordinator.group.modern.share.ShareGroupTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 817,
                    "endLine": 832,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testIsInStatesCaseInsensitive() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 821,
                    "endLine": 821,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 817,
                    "endLine": 832,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testIsInStatesCaseInsensitive() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 821,
                    "endLine": 821,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(0)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 828,
                    "endLine": 828,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(1)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 618,
        "extraction_results": {
            "success": true,
            "newCommitHash": "7e477e8eadf30c6142f19bbb76cff3b1bf7d8587",
            "newBranchName": "extract-idempotentCreateSnapshot-testIsInStatesCaseInsensitive-130af38"
        },
        "telemetry": {
            "id": "7ca1fc50-a8dc-4a65-a15e-e7b3ff608893",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 790,
                "lineStart": 56,
                "lineEnd": 845,
                "bodyLineStart": 56,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                "sourceCode": "public class ShareGroupTest {\n\n    @Test\n    public void testType() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        assertEquals(Group.GroupType.SHARE, shareGroup.type());\n    }\n\n    @Test\n    public void testProtocolType() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        assertEquals(\"share\", shareGroup.protocolType());\n    }\n\n    @Test\n    public void testGetOrCreateMember() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        // Create a member.\n        member = shareGroup.getOrMaybeCreateMember(\"member-id\", true);\n        assertEquals(\"member-id\", member.memberId());\n\n        // Add member to the group.\n        shareGroup.updateMember(member);\n\n        // Get that member back.\n        member = shareGroup.getOrMaybeCreateMember(\"member-id\", false);\n        assertEquals(\"member-id\", member.memberId());\n\n        assertThrows(UnknownMemberIdException.class, () ->\n            shareGroup.getOrMaybeCreateMember(\"does-not-exist\", false));\n    }\n\n    @Test\n    public void testUpdateMember() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        member = shareGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ShareGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(member, shareGroup.getOrMaybeCreateMember(\"member\", false));\n    }\n\n    @Test\n    public void testRemoveMember() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        ShareGroupMember member = shareGroup.getOrMaybeCreateMember(\"member\", true);\n        shareGroup.updateMember(member);\n        assertTrue(shareGroup.hasMember(\"member\"));\n\n        shareGroup.removeMember(\"member\");\n        assertFalse(shareGroup.hasMember(\"member\"));\n\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        member = new ShareGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(barTopicId, 6));\n\n        member = new ShareGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(barTopicId, 1, 2, 3)))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(11, shareGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(11, shareGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(11, shareGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n    }\n\n    @Test\n    public void testRemovePartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        // Removing should fail because there is no epoch set.\n        assertThrows(IllegalStateException.class, () -> shareGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        ));\n\n        ShareGroupMember m1 = new ShareGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        shareGroup.updateMember(m1);\n\n        // Removing should fail because the expected epoch is incorrect.\n        assertThrows(IllegalStateException.class, () -> shareGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testAddPartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        shareGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        );\n\n        // Changing the epoch should fail because the owner of the partition\n        // should remove it first.\n        assertThrows(IllegalStateException.class, () -> shareGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testDeletingMemberRemovesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        member = new ShareGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n\n        shareGroup.removeMember(member.memberId());\n\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n    }\n\n    @Test\n    public void testGroupState() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        assertEquals(ShareGroup.ShareGroupState.EMPTY, shareGroup.state());\n        assertEquals(\"Empty\", shareGroup.stateAsString());\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        shareGroup.updateMember(member1);\n        shareGroup.setGroupEpoch(1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertEquals(\"Stable\", shareGroup.stateAsString());\n    }\n\n    @Test\n    public void testGroupTypeFromString() {\n        assertEquals(Group.GroupType.parse(\"share\"), Group.GroupType.SHARE);\n        // Test case insensitivity.\n        assertEquals(Group.GroupType.parse(\"Share\"), Group.GroupType.SHARE);\n        assertEquals(Group.GroupType.parse(\"SHare\"), Group.GroupType.SHARE);\n    }\n\n    @Test\n    public void testUpdateSubscriptionMetadata() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addTopic(zarTopicId, \"zar\", 3)\n            .addRacks()\n            .build();\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ShareGroupMember member2 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n        ShareGroupMember member3 = new ShareGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Collections.singletonList(\"zar\"))\n            .build();\n\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, member1),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member1.\n        shareGroup.updateMember(member1);\n\n        // It should return foo now.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, member2),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member2.\n        shareGroup.updateMember(member2);\n\n        // It should return foo and bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(member2, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Removing member1 results in returning bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, member3),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating group with member3.\n        shareGroup.updateMember(member3);\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1, member 2 and member 3\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member1, member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2 and member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(Collections.singleton(member1)),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(Collections.emptySet()),\n                image.topics(),\n                image.cluster()\n            )\n        );\n    }\n\n    @Test\n    public void testUpdateSubscribedTopicNamesAndSubscriptionType() {\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ShareGroupMember member2 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n        ShareGroupMember member3 = new ShareGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.subscribedTopicNames()\n        );\n\n        // It should be Homogeneous by default.\n        assertEquals(\n            HOMOGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.updateMember(member1);\n\n        // It should be Homogeneous since there is just 1 member\n        assertEquals(\n            HOMOGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.updateMember(member2);\n\n        assertEquals(\n            HETEROGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.updateMember(member3);\n\n        assertEquals(\n            HETEROGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.removeMember(member1.memberId());\n\n        assertEquals(\n            HOMOGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        ShareGroupMember member4 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\", \"zar\"))\n            .build();\n\n        shareGroup.updateMember(member4);\n\n        assertEquals(\n            HETEROGENEOUS,\n            shareGroup.subscriptionType()\n        );\n    }\n\n    @Test\n    public void testUpdateInvertedAssignment() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"test-group\");\n        Uuid topicId = Uuid.randomUuid();\n        String memberId1 = \"member1\";\n        String memberId2 = \"member2\";\n\n        // Initial assignment for member1\n        Assignment initialAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        shareGroup.updateTargetAssignment(memberId1, initialAssignment);\n\n        // Verify that partition 0 is assigned to member1.\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(0, memberId1)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1\n        Assignment newAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        shareGroup.updateTargetAssignment(memberId1, newAssignment);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId1)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member2 to add partition 1\n        Assignment newAssignment2 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        shareGroup.updateTargetAssignment(memberId2, newAssignment2);\n\n        // Verify that partition 1 is assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1 to revoke partition 1 and assign partition 0\n        Assignment newAssignment1 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        shareGroup.updateTargetAssignment(memberId1, newAssignment1);\n\n        // Verify that partition 1 is still assigned to member2 and partition 0 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(\n                    mkEntry(0, memberId1),\n                    mkEntry(1, memberId2)\n                ))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // Test remove target assignment for member1\n        shareGroup.removeTargetAssignment(memberId1);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is still assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n    }\n\n    @Test\n    public void testMetadataRefreshDeadline() {\n        MockTime time = new MockTime();\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        // Group epoch starts at 0.\n        assertEquals(0, shareGroup.groupEpoch());\n\n        // The refresh time deadline should be empty when the group is created or loaded.\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, shareGroup.metadataRefreshDeadline().epoch);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        shareGroup.setMetadataRefreshDeadline(time.milliseconds() + 1000, shareGroup.groupEpoch());\n        assertFalse(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(shareGroup.groupEpoch(), shareGroup.metadataRefreshDeadline().epoch);\n\n        // Advance past the deadline. The metadata should have expired.\n        time.sleep(1001L);\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n\n        // Set the refresh time deadline with a higher group epoch. The metadata is considered\n        // as expired because the group epoch attached to the deadline is higher than the\n        // current group epoch.\n        shareGroup.setMetadataRefreshDeadline(time.milliseconds() + 1000, shareGroup.groupEpoch() + 1);\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(shareGroup.groupEpoch() + 1, shareGroup.metadataRefreshDeadline().epoch);\n\n        // Advance the group epoch.\n        shareGroup.setGroupEpoch(shareGroup.groupEpoch() + 1);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        shareGroup.setMetadataRefreshDeadline(time.milliseconds() + 1000, shareGroup.groupEpoch());\n        assertFalse(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(shareGroup.groupEpoch(), shareGroup.metadataRefreshDeadline().epoch);\n\n        // Request metadata refresh. The metadata expires immediately.\n        shareGroup.requestMetadataRefresh();\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, shareGroup.metadataRefreshDeadline().epoch);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateOffsetCommit(short version) {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, () ->\n            shareGroup.validateOffsetCommit(null, null, -1, false, version));\n    }\n\n    @Test\n    public void testAsListedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"group-foo\");\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ShareGroupState.EMPTY, shareGroup.state(0));\n        assertEquals(\"Empty\", shareGroup.stateAsString(0));\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertEquals(ShareGroupState.EMPTY, shareGroup.state(0));\n        assertEquals(\"Empty\", shareGroup.stateAsString(0));\n        assertEquals(ShareGroupState.STABLE, shareGroup.state(1));\n        assertEquals(\"Stable\", shareGroup.stateAsString(1));\n    }\n\n    @Test\n    public void testOffsetExpirationCondition() {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, shareGroup::offsetExpirationCondition);\n    }\n\n    @Test\n    public void testValidateOffsetFetch() {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, () ->\n            shareGroup.validateOffsetFetch(null, -1, -1));\n    }\n\n    @Test\n    public void testValidateOffsetDelete() {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, shareGroup::validateOffsetDelete);\n    }\n\n    @Test\n    public void testValidateDeleteGroup() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        assertEquals(ShareGroupState.EMPTY, shareGroup.state());\n        assertDoesNotThrow(shareGroup::validateDeleteGroup);\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n        shareGroup.updateMember(member1);\n\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertThrows(GroupNotEmptyException.class, shareGroup::validateDeleteGroup);\n\n        shareGroup.setGroupEpoch(1);\n\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertThrows(GroupNotEmptyException.class, shareGroup::validateDeleteGroup);\n\n        shareGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertThrows(GroupNotEmptyException.class, shareGroup::validateDeleteGroup);\n    }\n\n    @Test\n    public void testIsSubscribedToTopic() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addRacks()\n            .build();\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ShareGroupMember member2 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        shareGroup.updateMember(member1);\n        shareGroup.updateMember(member2);\n\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        assertTrue(shareGroup.isSubscribedToTopic(\"foo\"));\n        assertTrue(shareGroup.isSubscribedToTopic(\"bar\"));\n\n        shareGroup.removeMember(\"member1\");\n        assertFalse(shareGroup.isSubscribedToTopic(\"foo\"));\n\n        shareGroup.removeMember(\"member2\");\n        assertFalse(shareGroup.isSubscribedToTopic(\"bar\"));\n    }\n\n    @Test\n    public void testAsDescribedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"group-id-1\");\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ShareGroupState.EMPTY.toString(), shareGroup.stateAsString(0));\n\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member1\")\n                .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                .build());\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member2\")\n                .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n\n        ShareGroupDescribeResponseData.DescribedGroup expected = new ShareGroupDescribeResponseData.DescribedGroup()\n            .setGroupId(\"group-id-1\")\n            .setGroupState(ShareGroupState.STABLE.toString())\n            .setGroupEpoch(0)\n            .setAssignmentEpoch(0)\n            .setAssignorName(\"assignorName\")\n            .setMembers(Arrays.asList(\n                new ShareGroupDescribeResponseData.Member()\n                    .setMemberId(\"member1\")\n                    .setSubscribedTopicNames(Collections.singletonList(\"foo\")),\n                new ShareGroupDescribeResponseData.Member().setMemberId(\"member2\")\n            ));\n        ShareGroupDescribeResponseData.DescribedGroup actual = shareGroup.asDescribedGroup(1, \"assignorName\",\n            new MetadataImageBuilder().build().topics());\n\n        assertEquals(expected, actual);\n    }\n\n    @Test\n    public void testIsInStatesCaseInsensitive() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"group-foo\");\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertTrue(shareGroup.isInStates(Collections.singleton(\"empty\"), 0));\n        assertFalse(shareGroup.isInStates(Collections.singleton(\"Empty\"), 0));\n\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertTrue(shareGroup.isInStates(Collections.singleton(\"empty\"), 0));\n        assertTrue(shareGroup.isInStates(Collections.singleton(\"stable\"), 1));\n        assertFalse(shareGroup.isInStates(Collections.singleton(\"empty\"), 1));\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(0);\n    }\n\n    private ShareGroup createShareGroup(String groupId) {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        return new ShareGroup(\n            snapshotRegistry,\n            groupId\n        );\n    }\n}",
                "methodCount": 26
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 833,
                        "lineEnd": 835,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method idempotentCreateSnapshot() directly interacts with the getOrCreateSnapshot() method from the SnapshotRegistry class to create a snapshot if it does not exist already. This tight coupling indicates that the functionality is directly relevant to the SnapshotRegistry, aligning with the responsibilities of managing and creating snapshots within this class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createShareGroup",
                            "method_signature": "private createShareGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createShareGroup",
                            "method_signature": "private createShareGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createShareGroup",
                            "method_signature": "private createShareGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.17298872485207484
                },
                "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)": {
                    "first": {
                        "method_name": "testValidateOffsetCommit",
                        "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4109393189139836
                },
                "private createShareGroup(String groupId)": {
                    "first": {
                        "method_name": "createShareGroup",
                        "method_signature": "private createShareGroup(String groupId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.45189092700050687
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                    "private createShareGroup(String groupId)",
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                ],
                "llm_response_time": 4515
            },
            "targetClassMap": {
                "testValidateOffsetCommit": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "createShareGroup": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testAddEntriesWithSnapshots() : Map<Integer,String> in class org.apache.kafka.jmh.timeline.TimelineHashMapBenchmark & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/timeline/TimelineHashMapBenchmark.java",
                    "startLine": 70,
                    "endLine": 90,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testAddEntriesWithSnapshots() : Map<Integer,String>"
                },
                {
                    "filePath": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/timeline/TimelineHashMapBenchmark.java",
                    "startLine": 81,
                    "endLine": 81,
                    "startColumn": 17,
                    "endColumn": 61,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/timeline/TimelineHashMapBenchmark.java",
                    "startLine": 70,
                    "endLine": 90,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testAddEntriesWithSnapshots() : Map<Integer,String>"
                },
                {
                    "filePath": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/timeline/TimelineHashMapBenchmark.java",
                    "startLine": 81,
                    "endLine": 81,
                    "startColumn": 17,
                    "endColumn": 65,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(epoch)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 619,
        "extraction_results": {
            "success": true,
            "newCommitHash": "ebceb5d1219775a9e83c01195bca606578cff305",
            "newBranchName": "extract-idempotentCreateSnapshot-testAddEntriesWithSnapshots-130af38"
        },
        "telemetry": {
            "id": "7df99f4a-8f5c-4a14-9b4c-e1619e3b0e45",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 58,
                "lineStart": 38,
                "lineEnd": 95,
                "bodyLineStart": 38,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/timeline/TimelineHashMapBenchmark.java",
                "sourceCode": "@State(Scope.Benchmark)\n@Fork(value = 1)\n@Warmup(iterations = 3)\n@Measurement(iterations = 10)\n@BenchmarkMode(Mode.AverageTime)\n@OutputTimeUnit(TimeUnit.MILLISECONDS)\n\npublic class TimelineHashMapBenchmark {\n    private static final int NUM_ENTRIES = 1_000_000;\n\n    @Benchmark\n    public Map<Integer, String> testAddEntriesInHashMap() {\n        HashMap<Integer, String> map = new HashMap<>(NUM_ENTRIES);\n        for (int i = 0; i < NUM_ENTRIES; i++) {\n            int key = (int) (0xffffffff & ((i * 2862933555777941757L) + 3037000493L));\n            map.put(key, String.valueOf(key));\n        }\n        return map;\n    }\n\n    @Benchmark\n    public Map<Integer, String> testAddEntriesInTimelineMap() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        TimelineHashMap<Integer, String> map =\n            new TimelineHashMap<>(snapshotRegistry, NUM_ENTRIES);\n        for (int i = 0; i < NUM_ENTRIES; i++) {\n            int key = (int) (0xffffffff & ((i * 2862933555777941757L) + 3037000493L));\n            map.put(key, String.valueOf(key));\n        }\n        return map;\n    }\n\n    @Benchmark\n    public Map<Integer, String> testAddEntriesWithSnapshots() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        TimelineHashMap<Integer, String> map =\n            new TimelineHashMap<>(snapshotRegistry, NUM_ENTRIES);\n        long epoch = 0;\n        int j = 0;\n        for (int i = 0; i < NUM_ENTRIES; i++) {\n            int key = (int) (0xffffffff & ((i * 2862933555777941757L) + 3037000493L));\n            if (j > 10 && key % 3 == 0) {\n                snapshotRegistry.deleteSnapshotsUpTo(epoch - 1000);\n                idempotentCreateSnapshot(snapshotRegistry, epoch);\n                j = 0;\n            } else {\n                j++;\n            }\n            map.put(key, String.valueOf(key));\n            epoch++;\n        }\n        return map;\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry, long epoch) {\n        snapshotRegistry.getOrCreateSnapshot(epoch);\n    }\n}",
                "methodCount": 4
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 91,
                        "lineEnd": 93,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method idempotentCreateSnapshot relies exclusively on methods and data within the SnapshotRegistry class, specifically the getOrCreateSnapshot method and snapshots data field. By moving this method into SnapshotRegistry, we can encapsulate the snapshot creation logic within the class that's responsible for managing snapshots. This will maintain cohesion and ensure that all operations related to snapshot creation and management are centralized within a single class, making future maintenance and enhancements easier.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "testAddEntriesInHashMap",
                            "method_signature": "@Benchmark\n    public testAddEntriesInHashMap()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testAddEntriesInTimelineMap",
                            "method_signature": "@Benchmark\n    public testAddEntriesInTimelineMap()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testAddEntriesWithSnapshots",
                            "method_signature": "@Benchmark\n    public testAddEntriesWithSnapshots()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry, long epoch)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry, long epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testAddEntriesInHashMap",
                            "method_signature": "@Benchmark\n    public testAddEntriesInHashMap()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testAddEntriesWithSnapshots",
                            "method_signature": "@Benchmark\n    public testAddEntriesWithSnapshots()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testAddEntriesInTimelineMap",
                            "method_signature": "@Benchmark\n    public testAddEntriesInTimelineMap()",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry, long epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testAddEntriesInHashMap",
                            "method_signature": "@Benchmark\n    public testAddEntriesInHashMap()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testAddEntriesWithSnapshots",
                            "method_signature": "@Benchmark\n    public testAddEntriesWithSnapshots()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testAddEntriesInTimelineMap",
                            "method_signature": "@Benchmark\n    public testAddEntriesInTimelineMap()",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry, long epoch)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry, long epoch)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.21790681682020446
                },
                "@Benchmark\n    public testAddEntriesInHashMap()": {
                    "first": {
                        "method_name": "testAddEntriesInHashMap",
                        "method_signature": "@Benchmark\n    public testAddEntriesInHashMap()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.8331550504319885
                },
                "@Benchmark\n    public testAddEntriesWithSnapshots()": {
                    "first": {
                        "method_name": "testAddEntriesWithSnapshots",
                        "method_signature": "@Benchmark\n    public testAddEntriesWithSnapshots()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.8574573718791328
                },
                "@Benchmark\n    public testAddEntriesInTimelineMap()": {
                    "first": {
                        "method_name": "testAddEntriesInTimelineMap",
                        "method_signature": "@Benchmark\n    public testAddEntriesInTimelineMap()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.8733909728084096
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "@Benchmark\n    public testAddEntriesWithSnapshots()",
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry, long epoch)",
                    "@Benchmark\n    public testAddEntriesInHashMap()",
                    "@Benchmark\n    public testAddEntriesInTimelineMap()"
                ],
                "llm_response_time": 2084
            },
            "targetClassMap": {
                "testAddEntriesWithSnapshots": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 4380,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.27411570338737873
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 2095,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "testAddEntriesInHashMap": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3159,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "testAddEntriesInTimelineMap": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3161,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from private OffsetControlManager(logContext LogContext, snapshotRegistry SnapshotRegistry, metrics QuorumControllerMetrics, time Time) in class org.apache.kafka.controller.OffsetControlManager & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 142,
                    "endLine": 164,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private OffsetControlManager(logContext LogContext, snapshotRegistry SnapshotRegistry, metrics QuorumControllerMetrics, time Time)"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 159,
                    "endLine": 159,
                    "startColumn": 9,
                    "endColumn": 51,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 142,
                    "endLine": 164,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private OffsetControlManager(logContext LogContext, snapshotRegistry SnapshotRegistry, metrics QuorumControllerMetrics, time Time)"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 159,
                    "endLine": 159,
                    "startColumn": 9,
                    "endColumn": 55,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(-1L)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 620,
        "extraction_results": {
            "success": false
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from package activate(newNextWriteOffset long) : void in class org.apache.kafka.controller.OffsetControlManager & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 236,
                    "endLine": 255,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package activate(newNextWriteOffset long) : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 252,
                    "endLine": 252,
                    "startColumn": 9,
                    "endColumn": 64,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 236,
                    "endLine": 255,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package activate(newNextWriteOffset long) : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 252,
                    "endLine": 252,
                    "startColumn": 9,
                    "endColumn": 68,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(lastStableOffset)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 621,
        "extraction_results": {
            "success": true,
            "newCommitHash": "9eea6895d62aec7465dfbc4811e8b0bc61445662",
            "newBranchName": "extract-idempotentCreateSnapshot-activate-130af38"
        },
        "telemetry": {
            "id": "de72f93b-6759-4378-850f-e87f77601642",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 390,
                "lineStart": 37,
                "lineEnd": 426,
                "bodyLineStart": 37,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                "sourceCode": "/**\n * Manages read and write offsets, and in-memory snapshots.\n *\n * Also manages the following metrics:\n *      kafka.controller:type=KafkaController,name=ActiveControllerCount\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordLagMs\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordOffset\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordTimestamp\n *      kafka.controller:type=KafkaController,name=LastCommittedRecordOffset\n */\nclass OffsetControlManager {\n    public static class Builder {\n        private LogContext logContext = null;\n        private SnapshotRegistry snapshotRegistry = null;\n        private QuorumControllerMetrics metrics = null;\n        private Time time = Time.SYSTEM;\n\n        Builder setLogContext(LogContext logContext) {\n            this.logContext = logContext;\n            return this;\n        }\n\n        Builder setSnapshotRegistry(SnapshotRegistry snapshotRegistry) {\n            this.snapshotRegistry = snapshotRegistry;\n            return this;\n        }\n\n        Builder setMetrics(QuorumControllerMetrics metrics) {\n            this.metrics = metrics;\n            return this;\n        }\n\n        Builder setTime(Time time) {\n            this.time = time;\n            return this;\n        }\n\n        public OffsetControlManager build() {\n            if (logContext == null) logContext = new LogContext();\n            if (snapshotRegistry == null) snapshotRegistry = new SnapshotRegistry(logContext);\n            if (metrics == null) {\n                metrics = new QuorumControllerMetrics(Optional.empty(), time, false);\n            }\n            return new OffsetControlManager(logContext,\n                    snapshotRegistry,\n                    metrics,\n                    time);\n        }\n    }\n\n    /**\n     * The slf4j logger.\n     */\n    private final Logger log;\n\n    /**\n     * The snapshot registry.\n     */\n    private final SnapshotRegistry snapshotRegistry;\n\n    /**\n     * The quorum controller metrics.\n     */\n    private final QuorumControllerMetrics metrics;\n\n    /**\n     * The clock.\n     */\n    private final Time time;\n\n    /**\n     * The ID of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private OffsetAndEpoch currentSnapshotId;\n\n    /**\n     * The name of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private String currentSnapshotName;\n\n    /**\n     * The latest committed offset.\n     */\n    private long lastCommittedOffset;\n\n    /**\n     * The latest committed epoch.\n     */\n    private int lastCommittedEpoch;\n\n    /**\n     * The latest offset that it is safe to read from.\n     */\n    private long lastStableOffset;\n\n    /**\n     * The offset of the transaction we're in, or -1 if we are not in one.\n     */\n    private long transactionStartOffset;\n\n    /**\n     * The next offset we should write to, or -1 if the controller is not active. Exclusive offset.\n     */\n    private long nextWriteOffset;\n\n    private OffsetControlManager(\n        LogContext logContext,\n        SnapshotRegistry snapshotRegistry,\n        QuorumControllerMetrics metrics,\n        Time time\n    ) {\n        this.log = logContext.logger(OffsetControlManager.class);\n        this.snapshotRegistry = snapshotRegistry;\n        this.metrics = metrics;\n        this.time = time;\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        snapshotRegistry.getOrCreateSnapshot(-1L);\n        metrics.setActive(false);\n        metrics.setLastCommittedRecordOffset(-1L);\n        metrics.setLastAppliedRecordOffset(-1L);\n        metrics.setLastAppliedRecordTimestamp(-1L);\n    }\n\n    /**\n     *  @return The SnapshotRegistry used by this offset control manager.\n     */\n    SnapshotRegistry snapshotRegistry() {\n        return snapshotRegistry;\n    }\n\n    /**\n     * @return QuorumControllerMetrics managed by this offset control manager.\n     */\n    QuorumControllerMetrics metrics() {\n        return metrics;\n    }\n\n    /**\n     * @return the ID of the current snapshot.\n     */\n    OffsetAndEpoch currentSnapshotId() {\n        return currentSnapshotId;\n    }\n\n    /**\n     * @return the name of the current snapshot.\n     */\n    String currentSnapshotName() {\n        return currentSnapshotName;\n    }\n\n    /**\n     * @return the last committed offset.\n     */\n    long lastCommittedOffset() {\n        return lastCommittedOffset;\n    }\n\n    /**\n     * @return the last committed epoch.\n     */\n    int lastCommittedEpoch() {\n        return lastCommittedEpoch;\n    }\n\n    /**\n     * @return the latest offset that it is safe to read from.\n     */\n    long lastStableOffset() {\n        return lastStableOffset;\n    }\n\n    /**\n     * @return the transaction start offset, or -1 if there is no transaction.\n     */\n    long transactionStartOffset() {\n        return transactionStartOffset;\n    }\n\n    /**\n     * @return the next offset that the active controller should write to.\n     */\n    long nextWriteOffset() {\n        return nextWriteOffset;\n    }\n\n    /**\n     * @return true only if the manager is active.\n     */\n    boolean active() {\n        return nextWriteOffset != -1L;\n    }\n\n    /**\n     * Called when the QuorumController becomes active.\n     *\n     * @param newNextWriteOffset The new next write offset to use. Must be non-negative.\n     */\n    void activate(long newNextWriteOffset) {\n        if (active()) {\n            throw new RuntimeException(\"Can't activate already active OffsetControlManager.\");\n        }\n        if (newNextWriteOffset < 0) {\n            throw new RuntimeException(\"Invalid negative newNextWriteOffset \" +\n                    newNextWriteOffset + \".\");\n        }\n        // Before switching to active, create an in-memory snapshot at the last committed\n        // offset. This is required because the active controller assumes that there is always\n        // an in-memory snapshot at the last committed offset.\n        idempotentCreateSnapshot();\n        this.nextWriteOffset = newNextWriteOffset;\n        metrics.setActive(true);\n    }\n\n    private void idempotentCreateSnapshot() {\n        snapshotRegistry.getOrCreateSnapshot(lastStableOffset);\n    }\n\n    /**\n     * Called when the QuorumController becomes inactive.\n     */\n    void deactivate() {\n        if (!active()) {\n            throw new RuntimeException(\"Can't deactivate inactive OffsetControlManager.\");\n        }\n        metrics.setActive(false);\n        metrics.setLastAppliedRecordOffset(lastStableOffset);\n        this.nextWriteOffset = -1L;\n        if (!snapshotRegistry.hasSnapshot(lastStableOffset)) {\n            throw new RuntimeException(\"Unable to reset to last stable offset \" + lastStableOffset +\n                    \". No in-memory snapshot found for this offset.\");\n        }\n        snapshotRegistry.revertToSnapshot(lastStableOffset);\n    }\n\n    /**\n     * Handle the callback from the Raft layer indicating that a batch was committed.\n     *\n     * @param batch The batch that has been committed.\n     */\n    void handleCommitBatch(Batch<ApiMessageAndVersion> batch) {\n        this.lastCommittedOffset = batch.lastOffset();\n        this.lastCommittedEpoch = batch.epoch();\n        maybeAdvanceLastStableOffset();\n        metrics.setLastCommittedRecordOffset(batch.lastOffset());\n        if (!active()) {\n            // On standby controllers, the last applied record offset is equals to the last\n            // committed offset.\n            metrics.setLastAppliedRecordOffset(batch.lastOffset());\n            metrics.setLastAppliedRecordTimestamp(batch.appendTimestamp());\n        }\n    }\n\n    /**\n     * Called by the active controller after it has invoked scheduleAtomicAppend to schedule some\n     * records to be written.\n     *\n     * @param endOffset The offset of the last record that was written.\n     */\n    void handleScheduleAtomicAppend(long endOffset) {\n        this.nextWriteOffset = endOffset + 1;\n\n        snapshotRegistry.getOrCreateSnapshot(endOffset);\n\n        metrics.setLastAppliedRecordOffset(endOffset);\n\n        // This is not truly the append timestamp. The KRaft client doesn't expose the append\n        // time when scheduling a write. This is good enough because this is called right after\n        // the records were given to the KRAft client for appending and the default append linger\n        // for KRaft is 25ms.\n        metrics.setLastAppliedRecordTimestamp(time.milliseconds());\n    }\n\n    /**\n     * Advance the last stable offset if needed.\n     */\n    void maybeAdvanceLastStableOffset() {\n        long newLastStableOffset;\n        if (transactionStartOffset == -1L) {\n            newLastStableOffset = lastCommittedOffset;\n        } else {\n            newLastStableOffset = Math.min(transactionStartOffset - 1, lastCommittedOffset);\n        }\n        if (lastStableOffset < newLastStableOffset) {\n            lastStableOffset = newLastStableOffset;\n            snapshotRegistry.deleteSnapshotsUpTo(lastStableOffset);\n            if (!active()) {\n                idempotentCreateSnapshot();\n            }\n        }\n    }\n\n    /**\n     * Called before we load a Raft snapshot.\n     *\n     * @param snapshotId The Raft snapshot offset and epoch.\n     */\n    void beginLoadSnapshot(OffsetAndEpoch snapshotId) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"Can't begin reading snapshot for \" + snapshotId +\n                    \", because we are already reading \" + currentSnapshotId);\n        }\n        this.currentSnapshotId = snapshotId;\n        this.currentSnapshotName = Snapshots.filenameFromSnapshotId(snapshotId);\n        log.info(\"Starting to load snapshot {}. Previous lastCommittedOffset was {}. Previous \" +\n                \"transactionStartOffset was {}.\", currentSnapshotName, lastCommittedOffset,\n                transactionStartOffset);\n        this.snapshotRegistry.reset();\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n    }\n\n    /**\n     * Called after we have finished loading a Raft snapshot.\n     *\n     * @param timestamp The timestamp of the snapshot.\n     */\n    void endLoadSnapshot(long timestamp) {\n        if (currentSnapshotId == null) {\n            throw new RuntimeException(\"Can't end loading snapshot, because there is no \" +\n                    \"current snapshot.\");\n        }\n        log.info(\"Successfully loaded snapshot {}.\", currentSnapshotName);\n        this.snapshotRegistry.getOrCreateSnapshot(currentSnapshotId.offset());\n        this.lastCommittedOffset = currentSnapshotId.offset();\n        this.lastCommittedEpoch = currentSnapshotId.epoch();\n        this.lastStableOffset = currentSnapshotId.offset();\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        metrics.setLastCommittedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordTimestamp(timestamp);\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n    }\n\n    public void replay(BeginTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"BeginTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset != -1L) {\n            throw new RuntimeException(\"Can't replay a BeginTransactionRecord at \" + offset +\n                \" because the transaction at \" + transactionStartOffset + \" was never closed.\");\n        }\n        snapshotRegistry.getOrCreateSnapshot(offset - 1);\n        transactionStartOffset = offset;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    public void replay(EndTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"EndTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an EndTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    public void replay(AbortTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"AbortTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an AbortTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        long preTransactionOffset = transactionStartOffset - 1;\n        snapshotRegistry.revertToSnapshot(preTransactionOffset);\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}. Reverted to offset {}.\",\n                message, offset, preTransactionOffset);\n    }\n\n    // VisibleForTesting\n    void setNextWriteOffset(long newNextWriteOffset) {\n        this.nextWriteOffset = newNextWriteOffset;\n    }\n}",
                "methodCount": 28
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "active",
                            "method_signature": " active()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(EndTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(EndTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "active",
                            "method_signature": " active()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(EndTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "active",
                            "method_signature": " active()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot()": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3522953764676562
                },
                "public build()": {
                    "first": {
                        "method_name": "build",
                        "method_signature": "public build()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4107921022481485
                },
                " handleScheduleAtomicAppend(long endOffset)": {
                    "first": {
                        "method_name": "handleScheduleAtomicAppend",
                        "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4898501156284456
                },
                "public replay(BeginTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.49176728521446916
                },
                "public replay(EndTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(EndTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5410090425923333
                },
                "public replay(AbortTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.548174904233164
                },
                " active()": {
                    "first": {
                        "method_name": "active",
                        "method_signature": " active()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5493091259559221
                },
                " beginLoadSnapshot(OffsetAndEpoch snapshotId)": {
                    "first": {
                        "method_name": "beginLoadSnapshot",
                        "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5739944991388558
                },
                " maybeAdvanceLastStableOffset()": {
                    "first": {
                        "method_name": "maybeAdvanceLastStableOffset",
                        "method_signature": " maybeAdvanceLastStableOffset()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6233917680383996
                },
                " handleCommitBatch(Batch<ApiMessageAndVersion> batch)": {
                    "first": {
                        "method_name": "handleCommitBatch",
                        "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6236765689755942
                },
                " deactivate()": {
                    "first": {
                        "method_name": "deactivate",
                        "method_signature": " deactivate()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6248110645866022
                },
                " endLoadSnapshot(long timestamp)": {
                    "first": {
                        "method_name": "endLoadSnapshot",
                        "method_signature": " endLoadSnapshot(long timestamp)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6448925334742986
                },
                " activate(long newNextWriteOffset)": {
                    "first": {
                        "method_name": "activate",
                        "method_signature": " activate(long newNextWriteOffset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6504498646273721
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [],
                "llm_response_time": 2709
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from package handleScheduleAtomicAppend(endOffset long) : void in class org.apache.kafka.controller.OffsetControlManager & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 292,
                    "endLine": 310,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package handleScheduleAtomicAppend(endOffset long) : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 301,
                    "endLine": 301,
                    "startColumn": 9,
                    "endColumn": 57,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 292,
                    "endLine": 310,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package handleScheduleAtomicAppend(endOffset long) : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 301,
                    "endLine": 301,
                    "startColumn": 9,
                    "endColumn": 61,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(endOffset)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 622,
        "extraction_results": {
            "success": true,
            "newCommitHash": "2e8ab6ec5591a47e6a5217eac0b790aacf3695fb",
            "newBranchName": "extract-idempotentCreateSnapshot-handleScheduleAtomicAppend-130af38"
        },
        "telemetry": {
            "id": "61c76397-b69b-43e3-b998-11b40de46afc",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 390,
                "lineStart": 37,
                "lineEnd": 426,
                "bodyLineStart": 37,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                "sourceCode": "/**\n * Manages read and write offsets, and in-memory snapshots.\n *\n * Also manages the following metrics:\n *      kafka.controller:type=KafkaController,name=ActiveControllerCount\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordLagMs\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordOffset\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordTimestamp\n *      kafka.controller:type=KafkaController,name=LastCommittedRecordOffset\n */\nclass OffsetControlManager {\n    public static class Builder {\n        private LogContext logContext = null;\n        private SnapshotRegistry snapshotRegistry = null;\n        private QuorumControllerMetrics metrics = null;\n        private Time time = Time.SYSTEM;\n\n        Builder setLogContext(LogContext logContext) {\n            this.logContext = logContext;\n            return this;\n        }\n\n        Builder setSnapshotRegistry(SnapshotRegistry snapshotRegistry) {\n            this.snapshotRegistry = snapshotRegistry;\n            return this;\n        }\n\n        Builder setMetrics(QuorumControllerMetrics metrics) {\n            this.metrics = metrics;\n            return this;\n        }\n\n        Builder setTime(Time time) {\n            this.time = time;\n            return this;\n        }\n\n        public OffsetControlManager build() {\n            if (logContext == null) logContext = new LogContext();\n            if (snapshotRegistry == null) snapshotRegistry = new SnapshotRegistry(logContext);\n            if (metrics == null) {\n                metrics = new QuorumControllerMetrics(Optional.empty(), time, false);\n            }\n            return new OffsetControlManager(logContext,\n                    snapshotRegistry,\n                    metrics,\n                    time);\n        }\n    }\n\n    /**\n     * The slf4j logger.\n     */\n    private final Logger log;\n\n    /**\n     * The snapshot registry.\n     */\n    private final SnapshotRegistry snapshotRegistry;\n\n    /**\n     * The quorum controller metrics.\n     */\n    private final QuorumControllerMetrics metrics;\n\n    /**\n     * The clock.\n     */\n    private final Time time;\n\n    /**\n     * The ID of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private OffsetAndEpoch currentSnapshotId;\n\n    /**\n     * The name of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private String currentSnapshotName;\n\n    /**\n     * The latest committed offset.\n     */\n    private long lastCommittedOffset;\n\n    /**\n     * The latest committed epoch.\n     */\n    private int lastCommittedEpoch;\n\n    /**\n     * The latest offset that it is safe to read from.\n     */\n    private long lastStableOffset;\n\n    /**\n     * The offset of the transaction we're in, or -1 if we are not in one.\n     */\n    private long transactionStartOffset;\n\n    /**\n     * The next offset we should write to, or -1 if the controller is not active. Exclusive offset.\n     */\n    private long nextWriteOffset;\n\n    private OffsetControlManager(\n        LogContext logContext,\n        SnapshotRegistry snapshotRegistry,\n        QuorumControllerMetrics metrics,\n        Time time\n    ) {\n        this.log = logContext.logger(OffsetControlManager.class);\n        this.snapshotRegistry = snapshotRegistry;\n        this.metrics = metrics;\n        this.time = time;\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        snapshotRegistry.getOrCreateSnapshot(-1L);\n        metrics.setActive(false);\n        metrics.setLastCommittedRecordOffset(-1L);\n        metrics.setLastAppliedRecordOffset(-1L);\n        metrics.setLastAppliedRecordTimestamp(-1L);\n    }\n\n    /**\n     *  @return The SnapshotRegistry used by this offset control manager.\n     */\n    SnapshotRegistry snapshotRegistry() {\n        return snapshotRegistry;\n    }\n\n    /**\n     * @return QuorumControllerMetrics managed by this offset control manager.\n     */\n    QuorumControllerMetrics metrics() {\n        return metrics;\n    }\n\n    /**\n     * @return the ID of the current snapshot.\n     */\n    OffsetAndEpoch currentSnapshotId() {\n        return currentSnapshotId;\n    }\n\n    /**\n     * @return the name of the current snapshot.\n     */\n    String currentSnapshotName() {\n        return currentSnapshotName;\n    }\n\n    /**\n     * @return the last committed offset.\n     */\n    long lastCommittedOffset() {\n        return lastCommittedOffset;\n    }\n\n    /**\n     * @return the last committed epoch.\n     */\n    int lastCommittedEpoch() {\n        return lastCommittedEpoch;\n    }\n\n    /**\n     * @return the latest offset that it is safe to read from.\n     */\n    long lastStableOffset() {\n        return lastStableOffset;\n    }\n\n    /**\n     * @return the transaction start offset, or -1 if there is no transaction.\n     */\n    long transactionStartOffset() {\n        return transactionStartOffset;\n    }\n\n    /**\n     * @return the next offset that the active controller should write to.\n     */\n    long nextWriteOffset() {\n        return nextWriteOffset;\n    }\n\n    /**\n     * @return true only if the manager is active.\n     */\n    boolean active() {\n        return nextWriteOffset != -1L;\n    }\n\n    /**\n     * Called when the QuorumController becomes active.\n     *\n     * @param newNextWriteOffset The new next write offset to use. Must be non-negative.\n     */\n    void activate(long newNextWriteOffset) {\n        if (active()) {\n            throw new RuntimeException(\"Can't activate already active OffsetControlManager.\");\n        }\n        if (newNextWriteOffset < 0) {\n            throw new RuntimeException(\"Invalid negative newNextWriteOffset \" +\n                    newNextWriteOffset + \".\");\n        }\n        // Before switching to active, create an in-memory snapshot at the last committed\n        // offset. This is required because the active controller assumes that there is always\n        // an in-memory snapshot at the last committed offset.\n        idempotentCreateSnapshot(lastStableOffset);\n        this.nextWriteOffset = newNextWriteOffset;\n        metrics.setActive(true);\n    }\n\n    /**\n     * Called when the QuorumController becomes inactive.\n     */\n    void deactivate() {\n        if (!active()) {\n            throw new RuntimeException(\"Can't deactivate inactive OffsetControlManager.\");\n        }\n        metrics.setActive(false);\n        metrics.setLastAppliedRecordOffset(lastStableOffset);\n        this.nextWriteOffset = -1L;\n        if (!snapshotRegistry.hasSnapshot(lastStableOffset)) {\n            throw new RuntimeException(\"Unable to reset to last stable offset \" + lastStableOffset +\n                    \". No in-memory snapshot found for this offset.\");\n        }\n        snapshotRegistry.revertToSnapshot(lastStableOffset);\n    }\n\n    /**\n     * Handle the callback from the Raft layer indicating that a batch was committed.\n     *\n     * @param batch The batch that has been committed.\n     */\n    void handleCommitBatch(Batch<ApiMessageAndVersion> batch) {\n        this.lastCommittedOffset = batch.lastOffset();\n        this.lastCommittedEpoch = batch.epoch();\n        maybeAdvanceLastStableOffset();\n        metrics.setLastCommittedRecordOffset(batch.lastOffset());\n        if (!active()) {\n            // On standby controllers, the last applied record offset is equals to the last\n            // committed offset.\n            metrics.setLastAppliedRecordOffset(batch.lastOffset());\n            metrics.setLastAppliedRecordTimestamp(batch.appendTimestamp());\n        }\n    }\n\n    /**\n     * Called by the active controller after it has invoked scheduleAtomicAppend to schedule some\n     * records to be written.\n     *\n     * @param endOffset The offset of the last record that was written.\n     */\n    void handleScheduleAtomicAppend(long endOffset) {\n        this.nextWriteOffset = endOffset + 1;\n\n        idempotentCreateSnapshot(endOffset);\n\n        metrics.setLastAppliedRecordOffset(endOffset);\n\n        // This is not truly the append timestamp. The KRaft client doesn't expose the append\n        // time when scheduling a write. This is good enough because this is called right after\n        // the records were given to the KRAft client for appending and the default append linger\n        // for KRaft is 25ms.\n        metrics.setLastAppliedRecordTimestamp(time.milliseconds());\n    }\n\n    private void idempotentCreateSnapshot(long endOffset) {\n        snapshotRegistry.getOrCreateSnapshot(endOffset);\n    }\n\n    /**\n     * Advance the last stable offset if needed.\n     */\n    void maybeAdvanceLastStableOffset() {\n        long newLastStableOffset;\n        if (transactionStartOffset == -1L) {\n            newLastStableOffset = lastCommittedOffset;\n        } else {\n            newLastStableOffset = Math.min(transactionStartOffset - 1, lastCommittedOffset);\n        }\n        if (lastStableOffset < newLastStableOffset) {\n            lastStableOffset = newLastStableOffset;\n            snapshotRegistry.deleteSnapshotsUpTo(lastStableOffset);\n            if (!active()) {\n                idempotentCreateSnapshot(lastStableOffset);\n            }\n        }\n    }\n\n    /**\n     * Called before we load a Raft snapshot.\n     *\n     * @param snapshotId The Raft snapshot offset and epoch.\n     */\n    void beginLoadSnapshot(OffsetAndEpoch snapshotId) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"Can't begin reading snapshot for \" + snapshotId +\n                    \", because we are already reading \" + currentSnapshotId);\n        }\n        this.currentSnapshotId = snapshotId;\n        this.currentSnapshotName = Snapshots.filenameFromSnapshotId(snapshotId);\n        log.info(\"Starting to load snapshot {}. Previous lastCommittedOffset was {}. Previous \" +\n                \"transactionStartOffset was {}.\", currentSnapshotName, lastCommittedOffset,\n                transactionStartOffset);\n        this.snapshotRegistry.reset();\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n    }\n\n    /**\n     * Called after we have finished loading a Raft snapshot.\n     *\n     * @param timestamp The timestamp of the snapshot.\n     */\n    void endLoadSnapshot(long timestamp) {\n        if (currentSnapshotId == null) {\n            throw new RuntimeException(\"Can't end loading snapshot, because there is no \" +\n                    \"current snapshot.\");\n        }\n        log.info(\"Successfully loaded snapshot {}.\", currentSnapshotName);\n        this.snapshotRegistry.getOrCreateSnapshot(currentSnapshotId.offset());\n        this.lastCommittedOffset = currentSnapshotId.offset();\n        this.lastCommittedEpoch = currentSnapshotId.epoch();\n        this.lastStableOffset = currentSnapshotId.offset();\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        metrics.setLastCommittedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordTimestamp(timestamp);\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n    }\n\n    public void replay(BeginTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"BeginTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset != -1L) {\n            throw new RuntimeException(\"Can't replay a BeginTransactionRecord at \" + offset +\n                \" because the transaction at \" + transactionStartOffset + \" was never closed.\");\n        }\n        idempotentCreateSnapshot(offset - 1);\n        transactionStartOffset = offset;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    public void replay(EndTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"EndTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an EndTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    public void replay(AbortTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"AbortTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an AbortTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        long preTransactionOffset = transactionStartOffset - 1;\n        snapshotRegistry.revertToSnapshot(preTransactionOffset);\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}. Reverted to offset {}.\",\n                message, offset, preTransactionOffset);\n    }\n\n    // VisibleForTesting\n    void setNextWriteOffset(long newNextWriteOffset) {\n        this.nextWriteOffset = newNextWriteOffset;\n    }\n}",
                "methodCount": 28
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 5,
                "candidates": [
                    {
                        "lineStart": 73,
                        "lineEnd": 83,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method build to class Time",
                        "description": "Move method build to org.apache.kafka.common.utils.Time\nRationale: The provided method 'build' is not logically related to the given 'Time' interface, as its purpose is to construct an instance of 'OffsetControlManager' involving various components such as 'logContext', 'snapshotRegistry', and 'metrics'. The 'Time' interface is solely responsible for time-related operations and abstracting clock functionalities. This method should not move to the 'Time' class, as it would violate the principle of single responsibility and lead to poor cohesion.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 381,
                        "lineEnd": 392,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method replay to class Time",
                        "description": "Move method replay to org.apache.kafka.common.utils.Time\nRationale: null",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 394,
                        "lineEnd": 404,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method replay to class Time",
                        "description": "Move method replay to org.apache.kafka.common.utils.Time\nRationale: null",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 406,
                        "lineEnd": 419,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method replay to class Time",
                        "description": "Move method replay to org.apache.kafka.common.utils.Time\nRationale: null",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 311,
                        "lineEnd": 313,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method idempotentCreateSnapshot() directly interacts with the SnapshotRegistry class by calling its getOrCreateSnapshot() method. This indicates that it is closely related to the responsibilities and functionalities encapsulated by the SnapshotRegistry. Moving this method to SnapshotRegistry will keep related functionalities together, leading to better cohesion and easier maintenance.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "active",
                            "method_signature": " active()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(EndTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(EndTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "active",
                            "method_signature": " active()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(EndTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "active",
                            "method_signature": " active()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(long endOffset)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(long endOffset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3288576954716088
                },
                "public build()": {
                    "first": {
                        "method_name": "build",
                        "method_signature": "public build()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4107593789356938
                },
                " handleScheduleAtomicAppend(long endOffset)": {
                    "first": {
                        "method_name": "handleScheduleAtomicAppend",
                        "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.49031739719817796
                },
                "public replay(BeginTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4917270033851441
                },
                "public replay(EndTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(EndTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5409651277383632
                },
                "public replay(AbortTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5481299208581495
                },
                " active()": {
                    "first": {
                        "method_name": "active",
                        "method_signature": " active()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5492662742304564
                },
                " beginLoadSnapshot(OffsetAndEpoch snapshotId)": {
                    "first": {
                        "method_name": "beginLoadSnapshot",
                        "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.573945552037463
                },
                " maybeAdvanceLastStableOffset()": {
                    "first": {
                        "method_name": "maybeAdvanceLastStableOffset",
                        "method_signature": " maybeAdvanceLastStableOffset()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6233399231341831
                },
                " handleCommitBatch(Batch<ApiMessageAndVersion> batch)": {
                    "first": {
                        "method_name": "handleCommitBatch",
                        "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6236247721810713
                },
                " deactivate()": {
                    "first": {
                        "method_name": "deactivate",
                        "method_signature": " deactivate()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6247600158455892
                },
                " endLoadSnapshot(long timestamp)": {
                    "first": {
                        "method_name": "endLoadSnapshot",
                        "method_signature": " endLoadSnapshot(long timestamp)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.644837953258935
                },
                " activate(long newNextWriteOffset)": {
                    "first": {
                        "method_name": "activate",
                        "method_signature": " activate(long newNextWriteOffset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6503939239809882
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "public build()",
                    "public replay(BeginTransactionRecord message, long offset)",
                    "public replay(EndTransactionRecord message, long offset)",
                    "public replay(AbortTransactionRecord message, long offset)",
                    "private idempotentCreateSnapshot(long endOffset)"
                ],
                "llm_response_time": 6875
            },
            "targetClassMap": {
                "build": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.14779923645463774
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 2688,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "replay": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.2081809070085429
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 1993,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.1306631486813383
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry",
                        "Time"
                    ],
                    "llm_response_time": 2859,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from package maybeAdvanceLastStableOffset() : void in class org.apache.kafka.controller.OffsetControlManager & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 312,
                    "endLine": 329,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package maybeAdvanceLastStableOffset() : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 326,
                    "endLine": 326,
                    "startColumn": 17,
                    "endColumn": 72,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 312,
                    "endLine": 329,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package maybeAdvanceLastStableOffset() : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 326,
                    "endLine": 326,
                    "startColumn": 17,
                    "endColumn": 76,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(lastStableOffset)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 623,
        "extraction_results": {
            "success": true,
            "newCommitHash": "f02c9361287ce0b3d5874fb1da2a212cf97ae04f",
            "newBranchName": "extract-idempotentCreateSnapshot-maybeAdvanceLastStableOffset-130af38"
        },
        "telemetry": {
            "id": "60d7343f-cfd8-4c1b-a09b-8ef04543ab9c",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 390,
                "lineStart": 37,
                "lineEnd": 426,
                "bodyLineStart": 37,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                "sourceCode": "/**\n * Manages read and write offsets, and in-memory snapshots.\n *\n * Also manages the following metrics:\n *      kafka.controller:type=KafkaController,name=ActiveControllerCount\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordLagMs\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordOffset\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordTimestamp\n *      kafka.controller:type=KafkaController,name=LastCommittedRecordOffset\n */\nclass OffsetControlManager {\n    public static class Builder {\n        private LogContext logContext = null;\n        private SnapshotRegistry snapshotRegistry = null;\n        private QuorumControllerMetrics metrics = null;\n        private Time time = Time.SYSTEM;\n\n        Builder setLogContext(LogContext logContext) {\n            this.logContext = logContext;\n            return this;\n        }\n\n        Builder setSnapshotRegistry(SnapshotRegistry snapshotRegistry) {\n            this.snapshotRegistry = snapshotRegistry;\n            return this;\n        }\n\n        Builder setMetrics(QuorumControllerMetrics metrics) {\n            this.metrics = metrics;\n            return this;\n        }\n\n        Builder setTime(Time time) {\n            this.time = time;\n            return this;\n        }\n\n        public OffsetControlManager build() {\n            if (logContext == null) logContext = new LogContext();\n            if (snapshotRegistry == null) snapshotRegistry = new SnapshotRegistry(logContext);\n            if (metrics == null) {\n                metrics = new QuorumControllerMetrics(Optional.empty(), time, false);\n            }\n            return new OffsetControlManager(logContext,\n                    snapshotRegistry,\n                    metrics,\n                    time);\n        }\n    }\n\n    /**\n     * The slf4j logger.\n     */\n    private final Logger log;\n\n    /**\n     * The snapshot registry.\n     */\n    private final SnapshotRegistry snapshotRegistry;\n\n    /**\n     * The quorum controller metrics.\n     */\n    private final QuorumControllerMetrics metrics;\n\n    /**\n     * The clock.\n     */\n    private final Time time;\n\n    /**\n     * The ID of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private OffsetAndEpoch currentSnapshotId;\n\n    /**\n     * The name of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private String currentSnapshotName;\n\n    /**\n     * The latest committed offset.\n     */\n    private long lastCommittedOffset;\n\n    /**\n     * The latest committed epoch.\n     */\n    private int lastCommittedEpoch;\n\n    /**\n     * The latest offset that it is safe to read from.\n     */\n    private long lastStableOffset;\n\n    /**\n     * The offset of the transaction we're in, or -1 if we are not in one.\n     */\n    private long transactionStartOffset;\n\n    /**\n     * The next offset we should write to, or -1 if the controller is not active. Exclusive offset.\n     */\n    private long nextWriteOffset;\n\n    private OffsetControlManager(\n        LogContext logContext,\n        SnapshotRegistry snapshotRegistry,\n        QuorumControllerMetrics metrics,\n        Time time\n    ) {\n        this.log = logContext.logger(OffsetControlManager.class);\n        this.snapshotRegistry = snapshotRegistry;\n        this.metrics = metrics;\n        this.time = time;\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        snapshotRegistry.getOrCreateSnapshot(-1L);\n        metrics.setActive(false);\n        metrics.setLastCommittedRecordOffset(-1L);\n        metrics.setLastAppliedRecordOffset(-1L);\n        metrics.setLastAppliedRecordTimestamp(-1L);\n    }\n\n    /**\n     *  @return The SnapshotRegistry used by this offset control manager.\n     */\n    SnapshotRegistry snapshotRegistry() {\n        return snapshotRegistry;\n    }\n\n    /**\n     * @return QuorumControllerMetrics managed by this offset control manager.\n     */\n    QuorumControllerMetrics metrics() {\n        return metrics;\n    }\n\n    /**\n     * @return the ID of the current snapshot.\n     */\n    OffsetAndEpoch currentSnapshotId() {\n        return currentSnapshotId;\n    }\n\n    /**\n     * @return the name of the current snapshot.\n     */\n    String currentSnapshotName() {\n        return currentSnapshotName;\n    }\n\n    /**\n     * @return the last committed offset.\n     */\n    long lastCommittedOffset() {\n        return lastCommittedOffset;\n    }\n\n    /**\n     * @return the last committed epoch.\n     */\n    int lastCommittedEpoch() {\n        return lastCommittedEpoch;\n    }\n\n    /**\n     * @return the latest offset that it is safe to read from.\n     */\n    long lastStableOffset() {\n        return lastStableOffset;\n    }\n\n    /**\n     * @return the transaction start offset, or -1 if there is no transaction.\n     */\n    long transactionStartOffset() {\n        return transactionStartOffset;\n    }\n\n    /**\n     * @return the next offset that the active controller should write to.\n     */\n    long nextWriteOffset() {\n        return nextWriteOffset;\n    }\n\n    /**\n     * @return true only if the manager is active.\n     */\n    boolean active() {\n        return nextWriteOffset != -1L;\n    }\n\n    /**\n     * Called when the QuorumController becomes active.\n     *\n     * @param newNextWriteOffset The new next write offset to use. Must be non-negative.\n     */\n    void activate(long newNextWriteOffset) {\n        if (active()) {\n            throw new RuntimeException(\"Can't activate already active OffsetControlManager.\");\n        }\n        if (newNextWriteOffset < 0) {\n            throw new RuntimeException(\"Invalid negative newNextWriteOffset \" +\n                    newNextWriteOffset + \".\");\n        }\n        // Before switching to active, create an in-memory snapshot at the last committed\n        // offset. This is required because the active controller assumes that there is always\n        // an in-memory snapshot at the last committed offset.\n        idempotentCreateSnapshot();\n        this.nextWriteOffset = newNextWriteOffset;\n        metrics.setActive(true);\n    }\n\n    /**\n     * Called when the QuorumController becomes inactive.\n     */\n    void deactivate() {\n        if (!active()) {\n            throw new RuntimeException(\"Can't deactivate inactive OffsetControlManager.\");\n        }\n        metrics.setActive(false);\n        metrics.setLastAppliedRecordOffset(lastStableOffset);\n        this.nextWriteOffset = -1L;\n        if (!snapshotRegistry.hasSnapshot(lastStableOffset)) {\n            throw new RuntimeException(\"Unable to reset to last stable offset \" + lastStableOffset +\n                    \". No in-memory snapshot found for this offset.\");\n        }\n        snapshotRegistry.revertToSnapshot(lastStableOffset);\n    }\n\n    /**\n     * Handle the callback from the Raft layer indicating that a batch was committed.\n     *\n     * @param batch The batch that has been committed.\n     */\n    void handleCommitBatch(Batch<ApiMessageAndVersion> batch) {\n        this.lastCommittedOffset = batch.lastOffset();\n        this.lastCommittedEpoch = batch.epoch();\n        maybeAdvanceLastStableOffset();\n        metrics.setLastCommittedRecordOffset(batch.lastOffset());\n        if (!active()) {\n            // On standby controllers, the last applied record offset is equals to the last\n            // committed offset.\n            metrics.setLastAppliedRecordOffset(batch.lastOffset());\n            metrics.setLastAppliedRecordTimestamp(batch.appendTimestamp());\n        }\n    }\n\n    /**\n     * Called by the active controller after it has invoked scheduleAtomicAppend to schedule some\n     * records to be written.\n     *\n     * @param endOffset The offset of the last record that was written.\n     */\n    void handleScheduleAtomicAppend(long endOffset) {\n        this.nextWriteOffset = endOffset + 1;\n\n        snapshotRegistry.getOrCreateSnapshot(endOffset);\n\n        metrics.setLastAppliedRecordOffset(endOffset);\n\n        // This is not truly the append timestamp. The KRaft client doesn't expose the append\n        // time when scheduling a write. This is good enough because this is called right after\n        // the records were given to the KRAft client for appending and the default append linger\n        // for KRaft is 25ms.\n        metrics.setLastAppliedRecordTimestamp(time.milliseconds());\n    }\n\n    /**\n     * Advance the last stable offset if needed.\n     */\n    void maybeAdvanceLastStableOffset() {\n        long newLastStableOffset;\n        if (transactionStartOffset == -1L) {\n            newLastStableOffset = lastCommittedOffset;\n        } else {\n            newLastStableOffset = Math.min(transactionStartOffset - 1, lastCommittedOffset);\n        }\n        if (lastStableOffset < newLastStableOffset) {\n            lastStableOffset = newLastStableOffset;\n            snapshotRegistry.deleteSnapshotsUpTo(lastStableOffset);\n            if (!active()) {\n                idempotentCreateSnapshot();\n            }\n        }\n    }\n\n    private void idempotentCreateSnapshot() {\n        snapshotRegistry.getOrCreateSnapshot(lastStableOffset);\n    }\n\n    /**\n     * Called before we load a Raft snapshot.\n     *\n     * @param snapshotId The Raft snapshot offset and epoch.\n     */\n    void beginLoadSnapshot(OffsetAndEpoch snapshotId) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"Can't begin reading snapshot for \" + snapshotId +\n                    \", because we are already reading \" + currentSnapshotId);\n        }\n        this.currentSnapshotId = snapshotId;\n        this.currentSnapshotName = Snapshots.filenameFromSnapshotId(snapshotId);\n        log.info(\"Starting to load snapshot {}. Previous lastCommittedOffset was {}. Previous \" +\n                \"transactionStartOffset was {}.\", currentSnapshotName, lastCommittedOffset,\n                transactionStartOffset);\n        this.snapshotRegistry.reset();\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n    }\n\n    /**\n     * Called after we have finished loading a Raft snapshot.\n     *\n     * @param timestamp The timestamp of the snapshot.\n     */\n    void endLoadSnapshot(long timestamp) {\n        if (currentSnapshotId == null) {\n            throw new RuntimeException(\"Can't end loading snapshot, because there is no \" +\n                    \"current snapshot.\");\n        }\n        log.info(\"Successfully loaded snapshot {}.\", currentSnapshotName);\n        this.snapshotRegistry.getOrCreateSnapshot(currentSnapshotId.offset());\n        this.lastCommittedOffset = currentSnapshotId.offset();\n        this.lastCommittedEpoch = currentSnapshotId.epoch();\n        this.lastStableOffset = currentSnapshotId.offset();\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        metrics.setLastCommittedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordTimestamp(timestamp);\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n    }\n\n    public void replay(BeginTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"BeginTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset != -1L) {\n            throw new RuntimeException(\"Can't replay a BeginTransactionRecord at \" + offset +\n                \" because the transaction at \" + transactionStartOffset + \" was never closed.\");\n        }\n        snapshotRegistry.getOrCreateSnapshot(offset - 1);\n        transactionStartOffset = offset;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    public void replay(EndTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"EndTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an EndTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    public void replay(AbortTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"AbortTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an AbortTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        long preTransactionOffset = transactionStartOffset - 1;\n        snapshotRegistry.revertToSnapshot(preTransactionOffset);\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}. Reverted to offset {}.\",\n                message, offset, preTransactionOffset);\n    }\n\n    // VisibleForTesting\n    void setNextWriteOffset(long newNextWriteOffset) {\n        this.nextWriteOffset = newNextWriteOffset;\n    }\n}",
                "methodCount": 28
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "active",
                            "method_signature": " active()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(EndTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(EndTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "active",
                            "method_signature": " active()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(EndTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "active",
                            "method_signature": " active()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot()": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3522953764676562
                },
                "public build()": {
                    "first": {
                        "method_name": "build",
                        "method_signature": "public build()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4107921022481485
                },
                " handleScheduleAtomicAppend(long endOffset)": {
                    "first": {
                        "method_name": "handleScheduleAtomicAppend",
                        "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4898501156284456
                },
                "public replay(BeginTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.49176728521446916
                },
                "public replay(EndTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(EndTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5410090425923333
                },
                "public replay(AbortTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.548174904233164
                },
                " active()": {
                    "first": {
                        "method_name": "active",
                        "method_signature": " active()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5493091259559221
                },
                " beginLoadSnapshot(OffsetAndEpoch snapshotId)": {
                    "first": {
                        "method_name": "beginLoadSnapshot",
                        "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5739944991388558
                },
                " maybeAdvanceLastStableOffset()": {
                    "first": {
                        "method_name": "maybeAdvanceLastStableOffset",
                        "method_signature": " maybeAdvanceLastStableOffset()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6233917680383996
                },
                " handleCommitBatch(Batch<ApiMessageAndVersion> batch)": {
                    "first": {
                        "method_name": "handleCommitBatch",
                        "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6236765689755942
                },
                " deactivate()": {
                    "first": {
                        "method_name": "deactivate",
                        "method_signature": " deactivate()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6248110645866022
                },
                " endLoadSnapshot(long timestamp)": {
                    "first": {
                        "method_name": "endLoadSnapshot",
                        "method_signature": " endLoadSnapshot(long timestamp)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6448925334742986
                },
                " activate(long newNextWriteOffset)": {
                    "first": {
                        "method_name": "activate",
                        "method_signature": " activate(long newNextWriteOffset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6504498646273721
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [],
                "llm_response_time": 5673
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from package endLoadSnapshot(timestamp long) : void in class org.apache.kafka.controller.OffsetControlManager & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 354,
                    "endLine": 376,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package endLoadSnapshot(timestamp long) : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 365,
                    "endLine": 365,
                    "startColumn": 9,
                    "endColumn": 79,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 354,
                    "endLine": 376,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package endLoadSnapshot(timestamp long) : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 365,
                    "endLine": 365,
                    "startColumn": 9,
                    "endColumn": 83,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "this.snapshotRegistry.idempotentCreateSnapshot(currentSnapshotId.offset())"
                }
            ],
            "isStatic": false
        },
        "ref_id": 624,
        "extraction_results": {
            "success": true,
            "newCommitHash": "e4bf466a438e0f393135b84ed4b5611c7608ac3e",
            "newBranchName": "extract-idempotentCreateSnapshot-endLoadSnapshot-130af38"
        },
        "telemetry": {
            "id": "1aab5f44-520b-47d0-abc0-ebfaed77df33",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 390,
                "lineStart": 37,
                "lineEnd": 426,
                "bodyLineStart": 37,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                "sourceCode": "/**\n * Manages read and write offsets, and in-memory snapshots.\n *\n * Also manages the following metrics:\n *      kafka.controller:type=KafkaController,name=ActiveControllerCount\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordLagMs\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordOffset\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordTimestamp\n *      kafka.controller:type=KafkaController,name=LastCommittedRecordOffset\n */\nclass OffsetControlManager {\n    public static class Builder {\n        private LogContext logContext = null;\n        private SnapshotRegistry snapshotRegistry = null;\n        private QuorumControllerMetrics metrics = null;\n        private Time time = Time.SYSTEM;\n\n        Builder setLogContext(LogContext logContext) {\n            this.logContext = logContext;\n            return this;\n        }\n\n        Builder setSnapshotRegistry(SnapshotRegistry snapshotRegistry) {\n            this.snapshotRegistry = snapshotRegistry;\n            return this;\n        }\n\n        Builder setMetrics(QuorumControllerMetrics metrics) {\n            this.metrics = metrics;\n            return this;\n        }\n\n        Builder setTime(Time time) {\n            this.time = time;\n            return this;\n        }\n\n        public OffsetControlManager build() {\n            if (logContext == null) logContext = new LogContext();\n            if (snapshotRegistry == null) snapshotRegistry = new SnapshotRegistry(logContext);\n            if (metrics == null) {\n                metrics = new QuorumControllerMetrics(Optional.empty(), time, false);\n            }\n            return new OffsetControlManager(logContext,\n                    snapshotRegistry,\n                    metrics,\n                    time);\n        }\n    }\n\n    /**\n     * The slf4j logger.\n     */\n    private final Logger log;\n\n    /**\n     * The snapshot registry.\n     */\n    private final SnapshotRegistry snapshotRegistry;\n\n    /**\n     * The quorum controller metrics.\n     */\n    private final QuorumControllerMetrics metrics;\n\n    /**\n     * The clock.\n     */\n    private final Time time;\n\n    /**\n     * The ID of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private OffsetAndEpoch currentSnapshotId;\n\n    /**\n     * The name of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private String currentSnapshotName;\n\n    /**\n     * The latest committed offset.\n     */\n    private long lastCommittedOffset;\n\n    /**\n     * The latest committed epoch.\n     */\n    private int lastCommittedEpoch;\n\n    /**\n     * The latest offset that it is safe to read from.\n     */\n    private long lastStableOffset;\n\n    /**\n     * The offset of the transaction we're in, or -1 if we are not in one.\n     */\n    private long transactionStartOffset;\n\n    /**\n     * The next offset we should write to, or -1 if the controller is not active. Exclusive offset.\n     */\n    private long nextWriteOffset;\n\n    private OffsetControlManager(\n        LogContext logContext,\n        SnapshotRegistry snapshotRegistry,\n        QuorumControllerMetrics metrics,\n        Time time\n    ) {\n        this.log = logContext.logger(OffsetControlManager.class);\n        this.snapshotRegistry = snapshotRegistry;\n        this.metrics = metrics;\n        this.time = time;\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        snapshotRegistry.getOrCreateSnapshot(-1L);\n        metrics.setActive(false);\n        metrics.setLastCommittedRecordOffset(-1L);\n        metrics.setLastAppliedRecordOffset(-1L);\n        metrics.setLastAppliedRecordTimestamp(-1L);\n    }\n\n    /**\n     *  @return The SnapshotRegistry used by this offset control manager.\n     */\n    SnapshotRegistry snapshotRegistry() {\n        return snapshotRegistry;\n    }\n\n    /**\n     * @return QuorumControllerMetrics managed by this offset control manager.\n     */\n    QuorumControllerMetrics metrics() {\n        return metrics;\n    }\n\n    /**\n     * @return the ID of the current snapshot.\n     */\n    OffsetAndEpoch currentSnapshotId() {\n        return currentSnapshotId;\n    }\n\n    /**\n     * @return the name of the current snapshot.\n     */\n    String currentSnapshotName() {\n        return currentSnapshotName;\n    }\n\n    /**\n     * @return the last committed offset.\n     */\n    long lastCommittedOffset() {\n        return lastCommittedOffset;\n    }\n\n    /**\n     * @return the last committed epoch.\n     */\n    int lastCommittedEpoch() {\n        return lastCommittedEpoch;\n    }\n\n    /**\n     * @return the latest offset that it is safe to read from.\n     */\n    long lastStableOffset() {\n        return lastStableOffset;\n    }\n\n    /**\n     * @return the transaction start offset, or -1 if there is no transaction.\n     */\n    long transactionStartOffset() {\n        return transactionStartOffset;\n    }\n\n    /**\n     * @return the next offset that the active controller should write to.\n     */\n    long nextWriteOffset() {\n        return nextWriteOffset;\n    }\n\n    /**\n     * @return true only if the manager is active.\n     */\n    boolean active() {\n        return nextWriteOffset != -1L;\n    }\n\n    /**\n     * Called when the QuorumController becomes active.\n     *\n     * @param newNextWriteOffset The new next write offset to use. Must be non-negative.\n     */\n    void activate(long newNextWriteOffset) {\n        if (active()) {\n            throw new RuntimeException(\"Can't activate already active OffsetControlManager.\");\n        }\n        if (newNextWriteOffset < 0) {\n            throw new RuntimeException(\"Invalid negative newNextWriteOffset \" +\n                    newNextWriteOffset + \".\");\n        }\n        // Before switching to active, create an in-memory snapshot at the last committed\n        // offset. This is required because the active controller assumes that there is always\n        // an in-memory snapshot at the last committed offset.\n        snapshotRegistry.getOrCreateSnapshot(lastStableOffset);\n        this.nextWriteOffset = newNextWriteOffset;\n        metrics.setActive(true);\n    }\n\n    /**\n     * Called when the QuorumController becomes inactive.\n     */\n    void deactivate() {\n        if (!active()) {\n            throw new RuntimeException(\"Can't deactivate inactive OffsetControlManager.\");\n        }\n        metrics.setActive(false);\n        metrics.setLastAppliedRecordOffset(lastStableOffset);\n        this.nextWriteOffset = -1L;\n        if (!snapshotRegistry.hasSnapshot(lastStableOffset)) {\n            throw new RuntimeException(\"Unable to reset to last stable offset \" + lastStableOffset +\n                    \". No in-memory snapshot found for this offset.\");\n        }\n        snapshotRegistry.revertToSnapshot(lastStableOffset);\n    }\n\n    /**\n     * Handle the callback from the Raft layer indicating that a batch was committed.\n     *\n     * @param batch The batch that has been committed.\n     */\n    void handleCommitBatch(Batch<ApiMessageAndVersion> batch) {\n        this.lastCommittedOffset = batch.lastOffset();\n        this.lastCommittedEpoch = batch.epoch();\n        maybeAdvanceLastStableOffset();\n        metrics.setLastCommittedRecordOffset(batch.lastOffset());\n        if (!active()) {\n            // On standby controllers, the last applied record offset is equals to the last\n            // committed offset.\n            metrics.setLastAppliedRecordOffset(batch.lastOffset());\n            metrics.setLastAppliedRecordTimestamp(batch.appendTimestamp());\n        }\n    }\n\n    /**\n     * Called by the active controller after it has invoked scheduleAtomicAppend to schedule some\n     * records to be written.\n     *\n     * @param endOffset The offset of the last record that was written.\n     */\n    void handleScheduleAtomicAppend(long endOffset) {\n        this.nextWriteOffset = endOffset + 1;\n\n        snapshotRegistry.getOrCreateSnapshot(endOffset);\n\n        metrics.setLastAppliedRecordOffset(endOffset);\n\n        // This is not truly the append timestamp. The KRaft client doesn't expose the append\n        // time when scheduling a write. This is good enough because this is called right after\n        // the records were given to the KRAft client for appending and the default append linger\n        // for KRaft is 25ms.\n        metrics.setLastAppliedRecordTimestamp(time.milliseconds());\n    }\n\n    /**\n     * Advance the last stable offset if needed.\n     */\n    void maybeAdvanceLastStableOffset() {\n        long newLastStableOffset;\n        if (transactionStartOffset == -1L) {\n            newLastStableOffset = lastCommittedOffset;\n        } else {\n            newLastStableOffset = Math.min(transactionStartOffset - 1, lastCommittedOffset);\n        }\n        if (lastStableOffset < newLastStableOffset) {\n            lastStableOffset = newLastStableOffset;\n            snapshotRegistry.deleteSnapshotsUpTo(lastStableOffset);\n            if (!active()) {\n                snapshotRegistry.getOrCreateSnapshot(lastStableOffset);\n            }\n        }\n    }\n\n    /**\n     * Called before we load a Raft snapshot.\n     *\n     * @param snapshotId The Raft snapshot offset and epoch.\n     */\n    void beginLoadSnapshot(OffsetAndEpoch snapshotId) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"Can't begin reading snapshot for \" + snapshotId +\n                    \", because we are already reading \" + currentSnapshotId);\n        }\n        this.currentSnapshotId = snapshotId;\n        this.currentSnapshotName = Snapshots.filenameFromSnapshotId(snapshotId);\n        log.info(\"Starting to load snapshot {}. Previous lastCommittedOffset was {}. Previous \" +\n                \"transactionStartOffset was {}.\", currentSnapshotName, lastCommittedOffset,\n                transactionStartOffset);\n        this.snapshotRegistry.reset();\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n    }\n\n    /**\n     * Called after we have finished loading a Raft snapshot.\n     *\n     * @param timestamp The timestamp of the snapshot.\n     */\n    void endLoadSnapshot(long timestamp) {\n        if (currentSnapshotId == null) {\n            throw new RuntimeException(\"Can't end loading snapshot, because there is no \" +\n                    \"current snapshot.\");\n        }\n        log.info(\"Successfully loaded snapshot {}.\", currentSnapshotName);\n        idempotentCreateSnapshot();\n        this.lastCommittedOffset = currentSnapshotId.offset();\n        this.lastCommittedEpoch = currentSnapshotId.epoch();\n        this.lastStableOffset = currentSnapshotId.offset();\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        metrics.setLastCommittedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordTimestamp(timestamp);\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n    }\n\n    private void idempotentCreateSnapshot() {\n        this.snapshotRegistry.getOrCreateSnapshot(currentSnapshotId.offset());\n    }\n\n    public void replay(BeginTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"BeginTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset != -1L) {\n            throw new RuntimeException(\"Can't replay a BeginTransactionRecord at \" + offset +\n                \" because the transaction at \" + transactionStartOffset + \" was never closed.\");\n        }\n        snapshotRegistry.getOrCreateSnapshot(offset - 1);\n        transactionStartOffset = offset;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    public void replay(EndTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"EndTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an EndTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    public void replay(AbortTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"AbortTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an AbortTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        long preTransactionOffset = transactionStartOffset - 1;\n        snapshotRegistry.revertToSnapshot(preTransactionOffset);\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}. Reverted to offset {}.\",\n                message, offset, preTransactionOffset);\n    }\n\n    // VisibleForTesting\n    void setNextWriteOffset(long newNextWriteOffset) {\n        this.nextWriteOffset = newNextWriteOffset;\n    }\n}",
                "methodCount": 28
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 5,
                "candidates": [
                    {
                        "lineStart": 73,
                        "lineEnd": 83,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method build to class Time",
                        "description": "Move method build to org.apache.kafka.common.utils.Time\nRationale: The provided method 'build' is not logically related to the given 'Time' interface, as its purpose is to construct an instance of 'OffsetControlManager' involving various components such as 'logContext', 'snapshotRegistry', and 'metrics'. The 'Time' interface is solely responsible for time-related operations and abstracting clock functionalities. This method should not move to the 'Time' class, as it would violate the principle of single responsibility and lead to poor cohesion.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 377,
                        "lineEnd": 379,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class Time",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.common.utils.Time\nRationale: null",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 381,
                        "lineEnd": 392,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method replay to class Time",
                        "description": "Move method replay to org.apache.kafka.common.utils.Time\nRationale: The target class 'Time' is not suitable because it is intended for utility functions related to time measurement, sleeping, and waiting for conditions. The 'replay' method deals with transactional records and snapshots, which are outside the scope of time-related functionalities.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 394,
                        "lineEnd": 404,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method replay to class Time",
                        "description": "Move method replay to org.apache.kafka.common.utils.Time\nRationale: null",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 406,
                        "lineEnd": 419,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method replay to class Time",
                        "description": "Move method replay to org.apache.kafka.common.utils.Time\nRationale: null",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "active",
                            "method_signature": " active()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(EndTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(EndTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "active",
                            "method_signature": " active()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(EndTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "active",
                            "method_signature": " active()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot()": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3522953764676562
                },
                "public build()": {
                    "first": {
                        "method_name": "build",
                        "method_signature": "public build()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4107921022481485
                },
                " handleScheduleAtomicAppend(long endOffset)": {
                    "first": {
                        "method_name": "handleScheduleAtomicAppend",
                        "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4898501156284456
                },
                "public replay(BeginTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.49176728521446916
                },
                "public replay(EndTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(EndTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5410090425923333
                },
                "public replay(AbortTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.548174904233164
                },
                " active()": {
                    "first": {
                        "method_name": "active",
                        "method_signature": " active()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5493091259559221
                },
                " beginLoadSnapshot(OffsetAndEpoch snapshotId)": {
                    "first": {
                        "method_name": "beginLoadSnapshot",
                        "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5739944991388558
                },
                " maybeAdvanceLastStableOffset()": {
                    "first": {
                        "method_name": "maybeAdvanceLastStableOffset",
                        "method_signature": " maybeAdvanceLastStableOffset()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6233917680383996
                },
                " handleCommitBatch(Batch<ApiMessageAndVersion> batch)": {
                    "first": {
                        "method_name": "handleCommitBatch",
                        "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6236765689755942
                },
                " deactivate()": {
                    "first": {
                        "method_name": "deactivate",
                        "method_signature": " deactivate()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6248110645866022
                },
                " endLoadSnapshot(long timestamp)": {
                    "first": {
                        "method_name": "endLoadSnapshot",
                        "method_signature": " endLoadSnapshot(long timestamp)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6448925334742986
                },
                " activate(long newNextWriteOffset)": {
                    "first": {
                        "method_name": "activate",
                        "method_signature": " activate(long newNextWriteOffset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6504498646273721
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "public build()",
                    "private idempotentCreateSnapshot()",
                    "public replay(BeginTransactionRecord message, long offset)",
                    "public replay(EndTransactionRecord message, long offset)",
                    "public replay(AbortTransactionRecord message, long offset)"
                ],
                "llm_response_time": 2610
            },
            "targetClassMap": {
                "build": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.14779923645463774
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.14113233090687777
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 2400,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "replay": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.2081809070085429
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public replay(message BeginTransactionRecord, offset long) : void in class org.apache.kafka.controller.OffsetControlManager & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 378,
                    "endLine": 389,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public replay(message BeginTransactionRecord, offset long) : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 386,
                    "endLine": 386,
                    "startColumn": 9,
                    "endColumn": 58,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 378,
                    "endLine": 389,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public replay(message BeginTransactionRecord, offset long) : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 386,
                    "endLine": 386,
                    "startColumn": 9,
                    "endColumn": 62,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(offset - 1)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 625,
        "extraction_results": {
            "success": true,
            "newCommitHash": "d0d1ef2e01ed6532f34cdbb087e241a4f6880007",
            "newBranchName": "extract-idempotentCreateSnapshot-replay-130af38"
        },
        "telemetry": {
            "id": "dbbae064-eac6-4be3-b389-3d2625c6c4e5",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 390,
                "lineStart": 37,
                "lineEnd": 426,
                "bodyLineStart": 37,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                "sourceCode": "/**\n * Manages read and write offsets, and in-memory snapshots.\n *\n * Also manages the following metrics:\n *      kafka.controller:type=KafkaController,name=ActiveControllerCount\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordLagMs\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordOffset\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordTimestamp\n *      kafka.controller:type=KafkaController,name=LastCommittedRecordOffset\n */\nclass OffsetControlManager {\n    public static class Builder {\n        private LogContext logContext = null;\n        private SnapshotRegistry snapshotRegistry = null;\n        private QuorumControllerMetrics metrics = null;\n        private Time time = Time.SYSTEM;\n\n        Builder setLogContext(LogContext logContext) {\n            this.logContext = logContext;\n            return this;\n        }\n\n        Builder setSnapshotRegistry(SnapshotRegistry snapshotRegistry) {\n            this.snapshotRegistry = snapshotRegistry;\n            return this;\n        }\n\n        Builder setMetrics(QuorumControllerMetrics metrics) {\n            this.metrics = metrics;\n            return this;\n        }\n\n        Builder setTime(Time time) {\n            this.time = time;\n            return this;\n        }\n\n        public OffsetControlManager build() {\n            if (logContext == null) logContext = new LogContext();\n            if (snapshotRegistry == null) snapshotRegistry = new SnapshotRegistry(logContext);\n            if (metrics == null) {\n                metrics = new QuorumControllerMetrics(Optional.empty(), time, false);\n            }\n            return new OffsetControlManager(logContext,\n                    snapshotRegistry,\n                    metrics,\n                    time);\n        }\n    }\n\n    /**\n     * The slf4j logger.\n     */\n    private final Logger log;\n\n    /**\n     * The snapshot registry.\n     */\n    private final SnapshotRegistry snapshotRegistry;\n\n    /**\n     * The quorum controller metrics.\n     */\n    private final QuorumControllerMetrics metrics;\n\n    /**\n     * The clock.\n     */\n    private final Time time;\n\n    /**\n     * The ID of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private OffsetAndEpoch currentSnapshotId;\n\n    /**\n     * The name of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private String currentSnapshotName;\n\n    /**\n     * The latest committed offset.\n     */\n    private long lastCommittedOffset;\n\n    /**\n     * The latest committed epoch.\n     */\n    private int lastCommittedEpoch;\n\n    /**\n     * The latest offset that it is safe to read from.\n     */\n    private long lastStableOffset;\n\n    /**\n     * The offset of the transaction we're in, or -1 if we are not in one.\n     */\n    private long transactionStartOffset;\n\n    /**\n     * The next offset we should write to, or -1 if the controller is not active. Exclusive offset.\n     */\n    private long nextWriteOffset;\n\n    private OffsetControlManager(\n        LogContext logContext,\n        SnapshotRegistry snapshotRegistry,\n        QuorumControllerMetrics metrics,\n        Time time\n    ) {\n        this.log = logContext.logger(OffsetControlManager.class);\n        this.snapshotRegistry = snapshotRegistry;\n        this.metrics = metrics;\n        this.time = time;\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        snapshotRegistry.getOrCreateSnapshot(-1L);\n        metrics.setActive(false);\n        metrics.setLastCommittedRecordOffset(-1L);\n        metrics.setLastAppliedRecordOffset(-1L);\n        metrics.setLastAppliedRecordTimestamp(-1L);\n    }\n\n    /**\n     *  @return The SnapshotRegistry used by this offset control manager.\n     */\n    SnapshotRegistry snapshotRegistry() {\n        return snapshotRegistry;\n    }\n\n    /**\n     * @return QuorumControllerMetrics managed by this offset control manager.\n     */\n    QuorumControllerMetrics metrics() {\n        return metrics;\n    }\n\n    /**\n     * @return the ID of the current snapshot.\n     */\n    OffsetAndEpoch currentSnapshotId() {\n        return currentSnapshotId;\n    }\n\n    /**\n     * @return the name of the current snapshot.\n     */\n    String currentSnapshotName() {\n        return currentSnapshotName;\n    }\n\n    /**\n     * @return the last committed offset.\n     */\n    long lastCommittedOffset() {\n        return lastCommittedOffset;\n    }\n\n    /**\n     * @return the last committed epoch.\n     */\n    int lastCommittedEpoch() {\n        return lastCommittedEpoch;\n    }\n\n    /**\n     * @return the latest offset that it is safe to read from.\n     */\n    long lastStableOffset() {\n        return lastStableOffset;\n    }\n\n    /**\n     * @return the transaction start offset, or -1 if there is no transaction.\n     */\n    long transactionStartOffset() {\n        return transactionStartOffset;\n    }\n\n    /**\n     * @return the next offset that the active controller should write to.\n     */\n    long nextWriteOffset() {\n        return nextWriteOffset;\n    }\n\n    /**\n     * @return true only if the manager is active.\n     */\n    boolean active() {\n        return nextWriteOffset != -1L;\n    }\n\n    /**\n     * Called when the QuorumController becomes active.\n     *\n     * @param newNextWriteOffset The new next write offset to use. Must be non-negative.\n     */\n    void activate(long newNextWriteOffset) {\n        if (active()) {\n            throw new RuntimeException(\"Can't activate already active OffsetControlManager.\");\n        }\n        if (newNextWriteOffset < 0) {\n            throw new RuntimeException(\"Invalid negative newNextWriteOffset \" +\n                    newNextWriteOffset + \".\");\n        }\n        // Before switching to active, create an in-memory snapshot at the last committed\n        // offset. This is required because the active controller assumes that there is always\n        // an in-memory snapshot at the last committed offset.\n        snapshotRegistry.getOrCreateSnapshot(lastStableOffset);\n        this.nextWriteOffset = newNextWriteOffset;\n        metrics.setActive(true);\n    }\n\n    /**\n     * Called when the QuorumController becomes inactive.\n     */\n    void deactivate() {\n        if (!active()) {\n            throw new RuntimeException(\"Can't deactivate inactive OffsetControlManager.\");\n        }\n        metrics.setActive(false);\n        metrics.setLastAppliedRecordOffset(lastStableOffset);\n        this.nextWriteOffset = -1L;\n        if (!snapshotRegistry.hasSnapshot(lastStableOffset)) {\n            throw new RuntimeException(\"Unable to reset to last stable offset \" + lastStableOffset +\n                    \". No in-memory snapshot found for this offset.\");\n        }\n        snapshotRegistry.revertToSnapshot(lastStableOffset);\n    }\n\n    /**\n     * Handle the callback from the Raft layer indicating that a batch was committed.\n     *\n     * @param batch The batch that has been committed.\n     */\n    void handleCommitBatch(Batch<ApiMessageAndVersion> batch) {\n        this.lastCommittedOffset = batch.lastOffset();\n        this.lastCommittedEpoch = batch.epoch();\n        maybeAdvanceLastStableOffset();\n        metrics.setLastCommittedRecordOffset(batch.lastOffset());\n        if (!active()) {\n            // On standby controllers, the last applied record offset is equals to the last\n            // committed offset.\n            metrics.setLastAppliedRecordOffset(batch.lastOffset());\n            metrics.setLastAppliedRecordTimestamp(batch.appendTimestamp());\n        }\n    }\n\n    /**\n     * Called by the active controller after it has invoked scheduleAtomicAppend to schedule some\n     * records to be written.\n     *\n     * @param endOffset The offset of the last record that was written.\n     */\n    void handleScheduleAtomicAppend(long endOffset) {\n        this.nextWriteOffset = endOffset + 1;\n\n        snapshotRegistry.getOrCreateSnapshot(endOffset);\n\n        metrics.setLastAppliedRecordOffset(endOffset);\n\n        // This is not truly the append timestamp. The KRaft client doesn't expose the append\n        // time when scheduling a write. This is good enough because this is called right after\n        // the records were given to the KRAft client for appending and the default append linger\n        // for KRaft is 25ms.\n        metrics.setLastAppliedRecordTimestamp(time.milliseconds());\n    }\n\n    /**\n     * Advance the last stable offset if needed.\n     */\n    void maybeAdvanceLastStableOffset() {\n        long newLastStableOffset;\n        if (transactionStartOffset == -1L) {\n            newLastStableOffset = lastCommittedOffset;\n        } else {\n            newLastStableOffset = Math.min(transactionStartOffset - 1, lastCommittedOffset);\n        }\n        if (lastStableOffset < newLastStableOffset) {\n            lastStableOffset = newLastStableOffset;\n            snapshotRegistry.deleteSnapshotsUpTo(lastStableOffset);\n            if (!active()) {\n                snapshotRegistry.getOrCreateSnapshot(lastStableOffset);\n            }\n        }\n    }\n\n    /**\n     * Called before we load a Raft snapshot.\n     *\n     * @param snapshotId The Raft snapshot offset and epoch.\n     */\n    void beginLoadSnapshot(OffsetAndEpoch snapshotId) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"Can't begin reading snapshot for \" + snapshotId +\n                    \", because we are already reading \" + currentSnapshotId);\n        }\n        this.currentSnapshotId = snapshotId;\n        this.currentSnapshotName = Snapshots.filenameFromSnapshotId(snapshotId);\n        log.info(\"Starting to load snapshot {}. Previous lastCommittedOffset was {}. Previous \" +\n                \"transactionStartOffset was {}.\", currentSnapshotName, lastCommittedOffset,\n                transactionStartOffset);\n        this.snapshotRegistry.reset();\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n    }\n\n    /**\n     * Called after we have finished loading a Raft snapshot.\n     *\n     * @param timestamp The timestamp of the snapshot.\n     */\n    void endLoadSnapshot(long timestamp) {\n        if (currentSnapshotId == null) {\n            throw new RuntimeException(\"Can't end loading snapshot, because there is no \" +\n                    \"current snapshot.\");\n        }\n        log.info(\"Successfully loaded snapshot {}.\", currentSnapshotName);\n        this.snapshotRegistry.getOrCreateSnapshot(currentSnapshotId.offset());\n        this.lastCommittedOffset = currentSnapshotId.offset();\n        this.lastCommittedEpoch = currentSnapshotId.epoch();\n        this.lastStableOffset = currentSnapshotId.offset();\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        metrics.setLastCommittedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordTimestamp(timestamp);\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n    }\n\n    public void replay(BeginTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"BeginTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset != -1L) {\n            throw new RuntimeException(\"Can't replay a BeginTransactionRecord at \" + offset +\n                \" because the transaction at \" + transactionStartOffset + \" was never closed.\");\n        }\n        idempotentCreateSnapshot(offset);\n        transactionStartOffset = offset;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    private void idempotentCreateSnapshot(long offset) {\n        snapshotRegistry.getOrCreateSnapshot(offset - 1);\n    }\n\n    public void replay(EndTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"EndTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an EndTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    public void replay(AbortTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"AbortTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an AbortTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        long preTransactionOffset = transactionStartOffset - 1;\n        snapshotRegistry.revertToSnapshot(preTransactionOffset);\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}. Reverted to offset {}.\",\n                message, offset, preTransactionOffset);\n    }\n\n    // VisibleForTesting\n    void setNextWriteOffset(long newNextWriteOffset) {\n        this.nextWriteOffset = newNextWriteOffset;\n    }\n}",
                "methodCount": 28
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 5,
                "candidates": [
                    {
                        "lineStart": 73,
                        "lineEnd": 83,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method build to class Time",
                        "description": "Move method build to org.apache.kafka.common.utils.Time\nRationale: The provided method 'build' is not logically related to the given 'Time' interface, as its purpose is to construct an instance of 'OffsetControlManager' involving various components such as 'logContext', 'snapshotRegistry', and 'metrics'. The 'Time' interface is solely responsible for time-related operations and abstracting clock functionalities. This method should not move to the 'Time' class, as it would violate the principle of single responsibility and lead to poor cohesion.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 377,
                        "lineEnd": 388,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method replay to class Time",
                        "description": "Move method replay to org.apache.kafka.common.utils.Time\nRationale: null",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 394,
                        "lineEnd": 404,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method replay to class Time",
                        "description": "Move method replay to org.apache.kafka.common.utils.Time\nRationale: null",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 406,
                        "lineEnd": 419,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method replay to class Time",
                        "description": "Move method replay to org.apache.kafka.common.utils.Time\nRationale: null",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 390,
                        "lineEnd": 392,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method `idempotentCreateSnapshot(long offset)` interacts directly with functionality provided by the `SnapshotRegistry` class, particularly the `getOrCreateSnapshot` method. By moving this method into `SnapshotRegistry`, the encapsulation of all snapshot-related logic is maintained within the same class, ensuring cohesion and ease of maintenance. This also aligns with the Single Responsibility Principle, as `SnapshotRegistry` is responsible for managing all snapshots and related operations.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "active",
                            "method_signature": " active()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(EndTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(EndTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "active",
                            "method_signature": " active()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(EndTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "active",
                            "method_signature": " active()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(long offset)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.29957996542755694
                },
                "public build()": {
                    "first": {
                        "method_name": "build",
                        "method_signature": "public build()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4107157600145837
                },
                " handleScheduleAtomicAppend(long endOffset)": {
                    "first": {
                        "method_name": "handleScheduleAtomicAppend",
                        "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4897528859389902
                },
                "public replay(BeginTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.49499431983769177
                },
                "public replay(EndTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(EndTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5417384325651089
                },
                "public replay(AbortTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5488182866357066
                },
                " active()": {
                    "first": {
                        "method_name": "active",
                        "method_signature": " active()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5492091541934644
                },
                " beginLoadSnapshot(OffsetAndEpoch snapshotId)": {
                    "first": {
                        "method_name": "beginLoadSnapshot",
                        "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.57388030870975
                },
                " maybeAdvanceLastStableOffset()": {
                    "first": {
                        "method_name": "maybeAdvanceLastStableOffset",
                        "method_signature": " maybeAdvanceLastStableOffset()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6232708167123373
                },
                " handleCommitBatch(Batch<ApiMessageAndVersion> batch)": {
                    "first": {
                        "method_name": "handleCommitBatch",
                        "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6235557298590018
                },
                " deactivate()": {
                    "first": {
                        "method_name": "deactivate",
                        "method_signature": " deactivate()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6246919703173314
                },
                " endLoadSnapshot(long timestamp)": {
                    "first": {
                        "method_name": "endLoadSnapshot",
                        "method_signature": " endLoadSnapshot(long timestamp)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.644765201190868
                },
                " activate(long newNextWriteOffset)": {
                    "first": {
                        "method_name": "activate",
                        "method_signature": " activate(long newNextWriteOffset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6503193588991747
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "public build()",
                    "public replay(BeginTransactionRecord message, long offset)",
                    "public replay(EndTransactionRecord message, long offset)",
                    "public replay(AbortTransactionRecord message, long offset)",
                    "private idempotentCreateSnapshot(long offset)"
                ],
                "llm_response_time": 2883
            },
            "targetClassMap": {
                "build": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.14779923645463774
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "replay": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.2081809070085429
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.25127272810509715
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.12099576892601743
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry",
                        "Time"
                    ],
                    "llm_response_time": 3766,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testLoadSnapshot() : void in class org.apache.kafka.controller.AclControlManagerTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/AclControlManagerTest.java",
                    "startLine": 203,
                    "endLine": 238,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testLoadSnapshot() : void"
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/AclControlManagerTest.java",
                    "startLine": 206,
                    "endLine": 206,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/AclControlManagerTest.java",
                    "startLine": 203,
                    "endLine": 238,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testLoadSnapshot() : void"
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/AclControlManagerTest.java",
                    "startLine": 206,
                    "endLine": 206,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(0)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 626,
        "extraction_results": {
            "success": true,
            "newCommitHash": "41521201e2c0ea4f6a97d434a4a1b58e6b71b232",
            "newBranchName": "extract-idempotentCreateSnapshot-testLoadSnapshot-130af38"
        },
        "telemetry": {
            "id": "2d2d4068-92a0-4d7f-88f2-86d79361c4cf",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 265,
                "lineStart": 81,
                "lineEnd": 345,
                "bodyLineStart": 81,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/metadata/src/test/java/org/apache/kafka/controller/AclControlManagerTest.java",
                "sourceCode": "@Timeout(value = 40)\npublic class AclControlManagerTest {\n    /**\n     * Verify that validateNewAcl catches invalid ACLs.\n     */\n    @Test\n    public void testValidateNewAcl() {\n        AclControlManager.validateNewAcl(new AclBinding(\n            new ResourcePattern(TOPIC, \"*\", LITERAL),\n            new AccessControlEntry(\"User:*\", \"*\", ALTER, ALLOW)));\n        assertEquals(\"Invalid patternType UNKNOWN\",\n            assertThrows(InvalidRequestException.class, () ->\n                AclControlManager.validateNewAcl(new AclBinding(\n                    new ResourcePattern(TOPIC, \"*\", PatternType.UNKNOWN),\n                    new AccessControlEntry(\"User:*\", \"*\", ALTER, ALLOW)))).\n                getMessage());\n        assertEquals(\"Invalid resourceType UNKNOWN\",\n            assertThrows(InvalidRequestException.class, () ->\n                AclControlManager.validateNewAcl(new AclBinding(\n                    new ResourcePattern(ResourceType.UNKNOWN, \"*\", LITERAL),\n                    new AccessControlEntry(\"User:*\", \"*\", ALTER, ALLOW)))).\n                getMessage());\n        assertEquals(\"Invalid operation UNKNOWN\",\n            assertThrows(InvalidRequestException.class, () ->\n                AclControlManager.validateNewAcl(new AclBinding(\n                    new ResourcePattern(TOPIC, \"*\", LITERAL),\n                    new AccessControlEntry(\"User:*\", \"*\", AclOperation.UNKNOWN, ALLOW)))).\n                getMessage());\n        assertEquals(\"Invalid permissionType UNKNOWN\",\n            assertThrows(InvalidRequestException.class, () ->\n                AclControlManager.validateNewAcl(new AclBinding(\n                    new ResourcePattern(TOPIC, \"*\", LITERAL),\n                    new AccessControlEntry(\"User:*\", \"*\", ALTER, AclPermissionType.UNKNOWN)))).\n                getMessage());\n    }\n\n    /**\n     * Verify that validateFilter catches invalid filters.\n     */\n    @Test\n    public void testValidateFilter() {\n        AclControlManager.validateFilter(new AclBindingFilter(\n            new ResourcePatternFilter(ResourceType.ANY, \"*\", LITERAL),\n            new AccessControlEntryFilter(\"User:*\", \"*\", AclOperation.ANY, AclPermissionType.ANY)));\n        assertEquals(\"Unknown patternFilter.\",\n            assertThrows(InvalidRequestException.class, () ->\n                AclControlManager.validateFilter(new AclBindingFilter(\n                    new ResourcePatternFilter(ResourceType.ANY, \"*\", PatternType.UNKNOWN),\n                    new AccessControlEntryFilter(\"User:*\", \"*\", AclOperation.ANY, AclPermissionType.ANY)))).\n                getMessage());\n        assertEquals(\"Unknown entryFilter.\",\n            assertThrows(InvalidRequestException.class, () ->\n                AclControlManager.validateFilter(new AclBindingFilter(\n                    new ResourcePatternFilter(ResourceType.ANY, \"*\", MATCH),\n                    new AccessControlEntryFilter(\"User:*\", \"*\", AclOperation.ANY, AclPermissionType.UNKNOWN)))).\n                getMessage());\n    }\n\n    static class MockClusterMetadataAuthorizer implements ClusterMetadataAuthorizer {\n        Map<Uuid, StandardAcl> acls = Collections.emptyMap();\n\n        @Override\n        public void setAclMutator(AclMutator aclMutator) {\n            // do nothing\n        }\n\n        @Override\n        public AclMutator aclMutatorOrException() {\n            throw new NotControllerException(\"The current node is not the active controller.\");\n        }\n\n        @Override\n        public void completeInitialLoad() {\n            // do nothing\n        }\n\n        @Override\n        public void completeInitialLoad(Exception e) {\n            // do nothing\n        }\n\n        @Override\n        public void loadSnapshot(Map<Uuid, StandardAcl> acls) {\n            this.acls = new HashMap<>(acls);\n        }\n\n        @Override\n        public void addAcl(Uuid id, StandardAcl acl) {\n            // do nothing\n        }\n\n        @Override\n        public void removeAcl(Uuid id) {\n            // do nothing\n        }\n\n        @Override\n        public Map<Endpoint, ? extends CompletionStage<Void>> start(AuthorizerServerInfo serverInfo) {\n            return null; // do nothing\n        }\n\n        @Override\n        public List<AuthorizationResult> authorize(AuthorizableRequestContext requestContext, List<Action> actions) {\n            return null; // do nothing\n        }\n\n        @Override\n        public Iterable<AclBinding> acls(AclBindingFilter filter) {\n            return null; // do nothing\n        }\n\n        @Override\n        public void close() throws IOException {\n            // do nothing\n        }\n\n        @Override\n        public void configure(Map<String, ?> configs) {\n            // do nothing\n        }\n    }\n\n    @Test\n    public void testLoadSnapshot() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        idempotentCreateSnapshot(snapshotRegistry);\n        AclControlManager manager = new AclControlManager.Builder().\n            setSnapshotRegistry(snapshotRegistry).\n            build();\n\n        // Load TEST_ACLS into the AclControlManager.\n        Set<ApiMessageAndVersion> loadedAcls = new HashSet<>();\n        for (StandardAclWithId acl : TEST_ACLS) {\n            AccessControlEntryRecord record = acl.toRecord();\n            assertTrue(loadedAcls.add(new ApiMessageAndVersion(record, (short) 0)));\n            manager.replay(acl.toRecord());\n        }\n\n        // Verify that the ACLs stored in the AclControlManager match the ones we expect.\n        Set<ApiMessageAndVersion> foundAcls = new HashSet<>();\n        for (Map.Entry<Uuid, StandardAcl> entry : manager.idToAcl().entrySet()) {\n            foundAcls.add(new ApiMessageAndVersion(\n                    new StandardAclWithId(entry.getKey(), entry.getValue()).toRecord(), (short) 0));\n        }\n        assertEquals(loadedAcls, foundAcls);\n\n        // Once we complete the snapshot load, the ACLs should be reflected in the authorizer.\n        MockClusterMetadataAuthorizer authorizer = new MockClusterMetadataAuthorizer();\n        authorizer.loadSnapshot(manager.idToAcl());\n        assertEquals(new HashSet<>(StandardAclTest.TEST_ACLS), new HashSet<>(authorizer.acls.values()));\n\n        // Test reverting to an empty state and then completing the snapshot load without\n        // setting an authorizer. This simulates the case where the user didn't configure\n        // a cluster metadata authorizer.\n        snapshotRegistry.revertToSnapshot(0);\n        authorizer.loadSnapshot(manager.idToAcl());\n        assertTrue(manager.idToAcl().isEmpty());\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(0);\n    }\n\n    @Test\n    public void testAddAndDelete() {\n        AclControlManager manager = new AclControlManager.Builder().build();\n        MockClusterMetadataAuthorizer authorizer = new MockClusterMetadataAuthorizer();\n        authorizer.loadSnapshot(manager.idToAcl());\n        manager.replay(StandardAclWithIdTest.TEST_ACLS.get(0).toRecord());\n        manager.replay(new RemoveAccessControlEntryRecord().\n            setId(TEST_ACLS.get(0).id()));\n        assertTrue(manager.idToAcl().isEmpty());\n    }\n\n    @Test\n    public void testCreateAclDeleteAcl() {\n        AclControlManager manager = new AclControlManager.Builder().build();\n        MockClusterMetadataAuthorizer authorizer = new MockClusterMetadataAuthorizer();\n        authorizer.loadSnapshot(manager.idToAcl());\n\n        List<AclBinding> toCreate = new ArrayList<>();\n        for (int i = 0; i < 3; i++) {\n            toCreate.add(TEST_ACLS.get(i).toBinding());\n        }\n        toCreate.add(new AclBinding(\n            new ResourcePattern(TOPIC, \"*\", PatternType.UNKNOWN),\n            new AccessControlEntry(\"User:*\", \"*\", ALTER, ALLOW)));\n\n        ControllerResult<List<AclCreateResult>> createResult = manager.createAcls(toCreate);\n\n        List<AclCreateResult> expectedResults = new ArrayList<>();\n        for (int i = 0; i < 3; i++) {\n            expectedResults.add(AclCreateResult.SUCCESS);\n        }\n        expectedResults.add(new AclCreateResult(\n            new InvalidRequestException(\"Invalid patternType UNKNOWN\")));\n\n        for (int i = 0; i < expectedResults.size(); i++) {\n            AclCreateResult expectedResult = expectedResults.get(i);\n            if (expectedResult.exception().isPresent()) {\n                assertEquals(expectedResult.exception().get().getMessage(),\n                    createResult.response().get(i).exception().get().getMessage());\n            } else {\n                assertFalse(createResult.response().get(i).exception().isPresent());\n            }\n        }\n        RecordTestUtils.replayAll(manager, createResult.records());\n        assertFalse(manager.idToAcl().isEmpty());\n\n        ControllerResult<List<AclDeleteResult>> deleteResult =\n            manager.deleteAcls(Arrays.asList(\n                new AclBindingFilter(\n                    new ResourcePatternFilter(ResourceType.ANY, null, LITERAL),\n                        AccessControlEntryFilter.ANY),\n                new AclBindingFilter(\n                    new ResourcePatternFilter(ResourceType.UNKNOWN, null, LITERAL),\n                        AccessControlEntryFilter.ANY)));\n        assertEquals(2, deleteResult.response().size());\n        Set<AclBinding> deleted = new HashSet<>();\n        for (AclDeleteResult.AclBindingDeleteResult result :\n                deleteResult.response().get(0).aclBindingDeleteResults()) {\n            assertEquals(Optional.empty(), result.exception());\n            deleted.add(result.aclBinding());\n        }\n        assertEquals(new HashSet<>(Arrays.asList(\n            TEST_ACLS.get(0).toBinding(),\n                TEST_ACLS.get(2).toBinding())), deleted);\n        assertEquals(InvalidRequestException.class,\n            deleteResult.response().get(1).exception().get().getClass());\n        RecordTestUtils.replayAll(manager, deleteResult.records());\n\n        Iterator<Map.Entry<Uuid, StandardAcl>> iterator = manager.idToAcl().entrySet().iterator();\n        assertEquals(TEST_ACLS.get(1).acl(), iterator.next().getValue());\n        assertFalse(iterator.hasNext());\n    }\n\n    @Test\n    public void testDeleteDedupe() {\n        AclControlManager manager = new AclControlManager.Builder().build();\n        MockClusterMetadataAuthorizer authorizer = new MockClusterMetadataAuthorizer();\n        authorizer.loadSnapshot(manager.idToAcl());\n\n        AclBinding aclBinding = new AclBinding(new ResourcePattern(TOPIC, \"topic-1\", LITERAL),\n                new AccessControlEntry(\"User:user\", \"10.0.0.1\", AclOperation.ALL, ALLOW));\n\n        ControllerResult<List<AclCreateResult>> createResult = manager.createAcls(Collections.singletonList(aclBinding));\n        Uuid id = ((AccessControlEntryRecord) createResult.records().get(0).message()).id();\n        assertEquals(1, createResult.records().size());\n\n        ControllerResult<List<AclDeleteResult>> deleteAclResultsAnyFilter = manager.deleteAcls(Collections.singletonList(AclBindingFilter.ANY));\n        assertEquals(1, deleteAclResultsAnyFilter.records().size());\n        assertEquals(id, ((RemoveAccessControlEntryRecord) deleteAclResultsAnyFilter.records().get(0).message()).id());\n        assertEquals(1, deleteAclResultsAnyFilter.response().size());\n\n        ControllerResult<List<AclDeleteResult>> deleteAclResultsSpecificFilter = manager.deleteAcls(Collections.singletonList(aclBinding.toFilter()));\n        assertEquals(1, deleteAclResultsSpecificFilter.records().size());\n        assertEquals(id, ((RemoveAccessControlEntryRecord) deleteAclResultsSpecificFilter.records().get(0).message()).id());\n        assertEquals(1, deleteAclResultsSpecificFilter.response().size());\n\n        ControllerResult<List<AclDeleteResult>> deleteAclResultsBothFilters = manager.deleteAcls(Arrays.asList(AclBindingFilter.ANY, aclBinding.toFilter()));\n        assertEquals(1, deleteAclResultsBothFilters.records().size());\n        assertEquals(id, ((RemoveAccessControlEntryRecord) deleteAclResultsBothFilters.records().get(0).message()).id());\n        assertEquals(2, deleteAclResultsBothFilters.response().size());\n    }\n}",
                "methodCount": 19
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 239,
                        "lineEnd": 241,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method idempotentCreateSnapshot() directly interacts with the getOrCreateSnapshot() method from the SnapshotRegistry class to create a snapshot if it does not exist already. This tight coupling indicates that the functionality is directly relevant to the SnapshotRegistry, aligning with the responsibilities of managing and creating snapshots within this class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.31128640318234513
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                ],
                "llm_response_time": 1734
            },
            "targetClassMap": {
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testUpdateFeatures() : void in class org.apache.kafka.controller.FeatureControlManagerTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 95,
                    "endLine": 125,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testUpdateFeatures() : void"
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 103,
                    "endLine": 103,
                    "startColumn": 9,
                    "endColumn": 50,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 94,
                    "endLine": 124,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testUpdateFeatures() : void"
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 102,
                    "endLine": 102,
                    "startColumn": 9,
                    "endColumn": 54,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(-1)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 627,
        "extraction_results": {
            "success": true,
            "newCommitHash": "37798d9b1c134ab3b36c3c31004ba933de6687bd",
            "newBranchName": "extract-idempotentCreateSnapshot-testUpdateFeatures-130af38"
        },
        "telemetry": {
            "id": "f5e03934-491a-4a0e-85b1-cc14546ef863",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 405,
                "lineStart": 54,
                "lineEnd": 458,
                "bodyLineStart": 54,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                "sourceCode": "@Timeout(value = 40)\npublic class FeatureControlManagerTest {\n\n    @SuppressWarnings(\"unchecked\")\n    private static Map<String, VersionRange> rangeMap(Object... args) {\n        Map<String, VersionRange> result = new HashMap<>();\n        for (int i = 0; i < args.length; i += 3) {\n            String feature = (String) args[i];\n            Number low = (Number) args[i + 1];\n            Number high = (Number) args[i + 2];\n            result.put(feature, VersionRange.of(low.shortValue(), high.shortValue()));\n        }\n        return result;\n    }\n\n    private static Map<String, Short> versionMap(Object... args) {\n        Map<String, Short> result = new HashMap<>();\n        for (int i = 0; i < args.length; i += 2) {\n            String feature = (String) args[i];\n            Number ver = (Number) args[i + 1];\n            result.put(feature, ver.shortValue());\n        }\n        return result;\n    }\n\n    public static QuorumFeatures features(Object... args) {\n        Map<String, VersionRange> features = QuorumFeatures.defaultFeatureMap(true);\n        features.putAll(rangeMap(args));\n        return new QuorumFeatures(0, features, emptyList());\n    }\n\n    private static Map<String, Short> updateMap(Object... args) {\n        Map<String, Short> result = new HashMap<>();\n        for (int i = 0; i < args.length; i += 2) {\n            String feature = (String) args[i];\n            Number ver = (Number) args[i + 1];\n            result.put(feature, ver.shortValue());\n        }\n        return result;\n    }\n\n    @Test\n    public void testUpdateFeatures() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(features(\"foo\", 1, 2)).\n            setSnapshotRegistry(snapshotRegistry).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n            build();\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(new FinalizedControllerFeatures(Collections.singletonMap(\"metadata.version\", (short) 4), -1),\n            manager.finalizedFeatures(-1));\n        assertEquals(ControllerResult.atomicOf(emptyList(), Collections.\n                singletonMap(\"foo\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                    \"Invalid update version 3 for feature foo. Local controller 0 only supports versions 1-2\"))),\n            manager.updateFeatures(updateMap(\"foo\", 3),\n                Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                false));\n        ControllerResult<Map<String, ApiError>> result = manager.updateFeatures(\n                updateMap(\"foo\", 2, \"bar\", 1), Collections.emptyMap(),\n                false);\n        Map<String, ApiError> expectedMap = new HashMap<>();\n        expectedMap.put(\"foo\", ApiError.NONE);\n        expectedMap.put(\"bar\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 1 for feature bar. Local controller 0 does not support this feature.\"));\n        assertEquals(expectedMap, result.response());\n        List<ApiMessageAndVersion> expectedMessages = new ArrayList<>();\n        expectedMessages.add(new ApiMessageAndVersion(new FeatureLevelRecord().\n            setName(\"foo\").setFeatureLevel((short) 2),\n            (short) 0));\n        assertEquals(expectedMessages, result.records());\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(-1);\n    }\n\n    @Test\n    public void testReplay() {\n        LogContext logContext = new LogContext();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        FeatureLevelRecord record = new FeatureLevelRecord().\n            setName(\"foo\").setFeatureLevel((short) 2);\n\n        idempotentCreateSnapshot(snapshotRegistry);\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n                setLogContext(logContext).\n                setQuorumFeatures(features(\"foo\", 1, 2)).\n                setSnapshotRegistry(snapshotRegistry).\n                setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n                build();\n        manager.replay(record);\n        snapshotRegistry.getOrCreateSnapshot(123);\n        assertEquals(new FinalizedControllerFeatures(versionMap(\"metadata.version\", 4, \"foo\", 2), 123),\n            manager.finalizedFeatures(123));\n    }\n\n    static ClusterFeatureSupportDescriber createFakeClusterFeatureSupportDescriber(\n        List<Map.Entry<Integer, Map<String, VersionRange>>> brokerRanges,\n        List<Map.Entry<Integer, Map<String, VersionRange>>> controllerRanges\n    ) {\n        return new ClusterFeatureSupportDescriber() {\n            @Override\n            public Iterator<Map.Entry<Integer, Map<String, VersionRange>>> brokerSupported() {\n                return brokerRanges.iterator();\n            }\n\n            @Override\n            public Iterator<Map.Entry<Integer, Map<String, VersionRange>>> controllerSupported() {\n                return controllerRanges.iterator();\n            }\n        };\n    }\n\n    @Test\n    public void testUpdateFeaturesErrorCases() {\n        LogContext logContext = new LogContext();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setLogContext(logContext).\n            setQuorumFeatures(features(\"foo\", 1, 5, \"bar\", 0, 3)).\n            setSnapshotRegistry(snapshotRegistry).\n            setClusterFeatureSupportDescriber(createFakeClusterFeatureSupportDescriber(\n                Collections.singletonList(new SimpleImmutableEntry<>(5, singletonMap(\"bar\", VersionRange.of(0, 3)))),\n                emptyList())).\n            build();\n\n        assertEquals(ControllerResult.atomicOf(emptyList(),\n            Collections.singletonMap(\"foo\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 3 for feature foo. Broker 5 does not support this feature.\"))),\n                    manager.updateFeatures(updateMap(\"foo\", 3),\n                        Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                        false));\n\n        ControllerResult<Map<String, ApiError>> result = manager.updateFeatures(\n            updateMap(\"bar\", 3), Collections.emptyMap(), false);\n        assertEquals(Collections.singletonMap(\"bar\", ApiError.NONE), result.response());\n        manager.replay((FeatureLevelRecord) result.records().get(0).message());\n        snapshotRegistry.getOrCreateSnapshot(3);\n\n        assertEquals(ControllerResult.atomicOf(emptyList(), Collections.\n                singletonMap(\"bar\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                    \"Invalid update version 2 for feature bar. Can't downgrade the version of this feature \" +\n                    \"without setting the upgrade type to either safe or unsafe downgrade.\"))),\n            manager.updateFeatures(updateMap(\"bar\", 2), Collections.emptyMap(), false));\n\n        assertEquals(\n            ControllerResult.atomicOf(\n                Collections.singletonList(\n                    new ApiMessageAndVersion(\n                        new FeatureLevelRecord()\n                            .setName(\"bar\")\n                            .setFeatureLevel((short) 2),\n                        (short) 0\n                    )\n                ),\n                Collections.singletonMap(\"bar\", ApiError.NONE)\n            ),\n            manager.updateFeatures(\n                updateMap(\"bar\", 2),\n                Collections.singletonMap(\"bar\", FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                false)\n        );\n    }\n\n    @Test\n    public void testReplayRecords() throws Exception {\n        LogContext logContext = new LogContext();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setLogContext(logContext).\n            setQuorumFeatures(features(\"foo\", 1, 5, \"bar\", 1, 2)).\n            setSnapshotRegistry(snapshotRegistry).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n            build();\n        ControllerResult<Map<String, ApiError>> result = manager.\n            updateFeatures(updateMap(\"foo\", 5, \"bar\", 1), Collections.emptyMap(), false);\n        RecordTestUtils.replayAll(manager, result.records());\n        assertEquals(MetadataVersion.IBP_3_3_IV0, manager.metadataVersion());\n        assertEquals(Optional.of((short) 5), manager.finalizedFeatures(Long.MAX_VALUE).get(\"foo\"));\n        assertEquals(Optional.of((short) 1), manager.finalizedFeatures(Long.MAX_VALUE).get(\"bar\"));\n        assertEquals(new HashSet<>(Arrays.asList(\n            MetadataVersion.FEATURE_NAME, \"foo\", \"bar\")),\n                manager.finalizedFeatures(Long.MAX_VALUE).featureNames());\n    }\n\n    private static final FeatureControlManager.Builder TEST_MANAGER_BUILDER1 =\n        new FeatureControlManager.Builder().\n            setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                MetadataVersion.IBP_3_3_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV3.featureLevel())).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV2);\n\n    @Test\n    public void testApplyMetadataVersionChangeRecord() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        manager.replay(new FeatureLevelRecord().\n            setName(MetadataVersion.FEATURE_NAME).\n            setFeatureLevel(MetadataVersion.IBP_3_3_IV3.featureLevel()));\n        assertEquals(MetadataVersion.IBP_3_3_IV3, manager.metadataVersion());\n    }\n\n    @Test\n    public void testCannotDowngradeToVersionBeforeMinimumSupportedKraftVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 3 for feature metadata.version. Local controller 0 only \" +\n                \"supports versions 4-7\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_2_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotDowngradeToHigherVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 7 for feature metadata.version. Can't downgrade to a \" +\n                \"newer version.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV3.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotUnsafeDowngradeToHigherVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 7 for feature metadata.version. Can't downgrade to a \" +\n                \"newer version.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV3.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotUpgradeToLowerVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 4 for feature metadata.version. Can't downgrade the \" +\n                \"version of this feature without setting the upgrade type to either safe or \" +\n                \"unsafe downgrade.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n                true));\n    }\n\n    @Test\n    public void testCanUpgradeToHigherVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, ApiError.NONE)),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV3.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotUseSafeDowngradeIfMetadataChanged() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid metadata.version 4. Refusing to perform the requested downgrade because \" +\n                \"it might delete metadata information.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testUnsafeDowngradeIsTemporarilyDisabled() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                                \"Invalid metadata.version 4. Unsafe metadata downgrade is not supported in this version.\"))),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Disabled\n    @Test\n    public void testCanUseUnsafeDowngradeIfMetadataChanged() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, ApiError.NONE)),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Test\n    public void testCanUseSafeDowngradeIfMetadataDidNotChange() {\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n                setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                        MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV1.featureLevel())).\n                setMetadataVersion(MetadataVersion.IBP_3_1_IV0).\n                setMinimumBootstrapVersion(MetadataVersion.IBP_3_0_IV0).\n                build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, ApiError.NONE)),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_0_IV1.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Test\n    public void testCannotDowngradeBefore3_3_IV0() {\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV3.featureLevel())).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n            build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                        \"Invalid metadata.version 3. Unable to set a metadata.version less than 3.3-IV0\"))),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_2_IV0.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Test\n    public void testCreateFeatureLevelRecords() {\n        Map<String, VersionRange> localSupportedFeatures = new HashMap<>();\n        localSupportedFeatures.put(MetadataVersion.FEATURE_NAME, VersionRange.of(\n            MetadataVersion.IBP_3_0_IV1.featureLevel(), MetadataVersion.latestTesting().featureLevel()));\n        localSupportedFeatures.put(\"foo\", VersionRange.of(0, 2));\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(new QuorumFeatures(0, localSupportedFeatures, emptyList())).\n            setClusterFeatureSupportDescriber(createFakeClusterFeatureSupportDescriber(\n                Collections.singletonList(new SimpleImmutableEntry<>(1, singletonMap(\"foo\", VersionRange.of(0, 3)))),\n                emptyList())).\n                build();\n        ControllerResult<Map<String, ApiError>> result  = manager.updateFeatures(\n                Collections.singletonMap(\"foo\", (short) 1),\n                Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.UPGRADE),\n                false);\n        assertEquals(ControllerResult.atomicOf(Collections.singletonList(new ApiMessageAndVersion(\n                new FeatureLevelRecord().setName(\"foo\").setFeatureLevel((short) 1), (short) 0)),\n                        Collections.singletonMap(\"foo\", ApiError.NONE)), result);\n        RecordTestUtils.replayAll(manager, result.records());\n        assertEquals(Optional.of((short) 1), manager.finalizedFeatures(Long.MAX_VALUE).get(\"foo\"));\n\n        ControllerResult<Map<String, ApiError>> result2  = manager.updateFeatures(\n                Collections.singletonMap(\"foo\", (short) 0),\n                Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                false);\n        assertEquals(ControllerResult.atomicOf(Collections.singletonList(new ApiMessageAndVersion(\n                        new FeatureLevelRecord().setName(\"foo\").setFeatureLevel((short) 0), (short) 0)),\n                Collections.singletonMap(\"foo\", ApiError.NONE)), result2);\n        RecordTestUtils.replayAll(manager, result2.records());\n        assertEquals(Optional.empty(), manager.finalizedFeatures(Long.MAX_VALUE).get(\"foo\"));\n    }\n\n    @Test\n    public void testNoMetadataVersionChangeDuringMigration() {\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                    MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_5_IV1.featureLevel())).\n            setMetadataVersion(MetadataVersion.IBP_3_4_IV0).\n            build();\n        BootstrapMetadata bootstrapMetadata = BootstrapMetadata.fromVersion(MetadataVersion.IBP_3_4_IV0, \"FeatureControlManagerTest\");\n        RecordTestUtils.replayAll(manager, bootstrapMetadata.records());\n        RecordTestUtils.replayOne(manager, ZkMigrationState.PRE_MIGRATION.toRecord());\n\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid metadata.version 10. Unable to modify metadata.version while a ZK migration is in progress.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_5_IV1.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n                true));\n\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid metadata.version 4. Unable to modify metadata.version while a ZK migration is in progress.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                true));\n\n        // Complete the migration\n        RecordTestUtils.replayOne(manager, ZkMigrationState.POST_MIGRATION.toRecord());\n        ControllerResult<Map<String, ApiError>> result = manager.updateFeatures(\n            singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_5_IV1.featureLevel()),\n            singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n            false);\n        assertEquals(Errors.NONE, result.response().get(MetadataVersion.FEATURE_NAME).error());\n        RecordTestUtils.replayAll(manager, result.records());\n        assertEquals(MetadataVersion.IBP_3_5_IV1, manager.metadataVersion());\n    }\n}",
                "methodCount": 25
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 4,
                "candidates": [
                    {
                        "lineStart": 84,
                        "lineEnd": 92,
                        "refactoringType": "MyMoveStaticMethodRefactoring",
                        "refactoringInfo": "Move Static method updateMap to class SslConfigsBuilder",
                        "description": "move method updateMap to PsiClass:SslConfigsBuilder\nRationale: The 'updateMap' method is more closely aligned with the responsibilities of the SslConfigsBuilder class, which involves constructing and managing SSL configurations, including operations on Map objects that hold configuration details. The TestSslUtils class primarily deals with various SSL utility methods that generate certificates and handle KeyStores, whereas the OAuthBearerValidationUtilsTest class is focused on testing OAuth Bearer token validation. Hence, the method should move to SslConfigsBuilder for better cohesion and relevance to the class responsibilities.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 68,
                        "lineEnd": 76,
                        "refactoringType": "MyMoveStaticMethodRefactoring",
                        "refactoringInfo": "Move Static method versionMap to class SslConfigsBuilder",
                        "description": "move method versionMap to PsiClass:SslConfigsBuilder\nRationale: The method `versionMap` is responsible for generating a map from given arguments, which seems to fit better in a configuration or utility class related to managing configurations or creating mappings. `SslConfigsBuilder` already handles various configurations and is responsible for building SSL configurations. Adding `versionMap` to this class maintains cohesion as it could be used to manage version configurations as well. Moreover, `SslConfigsBuilder` already has several static utility methods, making it a natural home for the `versionMap` method.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 56,
                        "lineEnd": 66,
                        "refactoringType": "MyMoveStaticMethodRefactoring",
                        "refactoringInfo": "Move Static method rangeMap to class ScramParser",
                        "description": "move method rangeMap to PsiClass:ScramParser\nRationale: The rangeMap method processes an array of arguments to create a map of version ranges, which aligns with ScramParser's responsibility for parsing and assembling data structures from input arguments. Integrating this method into ScramParser aligns with Single Responsibility Principle and enhances cohesion. Additionally, ScramParser already contains methods related to parsing and structuring data, making it a logical home for rangeMap.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 126,
                        "lineEnd": 128,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method `idempotentCreateSnapshot` relies directly and solely on the `SnapshotRegistry` class to function. Moving this method to `SnapshotRegistry` ensures that all snapshot-related functionality is encapsulated within the `SnapshotRegistry` class. This promotes better cohesion, encapsulation, and maintainability since all related methods are in a single class. The `Builder` class is primarily concerned with setting up dependencies and initializing the `FeatureControlManager`, and including snapshot creation logic there would violate single responsibility principles.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "rangeMap",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "versionMap",
                            "method_signature": "private static versionMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "features",
                            "method_signature": "public static features(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateMap",
                            "method_signature": "private static updateMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "features",
                            "method_signature": "public static features(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "rangeMap",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "versionMap",
                            "method_signature": "private static versionMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateMap",
                            "method_signature": "private static updateMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "features",
                            "method_signature": "public static features(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "rangeMap",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "versionMap",
                            "method_signature": "private static versionMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateMap",
                            "method_signature": "private static updateMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.25734306657093914
                },
                "public static features(Object... args)": {
                    "first": {
                        "method_name": "features",
                        "method_signature": "public static features(Object... args)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.41132873798477343
                },
                "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)": {
                    "first": {
                        "method_name": "rangeMap",
                        "method_signature": "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4943269859019432
                },
                "private static versionMap(Object... args)": {
                    "first": {
                        "method_name": "versionMap",
                        "method_signature": "private static versionMap(Object... args)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5196640759393861
                },
                "private static updateMap(Object... args)": {
                    "first": {
                        "method_name": "updateMap",
                        "method_signature": "private static updateMap(Object... args)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5196640759393861
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "private static updateMap(Object... args)",
                    "private static versionMap(Object... args)",
                    "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)",
                    "public static features(Object... args)",
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                ],
                "llm_response_time": 5994
            },
            "targetClassMap": {
                "updateMap": {
                    "target_classes": [
                        {
                            "class_name": "ControllerMetricsTestUtils",
                            "similarity_score": 0.5230682148205612
                        },
                        {
                            "class_name": "ControllerRequestContextUtil",
                            "similarity_score": 0.3235991778623843
                        },
                        {
                            "class_name": "QuorumControllerIntegrationTestUtils",
                            "similarity_score": 0.41778030532391136
                        },
                        {
                            "class_name": "TestSslUtils",
                            "similarity_score": 0.5728068308660601
                        },
                        {
                            "class_name": "ThreadUtils",
                            "similarity_score": 0.3021660931112009
                        },
                        {
                            "class_name": "ClaimValidationUtils",
                            "similarity_score": 0.08960530171006027
                        },
                        {
                            "class_name": "SecurityUtils",
                            "similarity_score": 0.5334870336969845
                        },
                        {
                            "class_name": "ClientTelemetryUtils",
                            "similarity_score": 0.47936195012271127
                        },
                        {
                            "class_name": "AdminClientTestUtils",
                            "similarity_score": 0.3024186401562943
                        },
                        {
                            "class_name": "AdminUtils",
                            "similarity_score": 0.2976216922434756
                        },
                        {
                            "class_name": "CommandLineUtils",
                            "similarity_score": 0.33168373363760056
                        },
                        {
                            "class_name": "ConsumerUtils",
                            "similarity_score": 0.4733106517221084
                        },
                        {
                            "class_name": "ConfigUtils",
                            "similarity_score": 0.31239418510690087
                        },
                        {
                            "class_name": "ControlRecordUtils",
                            "similarity_score": 0.4147272803911595
                        },
                        {
                            "class_name": "MessageUtil",
                            "similarity_score": 0.44424497091197873
                        },
                        {
                            "class_name": "FetchUtils",
                            "similarity_score": 0.05206086001129595
                        },
                        {
                            "class_name": "MetricsUtils",
                            "similarity_score": 0.2663780856520238
                        },
                        {
                            "class_name": "PropertiesUtils",
                            "similarity_score": 0.3392833828023668
                        },
                        {
                            "class_name": "RaftUtil",
                            "similarity_score": 0.40802503529583395
                        },
                        {
                            "class_name": "RequestTestUtils",
                            "similarity_score": 0.37494370536553573
                        },
                        {
                            "class_name": "FutureUtils",
                            "similarity_score": 0.20854541300380916
                        },
                        {
                            "class_name": "NetworkTestUtils",
                            "similarity_score": 0.48101535834564235
                        },
                        {
                            "class_name": "RecordsUtil",
                            "similarity_score": 0.39092786730806
                        },
                        {
                            "class_name": "RecordTestUtils",
                            "similarity_score": 0.4858468349007344
                        },
                        {
                            "class_name": "ProducerTestUtils",
                            "similarity_score": 0.34877513670672466
                        },
                        {
                            "class_name": "Utils",
                            "similarity_score": 0.3073339579117863
                        },
                        {
                            "class_name": "ByteUtils",
                            "similarity_score": 0.2230517779778567
                        },
                        {
                            "class_name": "OAuthBearerValidationUtils",
                            "similarity_score": 0.16559104438052208
                        },
                        {
                            "class_name": "NetworkClientUtils",
                            "similarity_score": 0.23531956447757144
                        },
                        {
                            "class_name": "ScramCredentialUtils",
                            "similarity_score": 0.5017763968150261
                        },
                        {
                            "class_name": "CollectionUtils",
                            "similarity_score": 0.33828585081362084
                        },
                        {
                            "class_name": "RequestUtils",
                            "similarity_score": 0.40228814150092784
                        },
                        {
                            "class_name": "OffsetsForLeaderEpochUtils",
                            "similarity_score": 0.439524041295631
                        },
                        {
                            "class_name": "OAuthBearerScopeUtils",
                            "similarity_score": 0.16798530752762428
                        },
                        {
                            "class_name": "JaasUtils",
                            "similarity_score": 0.42920157150778865
                        },
                        {
                            "class_name": "OAuthBearerValidationUtilsTest",
                            "similarity_score": 0.5537474559362661
                        },
                        {
                            "class_name": "JaasOptionsUtils",
                            "similarity_score": 0.4713582375702075
                        },
                        {
                            "class_name": "CommandLineUtilsTest",
                            "similarity_score": 0.32994621264709656
                        },
                        {
                            "class_name": "UtilsTest",
                            "similarity_score": 0.540720367870316
                        },
                        {
                            "class_name": "CertificateBuilder",
                            "similarity_score": 0.4687887344586328
                        },
                        {
                            "class_name": "TestSslEngineFactory",
                            "similarity_score": 0.32528969464480484
                        },
                        {
                            "class_name": "TestThroughAllIntermediateImagesLeadingToFinalImageHelper",
                            "similarity_score": 0.5088785581827483
                        },
                        {
                            "class_name": "ScramCredentialUtilsTest",
                            "similarity_score": 0.3929882209189315
                        },
                        {
                            "class_name": "ThreadUtilsTest",
                            "similarity_score": 0.4391168504129238
                        },
                        {
                            "class_name": "ClaimValidationUtilsTest",
                            "similarity_score": 0.3251029475173706
                        },
                        {
                            "class_name": "SecurityUtilsTest",
                            "similarity_score": 0.5019791449522171
                        },
                        {
                            "class_name": "ClientTelemetryUtilsTest",
                            "similarity_score": 0.3455765205731853
                        },
                        {
                            "class_name": "TopicIdPartitionComparator",
                            "similarity_score": 0.4256282653793743
                        },
                        {
                            "class_name": "TopicPartitionComparator",
                            "similarity_score": 0.4517351373863803
                        },
                        {
                            "class_name": "ByteUtilsTest",
                            "similarity_score": 0.39369128935422
                        },
                        {
                            "class_name": "CollectionUtilsTest",
                            "similarity_score": 0.41890312442585187
                        },
                        {
                            "class_name": "ConfigurationUtils",
                            "similarity_score": 0.24353919907812213
                        },
                        {
                            "class_name": "ConfigurationUtilsTest",
                            "similarity_score": 0.29704166429653484
                        },
                        {
                            "class_name": "ConfigUtilsTest",
                            "similarity_score": 0.44960356879387064
                        },
                        {
                            "class_name": "ControlRecordUtilsTest",
                            "similarity_score": 0.3431409510633933
                        },
                        {
                            "class_name": "MessageUtilTest",
                            "similarity_score": 0.2577849670652873
                        },
                        {
                            "class_name": "SslConfigsBuilder",
                            "similarity_score": 0.5712337931469162
                        },
                        {
                            "class_name": "MetricsUtilsTest",
                            "similarity_score": 0.26473451003726783
                        },
                        {
                            "class_name": "PropertiesUtilsTest",
                            "similarity_score": 0.41845020827082885
                        },
                        {
                            "class_name": "ProtoUtilsTest",
                            "similarity_score": 0.15574226979044098
                        },
                        {
                            "class_name": "FutureUtilsTest",
                            "similarity_score": 0.381101994652465
                        },
                        {
                            "class_name": "OAuthBearerScopeUtilsTest",
                            "similarity_score": 0.3307942673698644
                        },
                        {
                            "class_name": "JaasOptionsUtilsTest",
                            "similarity_score": 0.47312184713712263
                        },
                        {
                            "class_name": "ImageDeltaPair",
                            "similarity_score": 0.317380885407937
                        },
                        {
                            "class_name": "ActivationRecordsGenerator",
                            "similarity_score": 0.2593517248884098
                        },
                        {
                            "class_name": "ControllerExceptions",
                            "similarity_score": 0.22302163169549574
                        },
                        {
                            "class_name": "Sanitizer",
                            "similarity_score": 0.3070603914522629
                        },
                        {
                            "class_name": "SaslConfigs",
                            "similarity_score": 0.289398870169027
                        },
                        {
                            "class_name": "DirectoryId",
                            "similarity_score": 0.2682321468668362
                        },
                        {
                            "class_name": "ScramParser",
                            "similarity_score": 0.5447008699976473
                        },
                        {
                            "class_name": "AppInfoParser",
                            "similarity_score": 0.485656830852641
                        },
                        {
                            "class_name": "AccessTokenRetrieverFactory",
                            "similarity_score": 0.4174235549683609
                        },
                        {
                            "class_name": "AccessTokenValidatorFactory",
                            "similarity_score": 0.4859714909220032
                        },
                        {
                            "class_name": "Serdes",
                            "similarity_score": 0.3503802333879169
                        },
                        {
                            "class_name": "Serializer",
                            "similarity_score": 0.4516571961342602
                        },
                        {
                            "class_name": "AssignmentsHelper",
                            "similarity_score": 0.48665579620546473
                        },
                        {
                            "class_name": "ServerTopicConfigSynonyms",
                            "similarity_score": 0.31518112457883296
                        },
                        {
                            "class_name": "Topic",
                            "similarity_score": 0.39298820109646987
                        },
                        {
                            "class_name": "ShareGroupHelper",
                            "similarity_score": 0.12921165649395971
                        },
                        {
                            "class_name": "CommonClientConfigs",
                            "similarity_score": 0.290038120710786
                        },
                        {
                            "class_name": "CompressionRatioEstimator",
                            "similarity_score": 0.392516780601977
                        },
                        {
                            "class_name": "ConsumerProtocol",
                            "similarity_score": 0.44854244220185563
                        },
                        {
                            "class_name": "Exit",
                            "similarity_score": 0.3653015133788107
                        },
                        {
                            "class_name": "Snapshots",
                            "similarity_score": 0.5219806204409719
                        },
                        {
                            "class_name": "SslConfigs",
                            "similarity_score": 0.314980247692483
                        },
                        {
                            "class_name": "MetricsBench",
                            "similarity_score": 0.5116429598912723
                        },
                        {
                            "class_name": "Csv",
                            "similarity_score": 0.32314097407232784
                        },
                        {
                            "class_name": "TelemetryMetricNamingConvention",
                            "similarity_score": 0.26451974684311724
                        },
                        {
                            "class_name": "VerificationKeyResolverFactory",
                            "similarity_score": 0.43248439857877846
                        },
                        {
                            "class_name": "Protocol",
                            "similarity_score": 0.5431281914792646
                        },
                        {
                            "class_name": "OffsetSpec",
                            "similarity_score": 0.22847956398461716
                        },
                        {
                            "class_name": "Replicas",
                            "similarity_score": 0.36758317751884195
                        },
                        {
                            "class_name": "QuotaConfigs",
                            "similarity_score": 0.3561781420553535
                        },
                        {
                            "class_name": "PrimitiveRef",
                            "similarity_score": 0.3554280338863895
                        },
                        {
                            "class_name": "PartitionFactory",
                            "similarity_score": 0.3060982830947443
                        },
                        {
                            "class_name": "Json",
                            "similarity_score": 0.23324759035367781
                        },
                        {
                            "class_name": "BrokerToElrsTest",
                            "similarity_score": 0.35577779597974063
                        },
                        {
                            "class_name": "ChannelBuilders",
                            "similarity_score": 0.3378457167053045
                        },
                        {
                            "class_name": "Checksums",
                            "similarity_score": 0.37396456490472985
                        },
                        {
                            "class_name": "ByteBufferUnmapper",
                            "similarity_score": 0.444128806861606
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SslConfigsBuilder",
                        "TestSslUtils",
                        "OAuthBearerValidationUtilsTest"
                    ],
                    "llm_response_time": 5435,
                    "similarity_computation_time": 21,
                    "similarity_metric": "cosine"
                },
                "versionMap": {
                    "target_classes": [
                        {
                            "class_name": "ControllerMetricsTestUtils",
                            "similarity_score": 0.5230682148205612
                        },
                        {
                            "class_name": "ControllerRequestContextUtil",
                            "similarity_score": 0.3235991778623843
                        },
                        {
                            "class_name": "QuorumControllerIntegrationTestUtils",
                            "similarity_score": 0.41778030532391136
                        },
                        {
                            "class_name": "ConsumerUtils",
                            "similarity_score": 0.4733106517221084
                        },
                        {
                            "class_name": "ControlRecordUtils",
                            "similarity_score": 0.4147272803911595
                        },
                        {
                            "class_name": "FetchUtils",
                            "similarity_score": 0.05206086001129595
                        },
                        {
                            "class_name": "ClaimValidationUtils",
                            "similarity_score": 0.08960530171006027
                        },
                        {
                            "class_name": "ClientTelemetryUtils",
                            "similarity_score": 0.47936195012271127
                        },
                        {
                            "class_name": "AdminClientTestUtils",
                            "similarity_score": 0.3024186401562943
                        },
                        {
                            "class_name": "AdminUtils",
                            "similarity_score": 0.2976216922434756
                        },
                        {
                            "class_name": "CommandLineUtils",
                            "similarity_score": 0.33168373363760056
                        },
                        {
                            "class_name": "ConfigUtils",
                            "similarity_score": 0.31239418510690087
                        },
                        {
                            "class_name": "FutureUtils",
                            "similarity_score": 0.20854541300380916
                        },
                        {
                            "class_name": "MessageUtil",
                            "similarity_score": 0.44424497091197873
                        },
                        {
                            "class_name": "MetricsUtils",
                            "similarity_score": 0.2663780856520238
                        },
                        {
                            "class_name": "PropertiesUtils",
                            "similarity_score": 0.3392833828023668
                        },
                        {
                            "class_name": "TestSslUtils",
                            "similarity_score": 0.5728068308660601
                        },
                        {
                            "class_name": "ThreadUtils",
                            "similarity_score": 0.3021660931112009
                        },
                        {
                            "class_name": "RaftUtil",
                            "similarity_score": 0.40802503529583395
                        },
                        {
                            "class_name": "SecurityUtils",
                            "similarity_score": 0.5334870336969845
                        },
                        {
                            "class_name": "RecordsUtil",
                            "similarity_score": 0.39092786730806
                        },
                        {
                            "class_name": "RecordTestUtils",
                            "similarity_score": 0.4858468349007344
                        },
                        {
                            "class_name": "NetworkTestUtils",
                            "similarity_score": 0.48101535834564235
                        },
                        {
                            "class_name": "RequestTestUtils",
                            "similarity_score": 0.37494370536553573
                        },
                        {
                            "class_name": "ProducerTestUtils",
                            "similarity_score": 0.34877513670672466
                        },
                        {
                            "class_name": "Utils",
                            "similarity_score": 0.3073339579117863
                        },
                        {
                            "class_name": "ByteUtils",
                            "similarity_score": 0.2230517779778567
                        },
                        {
                            "class_name": "OAuthBearerValidationUtils",
                            "similarity_score": 0.16559104438052208
                        },
                        {
                            "class_name": "NetworkClientUtils",
                            "similarity_score": 0.23531956447757144
                        },
                        {
                            "class_name": "CollectionUtils",
                            "similarity_score": 0.33828585081362084
                        },
                        {
                            "class_name": "ScramCredentialUtils",
                            "similarity_score": 0.5017763968150261
                        },
                        {
                            "class_name": "RequestUtils",
                            "similarity_score": 0.40228814150092784
                        },
                        {
                            "class_name": "JaasUtils",
                            "similarity_score": 0.42920157150778865
                        },
                        {
                            "class_name": "OffsetsForLeaderEpochUtils",
                            "similarity_score": 0.439524041295631
                        },
                        {
                            "class_name": "OAuthBearerScopeUtils",
                            "similarity_score": 0.16798530752762428
                        },
                        {
                            "class_name": "OAuthBearerValidationUtilsTest",
                            "similarity_score": 0.5537474559362661
                        },
                        {
                            "class_name": "JaasOptionsUtils",
                            "similarity_score": 0.4713582375702075
                        },
                        {
                            "class_name": "CommandLineUtilsTest",
                            "similarity_score": 0.32994621264709656
                        },
                        {
                            "class_name": "UtilsTest",
                            "similarity_score": 0.540720367870316
                        },
                        {
                            "class_name": "CertificateBuilder",
                            "similarity_score": 0.4687887344586328
                        },
                        {
                            "class_name": "ControlRecordUtilsTest",
                            "similarity_score": 0.3431409510633933
                        },
                        {
                            "class_name": "ClaimValidationUtilsTest",
                            "similarity_score": 0.3251029475173706
                        },
                        {
                            "class_name": "ClientTelemetryUtilsTest",
                            "similarity_score": 0.3455765205731853
                        },
                        {
                            "class_name": "CollectionUtilsTest",
                            "similarity_score": 0.41890312442585187
                        },
                        {
                            "class_name": "ImageDeltaPair",
                            "similarity_score": 0.317380885407937
                        },
                        {
                            "class_name": "ConfigurationUtils",
                            "similarity_score": 0.24353919907812213
                        },
                        {
                            "class_name": "ConfigurationUtilsTest",
                            "similarity_score": 0.29704166429653484
                        },
                        {
                            "class_name": "ConfigUtilsTest",
                            "similarity_score": 0.44960356879387064
                        },
                        {
                            "class_name": "ByteUtilsTest",
                            "similarity_score": 0.39369128935422
                        },
                        {
                            "class_name": "FutureUtilsTest",
                            "similarity_score": 0.381101994652465
                        },
                        {
                            "class_name": "MessageUtilTest",
                            "similarity_score": 0.2577849670652873
                        },
                        {
                            "class_name": "MetricsUtilsTest",
                            "similarity_score": 0.26473451003726783
                        },
                        {
                            "class_name": "JaasOptionsUtilsTest",
                            "similarity_score": 0.47312184713712263
                        },
                        {
                            "class_name": "PropertiesUtilsTest",
                            "similarity_score": 0.41845020827082885
                        },
                        {
                            "class_name": "TestSslEngineFactory",
                            "similarity_score": 0.32528969464480484
                        },
                        {
                            "class_name": "TestThroughAllIntermediateImagesLeadingToFinalImageHelper",
                            "similarity_score": 0.5088785581827483
                        },
                        {
                            "class_name": "ProtoUtilsTest",
                            "similarity_score": 0.15574226979044098
                        },
                        {
                            "class_name": "ThreadUtilsTest",
                            "similarity_score": 0.4391168504129238
                        },
                        {
                            "class_name": "ScramCredentialUtilsTest",
                            "similarity_score": 0.3929882209189315
                        },
                        {
                            "class_name": "SecurityUtilsTest",
                            "similarity_score": 0.5019791449522171
                        },
                        {
                            "class_name": "TopicIdPartitionComparator",
                            "similarity_score": 0.4256282653793743
                        },
                        {
                            "class_name": "TopicPartitionComparator",
                            "similarity_score": 0.4517351373863803
                        },
                        {
                            "class_name": "SslConfigsBuilder",
                            "similarity_score": 0.5712337931469162
                        },
                        {
                            "class_name": "OAuthBearerScopeUtilsTest",
                            "similarity_score": 0.3307942673698644
                        },
                        {
                            "class_name": "ControllerExceptions",
                            "similarity_score": 0.22302163169549574
                        },
                        {
                            "class_name": "ActivationRecordsGenerator",
                            "similarity_score": 0.2593517248884098
                        },
                        {
                            "class_name": "Exit",
                            "similarity_score": 0.3653015133788107
                        },
                        {
                            "class_name": "ConsumerProtocol",
                            "similarity_score": 0.44854244220185563
                        },
                        {
                            "class_name": "DirectoryId",
                            "similarity_score": 0.2682321468668362
                        },
                        {
                            "class_name": "Csv",
                            "similarity_score": 0.32314097407232784
                        },
                        {
                            "class_name": "AccessTokenRetrieverFactory",
                            "similarity_score": 0.4174235549683609
                        },
                        {
                            "class_name": "AccessTokenValidatorFactory",
                            "similarity_score": 0.4859714909220032
                        },
                        {
                            "class_name": "CompressionRatioEstimator",
                            "similarity_score": 0.392516780601977
                        },
                        {
                            "class_name": "CommonClientConfigs",
                            "similarity_score": 0.290038120710786
                        },
                        {
                            "class_name": "AppInfoParser",
                            "similarity_score": 0.485656830852641
                        },
                        {
                            "class_name": "AssignmentsHelper",
                            "similarity_score": 0.48665579620546473
                        },
                        {
                            "class_name": "Json",
                            "similarity_score": 0.23324759035367781
                        },
                        {
                            "class_name": "MetricsBench",
                            "similarity_score": 0.5116429598912723
                        },
                        {
                            "class_name": "Protocol",
                            "similarity_score": 0.5431281914792646
                        },
                        {
                            "class_name": "OffsetSpec",
                            "similarity_score": 0.22847956398461716
                        },
                        {
                            "class_name": "Sanitizer",
                            "similarity_score": 0.3070603914522629
                        },
                        {
                            "class_name": "SaslConfigs",
                            "similarity_score": 0.289398870169027
                        },
                        {
                            "class_name": "ScramParser",
                            "similarity_score": 0.5447008699976473
                        },
                        {
                            "class_name": "QuotaConfigs",
                            "similarity_score": 0.3561781420553535
                        },
                        {
                            "class_name": "VerificationKeyResolverFactory",
                            "similarity_score": 0.43248439857877846
                        },
                        {
                            "class_name": "Serdes",
                            "similarity_score": 0.3503802333879169
                        },
                        {
                            "class_name": "Serializer",
                            "similarity_score": 0.4516571961342602
                        },
                        {
                            "class_name": "Topic",
                            "similarity_score": 0.39298820109646987
                        },
                        {
                            "class_name": "ServerTopicConfigSynonyms",
                            "similarity_score": 0.31518112457883296
                        },
                        {
                            "class_name": "PartitionFactory",
                            "similarity_score": 0.3060982830947443
                        },
                        {
                            "class_name": "ShareGroupHelper",
                            "similarity_score": 0.12921165649395971
                        },
                        {
                            "class_name": "SslConfigs",
                            "similarity_score": 0.314980247692483
                        },
                        {
                            "class_name": "Replicas",
                            "similarity_score": 0.36758317751884195
                        },
                        {
                            "class_name": "PrimitiveRef",
                            "similarity_score": 0.3554280338863895
                        },
                        {
                            "class_name": "TelemetryMetricNamingConvention",
                            "similarity_score": 0.26451974684311724
                        },
                        {
                            "class_name": "Snapshots",
                            "similarity_score": 0.5219806204409719
                        },
                        {
                            "class_name": "BrokerToElrsTest",
                            "similarity_score": 0.35577779597974063
                        },
                        {
                            "class_name": "ChannelBuilders",
                            "similarity_score": 0.3378457167053045
                        },
                        {
                            "class_name": "Checksums",
                            "similarity_score": 0.37396456490472985
                        },
                        {
                            "class_name": "ByteBufferUnmapper",
                            "similarity_score": 0.444128806861606
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SslConfigsBuilder",
                        "TestSslUtils",
                        "OAuthBearerValidationUtilsTest"
                    ],
                    "llm_response_time": 5693,
                    "similarity_computation_time": 22,
                    "similarity_metric": "cosine"
                },
                "rangeMap": {
                    "target_classes": [
                        {
                            "class_name": "ControllerMetricsTestUtils",
                            "similarity_score": 0.490825920794188
                        },
                        {
                            "class_name": "ControllerRequestContextUtil",
                            "similarity_score": 0.28833765854913374
                        },
                        {
                            "class_name": "QuorumControllerIntegrationTestUtils",
                            "similarity_score": 0.4024668898971298
                        },
                        {
                            "class_name": "PropertiesUtils",
                            "similarity_score": 0.32556763957029866
                        },
                        {
                            "class_name": "RecordTestUtils",
                            "similarity_score": 0.4517693740509775
                        },
                        {
                            "class_name": "PropertiesUtilsTest",
                            "similarity_score": 0.4006905375132152
                        },
                        {
                            "class_name": "TestThroughAllIntermediateImagesLeadingToFinalImageHelper",
                            "similarity_score": 0.48639149973351714
                        },
                        {
                            "class_name": "ImageDeltaPair",
                            "similarity_score": 0.2906732603092963
                        },
                        {
                            "class_name": "ActivationRecordsGenerator",
                            "similarity_score": 0.24689213193069717
                        },
                        {
                            "class_name": "ControllerExceptions",
                            "similarity_score": 0.2011333106794384
                        },
                        {
                            "class_name": "ScramParser",
                            "similarity_score": 0.5397612556628663
                        },
                        {
                            "class_name": "Replicas",
                            "similarity_score": 0.3444564427364337
                        },
                        {
                            "class_name": "BrokerToElrsTest",
                            "similarity_score": 0.326207872833152
                        },
                        {
                            "class_name": "ClientQuotaControlManager",
                            "similarity_score": 0.48150809531455746
                        },
                        {
                            "class_name": "ControllerMetadataMetricsPublisherTest",
                            "similarity_score": 0.39219806645940514
                        },
                        {
                            "class_name": "BrokersToIsrsTest",
                            "similarity_score": 0.2597288702942012
                        },
                        {
                            "class_name": "ScramControlManager",
                            "similarity_score": 0.5047690255303079
                        },
                        {
                            "class_name": "StripedReplicaPlacer",
                            "similarity_score": 0.26898082181806393
                        },
                        {
                            "class_name": "ClientQuotaControlManagerTest",
                            "similarity_score": 0.3924124960571338
                        },
                        {
                            "class_name": "QuorumControllerMetricsTest",
                            "similarity_score": 0.5281082552729583
                        },
                        {
                            "class_name": "ScramImageTest",
                            "similarity_score": 0.5073554423970249
                        },
                        {
                            "class_name": "AclsImageTest",
                            "similarity_score": 0.5328370302677876
                        },
                        {
                            "class_name": "PartitionChangeBuilderTest",
                            "similarity_score": 0.46346240467307653
                        },
                        {
                            "class_name": "ProducerIdsImageTest",
                            "similarity_score": 0.3865422274694206
                        },
                        {
                            "class_name": "MetadataImageTest",
                            "similarity_score": 0.3767545863286342
                        },
                        {
                            "class_name": "ClientQuotasImageNode",
                            "similarity_score": 0.5318624039693715
                        },
                        {
                            "class_name": "ClientQuotasImageTest",
                            "similarity_score": 0.46842593482303035
                        },
                        {
                            "class_name": "ConfigurationsImageTest",
                            "similarity_score": 0.4341478589590008
                        },
                        {
                            "class_name": "ConfigurationControlManagerTest",
                            "similarity_score": 0.43285715704590727
                        },
                        {
                            "class_name": "PartitionReassignmentReplicas",
                            "similarity_score": 0.47236654654100074
                        },
                        {
                            "class_name": "DelegationTokenImageTest",
                            "similarity_score": 0.47725358990182626
                        },
                        {
                            "class_name": "AclControlManager",
                            "similarity_score": 0.4883524617908616
                        },
                        {
                            "class_name": "ControllerResult",
                            "similarity_score": 0.3332624038010199
                        },
                        {
                            "class_name": "BrokerHeartbeatManagerTest",
                            "similarity_score": 0.4143886436312194
                        },
                        {
                            "class_name": "ResultOrError",
                            "similarity_score": 0.38640580200395774
                        },
                        {
                            "class_name": "EventHandlerExceptionInfo",
                            "similarity_score": 0.31798881840324217
                        },
                        {
                            "class_name": "ControllerMetricsChangesTest",
                            "similarity_score": 0.3444351279257109
                        },
                        {
                            "class_name": "ControllerResultAndOffset",
                            "similarity_score": 0.30606706665315775
                        },
                        {
                            "class_name": "QuorumFeatures",
                            "similarity_score": 0.4676299059669639
                        },
                        {
                            "class_name": "ListenerInfo",
                            "similarity_score": 0.32156712288340583
                        },
                        {
                            "class_name": "ProducerIdControlManagerTest",
                            "similarity_score": 0.46249441560786475
                        },
                        {
                            "class_name": "QuorumControllerTest",
                            "similarity_score": 0.5231275986653078
                        },
                        {
                            "class_name": "TopicsImageTest",
                            "similarity_score": 0.4274748652364346
                        },
                        {
                            "class_name": "FeaturesImageTest",
                            "similarity_score": 0.4256432064749635
                        },
                        {
                            "class_name": "ClusterImageTest",
                            "similarity_score": 0.3633567713966603
                        },
                        {
                            "class_name": "ControllerRequestContext",
                            "similarity_score": 0.2891385481295669
                        },
                        {
                            "class_name": "ControllerMetadataMetricsTest",
                            "similarity_score": 0.26235446452512573
                        },
                        {
                            "class_name": "ControllerExceptionsTest",
                            "similarity_score": 0.2223965741461595
                        },
                        {
                            "class_name": "ReplicationControlManager",
                            "similarity_score": 0.4944428785628315
                        },
                        {
                            "class_name": "ImageDowngradeTest",
                            "similarity_score": 0.3098643139083865
                        },
                        {
                            "class_name": "BootstrapMetadata",
                            "similarity_score": 0.4496604085902668
                        },
                        {
                            "class_name": "DelegationTokenControlManager",
                            "similarity_score": 0.5274386119366898
                        },
                        {
                            "class_name": "ReplicationControlManagerTest",
                            "similarity_score": 0.4886216687796293
                        },
                        {
                            "class_name": "OffsetControlManagerTest",
                            "similarity_score": 0.32354436937971476
                        },
                        {
                            "class_name": "ClusterControlManagerTest",
                            "similarity_score": 0.4690519693274538
                        },
                        {
                            "class_name": "StandardAuthorizerData",
                            "similarity_score": 0.4101573907472663
                        },
                        {
                            "class_name": "QuorumControllerMetrics",
                            "similarity_score": 0.41473350569819456
                        },
                        {
                            "class_name": "ControllerMetadataMetrics",
                            "similarity_score": 0.3676506025299547
                        },
                        {
                            "class_name": "PartitionChangeBuilder",
                            "similarity_score": 0.3804947214517758
                        },
                        {
                            "class_name": "QuorumController",
                            "similarity_score": 0.4513821378847811
                        },
                        {
                            "class_name": "TestExistenceChecker",
                            "similarity_score": 0.3159413277642021
                        },
                        {
                            "class_name": "OffsetControlManager",
                            "similarity_score": 0.38691807264668643
                        },
                        {
                            "class_name": "MigrationRecordConsumer",
                            "similarity_score": 0.36676874353846095
                        },
                        {
                            "class_name": "MigrationWriteOperation",
                            "similarity_score": 0.3654236656029014
                        },
                        {
                            "class_name": "ScramCredentialDataNode",
                            "similarity_score": 0.31437751023734767
                        },
                        {
                            "class_name": "ScramCredentialKey",
                            "similarity_score": 0.40213532475552316
                        },
                        {
                            "class_name": "ScramCredentialValue",
                            "similarity_score": 0.38258351848376515
                        },
                        {
                            "class_name": "ScramCredentialValueTest",
                            "similarity_score": 0.42148902413687467
                        },
                        {
                            "class_name": "QuorumClusterFeatureSupportDescriber",
                            "similarity_score": 0.2625569846962102
                        },
                        {
                            "class_name": "QuorumControllerMetricsIntegrationTest",
                            "similarity_score": 0.4130644138426687
                        },
                        {
                            "class_name": "MockAclMutator",
                            "similarity_score": 0.36623560618118794
                        },
                        {
                            "class_name": "QuorumControllerTestEnv",
                            "similarity_score": 0.4807879507844991
                        },
                        {
                            "class_name": "QuorumFeaturesTest",
                            "similarity_score": 0.42088276210118825
                        },
                        {
                            "class_name": "QuorumMetaLogListener",
                            "similarity_score": 0.4024600404344965
                        },
                        {
                            "class_name": "MockClusterMetadataAuthorizer",
                            "similarity_score": 0.21401432573941534
                        },
                        {
                            "class_name": "MockControllerMetrics",
                            "similarity_score": 0.3138032019216492
                        },
                        {
                            "class_name": "ReadyBrokersFuture",
                            "similarity_score": 0.3922695246118537
                        },
                        {
                            "class_name": "AclControlManagerTest",
                            "similarity_score": 0.4223052188719406
                        },
                        {
                            "class_name": "ActivationRecordsGeneratorTest",
                            "similarity_score": 0.19405971327997806
                        },
                        {
                            "class_name": "TopicControlInfo",
                            "similarity_score": 0.3802379437965301
                        },
                        {
                            "class_name": "PartitionAssignmentTest",
                            "similarity_score": 0.4652434428686902
                        },
                        {
                            "class_name": "ProducerIdControlManager",
                            "similarity_score": 0.5034197562351374
                        },
                        {
                            "class_name": "PartitionReassignmentReplicasTest",
                            "similarity_score": 0.25167237936472336
                        },
                        {
                            "class_name": "PartitionReassignmentRevert",
                            "similarity_score": 0.32720362820116433
                        },
                        {
                            "class_name": "PartitionReassignmentRevertTest",
                            "similarity_score": 0.25548560716243884
                        },
                        {
                            "class_name": "PartitionRegistration",
                            "similarity_score": 0.45856235371366
                        },
                        {
                            "class_name": "PartitionsOnReplicaIterator",
                            "similarity_score": 0.5271768022848743
                        },
                        {
                            "class_name": "LogReplayTracker",
                            "similarity_score": 0.38348902172974136
                        },
                        {
                            "class_name": "LogReplayTrackerTest",
                            "similarity_score": 0.351972576457172
                        },
                        {
                            "class_name": "MetadataOffsetComparator",
                            "similarity_score": 0.3124262542812672
                        },
                        {
                            "class_name": "ResultOrErrorTest",
                            "similarity_score": 0.3514651270927317
                        },
                        {
                            "class_name": "UsableBrokerIterator",
                            "similarity_score": 0.49203142821393586
                        },
                        {
                            "class_name": "FeatureControlManager",
                            "similarity_score": 0.5055858266909106
                        },
                        {
                            "class_name": "ElectionResult",
                            "similarity_score": 0.32616403652672105
                        },
                        {
                            "class_name": "StandardAclTest",
                            "similarity_score": 0.40037142663355235
                        },
                        {
                            "class_name": "InitialSnapshot",
                            "similarity_score": 0.389648026364433
                        },
                        {
                            "class_name": "VersionRangeTest",
                            "similarity_score": 0.187594426652375
                        },
                        {
                            "class_name": "KafkaConfigSchemaTest",
                            "similarity_score": 0.36873950564456526
                        },
                        {
                            "class_name": "EventHandlerExceptionInfoTest",
                            "similarity_score": 0.28693857265149986
                        },
                        {
                            "class_name": "KRaftClusterDescriber",
                            "similarity_score": 0.2619732711758709
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "ScramParser",
                        "ClientQuotasImageNode",
                        "AclsImageTest"
                    ],
                    "llm_response_time": 3819,
                    "similarity_computation_time": 28,
                    "similarity_metric": "cosine"
                },
                "features": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2007,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        },
                        {
                            "class_name": "Builder",
                            "similarity_score": 0.3710212605396185
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry",
                        "Builder"
                    ],
                    "llm_response_time": 3659,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testReplay() : void in class org.apache.kafka.controller.FeatureControlManagerTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 127,
                    "endLine": 145,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testReplay() : void"
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 134,
                    "endLine": 134,
                    "startColumn": 9,
                    "endColumn": 50,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 126,
                    "endLine": 144,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testReplay() : void"
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 133,
                    "endLine": 133,
                    "startColumn": 9,
                    "endColumn": 54,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(-1)"
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 141,
                    "endLine": 141,
                    "startColumn": 9,
                    "endColumn": 55,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(123)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 628,
        "extraction_results": {
            "success": true,
            "newCommitHash": "8f45b80fb602932dd6473cbc2a55946fd956e4b5",
            "newBranchName": "extract-idempotentCreateSnapshot-testReplay-130af38"
        },
        "telemetry": {
            "id": "8bc9acfb-eb35-4e78-948e-01a42d856f4e",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 405,
                "lineStart": 54,
                "lineEnd": 458,
                "bodyLineStart": 54,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                "sourceCode": "@Timeout(value = 40)\npublic class FeatureControlManagerTest {\n\n    @SuppressWarnings(\"unchecked\")\n    private static Map<String, VersionRange> rangeMap(Object... args) {\n        Map<String, VersionRange> result = new HashMap<>();\n        for (int i = 0; i < args.length; i += 3) {\n            String feature = (String) args[i];\n            Number low = (Number) args[i + 1];\n            Number high = (Number) args[i + 2];\n            result.put(feature, VersionRange.of(low.shortValue(), high.shortValue()));\n        }\n        return result;\n    }\n\n    private static Map<String, Short> versionMap(Object... args) {\n        Map<String, Short> result = new HashMap<>();\n        for (int i = 0; i < args.length; i += 2) {\n            String feature = (String) args[i];\n            Number ver = (Number) args[i + 1];\n            result.put(feature, ver.shortValue());\n        }\n        return result;\n    }\n\n    public static QuorumFeatures features(Object... args) {\n        Map<String, VersionRange> features = QuorumFeatures.defaultFeatureMap(true);\n        features.putAll(rangeMap(args));\n        return new QuorumFeatures(0, features, emptyList());\n    }\n\n    private static Map<String, Short> updateMap(Object... args) {\n        Map<String, Short> result = new HashMap<>();\n        for (int i = 0; i < args.length; i += 2) {\n            String feature = (String) args[i];\n            Number ver = (Number) args[i + 1];\n            result.put(feature, ver.shortValue());\n        }\n        return result;\n    }\n\n    @Test\n    public void testUpdateFeatures() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(features(\"foo\", 1, 2)).\n            setSnapshotRegistry(snapshotRegistry).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n            build();\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(new FinalizedControllerFeatures(Collections.singletonMap(\"metadata.version\", (short) 4), -1),\n            manager.finalizedFeatures(-1));\n        assertEquals(ControllerResult.atomicOf(emptyList(), Collections.\n                singletonMap(\"foo\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                    \"Invalid update version 3 for feature foo. Local controller 0 only supports versions 1-2\"))),\n            manager.updateFeatures(updateMap(\"foo\", 3),\n                Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                false));\n        ControllerResult<Map<String, ApiError>> result = manager.updateFeatures(\n                updateMap(\"foo\", 2, \"bar\", 1), Collections.emptyMap(),\n                false);\n        Map<String, ApiError> expectedMap = new HashMap<>();\n        expectedMap.put(\"foo\", ApiError.NONE);\n        expectedMap.put(\"bar\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 1 for feature bar. Local controller 0 does not support this feature.\"));\n        assertEquals(expectedMap, result.response());\n        List<ApiMessageAndVersion> expectedMessages = new ArrayList<>();\n        expectedMessages.add(new ApiMessageAndVersion(new FeatureLevelRecord().\n            setName(\"foo\").setFeatureLevel((short) 2),\n            (short) 0));\n        assertEquals(expectedMessages, result.records());\n    }\n\n    @Test\n    public void testReplay() {\n        LogContext logContext = new LogContext();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        FeatureLevelRecord record = new FeatureLevelRecord().\n            setName(\"foo\").setFeatureLevel((short) 2);\n\n        idempotentCreateSnapshot(snapshotRegistry);\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n                setLogContext(logContext).\n                setQuorumFeatures(features(\"foo\", 1, 2)).\n                setSnapshotRegistry(snapshotRegistry).\n                setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n                build();\n        manager.replay(record);\n        snapshotRegistry.getOrCreateSnapshot(123);\n        assertEquals(new FinalizedControllerFeatures(versionMap(\"metadata.version\", 4, \"foo\", 2), 123),\n            manager.finalizedFeatures(123));\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(-1);\n    }\n\n    static ClusterFeatureSupportDescriber createFakeClusterFeatureSupportDescriber(\n        List<Map.Entry<Integer, Map<String, VersionRange>>> brokerRanges,\n        List<Map.Entry<Integer, Map<String, VersionRange>>> controllerRanges\n    ) {\n        return new ClusterFeatureSupportDescriber() {\n            @Override\n            public Iterator<Map.Entry<Integer, Map<String, VersionRange>>> brokerSupported() {\n                return brokerRanges.iterator();\n            }\n\n            @Override\n            public Iterator<Map.Entry<Integer, Map<String, VersionRange>>> controllerSupported() {\n                return controllerRanges.iterator();\n            }\n        };\n    }\n\n    @Test\n    public void testUpdateFeaturesErrorCases() {\n        LogContext logContext = new LogContext();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setLogContext(logContext).\n            setQuorumFeatures(features(\"foo\", 1, 5, \"bar\", 0, 3)).\n            setSnapshotRegistry(snapshotRegistry).\n            setClusterFeatureSupportDescriber(createFakeClusterFeatureSupportDescriber(\n                Collections.singletonList(new SimpleImmutableEntry<>(5, singletonMap(\"bar\", VersionRange.of(0, 3)))),\n                emptyList())).\n            build();\n\n        assertEquals(ControllerResult.atomicOf(emptyList(),\n            Collections.singletonMap(\"foo\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 3 for feature foo. Broker 5 does not support this feature.\"))),\n                    manager.updateFeatures(updateMap(\"foo\", 3),\n                        Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                        false));\n\n        ControllerResult<Map<String, ApiError>> result = manager.updateFeatures(\n            updateMap(\"bar\", 3), Collections.emptyMap(), false);\n        assertEquals(Collections.singletonMap(\"bar\", ApiError.NONE), result.response());\n        manager.replay((FeatureLevelRecord) result.records().get(0).message());\n        snapshotRegistry.getOrCreateSnapshot(3);\n\n        assertEquals(ControllerResult.atomicOf(emptyList(), Collections.\n                singletonMap(\"bar\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                    \"Invalid update version 2 for feature bar. Can't downgrade the version of this feature \" +\n                    \"without setting the upgrade type to either safe or unsafe downgrade.\"))),\n            manager.updateFeatures(updateMap(\"bar\", 2), Collections.emptyMap(), false));\n\n        assertEquals(\n            ControllerResult.atomicOf(\n                Collections.singletonList(\n                    new ApiMessageAndVersion(\n                        new FeatureLevelRecord()\n                            .setName(\"bar\")\n                            .setFeatureLevel((short) 2),\n                        (short) 0\n                    )\n                ),\n                Collections.singletonMap(\"bar\", ApiError.NONE)\n            ),\n            manager.updateFeatures(\n                updateMap(\"bar\", 2),\n                Collections.singletonMap(\"bar\", FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                false)\n        );\n    }\n\n    @Test\n    public void testReplayRecords() throws Exception {\n        LogContext logContext = new LogContext();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setLogContext(logContext).\n            setQuorumFeatures(features(\"foo\", 1, 5, \"bar\", 1, 2)).\n            setSnapshotRegistry(snapshotRegistry).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n            build();\n        ControllerResult<Map<String, ApiError>> result = manager.\n            updateFeatures(updateMap(\"foo\", 5, \"bar\", 1), Collections.emptyMap(), false);\n        RecordTestUtils.replayAll(manager, result.records());\n        assertEquals(MetadataVersion.IBP_3_3_IV0, manager.metadataVersion());\n        assertEquals(Optional.of((short) 5), manager.finalizedFeatures(Long.MAX_VALUE).get(\"foo\"));\n        assertEquals(Optional.of((short) 1), manager.finalizedFeatures(Long.MAX_VALUE).get(\"bar\"));\n        assertEquals(new HashSet<>(Arrays.asList(\n            MetadataVersion.FEATURE_NAME, \"foo\", \"bar\")),\n                manager.finalizedFeatures(Long.MAX_VALUE).featureNames());\n    }\n\n    private static final FeatureControlManager.Builder TEST_MANAGER_BUILDER1 =\n        new FeatureControlManager.Builder().\n            setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                MetadataVersion.IBP_3_3_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV3.featureLevel())).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV2);\n\n    @Test\n    public void testApplyMetadataVersionChangeRecord() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        manager.replay(new FeatureLevelRecord().\n            setName(MetadataVersion.FEATURE_NAME).\n            setFeatureLevel(MetadataVersion.IBP_3_3_IV3.featureLevel()));\n        assertEquals(MetadataVersion.IBP_3_3_IV3, manager.metadataVersion());\n    }\n\n    @Test\n    public void testCannotDowngradeToVersionBeforeMinimumSupportedKraftVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 3 for feature metadata.version. Local controller 0 only \" +\n                \"supports versions 4-7\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_2_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotDowngradeToHigherVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 7 for feature metadata.version. Can't downgrade to a \" +\n                \"newer version.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV3.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotUnsafeDowngradeToHigherVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 7 for feature metadata.version. Can't downgrade to a \" +\n                \"newer version.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV3.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotUpgradeToLowerVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 4 for feature metadata.version. Can't downgrade the \" +\n                \"version of this feature without setting the upgrade type to either safe or \" +\n                \"unsafe downgrade.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n                true));\n    }\n\n    @Test\n    public void testCanUpgradeToHigherVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, ApiError.NONE)),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV3.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotUseSafeDowngradeIfMetadataChanged() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid metadata.version 4. Refusing to perform the requested downgrade because \" +\n                \"it might delete metadata information.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testUnsafeDowngradeIsTemporarilyDisabled() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                                \"Invalid metadata.version 4. Unsafe metadata downgrade is not supported in this version.\"))),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Disabled\n    @Test\n    public void testCanUseUnsafeDowngradeIfMetadataChanged() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, ApiError.NONE)),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Test\n    public void testCanUseSafeDowngradeIfMetadataDidNotChange() {\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n                setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                        MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV1.featureLevel())).\n                setMetadataVersion(MetadataVersion.IBP_3_1_IV0).\n                setMinimumBootstrapVersion(MetadataVersion.IBP_3_0_IV0).\n                build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, ApiError.NONE)),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_0_IV1.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Test\n    public void testCannotDowngradeBefore3_3_IV0() {\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV3.featureLevel())).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n            build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                        \"Invalid metadata.version 3. Unable to set a metadata.version less than 3.3-IV0\"))),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_2_IV0.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Test\n    public void testCreateFeatureLevelRecords() {\n        Map<String, VersionRange> localSupportedFeatures = new HashMap<>();\n        localSupportedFeatures.put(MetadataVersion.FEATURE_NAME, VersionRange.of(\n            MetadataVersion.IBP_3_0_IV1.featureLevel(), MetadataVersion.latestTesting().featureLevel()));\n        localSupportedFeatures.put(\"foo\", VersionRange.of(0, 2));\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(new QuorumFeatures(0, localSupportedFeatures, emptyList())).\n            setClusterFeatureSupportDescriber(createFakeClusterFeatureSupportDescriber(\n                Collections.singletonList(new SimpleImmutableEntry<>(1, singletonMap(\"foo\", VersionRange.of(0, 3)))),\n                emptyList())).\n                build();\n        ControllerResult<Map<String, ApiError>> result  = manager.updateFeatures(\n                Collections.singletonMap(\"foo\", (short) 1),\n                Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.UPGRADE),\n                false);\n        assertEquals(ControllerResult.atomicOf(Collections.singletonList(new ApiMessageAndVersion(\n                new FeatureLevelRecord().setName(\"foo\").setFeatureLevel((short) 1), (short) 0)),\n                        Collections.singletonMap(\"foo\", ApiError.NONE)), result);\n        RecordTestUtils.replayAll(manager, result.records());\n        assertEquals(Optional.of((short) 1), manager.finalizedFeatures(Long.MAX_VALUE).get(\"foo\"));\n\n        ControllerResult<Map<String, ApiError>> result2  = manager.updateFeatures(\n                Collections.singletonMap(\"foo\", (short) 0),\n                Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                false);\n        assertEquals(ControllerResult.atomicOf(Collections.singletonList(new ApiMessageAndVersion(\n                        new FeatureLevelRecord().setName(\"foo\").setFeatureLevel((short) 0), (short) 0)),\n                Collections.singletonMap(\"foo\", ApiError.NONE)), result2);\n        RecordTestUtils.replayAll(manager, result2.records());\n        assertEquals(Optional.empty(), manager.finalizedFeatures(Long.MAX_VALUE).get(\"foo\"));\n    }\n\n    @Test\n    public void testNoMetadataVersionChangeDuringMigration() {\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                    MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_5_IV1.featureLevel())).\n            setMetadataVersion(MetadataVersion.IBP_3_4_IV0).\n            build();\n        BootstrapMetadata bootstrapMetadata = BootstrapMetadata.fromVersion(MetadataVersion.IBP_3_4_IV0, \"FeatureControlManagerTest\");\n        RecordTestUtils.replayAll(manager, bootstrapMetadata.records());\n        RecordTestUtils.replayOne(manager, ZkMigrationState.PRE_MIGRATION.toRecord());\n\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid metadata.version 10. Unable to modify metadata.version while a ZK migration is in progress.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_5_IV1.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n                true));\n\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid metadata.version 4. Unable to modify metadata.version while a ZK migration is in progress.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                true));\n\n        // Complete the migration\n        RecordTestUtils.replayOne(manager, ZkMigrationState.POST_MIGRATION.toRecord());\n        ControllerResult<Map<String, ApiError>> result = manager.updateFeatures(\n            singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_5_IV1.featureLevel()),\n            singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n            false);\n        assertEquals(Errors.NONE, result.response().get(MetadataVersion.FEATURE_NAME).error());\n        RecordTestUtils.replayAll(manager, result.records());\n        assertEquals(MetadataVersion.IBP_3_5_IV1, manager.metadataVersion());\n    }\n}",
                "methodCount": 25
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 4,
                "candidates": [
                    {
                        "lineStart": 84,
                        "lineEnd": 92,
                        "refactoringType": "MyMoveStaticMethodRefactoring",
                        "refactoringInfo": "Move Static method updateMap to class SslConfigsBuilder",
                        "description": "move method updateMap to PsiClass:SslConfigsBuilder\nRationale: The 'updateMap' method is more closely aligned with the responsibilities of the SslConfigsBuilder class, which involves constructing and managing SSL configurations, including operations on Map objects that hold configuration details. The TestSslUtils class primarily deals with various SSL utility methods that generate certificates and handle KeyStores, whereas the OAuthBearerValidationUtilsTest class is focused on testing OAuth Bearer token validation. Hence, the method should move to SslConfigsBuilder for better cohesion and relevance to the class responsibilities.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 56,
                        "lineEnd": 66,
                        "refactoringType": "MyMoveStaticMethodRefactoring",
                        "refactoringInfo": "Move Static method rangeMap to class ScramParser",
                        "description": "move method rangeMap to PsiClass:ScramParser\nRationale: The rangeMap method processes an array of arguments to create a map of version ranges, which aligns with ScramParser's responsibility for parsing and assembling data structures from input arguments. Integrating this method into ScramParser aligns with Single Responsibility Principle and enhances cohesion. Additionally, ScramParser already contains methods related to parsing and structuring data, making it a logical home for rangeMap.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 68,
                        "lineEnd": 76,
                        "refactoringType": "MyMoveStaticMethodRefactoring",
                        "refactoringInfo": "Move Static method versionMap to class SslConfigsBuilder",
                        "description": "move method versionMap to PsiClass:SslConfigsBuilder\nRationale: The method `versionMap` is responsible for generating a map from given arguments, which seems to fit better in a configuration or utility class related to managing configurations or creating mappings. `SslConfigsBuilder` already handles various configurations and is responsible for building SSL configurations. Adding `versionMap` to this class maintains cohesion as it could be used to manage version configurations as well. Moreover, `SslConfigsBuilder` already has several static utility methods, making it a natural home for the `versionMap` method.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 146,
                        "lineEnd": 148,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method `idempotentCreateSnapshot` relies directly and solely on the `SnapshotRegistry` class to function. Moving this method to `SnapshotRegistry` ensures that all snapshot-related functionality is encapsulated within the `SnapshotRegistry` class. This promotes better cohesion, encapsulation, and maintainability since all related methods are in a single class. The `Builder` class is primarily concerned with setting up dependencies and initializing the `FeatureControlManager`, and including snapshot creation logic there would violate single responsibility principles.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "rangeMap",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "versionMap",
                            "method_signature": "private static versionMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "features",
                            "method_signature": "public static features(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateMap",
                            "method_signature": "private static updateMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "features",
                            "method_signature": "public static features(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "rangeMap",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "versionMap",
                            "method_signature": "private static versionMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateMap",
                            "method_signature": "private static updateMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "features",
                            "method_signature": "public static features(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "rangeMap",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "versionMap",
                            "method_signature": "private static versionMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateMap",
                            "method_signature": "private static updateMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.25734306657093914
                },
                "public static features(Object... args)": {
                    "first": {
                        "method_name": "features",
                        "method_signature": "public static features(Object... args)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.41132873798477343
                },
                "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)": {
                    "first": {
                        "method_name": "rangeMap",
                        "method_signature": "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4943269859019432
                },
                "private static versionMap(Object... args)": {
                    "first": {
                        "method_name": "versionMap",
                        "method_signature": "private static versionMap(Object... args)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5196640759393861
                },
                "private static updateMap(Object... args)": {
                    "first": {
                        "method_name": "updateMap",
                        "method_signature": "private static updateMap(Object... args)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5196640759393861
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "public static features(Object... args)",
                    "private static updateMap(Object... args)",
                    "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)",
                    "private static versionMap(Object... args)",
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                ],
                "llm_response_time": 4311
            },
            "targetClassMap": {
                "features": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "updateMap": {
                    "target_classes": [
                        {
                            "class_name": "ControllerMetricsTestUtils",
                            "similarity_score": 0.5230682148205612
                        },
                        {
                            "class_name": "ControllerRequestContextUtil",
                            "similarity_score": 0.3235991778623843
                        },
                        {
                            "class_name": "QuorumControllerIntegrationTestUtils",
                            "similarity_score": 0.41778030532391136
                        },
                        {
                            "class_name": "TestSslUtils",
                            "similarity_score": 0.5728068308660601
                        },
                        {
                            "class_name": "MetricsUtils",
                            "similarity_score": 0.2663780856520238
                        },
                        {
                            "class_name": "ThreadUtils",
                            "similarity_score": 0.3021660931112009
                        },
                        {
                            "class_name": "SecurityUtils",
                            "similarity_score": 0.5334870336969845
                        },
                        {
                            "class_name": "AdminClientTestUtils",
                            "similarity_score": 0.3024186401562943
                        },
                        {
                            "class_name": "AdminUtils",
                            "similarity_score": 0.2976216922434756
                        },
                        {
                            "class_name": "MessageUtil",
                            "similarity_score": 0.44424497091197873
                        },
                        {
                            "class_name": "ConsumerUtils",
                            "similarity_score": 0.4733106517221084
                        },
                        {
                            "class_name": "ControlRecordUtils",
                            "similarity_score": 0.4147272803911595
                        },
                        {
                            "class_name": "NetworkTestUtils",
                            "similarity_score": 0.48101535834564235
                        },
                        {
                            "class_name": "ClaimValidationUtils",
                            "similarity_score": 0.08960530171006027
                        },
                        {
                            "class_name": "ClientTelemetryUtils",
                            "similarity_score": 0.47936195012271127
                        },
                        {
                            "class_name": "ProducerTestUtils",
                            "similarity_score": 0.34877513670672466
                        },
                        {
                            "class_name": "PropertiesUtils",
                            "similarity_score": 0.3392833828023668
                        },
                        {
                            "class_name": "ConfigUtils",
                            "similarity_score": 0.31239418510690087
                        },
                        {
                            "class_name": "CommandLineUtils",
                            "similarity_score": 0.33168373363760056
                        },
                        {
                            "class_name": "RaftUtil",
                            "similarity_score": 0.40802503529583395
                        },
                        {
                            "class_name": "FetchUtils",
                            "similarity_score": 0.05206086001129595
                        },
                        {
                            "class_name": "RecordsUtil",
                            "similarity_score": 0.39092786730806
                        },
                        {
                            "class_name": "RecordTestUtils",
                            "similarity_score": 0.4858468349007344
                        },
                        {
                            "class_name": "RequestTestUtils",
                            "similarity_score": 0.37494370536553573
                        },
                        {
                            "class_name": "FutureUtils",
                            "similarity_score": 0.20854541300380916
                        },
                        {
                            "class_name": "Utils",
                            "similarity_score": 0.3073339579117863
                        },
                        {
                            "class_name": "ByteUtils",
                            "similarity_score": 0.2230517779778567
                        },
                        {
                            "class_name": "OAuthBearerValidationUtils",
                            "similarity_score": 0.16559104438052208
                        },
                        {
                            "class_name": "NetworkClientUtils",
                            "similarity_score": 0.23531956447757144
                        },
                        {
                            "class_name": "ScramCredentialUtils",
                            "similarity_score": 0.5017763968150261
                        },
                        {
                            "class_name": "CollectionUtils",
                            "similarity_score": 0.33828585081362084
                        },
                        {
                            "class_name": "RequestUtils",
                            "similarity_score": 0.40228814150092784
                        },
                        {
                            "class_name": "OffsetsForLeaderEpochUtils",
                            "similarity_score": 0.439524041295631
                        },
                        {
                            "class_name": "OAuthBearerScopeUtils",
                            "similarity_score": 0.16798530752762428
                        },
                        {
                            "class_name": "JaasUtils",
                            "similarity_score": 0.42920157150778865
                        },
                        {
                            "class_name": "OAuthBearerValidationUtilsTest",
                            "similarity_score": 0.5537474559362661
                        },
                        {
                            "class_name": "JaasOptionsUtils",
                            "similarity_score": 0.4713582375702075
                        },
                        {
                            "class_name": "CommandLineUtilsTest",
                            "similarity_score": 0.32994621264709656
                        },
                        {
                            "class_name": "UtilsTest",
                            "similarity_score": 0.540720367870316
                        },
                        {
                            "class_name": "TestSslEngineFactory",
                            "similarity_score": 0.32528969464480484
                        },
                        {
                            "class_name": "TestThroughAllIntermediateImagesLeadingToFinalImageHelper",
                            "similarity_score": 0.5088785581827483
                        },
                        {
                            "class_name": "MetricsUtilsTest",
                            "similarity_score": 0.26473451003726783
                        },
                        {
                            "class_name": "ThreadUtilsTest",
                            "similarity_score": 0.4391168504129238
                        },
                        {
                            "class_name": "ScramCredentialUtilsTest",
                            "similarity_score": 0.3929882209189315
                        },
                        {
                            "class_name": "SecurityUtilsTest",
                            "similarity_score": 0.5019791449522171
                        },
                        {
                            "class_name": "TopicIdPartitionComparator",
                            "similarity_score": 0.4256282653793743
                        },
                        {
                            "class_name": "TopicPartitionComparator",
                            "similarity_score": 0.4517351373863803
                        },
                        {
                            "class_name": "MessageUtilTest",
                            "similarity_score": 0.2577849670652873
                        },
                        {
                            "class_name": "CertificateBuilder",
                            "similarity_score": 0.4687887344586328
                        },
                        {
                            "class_name": "ControlRecordUtilsTest",
                            "similarity_score": 0.3431409510633933
                        },
                        {
                            "class_name": "ClaimValidationUtilsTest",
                            "similarity_score": 0.3251029475173706
                        },
                        {
                            "class_name": "OAuthBearerScopeUtilsTest",
                            "similarity_score": 0.3307942673698644
                        },
                        {
                            "class_name": "ClientTelemetryUtilsTest",
                            "similarity_score": 0.3455765205731853
                        },
                        {
                            "class_name": "PropertiesUtilsTest",
                            "similarity_score": 0.41845020827082885
                        },
                        {
                            "class_name": "ProtoUtilsTest",
                            "similarity_score": 0.15574226979044098
                        },
                        {
                            "class_name": "CollectionUtilsTest",
                            "similarity_score": 0.41890312442585187
                        },
                        {
                            "class_name": "SslConfigsBuilder",
                            "similarity_score": 0.5712337931469162
                        },
                        {
                            "class_name": "ConfigurationUtils",
                            "similarity_score": 0.24353919907812213
                        },
                        {
                            "class_name": "ConfigurationUtilsTest",
                            "similarity_score": 0.29704166429653484
                        },
                        {
                            "class_name": "ConfigUtilsTest",
                            "similarity_score": 0.44960356879387064
                        },
                        {
                            "class_name": "ByteUtilsTest",
                            "similarity_score": 0.39369128935422
                        },
                        {
                            "class_name": "FutureUtilsTest",
                            "similarity_score": 0.381101994652465
                        },
                        {
                            "class_name": "JaasOptionsUtilsTest",
                            "similarity_score": 0.47312184713712263
                        },
                        {
                            "class_name": "ImageDeltaPair",
                            "similarity_score": 0.317380885407937
                        },
                        {
                            "class_name": "ActivationRecordsGenerator",
                            "similarity_score": 0.2593517248884098
                        },
                        {
                            "class_name": "ControllerExceptions",
                            "similarity_score": 0.22302163169549574
                        },
                        {
                            "class_name": "OffsetSpec",
                            "similarity_score": 0.22847956398461716
                        },
                        {
                            "class_name": "Sanitizer",
                            "similarity_score": 0.3070603914522629
                        },
                        {
                            "class_name": "SaslConfigs",
                            "similarity_score": 0.289398870169027
                        },
                        {
                            "class_name": "MetricsBench",
                            "similarity_score": 0.5116429598912723
                        },
                        {
                            "class_name": "DirectoryId",
                            "similarity_score": 0.2682321468668362
                        },
                        {
                            "class_name": "ScramParser",
                            "similarity_score": 0.5447008699976473
                        },
                        {
                            "class_name": "AccessTokenRetrieverFactory",
                            "similarity_score": 0.4174235549683609
                        },
                        {
                            "class_name": "AccessTokenValidatorFactory",
                            "similarity_score": 0.4859714909220032
                        },
                        {
                            "class_name": "Serdes",
                            "similarity_score": 0.3503802333879169
                        },
                        {
                            "class_name": "Serializer",
                            "similarity_score": 0.4516571961342602
                        },
                        {
                            "class_name": "Topic",
                            "similarity_score": 0.39298820109646987
                        },
                        {
                            "class_name": "PartitionFactory",
                            "similarity_score": 0.3060982830947443
                        },
                        {
                            "class_name": "ServerTopicConfigSynonyms",
                            "similarity_score": 0.31518112457883296
                        },
                        {
                            "class_name": "ConsumerProtocol",
                            "similarity_score": 0.44854244220185563
                        },
                        {
                            "class_name": "ShareGroupHelper",
                            "similarity_score": 0.12921165649395971
                        },
                        {
                            "class_name": "AppInfoParser",
                            "similarity_score": 0.485656830852641
                        },
                        {
                            "class_name": "Csv",
                            "similarity_score": 0.32314097407232784
                        },
                        {
                            "class_name": "PrimitiveRef",
                            "similarity_score": 0.3554280338863895
                        },
                        {
                            "class_name": "AssignmentsHelper",
                            "similarity_score": 0.48665579620546473
                        },
                        {
                            "class_name": "Snapshots",
                            "similarity_score": 0.5219806204409719
                        },
                        {
                            "class_name": "Exit",
                            "similarity_score": 0.3653015133788107
                        },
                        {
                            "class_name": "Protocol",
                            "similarity_score": 0.5431281914792646
                        },
                        {
                            "class_name": "CompressionRatioEstimator",
                            "similarity_score": 0.392516780601977
                        },
                        {
                            "class_name": "SslConfigs",
                            "similarity_score": 0.314980247692483
                        },
                        {
                            "class_name": "CommonClientConfigs",
                            "similarity_score": 0.290038120710786
                        },
                        {
                            "class_name": "QuotaConfigs",
                            "similarity_score": 0.3561781420553535
                        },
                        {
                            "class_name": "VerificationKeyResolverFactory",
                            "similarity_score": 0.43248439857877846
                        },
                        {
                            "class_name": "Replicas",
                            "similarity_score": 0.36758317751884195
                        },
                        {
                            "class_name": "TelemetryMetricNamingConvention",
                            "similarity_score": 0.26451974684311724
                        },
                        {
                            "class_name": "Json",
                            "similarity_score": 0.23324759035367781
                        },
                        {
                            "class_name": "BrokerToElrsTest",
                            "similarity_score": 0.35577779597974063
                        },
                        {
                            "class_name": "ChannelBuilders",
                            "similarity_score": 0.3378457167053045
                        },
                        {
                            "class_name": "Checksums",
                            "similarity_score": 0.37396456490472985
                        },
                        {
                            "class_name": "ByteBufferUnmapper",
                            "similarity_score": 0.444128806861606
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SslConfigsBuilder",
                        "TestSslUtils",
                        "OAuthBearerValidationUtilsTest"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 22,
                    "similarity_metric": "cosine"
                },
                "rangeMap": {
                    "target_classes": [
                        {
                            "class_name": "ControllerMetricsTestUtils",
                            "similarity_score": 0.490825920794188
                        },
                        {
                            "class_name": "ControllerRequestContextUtil",
                            "similarity_score": 0.28833765854913374
                        },
                        {
                            "class_name": "QuorumControllerIntegrationTestUtils",
                            "similarity_score": 0.4024668898971298
                        },
                        {
                            "class_name": "PropertiesUtils",
                            "similarity_score": 0.32556763957029866
                        },
                        {
                            "class_name": "RecordTestUtils",
                            "similarity_score": 0.4517693740509775
                        },
                        {
                            "class_name": "ImageDeltaPair",
                            "similarity_score": 0.2906732603092963
                        },
                        {
                            "class_name": "TestThroughAllIntermediateImagesLeadingToFinalImageHelper",
                            "similarity_score": 0.48639149973351714
                        },
                        {
                            "class_name": "PropertiesUtilsTest",
                            "similarity_score": 0.4006905375132152
                        },
                        {
                            "class_name": "ActivationRecordsGenerator",
                            "similarity_score": 0.24689213193069717
                        },
                        {
                            "class_name": "ControllerExceptions",
                            "similarity_score": 0.2011333106794384
                        },
                        {
                            "class_name": "ScramParser",
                            "similarity_score": 0.5397612556628663
                        },
                        {
                            "class_name": "Replicas",
                            "similarity_score": 0.3444564427364337
                        },
                        {
                            "class_name": "BrokerToElrsTest",
                            "similarity_score": 0.326207872833152
                        },
                        {
                            "class_name": "ClientQuotaControlManager",
                            "similarity_score": 0.48150809531455746
                        },
                        {
                            "class_name": "BrokersToIsrsTest",
                            "similarity_score": 0.2597288702942012
                        },
                        {
                            "class_name": "ControllerMetadataMetricsPublisherTest",
                            "similarity_score": 0.39219806645940514
                        },
                        {
                            "class_name": "ScramControlManager",
                            "similarity_score": 0.5047690255303079
                        },
                        {
                            "class_name": "StripedReplicaPlacer",
                            "similarity_score": 0.26898082181806393
                        },
                        {
                            "class_name": "ClientQuotaControlManagerTest",
                            "similarity_score": 0.3924124960571338
                        },
                        {
                            "class_name": "QuorumControllerMetricsTest",
                            "similarity_score": 0.5281082552729583
                        },
                        {
                            "class_name": "AclsImageTest",
                            "similarity_score": 0.5328370302677876
                        },
                        {
                            "class_name": "ClientQuotasImageNode",
                            "similarity_score": 0.5318624039693715
                        },
                        {
                            "class_name": "ClientQuotasImageTest",
                            "similarity_score": 0.46842593482303035
                        },
                        {
                            "class_name": "ConfigurationsImageTest",
                            "similarity_score": 0.4341478589590008
                        },
                        {
                            "class_name": "MetadataImageTest",
                            "similarity_score": 0.3767545863286342
                        },
                        {
                            "class_name": "ScramImageTest",
                            "similarity_score": 0.5073554423970249
                        },
                        {
                            "class_name": "PartitionChangeBuilderTest",
                            "similarity_score": 0.46346240467307653
                        },
                        {
                            "class_name": "ProducerIdsImageTest",
                            "similarity_score": 0.3865422274694206
                        },
                        {
                            "class_name": "ConfigurationControlManagerTest",
                            "similarity_score": 0.43285715704590727
                        },
                        {
                            "class_name": "PartitionReassignmentReplicas",
                            "similarity_score": 0.47236654654100074
                        },
                        {
                            "class_name": "DelegationTokenImageTest",
                            "similarity_score": 0.47725358990182626
                        },
                        {
                            "class_name": "AclControlManager",
                            "similarity_score": 0.4883524617908616
                        },
                        {
                            "class_name": "ControllerResult",
                            "similarity_score": 0.3332624038010199
                        },
                        {
                            "class_name": "BrokerHeartbeatManagerTest",
                            "similarity_score": 0.4143886436312194
                        },
                        {
                            "class_name": "EventHandlerExceptionInfo",
                            "similarity_score": 0.31798881840324217
                        },
                        {
                            "class_name": "ControllerMetricsChangesTest",
                            "similarity_score": 0.3444351279257109
                        },
                        {
                            "class_name": "ControllerResultAndOffset",
                            "similarity_score": 0.30606706665315775
                        },
                        {
                            "class_name": "ResultOrError",
                            "similarity_score": 0.38640580200395774
                        },
                        {
                            "class_name": "QuorumFeatures",
                            "similarity_score": 0.4676299059669639
                        },
                        {
                            "class_name": "ListenerInfo",
                            "similarity_score": 0.32156712288340583
                        },
                        {
                            "class_name": "ProducerIdControlManagerTest",
                            "similarity_score": 0.46249441560786475
                        },
                        {
                            "class_name": "QuorumControllerTest",
                            "similarity_score": 0.5231275986653078
                        },
                        {
                            "class_name": "TopicsImageTest",
                            "similarity_score": 0.4274748652364346
                        },
                        {
                            "class_name": "FeaturesImageTest",
                            "similarity_score": 0.4256432064749635
                        },
                        {
                            "class_name": "ClusterImageTest",
                            "similarity_score": 0.3633567713966603
                        },
                        {
                            "class_name": "ControllerRequestContext",
                            "similarity_score": 0.2891385481295669
                        },
                        {
                            "class_name": "ControllerMetadataMetricsTest",
                            "similarity_score": 0.26235446452512573
                        },
                        {
                            "class_name": "ControllerExceptionsTest",
                            "similarity_score": 0.2223965741461595
                        },
                        {
                            "class_name": "ReplicationControlManager",
                            "similarity_score": 0.4944428785628315
                        },
                        {
                            "class_name": "BootstrapMetadata",
                            "similarity_score": 0.4496604085902668
                        },
                        {
                            "class_name": "ImageDowngradeTest",
                            "similarity_score": 0.3098643139083865
                        },
                        {
                            "class_name": "DelegationTokenControlManager",
                            "similarity_score": 0.5274386119366898
                        },
                        {
                            "class_name": "ReplicationControlManagerTest",
                            "similarity_score": 0.4886216687796293
                        },
                        {
                            "class_name": "ClusterControlManagerTest",
                            "similarity_score": 0.4690519693274538
                        },
                        {
                            "class_name": "OffsetControlManagerTest",
                            "similarity_score": 0.32354436937971476
                        },
                        {
                            "class_name": "StandardAuthorizerData",
                            "similarity_score": 0.4101573907472663
                        },
                        {
                            "class_name": "QuorumControllerMetrics",
                            "similarity_score": 0.41473350569819456
                        },
                        {
                            "class_name": "ControllerMetadataMetrics",
                            "similarity_score": 0.3676506025299547
                        },
                        {
                            "class_name": "PartitionChangeBuilder",
                            "similarity_score": 0.3804947214517758
                        },
                        {
                            "class_name": "QuorumController",
                            "similarity_score": 0.4513821378847811
                        },
                        {
                            "class_name": "FeatureControlManager",
                            "similarity_score": 0.5055858266909106
                        },
                        {
                            "class_name": "AclControlManagerTest",
                            "similarity_score": 0.4223052188719406
                        },
                        {
                            "class_name": "ActivationRecordsGeneratorTest",
                            "similarity_score": 0.19405971327997806
                        },
                        {
                            "class_name": "ClusterControlManager",
                            "similarity_score": 0.4691705663517866
                        },
                        {
                            "class_name": "LogReplayTracker",
                            "similarity_score": 0.38348902172974136
                        },
                        {
                            "class_name": "LogReplayTrackerTest",
                            "similarity_score": 0.351972576457172
                        },
                        {
                            "class_name": "ElectionResult",
                            "similarity_score": 0.32616403652672105
                        },
                        {
                            "class_name": "KafkaConfigSchemaTest",
                            "similarity_score": 0.36873950564456526
                        },
                        {
                            "class_name": "BrokerControlStates",
                            "similarity_score": 0.3698352545411534
                        },
                        {
                            "class_name": "BrokerHeartbeatManager",
                            "similarity_score": 0.34110916657081153
                        },
                        {
                            "class_name": "BrokerHeartbeatState",
                            "similarity_score": 0.16043010139335345
                        },
                        {
                            "class_name": "BrokerHeartbeatStateIterator",
                            "similarity_score": 0.4154637515575287
                        },
                        {
                            "class_name": "BrokerHeartbeatStateList",
                            "similarity_score": 0.4322255076525336
                        },
                        {
                            "class_name": "BrokersToElrs",
                            "similarity_score": 0.47440536703770286
                        },
                        {
                            "class_name": "BrokersToIsrs",
                            "similarity_score": 0.5036125520521585
                        },
                        {
                            "class_name": "CompleteActivationEvent",
                            "similarity_score": 0.19388915267323256
                        },
                        {
                            "class_name": "CompletedReassignment",
                            "similarity_score": 0.30238833168763435
                        },
                        {
                            "class_name": "InitialSnapshot",
                            "similarity_score": 0.389648026364433
                        },
                        {
                            "class_name": "KRaftClusterDescriber",
                            "similarity_score": 0.2619732711758709
                        },
                        {
                            "class_name": "ConfigResourceExistenceChecker",
                            "similarity_score": 0.2681771284015011
                        },
                        {
                            "class_name": "ConfigurationControlManager",
                            "similarity_score": 0.5461799970971094
                        },
                        {
                            "class_name": "ConfigurationsImageNode",
                            "similarity_score": 0.4183964306705311
                        },
                        {
                            "class_name": "EventHandlerExceptionInfoTest",
                            "similarity_score": 0.28693857265149986
                        },
                        {
                            "class_name": "MetadataOffsetComparator",
                            "similarity_score": 0.3124262542812672
                        },
                        {
                            "class_name": "TestExistenceChecker",
                            "similarity_score": 0.3159413277642021
                        },
                        {
                            "class_name": "UsableBrokerIterator",
                            "similarity_score": 0.49203142821393586
                        },
                        {
                            "class_name": "ControllerEvent",
                            "similarity_score": 0.3924741683232832
                        },
                        {
                            "class_name": "ControllerMetadataMetricsPublisher",
                            "similarity_score": 0.4140357192728847
                        },
                        {
                            "class_name": "ControllerReadEvent",
                            "similarity_score": 0.4542535579801632
                        },
                        {
                            "class_name": "ControllerWriteEvent",
                            "similarity_score": 0.4043818348039314
                        },
                        {
                            "class_name": "MigrationRecordConsumer",
                            "similarity_score": 0.36676874353846095
                        },
                        {
                            "class_name": "MigrationWriteOperation",
                            "similarity_score": 0.3654236656029014
                        },
                        {
                            "class_name": "MockAclMutator",
                            "similarity_score": 0.36623560618118794
                        },
                        {
                            "class_name": "ScramCredentialDataNode",
                            "similarity_score": 0.31437751023734767
                        },
                        {
                            "class_name": "ScramCredentialKey",
                            "similarity_score": 0.40213532475552316
                        },
                        {
                            "class_name": "ScramCredentialValue",
                            "similarity_score": 0.38258351848376515
                        },
                        {
                            "class_name": "ScramCredentialValueTest",
                            "similarity_score": 0.42148902413687467
                        },
                        {
                            "class_name": "MockClusterMetadataAuthorizer",
                            "similarity_score": 0.21401432573941534
                        },
                        {
                            "class_name": "MockControllerMetrics",
                            "similarity_score": 0.3138032019216492
                        },
                        {
                            "class_name": "StandardAclTest",
                            "similarity_score": 0.40037142663355235
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "ScramParser",
                        "AclsImageTest",
                        "ConfigurationControlManager"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 36,
                    "similarity_metric": "cosine"
                },
                "versionMap": {
                    "target_classes": [
                        {
                            "class_name": "ControllerMetricsTestUtils",
                            "similarity_score": 0.5230682148205612
                        },
                        {
                            "class_name": "ControllerRequestContextUtil",
                            "similarity_score": 0.3235991778623843
                        },
                        {
                            "class_name": "QuorumControllerIntegrationTestUtils",
                            "similarity_score": 0.41778030532391136
                        },
                        {
                            "class_name": "TestSslUtils",
                            "similarity_score": 0.5728068308660601
                        },
                        {
                            "class_name": "ThreadUtils",
                            "similarity_score": 0.3021660931112009
                        },
                        {
                            "class_name": "SecurityUtils",
                            "similarity_score": 0.5334870336969845
                        },
                        {
                            "class_name": "ClaimValidationUtils",
                            "similarity_score": 0.08960530171006027
                        },
                        {
                            "class_name": "AdminClientTestUtils",
                            "similarity_score": 0.3024186401562943
                        },
                        {
                            "class_name": "AdminUtils",
                            "similarity_score": 0.2976216922434756
                        },
                        {
                            "class_name": "ClientTelemetryUtils",
                            "similarity_score": 0.47936195012271127
                        },
                        {
                            "class_name": "CommandLineUtils",
                            "similarity_score": 0.33168373363760056
                        },
                        {
                            "class_name": "ConfigUtils",
                            "similarity_score": 0.31239418510690087
                        },
                        {
                            "class_name": "MessageUtil",
                            "similarity_score": 0.44424497091197873
                        },
                        {
                            "class_name": "ProducerTestUtils",
                            "similarity_score": 0.34877513670672466
                        },
                        {
                            "class_name": "PropertiesUtils",
                            "similarity_score": 0.3392833828023668
                        },
                        {
                            "class_name": "ConsumerUtils",
                            "similarity_score": 0.4733106517221084
                        },
                        {
                            "class_name": "MetricsUtils",
                            "similarity_score": 0.2663780856520238
                        },
                        {
                            "class_name": "ControlRecordUtils",
                            "similarity_score": 0.4147272803911595
                        },
                        {
                            "class_name": "FetchUtils",
                            "similarity_score": 0.05206086001129595
                        },
                        {
                            "class_name": "RaftUtil",
                            "similarity_score": 0.40802503529583395
                        },
                        {
                            "class_name": "RecordsUtil",
                            "similarity_score": 0.39092786730806
                        },
                        {
                            "class_name": "RecordTestUtils",
                            "similarity_score": 0.4858468349007344
                        },
                        {
                            "class_name": "NetworkTestUtils",
                            "similarity_score": 0.48101535834564235
                        },
                        {
                            "class_name": "RequestTestUtils",
                            "similarity_score": 0.37494370536553573
                        },
                        {
                            "class_name": "FutureUtils",
                            "similarity_score": 0.20854541300380916
                        },
                        {
                            "class_name": "Utils",
                            "similarity_score": 0.3073339579117863
                        },
                        {
                            "class_name": "ByteUtils",
                            "similarity_score": 0.2230517779778567
                        },
                        {
                            "class_name": "OAuthBearerValidationUtils",
                            "similarity_score": 0.16559104438052208
                        },
                        {
                            "class_name": "NetworkClientUtils",
                            "similarity_score": 0.23531956447757144
                        },
                        {
                            "class_name": "ScramCredentialUtils",
                            "similarity_score": 0.5017763968150261
                        },
                        {
                            "class_name": "CollectionUtils",
                            "similarity_score": 0.33828585081362084
                        },
                        {
                            "class_name": "RequestUtils",
                            "similarity_score": 0.40228814150092784
                        },
                        {
                            "class_name": "OffsetsForLeaderEpochUtils",
                            "similarity_score": 0.439524041295631
                        },
                        {
                            "class_name": "OAuthBearerScopeUtils",
                            "similarity_score": 0.16798530752762428
                        },
                        {
                            "class_name": "JaasUtils",
                            "similarity_score": 0.42920157150778865
                        },
                        {
                            "class_name": "OAuthBearerValidationUtilsTest",
                            "similarity_score": 0.5537474559362661
                        },
                        {
                            "class_name": "JaasOptionsUtils",
                            "similarity_score": 0.4713582375702075
                        },
                        {
                            "class_name": "CommandLineUtilsTest",
                            "similarity_score": 0.32994621264709656
                        },
                        {
                            "class_name": "UtilsTest",
                            "similarity_score": 0.540720367870316
                        },
                        {
                            "class_name": "TestSslEngineFactory",
                            "similarity_score": 0.32528969464480484
                        },
                        {
                            "class_name": "CertificateBuilder",
                            "similarity_score": 0.4687887344586328
                        },
                        {
                            "class_name": "TestThroughAllIntermediateImagesLeadingToFinalImageHelper",
                            "similarity_score": 0.5088785581827483
                        },
                        {
                            "class_name": "ThreadUtilsTest",
                            "similarity_score": 0.4391168504129238
                        },
                        {
                            "class_name": "ScramCredentialUtilsTest",
                            "similarity_score": 0.3929882209189315
                        },
                        {
                            "class_name": "SecurityUtilsTest",
                            "similarity_score": 0.5019791449522171
                        },
                        {
                            "class_name": "ClaimValidationUtilsTest",
                            "similarity_score": 0.3251029475173706
                        },
                        {
                            "class_name": "TopicIdPartitionComparator",
                            "similarity_score": 0.4256282653793743
                        },
                        {
                            "class_name": "TopicPartitionComparator",
                            "similarity_score": 0.4517351373863803
                        },
                        {
                            "class_name": "ClientTelemetryUtilsTest",
                            "similarity_score": 0.3455765205731853
                        },
                        {
                            "class_name": "CollectionUtilsTest",
                            "similarity_score": 0.41890312442585187
                        },
                        {
                            "class_name": "ByteUtilsTest",
                            "similarity_score": 0.39369128935422
                        },
                        {
                            "class_name": "ConfigurationUtils",
                            "similarity_score": 0.24353919907812213
                        },
                        {
                            "class_name": "ConfigurationUtilsTest",
                            "similarity_score": 0.29704166429653484
                        },
                        {
                            "class_name": "ConfigUtilsTest",
                            "similarity_score": 0.44960356879387064
                        },
                        {
                            "class_name": "MessageUtilTest",
                            "similarity_score": 0.2577849670652873
                        },
                        {
                            "class_name": "PropertiesUtilsTest",
                            "similarity_score": 0.41845020827082885
                        },
                        {
                            "class_name": "ProtoUtilsTest",
                            "similarity_score": 0.15574226979044098
                        },
                        {
                            "class_name": "MetricsUtilsTest",
                            "similarity_score": 0.26473451003726783
                        },
                        {
                            "class_name": "SslConfigsBuilder",
                            "similarity_score": 0.5712337931469162
                        },
                        {
                            "class_name": "ControlRecordUtilsTest",
                            "similarity_score": 0.3431409510633933
                        },
                        {
                            "class_name": "ImageDeltaPair",
                            "similarity_score": 0.317380885407937
                        },
                        {
                            "class_name": "FutureUtilsTest",
                            "similarity_score": 0.381101994652465
                        },
                        {
                            "class_name": "OAuthBearerScopeUtilsTest",
                            "similarity_score": 0.3307942673698644
                        },
                        {
                            "class_name": "JaasOptionsUtilsTest",
                            "similarity_score": 0.47312184713712263
                        },
                        {
                            "class_name": "ActivationRecordsGenerator",
                            "similarity_score": 0.2593517248884098
                        },
                        {
                            "class_name": "ControllerExceptions",
                            "similarity_score": 0.22302163169549574
                        },
                        {
                            "class_name": "OffsetSpec",
                            "similarity_score": 0.22847956398461716
                        },
                        {
                            "class_name": "Sanitizer",
                            "similarity_score": 0.3070603914522629
                        },
                        {
                            "class_name": "SaslConfigs",
                            "similarity_score": 0.289398870169027
                        },
                        {
                            "class_name": "DirectoryId",
                            "similarity_score": 0.2682321468668362
                        },
                        {
                            "class_name": "ScramParser",
                            "similarity_score": 0.5447008699976473
                        },
                        {
                            "class_name": "AccessTokenRetrieverFactory",
                            "similarity_score": 0.4174235549683609
                        },
                        {
                            "class_name": "AccessTokenValidatorFactory",
                            "similarity_score": 0.4859714909220032
                        },
                        {
                            "class_name": "Serdes",
                            "similarity_score": 0.3503802333879169
                        },
                        {
                            "class_name": "Topic",
                            "similarity_score": 0.39298820109646987
                        },
                        {
                            "class_name": "Serializer",
                            "similarity_score": 0.4516571961342602
                        },
                        {
                            "class_name": "PartitionFactory",
                            "similarity_score": 0.3060982830947443
                        },
                        {
                            "class_name": "ServerTopicConfigSynonyms",
                            "similarity_score": 0.31518112457883296
                        },
                        {
                            "class_name": "ShareGroupHelper",
                            "similarity_score": 0.12921165649395971
                        },
                        {
                            "class_name": "CommonClientConfigs",
                            "similarity_score": 0.290038120710786
                        },
                        {
                            "class_name": "AppInfoParser",
                            "similarity_score": 0.485656830852641
                        },
                        {
                            "class_name": "CompressionRatioEstimator",
                            "similarity_score": 0.392516780601977
                        },
                        {
                            "class_name": "PrimitiveRef",
                            "similarity_score": 0.3554280338863895
                        },
                        {
                            "class_name": "Snapshots",
                            "similarity_score": 0.5219806204409719
                        },
                        {
                            "class_name": "AssignmentsHelper",
                            "similarity_score": 0.48665579620546473
                        },
                        {
                            "class_name": "Exit",
                            "similarity_score": 0.3653015133788107
                        },
                        {
                            "class_name": "Protocol",
                            "similarity_score": 0.5431281914792646
                        },
                        {
                            "class_name": "ConsumerProtocol",
                            "similarity_score": 0.44854244220185563
                        },
                        {
                            "class_name": "MetricsBench",
                            "similarity_score": 0.5116429598912723
                        },
                        {
                            "class_name": "SslConfigs",
                            "similarity_score": 0.314980247692483
                        },
                        {
                            "class_name": "QuotaConfigs",
                            "similarity_score": 0.3561781420553535
                        },
                        {
                            "class_name": "VerificationKeyResolverFactory",
                            "similarity_score": 0.43248439857877846
                        },
                        {
                            "class_name": "Csv",
                            "similarity_score": 0.32314097407232784
                        },
                        {
                            "class_name": "Replicas",
                            "similarity_score": 0.36758317751884195
                        },
                        {
                            "class_name": "TelemetryMetricNamingConvention",
                            "similarity_score": 0.26451974684311724
                        },
                        {
                            "class_name": "Json",
                            "similarity_score": 0.23324759035367781
                        },
                        {
                            "class_name": "BrokerToElrsTest",
                            "similarity_score": 0.35577779597974063
                        },
                        {
                            "class_name": "ChannelBuilders",
                            "similarity_score": 0.3378457167053045
                        },
                        {
                            "class_name": "Checksums",
                            "similarity_score": 0.37396456490472985
                        },
                        {
                            "class_name": "ByteBufferUnmapper",
                            "similarity_score": 0.444128806861606
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SslConfigsBuilder",
                        "TestSslUtils",
                        "OAuthBearerValidationUtilsTest"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 20,
                    "similarity_metric": "cosine"
                },
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        },
                        {
                            "class_name": "Builder",
                            "similarity_score": 0.3710212605396185
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry",
                        "Builder"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testUpdateFeaturesErrorCases() : void in class org.apache.kafka.controller.FeatureControlManagerTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 164,
                    "endLine": 213,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testUpdateFeaturesErrorCases() : void"
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 188,
                    "endLine": 188,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 163,
                    "endLine": 212,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testUpdateFeaturesErrorCases() : void"
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 187,
                    "endLine": 187,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(3)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 629,
        "extraction_results": {
            "success": true,
            "newCommitHash": "bed236d73c831ae8aa19c7bdaed32680482e4f73",
            "newBranchName": "extract-idempotentCreateSnapshot-testUpdateFeaturesErrorCases-130af38"
        },
        "telemetry": {
            "id": "25f82d3c-c37e-4798-ba3d-f065ce3537ab",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 405,
                "lineStart": 54,
                "lineEnd": 458,
                "bodyLineStart": 54,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                "sourceCode": "@Timeout(value = 40)\npublic class FeatureControlManagerTest {\n\n    @SuppressWarnings(\"unchecked\")\n    private static Map<String, VersionRange> rangeMap(Object... args) {\n        Map<String, VersionRange> result = new HashMap<>();\n        for (int i = 0; i < args.length; i += 3) {\n            String feature = (String) args[i];\n            Number low = (Number) args[i + 1];\n            Number high = (Number) args[i + 2];\n            result.put(feature, VersionRange.of(low.shortValue(), high.shortValue()));\n        }\n        return result;\n    }\n\n    private static Map<String, Short> versionMap(Object... args) {\n        Map<String, Short> result = new HashMap<>();\n        for (int i = 0; i < args.length; i += 2) {\n            String feature = (String) args[i];\n            Number ver = (Number) args[i + 1];\n            result.put(feature, ver.shortValue());\n        }\n        return result;\n    }\n\n    public static QuorumFeatures features(Object... args) {\n        Map<String, VersionRange> features = QuorumFeatures.defaultFeatureMap(true);\n        features.putAll(rangeMap(args));\n        return new QuorumFeatures(0, features, emptyList());\n    }\n\n    private static Map<String, Short> updateMap(Object... args) {\n        Map<String, Short> result = new HashMap<>();\n        for (int i = 0; i < args.length; i += 2) {\n            String feature = (String) args[i];\n            Number ver = (Number) args[i + 1];\n            result.put(feature, ver.shortValue());\n        }\n        return result;\n    }\n\n    @Test\n    public void testUpdateFeatures() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(features(\"foo\", 1, 2)).\n            setSnapshotRegistry(snapshotRegistry).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n            build();\n        snapshotRegistry.getOrCreateSnapshot(-1);\n        assertEquals(new FinalizedControllerFeatures(Collections.singletonMap(\"metadata.version\", (short) 4), -1),\n            manager.finalizedFeatures(-1));\n        assertEquals(ControllerResult.atomicOf(emptyList(), Collections.\n                singletonMap(\"foo\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                    \"Invalid update version 3 for feature foo. Local controller 0 only supports versions 1-2\"))),\n            manager.updateFeatures(updateMap(\"foo\", 3),\n                Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                false));\n        ControllerResult<Map<String, ApiError>> result = manager.updateFeatures(\n                updateMap(\"foo\", 2, \"bar\", 1), Collections.emptyMap(),\n                false);\n        Map<String, ApiError> expectedMap = new HashMap<>();\n        expectedMap.put(\"foo\", ApiError.NONE);\n        expectedMap.put(\"bar\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 1 for feature bar. Local controller 0 does not support this feature.\"));\n        assertEquals(expectedMap, result.response());\n        List<ApiMessageAndVersion> expectedMessages = new ArrayList<>();\n        expectedMessages.add(new ApiMessageAndVersion(new FeatureLevelRecord().\n            setName(\"foo\").setFeatureLevel((short) 2),\n            (short) 0));\n        assertEquals(expectedMessages, result.records());\n    }\n\n    @Test\n    public void testReplay() {\n        LogContext logContext = new LogContext();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        FeatureLevelRecord record = new FeatureLevelRecord().\n            setName(\"foo\").setFeatureLevel((short) 2);\n\n        snapshotRegistry.getOrCreateSnapshot(-1);\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n                setLogContext(logContext).\n                setQuorumFeatures(features(\"foo\", 1, 2)).\n                setSnapshotRegistry(snapshotRegistry).\n                setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n                build();\n        manager.replay(record);\n        snapshotRegistry.getOrCreateSnapshot(123);\n        assertEquals(new FinalizedControllerFeatures(versionMap(\"metadata.version\", 4, \"foo\", 2), 123),\n            manager.finalizedFeatures(123));\n    }\n\n    static ClusterFeatureSupportDescriber createFakeClusterFeatureSupportDescriber(\n        List<Map.Entry<Integer, Map<String, VersionRange>>> brokerRanges,\n        List<Map.Entry<Integer, Map<String, VersionRange>>> controllerRanges\n    ) {\n        return new ClusterFeatureSupportDescriber() {\n            @Override\n            public Iterator<Map.Entry<Integer, Map<String, VersionRange>>> brokerSupported() {\n                return brokerRanges.iterator();\n            }\n\n            @Override\n            public Iterator<Map.Entry<Integer, Map<String, VersionRange>>> controllerSupported() {\n                return controllerRanges.iterator();\n            }\n        };\n    }\n\n    @Test\n    public void testUpdateFeaturesErrorCases() {\n        LogContext logContext = new LogContext();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setLogContext(logContext).\n            setQuorumFeatures(features(\"foo\", 1, 5, \"bar\", 0, 3)).\n            setSnapshotRegistry(snapshotRegistry).\n            setClusterFeatureSupportDescriber(createFakeClusterFeatureSupportDescriber(\n                Collections.singletonList(new SimpleImmutableEntry<>(5, singletonMap(\"bar\", VersionRange.of(0, 3)))),\n                emptyList())).\n            build();\n\n        assertEquals(ControllerResult.atomicOf(emptyList(),\n            Collections.singletonMap(\"foo\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 3 for feature foo. Broker 5 does not support this feature.\"))),\n                    manager.updateFeatures(updateMap(\"foo\", 3),\n                        Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                        false));\n\n        ControllerResult<Map<String, ApiError>> result = manager.updateFeatures(\n            updateMap(\"bar\", 3), Collections.emptyMap(), false);\n        assertEquals(Collections.singletonMap(\"bar\", ApiError.NONE), result.response());\n        manager.replay((FeatureLevelRecord) result.records().get(0).message());\n        idempotentCreateSnapshot(snapshotRegistry);\n\n        assertEquals(ControllerResult.atomicOf(emptyList(), Collections.\n                singletonMap(\"bar\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                    \"Invalid update version 2 for feature bar. Can't downgrade the version of this feature \" +\n                    \"without setting the upgrade type to either safe or unsafe downgrade.\"))),\n            manager.updateFeatures(updateMap(\"bar\", 2), Collections.emptyMap(), false));\n\n        assertEquals(\n            ControllerResult.atomicOf(\n                Collections.singletonList(\n                    new ApiMessageAndVersion(\n                        new FeatureLevelRecord()\n                            .setName(\"bar\")\n                            .setFeatureLevel((short) 2),\n                        (short) 0\n                    )\n                ),\n                Collections.singletonMap(\"bar\", ApiError.NONE)\n            ),\n            manager.updateFeatures(\n                updateMap(\"bar\", 2),\n                Collections.singletonMap(\"bar\", FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                false)\n        );\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(3);\n    }\n\n    @Test\n    public void testReplayRecords() throws Exception {\n        LogContext logContext = new LogContext();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setLogContext(logContext).\n            setQuorumFeatures(features(\"foo\", 1, 5, \"bar\", 1, 2)).\n            setSnapshotRegistry(snapshotRegistry).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n            build();\n        ControllerResult<Map<String, ApiError>> result = manager.\n            updateFeatures(updateMap(\"foo\", 5, \"bar\", 1), Collections.emptyMap(), false);\n        RecordTestUtils.replayAll(manager, result.records());\n        assertEquals(MetadataVersion.IBP_3_3_IV0, manager.metadataVersion());\n        assertEquals(Optional.of((short) 5), manager.finalizedFeatures(Long.MAX_VALUE).get(\"foo\"));\n        assertEquals(Optional.of((short) 1), manager.finalizedFeatures(Long.MAX_VALUE).get(\"bar\"));\n        assertEquals(new HashSet<>(Arrays.asList(\n            MetadataVersion.FEATURE_NAME, \"foo\", \"bar\")),\n                manager.finalizedFeatures(Long.MAX_VALUE).featureNames());\n    }\n\n    private static final FeatureControlManager.Builder TEST_MANAGER_BUILDER1 =\n        new FeatureControlManager.Builder().\n            setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                MetadataVersion.IBP_3_3_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV3.featureLevel())).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV2);\n\n    @Test\n    public void testApplyMetadataVersionChangeRecord() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        manager.replay(new FeatureLevelRecord().\n            setName(MetadataVersion.FEATURE_NAME).\n            setFeatureLevel(MetadataVersion.IBP_3_3_IV3.featureLevel()));\n        assertEquals(MetadataVersion.IBP_3_3_IV3, manager.metadataVersion());\n    }\n\n    @Test\n    public void testCannotDowngradeToVersionBeforeMinimumSupportedKraftVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 3 for feature metadata.version. Local controller 0 only \" +\n                \"supports versions 4-7\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_2_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotDowngradeToHigherVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 7 for feature metadata.version. Can't downgrade to a \" +\n                \"newer version.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV3.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotUnsafeDowngradeToHigherVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 7 for feature metadata.version. Can't downgrade to a \" +\n                \"newer version.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV3.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotUpgradeToLowerVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 4 for feature metadata.version. Can't downgrade the \" +\n                \"version of this feature without setting the upgrade type to either safe or \" +\n                \"unsafe downgrade.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n                true));\n    }\n\n    @Test\n    public void testCanUpgradeToHigherVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, ApiError.NONE)),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV3.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotUseSafeDowngradeIfMetadataChanged() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid metadata.version 4. Refusing to perform the requested downgrade because \" +\n                \"it might delete metadata information.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testUnsafeDowngradeIsTemporarilyDisabled() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                                \"Invalid metadata.version 4. Unsafe metadata downgrade is not supported in this version.\"))),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Disabled\n    @Test\n    public void testCanUseUnsafeDowngradeIfMetadataChanged() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, ApiError.NONE)),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Test\n    public void testCanUseSafeDowngradeIfMetadataDidNotChange() {\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n                setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                        MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV1.featureLevel())).\n                setMetadataVersion(MetadataVersion.IBP_3_1_IV0).\n                setMinimumBootstrapVersion(MetadataVersion.IBP_3_0_IV0).\n                build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, ApiError.NONE)),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_0_IV1.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Test\n    public void testCannotDowngradeBefore3_3_IV0() {\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV3.featureLevel())).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n            build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                        \"Invalid metadata.version 3. Unable to set a metadata.version less than 3.3-IV0\"))),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_2_IV0.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Test\n    public void testCreateFeatureLevelRecords() {\n        Map<String, VersionRange> localSupportedFeatures = new HashMap<>();\n        localSupportedFeatures.put(MetadataVersion.FEATURE_NAME, VersionRange.of(\n            MetadataVersion.IBP_3_0_IV1.featureLevel(), MetadataVersion.latestTesting().featureLevel()));\n        localSupportedFeatures.put(\"foo\", VersionRange.of(0, 2));\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(new QuorumFeatures(0, localSupportedFeatures, emptyList())).\n            setClusterFeatureSupportDescriber(createFakeClusterFeatureSupportDescriber(\n                Collections.singletonList(new SimpleImmutableEntry<>(1, singletonMap(\"foo\", VersionRange.of(0, 3)))),\n                emptyList())).\n                build();\n        ControllerResult<Map<String, ApiError>> result  = manager.updateFeatures(\n                Collections.singletonMap(\"foo\", (short) 1),\n                Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.UPGRADE),\n                false);\n        assertEquals(ControllerResult.atomicOf(Collections.singletonList(new ApiMessageAndVersion(\n                new FeatureLevelRecord().setName(\"foo\").setFeatureLevel((short) 1), (short) 0)),\n                        Collections.singletonMap(\"foo\", ApiError.NONE)), result);\n        RecordTestUtils.replayAll(manager, result.records());\n        assertEquals(Optional.of((short) 1), manager.finalizedFeatures(Long.MAX_VALUE).get(\"foo\"));\n\n        ControllerResult<Map<String, ApiError>> result2  = manager.updateFeatures(\n                Collections.singletonMap(\"foo\", (short) 0),\n                Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                false);\n        assertEquals(ControllerResult.atomicOf(Collections.singletonList(new ApiMessageAndVersion(\n                        new FeatureLevelRecord().setName(\"foo\").setFeatureLevel((short) 0), (short) 0)),\n                Collections.singletonMap(\"foo\", ApiError.NONE)), result2);\n        RecordTestUtils.replayAll(manager, result2.records());\n        assertEquals(Optional.empty(), manager.finalizedFeatures(Long.MAX_VALUE).get(\"foo\"));\n    }\n\n    @Test\n    public void testNoMetadataVersionChangeDuringMigration() {\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                    MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_5_IV1.featureLevel())).\n            setMetadataVersion(MetadataVersion.IBP_3_4_IV0).\n            build();\n        BootstrapMetadata bootstrapMetadata = BootstrapMetadata.fromVersion(MetadataVersion.IBP_3_4_IV0, \"FeatureControlManagerTest\");\n        RecordTestUtils.replayAll(manager, bootstrapMetadata.records());\n        RecordTestUtils.replayOne(manager, ZkMigrationState.PRE_MIGRATION.toRecord());\n\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid metadata.version 10. Unable to modify metadata.version while a ZK migration is in progress.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_5_IV1.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n                true));\n\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid metadata.version 4. Unable to modify metadata.version while a ZK migration is in progress.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                true));\n\n        // Complete the migration\n        RecordTestUtils.replayOne(manager, ZkMigrationState.POST_MIGRATION.toRecord());\n        ControllerResult<Map<String, ApiError>> result = manager.updateFeatures(\n            singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_5_IV1.featureLevel()),\n            singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n            false);\n        assertEquals(Errors.NONE, result.response().get(MetadataVersion.FEATURE_NAME).error());\n        RecordTestUtils.replayAll(manager, result.records());\n        assertEquals(MetadataVersion.IBP_3_5_IV1, manager.metadataVersion());\n    }\n}",
                "methodCount": 25
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 4,
                "candidates": [
                    {
                        "lineStart": 214,
                        "lineEnd": 216,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method idempotentCreateSnapshot() is directly related to the functions of the SnapshotRegistry class, as it interacts with the snapshot management functionalities (getOrCreateSnapshot). Embedding it within SnapshotRegistry ensures that snapshot creation logic remains encapsulated, adheres to the Single Responsibility Principle, and enhances maintainability. Moving it here aligns the method with existing snapshot-related methods, making the codebase more intuitive and consistent.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 84,
                        "lineEnd": 92,
                        "refactoringType": "MyMoveStaticMethodRefactoring",
                        "refactoringInfo": "Move Static method updateMap to class SslConfigsBuilder",
                        "description": "move method updateMap to PsiClass:SslConfigsBuilder\nRationale: The 'updateMap' method is more closely aligned with the responsibilities of the SslConfigsBuilder class, which involves constructing and managing SSL configurations, including operations on Map objects that hold configuration details. The TestSslUtils class primarily deals with various SSL utility methods that generate certificates and handle KeyStores, whereas the OAuthBearerValidationUtilsTest class is focused on testing OAuth Bearer token validation. Hence, the method should move to SslConfigsBuilder for better cohesion and relevance to the class responsibilities.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 68,
                        "lineEnd": 76,
                        "refactoringType": "MyMoveStaticMethodRefactoring",
                        "refactoringInfo": "Move Static method versionMap to class SslConfigsBuilder",
                        "description": "move method versionMap to PsiClass:SslConfigsBuilder\nRationale: The method `versionMap` is responsible for generating a map from given arguments, which seems to fit better in a configuration or utility class related to managing configurations or creating mappings. `SslConfigsBuilder` already handles various configurations and is responsible for building SSL configurations. Adding `versionMap` to this class maintains cohesion as it could be used to manage version configurations as well. Moreover, `SslConfigsBuilder` already has several static utility methods, making it a natural home for the `versionMap` method.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 56,
                        "lineEnd": 66,
                        "refactoringType": "MyMoveStaticMethodRefactoring",
                        "refactoringInfo": "Move Static method rangeMap to class ScramParser",
                        "description": "move method rangeMap to PsiClass:ScramParser\nRationale: The rangeMap method processes an array of arguments to create a map of version ranges, which aligns with ScramParser's responsibility for parsing and assembling data structures from input arguments. Integrating this method into ScramParser aligns with Single Responsibility Principle and enhances cohesion. Additionally, ScramParser already contains methods related to parsing and structuring data, making it a logical home for rangeMap.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "rangeMap",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "versionMap",
                            "method_signature": "private static versionMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "features",
                            "method_signature": "public static features(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateMap",
                            "method_signature": "private static updateMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "features",
                            "method_signature": "public static features(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "rangeMap",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "versionMap",
                            "method_signature": "private static versionMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateMap",
                            "method_signature": "private static updateMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "features",
                            "method_signature": "public static features(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "rangeMap",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "versionMap",
                            "method_signature": "private static versionMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateMap",
                            "method_signature": "private static updateMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.25734306657093914
                },
                "public static features(Object... args)": {
                    "first": {
                        "method_name": "features",
                        "method_signature": "public static features(Object... args)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.41132873798477343
                },
                "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)": {
                    "first": {
                        "method_name": "rangeMap",
                        "method_signature": "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4943269859019432
                },
                "private static versionMap(Object... args)": {
                    "first": {
                        "method_name": "versionMap",
                        "method_signature": "private static versionMap(Object... args)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5196640759393861
                },
                "private static updateMap(Object... args)": {
                    "first": {
                        "method_name": "updateMap",
                        "method_signature": "private static updateMap(Object... args)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5196640759393861
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                    "private static updateMap(Object... args)",
                    "private static versionMap(Object... args)",
                    "public static features(Object... args)",
                    "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)"
                ],
                "llm_response_time": 6303
            },
            "targetClassMap": {
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        },
                        {
                            "class_name": "Builder",
                            "similarity_score": 0.3710212605396185
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry",
                        "Builder"
                    ],
                    "llm_response_time": 3373,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "updateMap": {
                    "target_classes": [
                        {
                            "class_name": "ControllerMetricsTestUtils",
                            "similarity_score": 0.5230682148205612
                        },
                        {
                            "class_name": "ControllerRequestContextUtil",
                            "similarity_score": 0.3235991778623843
                        },
                        {
                            "class_name": "QuorumControllerIntegrationTestUtils",
                            "similarity_score": 0.41778030532391136
                        },
                        {
                            "class_name": "ClaimValidationUtils",
                            "similarity_score": 0.08960530171006027
                        },
                        {
                            "class_name": "AdminClientTestUtils",
                            "similarity_score": 0.3024186401562943
                        },
                        {
                            "class_name": "AdminUtils",
                            "similarity_score": 0.2976216922434756
                        },
                        {
                            "class_name": "ClientTelemetryUtils",
                            "similarity_score": 0.47936195012271127
                        },
                        {
                            "class_name": "TestSslUtils",
                            "similarity_score": 0.5728068308660601
                        },
                        {
                            "class_name": "ThreadUtils",
                            "similarity_score": 0.3021660931112009
                        },
                        {
                            "class_name": "CommandLineUtils",
                            "similarity_score": 0.33168373363760056
                        },
                        {
                            "class_name": "SecurityUtils",
                            "similarity_score": 0.5334870336969845
                        },
                        {
                            "class_name": "ConfigUtils",
                            "similarity_score": 0.31239418510690087
                        },
                        {
                            "class_name": "MessageUtil",
                            "similarity_score": 0.44424497091197873
                        },
                        {
                            "class_name": "ConsumerUtils",
                            "similarity_score": 0.4733106517221084
                        },
                        {
                            "class_name": "MetricsUtils",
                            "similarity_score": 0.2663780856520238
                        },
                        {
                            "class_name": "ControlRecordUtils",
                            "similarity_score": 0.4147272803911595
                        },
                        {
                            "class_name": "FetchUtils",
                            "similarity_score": 0.05206086001129595
                        },
                        {
                            "class_name": "NetworkTestUtils",
                            "similarity_score": 0.48101535834564235
                        },
                        {
                            "class_name": "FutureUtils",
                            "similarity_score": 0.20854541300380916
                        },
                        {
                            "class_name": "PropertiesUtils",
                            "similarity_score": 0.3392833828023668
                        },
                        {
                            "class_name": "RequestTestUtils",
                            "similarity_score": 0.37494370536553573
                        },
                        {
                            "class_name": "RaftUtil",
                            "similarity_score": 0.40802503529583395
                        },
                        {
                            "class_name": "RecordsUtil",
                            "similarity_score": 0.39092786730806
                        },
                        {
                            "class_name": "RecordTestUtils",
                            "similarity_score": 0.4858468349007344
                        },
                        {
                            "class_name": "ProducerTestUtils",
                            "similarity_score": 0.34877513670672466
                        },
                        {
                            "class_name": "Utils",
                            "similarity_score": 0.3073339579117863
                        },
                        {
                            "class_name": "ByteUtils",
                            "similarity_score": 0.2230517779778567
                        },
                        {
                            "class_name": "OAuthBearerValidationUtils",
                            "similarity_score": 0.16559104438052208
                        },
                        {
                            "class_name": "NetworkClientUtils",
                            "similarity_score": 0.23531956447757144
                        },
                        {
                            "class_name": "ScramCredentialUtils",
                            "similarity_score": 0.5017763968150261
                        },
                        {
                            "class_name": "CollectionUtils",
                            "similarity_score": 0.33828585081362084
                        },
                        {
                            "class_name": "RequestUtils",
                            "similarity_score": 0.40228814150092784
                        },
                        {
                            "class_name": "JaasUtils",
                            "similarity_score": 0.42920157150778865
                        },
                        {
                            "class_name": "OffsetsForLeaderEpochUtils",
                            "similarity_score": 0.439524041295631
                        },
                        {
                            "class_name": "OAuthBearerScopeUtils",
                            "similarity_score": 0.16798530752762428
                        },
                        {
                            "class_name": "OAuthBearerValidationUtilsTest",
                            "similarity_score": 0.5537474559362661
                        },
                        {
                            "class_name": "JaasOptionsUtils",
                            "similarity_score": 0.4713582375702075
                        },
                        {
                            "class_name": "CommandLineUtilsTest",
                            "similarity_score": 0.32994621264709656
                        },
                        {
                            "class_name": "UtilsTest",
                            "similarity_score": 0.540720367870316
                        },
                        {
                            "class_name": "CertificateBuilder",
                            "similarity_score": 0.4687887344586328
                        },
                        {
                            "class_name": "ClaimValidationUtilsTest",
                            "similarity_score": 0.3251029475173706
                        },
                        {
                            "class_name": "ClientTelemetryUtilsTest",
                            "similarity_score": 0.3455765205731853
                        },
                        {
                            "class_name": "TestSslEngineFactory",
                            "similarity_score": 0.32528969464480484
                        },
                        {
                            "class_name": "TestThroughAllIntermediateImagesLeadingToFinalImageHelper",
                            "similarity_score": 0.5088785581827483
                        },
                        {
                            "class_name": "ThreadUtilsTest",
                            "similarity_score": 0.4391168504129238
                        },
                        {
                            "class_name": "ScramCredentialUtilsTest",
                            "similarity_score": 0.3929882209189315
                        },
                        {
                            "class_name": "ImageDeltaPair",
                            "similarity_score": 0.317380885407937
                        },
                        {
                            "class_name": "CollectionUtilsTest",
                            "similarity_score": 0.41890312442585187
                        },
                        {
                            "class_name": "SecurityUtilsTest",
                            "similarity_score": 0.5019791449522171
                        },
                        {
                            "class_name": "ByteUtilsTest",
                            "similarity_score": 0.39369128935422
                        },
                        {
                            "class_name": "ConfigurationUtils",
                            "similarity_score": 0.24353919907812213
                        },
                        {
                            "class_name": "ConfigurationUtilsTest",
                            "similarity_score": 0.29704166429653484
                        },
                        {
                            "class_name": "ConfigUtilsTest",
                            "similarity_score": 0.44960356879387064
                        },
                        {
                            "class_name": "TopicIdPartitionComparator",
                            "similarity_score": 0.4256282653793743
                        },
                        {
                            "class_name": "TopicPartitionComparator",
                            "similarity_score": 0.4517351373863803
                        },
                        {
                            "class_name": "MessageUtilTest",
                            "similarity_score": 0.2577849670652873
                        },
                        {
                            "class_name": "SslConfigsBuilder",
                            "similarity_score": 0.5712337931469162
                        },
                        {
                            "class_name": "JaasOptionsUtilsTest",
                            "similarity_score": 0.47312184713712263
                        },
                        {
                            "class_name": "MetricsUtilsTest",
                            "similarity_score": 0.26473451003726783
                        },
                        {
                            "class_name": "ControlRecordUtilsTest",
                            "similarity_score": 0.3431409510633933
                        },
                        {
                            "class_name": "FutureUtilsTest",
                            "similarity_score": 0.381101994652465
                        },
                        {
                            "class_name": "PropertiesUtilsTest",
                            "similarity_score": 0.41845020827082885
                        },
                        {
                            "class_name": "OAuthBearerScopeUtilsTest",
                            "similarity_score": 0.3307942673698644
                        },
                        {
                            "class_name": "ProtoUtilsTest",
                            "similarity_score": 0.15574226979044098
                        },
                        {
                            "class_name": "ActivationRecordsGenerator",
                            "similarity_score": 0.2593517248884098
                        },
                        {
                            "class_name": "ControllerExceptions",
                            "similarity_score": 0.22302163169549574
                        },
                        {
                            "class_name": "DirectoryId",
                            "similarity_score": 0.2682321468668362
                        },
                        {
                            "class_name": "AssignmentsHelper",
                            "similarity_score": 0.48665579620546473
                        },
                        {
                            "class_name": "AppInfoParser",
                            "similarity_score": 0.485656830852641
                        },
                        {
                            "class_name": "AccessTokenRetrieverFactory",
                            "similarity_score": 0.4174235549683609
                        },
                        {
                            "class_name": "AccessTokenValidatorFactory",
                            "similarity_score": 0.4859714909220032
                        },
                        {
                            "class_name": "Sanitizer",
                            "similarity_score": 0.3070603914522629
                        },
                        {
                            "class_name": "SaslConfigs",
                            "similarity_score": 0.289398870169027
                        },
                        {
                            "class_name": "ScramParser",
                            "similarity_score": 0.5447008699976473
                        },
                        {
                            "class_name": "CommonClientConfigs",
                            "similarity_score": 0.290038120710786
                        },
                        {
                            "class_name": "Serdes",
                            "similarity_score": 0.3503802333879169
                        },
                        {
                            "class_name": "Serializer",
                            "similarity_score": 0.4516571961342602
                        },
                        {
                            "class_name": "CompressionRatioEstimator",
                            "similarity_score": 0.392516780601977
                        },
                        {
                            "class_name": "ServerTopicConfigSynonyms",
                            "similarity_score": 0.31518112457883296
                        },
                        {
                            "class_name": "Topic",
                            "similarity_score": 0.39298820109646987
                        },
                        {
                            "class_name": "Exit",
                            "similarity_score": 0.3653015133788107
                        },
                        {
                            "class_name": "ShareGroupHelper",
                            "similarity_score": 0.12921165649395971
                        },
                        {
                            "class_name": "ConsumerProtocol",
                            "similarity_score": 0.44854244220185563
                        },
                        {
                            "class_name": "SslConfigs",
                            "similarity_score": 0.314980247692483
                        },
                        {
                            "class_name": "MetricsBench",
                            "similarity_score": 0.5116429598912723
                        },
                        {
                            "class_name": "Snapshots",
                            "similarity_score": 0.5219806204409719
                        },
                        {
                            "class_name": "Json",
                            "similarity_score": 0.23324759035367781
                        },
                        {
                            "class_name": "Csv",
                            "similarity_score": 0.32314097407232784
                        },
                        {
                            "class_name": "OffsetSpec",
                            "similarity_score": 0.22847956398461716
                        },
                        {
                            "class_name": "VerificationKeyResolverFactory",
                            "similarity_score": 0.43248439857877846
                        },
                        {
                            "class_name": "PartitionFactory",
                            "similarity_score": 0.3060982830947443
                        },
                        {
                            "class_name": "Protocol",
                            "similarity_score": 0.5431281914792646
                        },
                        {
                            "class_name": "TelemetryMetricNamingConvention",
                            "similarity_score": 0.26451974684311724
                        },
                        {
                            "class_name": "Replicas",
                            "similarity_score": 0.36758317751884195
                        },
                        {
                            "class_name": "PrimitiveRef",
                            "similarity_score": 0.3554280338863895
                        },
                        {
                            "class_name": "QuotaConfigs",
                            "similarity_score": 0.3561781420553535
                        },
                        {
                            "class_name": "BrokerToElrsTest",
                            "similarity_score": 0.35577779597974063
                        },
                        {
                            "class_name": "ChannelBuilders",
                            "similarity_score": 0.3378457167053045
                        },
                        {
                            "class_name": "Checksums",
                            "similarity_score": 0.37396456490472985
                        },
                        {
                            "class_name": "ByteBufferUnmapper",
                            "similarity_score": 0.444128806861606
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SslConfigsBuilder",
                        "TestSslUtils",
                        "OAuthBearerValidationUtilsTest"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 47,
                    "similarity_metric": "cosine"
                },
                "versionMap": {
                    "target_classes": [
                        {
                            "class_name": "ControllerMetricsTestUtils",
                            "similarity_score": 0.5230682148205612
                        },
                        {
                            "class_name": "ControllerRequestContextUtil",
                            "similarity_score": 0.3235991778623843
                        },
                        {
                            "class_name": "QuorumControllerIntegrationTestUtils",
                            "similarity_score": 0.41778030532391136
                        },
                        {
                            "class_name": "ClaimValidationUtils",
                            "similarity_score": 0.08960530171006027
                        },
                        {
                            "class_name": "SecurityUtils",
                            "similarity_score": 0.5334870336969845
                        },
                        {
                            "class_name": "ClientTelemetryUtils",
                            "similarity_score": 0.47936195012271127
                        },
                        {
                            "class_name": "CommandLineUtils",
                            "similarity_score": 0.33168373363760056
                        },
                        {
                            "class_name": "ConfigUtils",
                            "similarity_score": 0.31239418510690087
                        },
                        {
                            "class_name": "MessageUtil",
                            "similarity_score": 0.44424497091197873
                        },
                        {
                            "class_name": "ProducerTestUtils",
                            "similarity_score": 0.34877513670672466
                        },
                        {
                            "class_name": "MetricsUtils",
                            "similarity_score": 0.2663780856520238
                        },
                        {
                            "class_name": "PropertiesUtils",
                            "similarity_score": 0.3392833828023668
                        },
                        {
                            "class_name": "ConsumerUtils",
                            "similarity_score": 0.4733106517221084
                        },
                        {
                            "class_name": "ControlRecordUtils",
                            "similarity_score": 0.4147272803911595
                        },
                        {
                            "class_name": "FetchUtils",
                            "similarity_score": 0.05206086001129595
                        },
                        {
                            "class_name": "RaftUtil",
                            "similarity_score": 0.40802503529583395
                        },
                        {
                            "class_name": "RecordsUtil",
                            "similarity_score": 0.39092786730806
                        },
                        {
                            "class_name": "RecordTestUtils",
                            "similarity_score": 0.4858468349007344
                        },
                        {
                            "class_name": "AdminClientTestUtils",
                            "similarity_score": 0.3024186401562943
                        },
                        {
                            "class_name": "AdminUtils",
                            "similarity_score": 0.2976216922434756
                        },
                        {
                            "class_name": "NetworkTestUtils",
                            "similarity_score": 0.48101535834564235
                        },
                        {
                            "class_name": "RequestTestUtils",
                            "similarity_score": 0.37494370536553573
                        },
                        {
                            "class_name": "FutureUtils",
                            "similarity_score": 0.20854541300380916
                        },
                        {
                            "class_name": "TestSslUtils",
                            "similarity_score": 0.5728068308660601
                        },
                        {
                            "class_name": "ThreadUtils",
                            "similarity_score": 0.3021660931112009
                        },
                        {
                            "class_name": "Utils",
                            "similarity_score": 0.3073339579117863
                        },
                        {
                            "class_name": "ByteUtils",
                            "similarity_score": 0.2230517779778567
                        },
                        {
                            "class_name": "OAuthBearerValidationUtils",
                            "similarity_score": 0.16559104438052208
                        },
                        {
                            "class_name": "NetworkClientUtils",
                            "similarity_score": 0.23531956447757144
                        },
                        {
                            "class_name": "ScramCredentialUtils",
                            "similarity_score": 0.5017763968150261
                        },
                        {
                            "class_name": "CollectionUtils",
                            "similarity_score": 0.33828585081362084
                        },
                        {
                            "class_name": "RequestUtils",
                            "similarity_score": 0.40228814150092784
                        },
                        {
                            "class_name": "OffsetsForLeaderEpochUtils",
                            "similarity_score": 0.439524041295631
                        },
                        {
                            "class_name": "JaasUtils",
                            "similarity_score": 0.42920157150778865
                        },
                        {
                            "class_name": "OAuthBearerScopeUtils",
                            "similarity_score": 0.16798530752762428
                        },
                        {
                            "class_name": "OAuthBearerValidationUtilsTest",
                            "similarity_score": 0.5537474559362661
                        },
                        {
                            "class_name": "JaasOptionsUtils",
                            "similarity_score": 0.4713582375702075
                        },
                        {
                            "class_name": "CommandLineUtilsTest",
                            "similarity_score": 0.32994621264709656
                        },
                        {
                            "class_name": "UtilsTest",
                            "similarity_score": 0.540720367870316
                        },
                        {
                            "class_name": "CertificateBuilder",
                            "similarity_score": 0.4687887344586328
                        },
                        {
                            "class_name": "ScramCredentialUtilsTest",
                            "similarity_score": 0.3929882209189315
                        },
                        {
                            "class_name": "ClaimValidationUtilsTest",
                            "similarity_score": 0.3251029475173706
                        },
                        {
                            "class_name": "SecurityUtilsTest",
                            "similarity_score": 0.5019791449522171
                        },
                        {
                            "class_name": "ClientTelemetryUtilsTest",
                            "similarity_score": 0.3455765205731853
                        },
                        {
                            "class_name": "CollectionUtilsTest",
                            "similarity_score": 0.41890312442585187
                        },
                        {
                            "class_name": "ImageDeltaPair",
                            "similarity_score": 0.317380885407937
                        },
                        {
                            "class_name": "ByteUtilsTest",
                            "similarity_score": 0.39369128935422
                        },
                        {
                            "class_name": "ConfigurationUtils",
                            "similarity_score": 0.24353919907812213
                        },
                        {
                            "class_name": "ConfigurationUtilsTest",
                            "similarity_score": 0.29704166429653484
                        },
                        {
                            "class_name": "ConfigUtilsTest",
                            "similarity_score": 0.44960356879387064
                        },
                        {
                            "class_name": "MessageUtilTest",
                            "similarity_score": 0.2577849670652873
                        },
                        {
                            "class_name": "MetricsUtilsTest",
                            "similarity_score": 0.26473451003726783
                        },
                        {
                            "class_name": "PropertiesUtilsTest",
                            "similarity_score": 0.41845020827082885
                        },
                        {
                            "class_name": "ProtoUtilsTest",
                            "similarity_score": 0.15574226979044098
                        },
                        {
                            "class_name": "ControlRecordUtilsTest",
                            "similarity_score": 0.3431409510633933
                        },
                        {
                            "class_name": "SslConfigsBuilder",
                            "similarity_score": 0.5712337931469162
                        },
                        {
                            "class_name": "JaasOptionsUtilsTest",
                            "similarity_score": 0.47312184713712263
                        },
                        {
                            "class_name": "OAuthBearerScopeUtilsTest",
                            "similarity_score": 0.3307942673698644
                        },
                        {
                            "class_name": "FutureUtilsTest",
                            "similarity_score": 0.381101994652465
                        },
                        {
                            "class_name": "TestSslEngineFactory",
                            "similarity_score": 0.32528969464480484
                        },
                        {
                            "class_name": "TestThroughAllIntermediateImagesLeadingToFinalImageHelper",
                            "similarity_score": 0.5088785581827483
                        },
                        {
                            "class_name": "ThreadUtilsTest",
                            "similarity_score": 0.4391168504129238
                        },
                        {
                            "class_name": "TopicPartitionComparator",
                            "similarity_score": 0.4517351373863803
                        },
                        {
                            "class_name": "TopicIdPartitionComparator",
                            "similarity_score": 0.4256282653793743
                        },
                        {
                            "class_name": "ActivationRecordsGenerator",
                            "similarity_score": 0.2593517248884098
                        },
                        {
                            "class_name": "ControllerExceptions",
                            "similarity_score": 0.22302163169549574
                        },
                        {
                            "class_name": "OffsetSpec",
                            "similarity_score": 0.22847956398461716
                        },
                        {
                            "class_name": "Sanitizer",
                            "similarity_score": 0.3070603914522629
                        },
                        {
                            "class_name": "SaslConfigs",
                            "similarity_score": 0.289398870169027
                        },
                        {
                            "class_name": "DirectoryId",
                            "similarity_score": 0.2682321468668362
                        },
                        {
                            "class_name": "ScramParser",
                            "similarity_score": 0.5447008699976473
                        },
                        {
                            "class_name": "Serdes",
                            "similarity_score": 0.3503802333879169
                        },
                        {
                            "class_name": "AccessTokenRetrieverFactory",
                            "similarity_score": 0.4174235549683609
                        },
                        {
                            "class_name": "AccessTokenValidatorFactory",
                            "similarity_score": 0.4859714909220032
                        },
                        {
                            "class_name": "Serializer",
                            "similarity_score": 0.4516571961342602
                        },
                        {
                            "class_name": "PartitionFactory",
                            "similarity_score": 0.3060982830947443
                        },
                        {
                            "class_name": "ServerTopicConfigSynonyms",
                            "similarity_score": 0.31518112457883296
                        },
                        {
                            "class_name": "ShareGroupHelper",
                            "similarity_score": 0.12921165649395971
                        },
                        {
                            "class_name": "CommonClientConfigs",
                            "similarity_score": 0.290038120710786
                        },
                        {
                            "class_name": "CompressionRatioEstimator",
                            "similarity_score": 0.392516780601977
                        },
                        {
                            "class_name": "PrimitiveRef",
                            "similarity_score": 0.3554280338863895
                        },
                        {
                            "class_name": "Snapshots",
                            "similarity_score": 0.5219806204409719
                        },
                        {
                            "class_name": "Exit",
                            "similarity_score": 0.3653015133788107
                        },
                        {
                            "class_name": "MetricsBench",
                            "similarity_score": 0.5116429598912723
                        },
                        {
                            "class_name": "ConsumerProtocol",
                            "similarity_score": 0.44854244220185563
                        },
                        {
                            "class_name": "Protocol",
                            "similarity_score": 0.5431281914792646
                        },
                        {
                            "class_name": "SslConfigs",
                            "similarity_score": 0.314980247692483
                        },
                        {
                            "class_name": "AppInfoParser",
                            "similarity_score": 0.485656830852641
                        },
                        {
                            "class_name": "QuotaConfigs",
                            "similarity_score": 0.3561781420553535
                        },
                        {
                            "class_name": "Csv",
                            "similarity_score": 0.32314097407232784
                        },
                        {
                            "class_name": "AssignmentsHelper",
                            "similarity_score": 0.48665579620546473
                        },
                        {
                            "class_name": "Json",
                            "similarity_score": 0.23324759035367781
                        },
                        {
                            "class_name": "Replicas",
                            "similarity_score": 0.36758317751884195
                        },
                        {
                            "class_name": "TelemetryMetricNamingConvention",
                            "similarity_score": 0.26451974684311724
                        },
                        {
                            "class_name": "VerificationKeyResolverFactory",
                            "similarity_score": 0.43248439857877846
                        },
                        {
                            "class_name": "Topic",
                            "similarity_score": 0.39298820109646987
                        },
                        {
                            "class_name": "BrokerToElrsTest",
                            "similarity_score": 0.35577779597974063
                        },
                        {
                            "class_name": "ChannelBuilders",
                            "similarity_score": 0.3378457167053045
                        },
                        {
                            "class_name": "Checksums",
                            "similarity_score": 0.37396456490472985
                        },
                        {
                            "class_name": "ByteBufferUnmapper",
                            "similarity_score": 0.444128806861606
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SslConfigsBuilder",
                        "TestSslUtils",
                        "OAuthBearerValidationUtilsTest"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 22,
                    "similarity_metric": "cosine"
                },
                "features": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "rangeMap": {
                    "target_classes": [
                        {
                            "class_name": "ControllerMetricsTestUtils",
                            "similarity_score": 0.490825920794188
                        },
                        {
                            "class_name": "ControllerRequestContextUtil",
                            "similarity_score": 0.28833765854913374
                        },
                        {
                            "class_name": "QuorumControllerIntegrationTestUtils",
                            "similarity_score": 0.4024668898971298
                        },
                        {
                            "class_name": "PropertiesUtils",
                            "similarity_score": 0.32556763957029866
                        },
                        {
                            "class_name": "RecordTestUtils",
                            "similarity_score": 0.4517693740509775
                        },
                        {
                            "class_name": "TestThroughAllIntermediateImagesLeadingToFinalImageHelper",
                            "similarity_score": 0.48639149973351714
                        },
                        {
                            "class_name": "PropertiesUtilsTest",
                            "similarity_score": 0.4006905375132152
                        },
                        {
                            "class_name": "ImageDeltaPair",
                            "similarity_score": 0.2906732603092963
                        },
                        {
                            "class_name": "ActivationRecordsGenerator",
                            "similarity_score": 0.24689213193069717
                        },
                        {
                            "class_name": "ControllerExceptions",
                            "similarity_score": 0.2011333106794384
                        },
                        {
                            "class_name": "ScramParser",
                            "similarity_score": 0.5397612556628663
                        },
                        {
                            "class_name": "Replicas",
                            "similarity_score": 0.3444564427364337
                        },
                        {
                            "class_name": "BrokerToElrsTest",
                            "similarity_score": 0.326207872833152
                        },
                        {
                            "class_name": "ClientQuotaControlManager",
                            "similarity_score": 0.48150809531455746
                        },
                        {
                            "class_name": "BrokersToIsrsTest",
                            "similarity_score": 0.2597288702942012
                        },
                        {
                            "class_name": "ControllerMetadataMetricsPublisherTest",
                            "similarity_score": 0.39219806645940514
                        },
                        {
                            "class_name": "ScramControlManager",
                            "similarity_score": 0.5047690255303079
                        },
                        {
                            "class_name": "StripedReplicaPlacer",
                            "similarity_score": 0.26898082181806393
                        },
                        {
                            "class_name": "ClientQuotaControlManagerTest",
                            "similarity_score": 0.3924124960571338
                        },
                        {
                            "class_name": "QuorumControllerMetricsTest",
                            "similarity_score": 0.5281082552729583
                        },
                        {
                            "class_name": "ScramImageTest",
                            "similarity_score": 0.5073554423970249
                        },
                        {
                            "class_name": "AclsImageTest",
                            "similarity_score": 0.5328370302677876
                        },
                        {
                            "class_name": "PartitionChangeBuilderTest",
                            "similarity_score": 0.46346240467307653
                        },
                        {
                            "class_name": "ClientQuotasImageNode",
                            "similarity_score": 0.5318624039693715
                        },
                        {
                            "class_name": "ClientQuotasImageTest",
                            "similarity_score": 0.46842593482303035
                        },
                        {
                            "class_name": "ConfigurationsImageTest",
                            "similarity_score": 0.4341478589590008
                        },
                        {
                            "class_name": "MetadataImageTest",
                            "similarity_score": 0.3767545863286342
                        },
                        {
                            "class_name": "ProducerIdsImageTest",
                            "similarity_score": 0.3865422274694206
                        },
                        {
                            "class_name": "ConfigurationControlManagerTest",
                            "similarity_score": 0.43285715704590727
                        },
                        {
                            "class_name": "PartitionReassignmentReplicas",
                            "similarity_score": 0.47236654654100074
                        },
                        {
                            "class_name": "DelegationTokenImageTest",
                            "similarity_score": 0.47725358990182626
                        },
                        {
                            "class_name": "AclControlManager",
                            "similarity_score": 0.4883524617908616
                        },
                        {
                            "class_name": "ControllerResult",
                            "similarity_score": 0.3332624038010199
                        },
                        {
                            "class_name": "BrokerHeartbeatManagerTest",
                            "similarity_score": 0.4143886436312194
                        },
                        {
                            "class_name": "EventHandlerExceptionInfo",
                            "similarity_score": 0.31798881840324217
                        },
                        {
                            "class_name": "ControllerMetricsChangesTest",
                            "similarity_score": 0.3444351279257109
                        },
                        {
                            "class_name": "ControllerResultAndOffset",
                            "similarity_score": 0.30606706665315775
                        },
                        {
                            "class_name": "ResultOrError",
                            "similarity_score": 0.38640580200395774
                        },
                        {
                            "class_name": "QuorumFeatures",
                            "similarity_score": 0.4676299059669639
                        },
                        {
                            "class_name": "ListenerInfo",
                            "similarity_score": 0.32156712288340583
                        },
                        {
                            "class_name": "ProducerIdControlManagerTest",
                            "similarity_score": 0.46249441560786475
                        },
                        {
                            "class_name": "QuorumControllerTest",
                            "similarity_score": 0.5231275986653078
                        },
                        {
                            "class_name": "TopicsImageTest",
                            "similarity_score": 0.4274748652364346
                        },
                        {
                            "class_name": "ClusterImageTest",
                            "similarity_score": 0.3633567713966603
                        },
                        {
                            "class_name": "FeaturesImageTest",
                            "similarity_score": 0.4256432064749635
                        },
                        {
                            "class_name": "ControllerRequestContext",
                            "similarity_score": 0.2891385481295669
                        },
                        {
                            "class_name": "ControllerMetadataMetricsTest",
                            "similarity_score": 0.26235446452512573
                        },
                        {
                            "class_name": "ControllerExceptionsTest",
                            "similarity_score": 0.2223965741461595
                        },
                        {
                            "class_name": "ReplicationControlManager",
                            "similarity_score": 0.4944428785628315
                        },
                        {
                            "class_name": "BootstrapMetadata",
                            "similarity_score": 0.4496604085902668
                        },
                        {
                            "class_name": "DelegationTokenControlManager",
                            "similarity_score": 0.5274386119366898
                        },
                        {
                            "class_name": "ImageDowngradeTest",
                            "similarity_score": 0.3098643139083865
                        },
                        {
                            "class_name": "ReplicationControlManagerTest",
                            "similarity_score": 0.4886216687796293
                        },
                        {
                            "class_name": "OffsetControlManagerTest",
                            "similarity_score": 0.32354436937971476
                        },
                        {
                            "class_name": "ClusterControlManagerTest",
                            "similarity_score": 0.4690519693274538
                        },
                        {
                            "class_name": "StandardAuthorizerData",
                            "similarity_score": 0.4101573907472663
                        },
                        {
                            "class_name": "QuorumControllerMetrics",
                            "similarity_score": 0.41473350569819456
                        },
                        {
                            "class_name": "ControllerMetadataMetrics",
                            "similarity_score": 0.3676506025299547
                        },
                        {
                            "class_name": "PartitionChangeBuilder",
                            "similarity_score": 0.3804947214517758
                        },
                        {
                            "class_name": "QuorumController",
                            "similarity_score": 0.4513821378847811
                        },
                        {
                            "class_name": "TestExistenceChecker",
                            "similarity_score": 0.3159413277642021
                        },
                        {
                            "class_name": "OffsetControlManager",
                            "similarity_score": 0.38691807264668643
                        },
                        {
                            "class_name": "ScramCredentialDataNode",
                            "similarity_score": 0.31437751023734767
                        },
                        {
                            "class_name": "ScramCredentialKey",
                            "similarity_score": 0.40213532475552316
                        },
                        {
                            "class_name": "ScramCredentialValue",
                            "similarity_score": 0.38258351848376515
                        },
                        {
                            "class_name": "ScramCredentialValueTest",
                            "similarity_score": 0.42148902413687467
                        },
                        {
                            "class_name": "AclControlManagerTest",
                            "similarity_score": 0.4223052188719406
                        },
                        {
                            "class_name": "ActivationRecordsGeneratorTest",
                            "similarity_score": 0.19405971327997806
                        },
                        {
                            "class_name": "TopicControlInfo",
                            "similarity_score": 0.3802379437965301
                        },
                        {
                            "class_name": "PartitionAssignmentTest",
                            "similarity_score": 0.4652434428686902
                        },
                        {
                            "class_name": "PartitionReassignmentReplicasTest",
                            "similarity_score": 0.25167237936472336
                        },
                        {
                            "class_name": "PartitionReassignmentRevert",
                            "similarity_score": 0.32720362820116433
                        },
                        {
                            "class_name": "PartitionReassignmentRevertTest",
                            "similarity_score": 0.25548560716243884
                        },
                        {
                            "class_name": "PartitionRegistration",
                            "similarity_score": 0.45856235371366
                        },
                        {
                            "class_name": "PartitionsOnReplicaIterator",
                            "similarity_score": 0.5271768022848743
                        },
                        {
                            "class_name": "ClusterControlManager",
                            "similarity_score": 0.4691705663517866
                        },
                        {
                            "class_name": "LogReplayTracker",
                            "similarity_score": 0.38348902172974136
                        },
                        {
                            "class_name": "LogReplayTrackerTest",
                            "similarity_score": 0.351972576457172
                        },
                        {
                            "class_name": "ElectionResult",
                            "similarity_score": 0.32616403652672105
                        },
                        {
                            "class_name": "BrokerControlStates",
                            "similarity_score": 0.3698352545411534
                        },
                        {
                            "class_name": "BrokerHeartbeatManager",
                            "similarity_score": 0.34110916657081153
                        },
                        {
                            "class_name": "BrokerHeartbeatState",
                            "similarity_score": 0.16043010139335345
                        },
                        {
                            "class_name": "BrokerHeartbeatStateIterator",
                            "similarity_score": 0.4154637515575287
                        },
                        {
                            "class_name": "BrokerHeartbeatStateList",
                            "similarity_score": 0.4322255076525336
                        },
                        {
                            "class_name": "BrokersToElrs",
                            "similarity_score": 0.47440536703770286
                        },
                        {
                            "class_name": "BrokersToIsrs",
                            "similarity_score": 0.5036125520521585
                        },
                        {
                            "class_name": "CompleteActivationEvent",
                            "similarity_score": 0.19388915267323256
                        },
                        {
                            "class_name": "CompletedReassignment",
                            "similarity_score": 0.30238833168763435
                        },
                        {
                            "class_name": "ConfigResourceExistenceChecker",
                            "similarity_score": 0.2681771284015011
                        },
                        {
                            "class_name": "ConfigurationControlManager",
                            "similarity_score": 0.5461799970971094
                        },
                        {
                            "class_name": "ConfigurationsImageNode",
                            "similarity_score": 0.4183964306705311
                        },
                        {
                            "class_name": "EventHandlerExceptionInfoTest",
                            "similarity_score": 0.28693857265149986
                        },
                        {
                            "class_name": "MetadataOffsetComparator",
                            "similarity_score": 0.3124262542812672
                        },
                        {
                            "class_name": "ProducerIdControlManager",
                            "similarity_score": 0.5034197562351374
                        },
                        {
                            "class_name": "UsableBrokerIterator",
                            "similarity_score": 0.49203142821393586
                        },
                        {
                            "class_name": "ControllerEvent",
                            "similarity_score": 0.3924741683232832
                        },
                        {
                            "class_name": "ControllerMetadataMetricsPublisher",
                            "similarity_score": 0.4140357192728847
                        },
                        {
                            "class_name": "ControllerReadEvent",
                            "similarity_score": 0.4542535579801632
                        },
                        {
                            "class_name": "ControllerWriteEvent",
                            "similarity_score": 0.4043818348039314
                        },
                        {
                            "class_name": "MigrationRecordConsumer",
                            "similarity_score": 0.36676874353846095
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "ScramParser",
                        "AclsImageTest",
                        "ConfigurationControlManager"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 34,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "b4d5f163a9c5a480efe0a1de3bebf12e35be2a7f",
        "url": "https://github.com/apache/kafka/commit/b4d5f163a9c5a480efe0a1de3bebf12e35be2a7f",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public votedKey() : Optional<ReplicaKey> extracted from public testElectionTimeout() : void in class org.apache.kafka.raft.VotedStateTest & moved to class org.apache.kafka.raft.UnattachedState",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 57,
                    "endLine": 78,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testElectionTimeout() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 63,
                    "endLine": 63,
                    "startColumn": 9,
                    "endColumn": 50,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/UnattachedState.java",
                    "startLine": 99,
                    "endLine": 101,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public votedKey() : Optional<ReplicaKey>"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/UnattachedState.java",
                    "startLine": 100,
                    "endLine": 100,
                    "startColumn": 9,
                    "endColumn": 25,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/UnattachedStateWithVoteTest.java",
                    "startLine": 59,
                    "endLine": 80,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testElectionTimeout() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/UnattachedStateWithVoteTest.java",
                    "startLine": 65,
                    "endLine": 65,
                    "startColumn": 32,
                    "endColumn": 48,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "state.votedKey()"
                }
            ],
            "isStatic": false
        },
        "ref_id": 630,
        "extraction_results": {
            "success": true,
            "newCommitHash": "0545041c57c66aa4d42a43367f40b0139da63df4",
            "newBranchName": "extract-votedKey-testElectionTimeout-e1b2ade"
        },
        "telemetry": {
            "id": "5f16fbf7-7ffb-4334-83ce-89f9ab0279d3",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 93,
                "lineStart": 35,
                "lineEnd": 127,
                "bodyLineStart": 35,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                "sourceCode": "class VotedStateTest {\n\n    private final MockTime time = new MockTime();\n    private final LogContext logContext = new LogContext();\n    private final int epoch = 5;\n    private final int votedId = 1;\n    private final int electionTimeoutMs = 10000;\n\n    private VotedState newVotedState(\n        Uuid votedDirectoryId\n    ) {\n        return new VotedState(\n            time,\n            epoch,\n            ReplicaKey.of(votedId, votedDirectoryId),\n            Collections.emptySet(),\n            Optional.empty(),\n            electionTimeoutMs,\n            logContext\n        );\n    }\n\n    @Test\n    public void testElectionTimeout() {\n        VotedState state = newVotedState(ReplicaKey.NO_DIRECTORY_ID);\n        ReplicaKey votedKey  = ReplicaKey.of(votedId, ReplicaKey.NO_DIRECTORY_ID);\n\n        assertEquals(epoch, state.epoch());\n        votedKey(state, votedKey);\n        assertEquals(\n            ElectionState.withVotedCandidate(epoch, votedKey, Collections.emptySet()),\n            state.election()\n        );\n        assertEquals(electionTimeoutMs, state.remainingElectionTimeMs(time.milliseconds()));\n        assertFalse(state.hasElectionTimeoutExpired(time.milliseconds()));\n\n        time.sleep(5000);\n        assertEquals(electionTimeoutMs - 5000, state.remainingElectionTimeMs(time.milliseconds()));\n        assertFalse(state.hasElectionTimeoutExpired(time.milliseconds()));\n\n        time.sleep(5000);\n        assertEquals(0, state.remainingElectionTimeMs(time.milliseconds()));\n        assertTrue(state.hasElectionTimeoutExpired(time.milliseconds()));\n    }\n\n    private void votedKey(VotedState state, ReplicaKey votedKey) {\n        assertEquals(votedKey, state.votedKey());\n    }\n\n    @ParameterizedTest\n    @ValueSource(booleans = {true, false})\n    public void testCanGrantVoteWithoutDirectoryId(boolean isLogUpToDate) {\n        VotedState state = newVotedState(ReplicaKey.NO_DIRECTORY_ID);\n\n        assertTrue(\n            state.canGrantVote(ReplicaKey.of(votedId, ReplicaKey.NO_DIRECTORY_ID), isLogUpToDate)\n        );\n        assertTrue(\n            state.canGrantVote(\n                ReplicaKey.of(votedId, Uuid.randomUuid()),\n                isLogUpToDate\n            )\n        );\n\n        assertFalse(\n            state.canGrantVote(ReplicaKey.of(votedId + 1, ReplicaKey.NO_DIRECTORY_ID), isLogUpToDate)\n        );\n    }\n\n    @Test\n    void testCanGrantVoteWithDirectoryId() {\n        Uuid votedDirectoryId = Uuid.randomUuid();\n        VotedState state = newVotedState(votedDirectoryId);\n\n        assertTrue(state.canGrantVote(ReplicaKey.of(votedId, votedDirectoryId), false));\n\n        assertFalse(\n            state.canGrantVote(ReplicaKey.of(votedId, Uuid.randomUuid()), false)\n        );\n        assertFalse(state.canGrantVote(ReplicaKey.of(votedId, ReplicaKey.NO_DIRECTORY_ID), false));\n\n        assertFalse(state.canGrantVote(ReplicaKey.of(votedId + 1, votedDirectoryId), false));\n        assertFalse(state.canGrantVote(ReplicaKey.of(votedId + 1, ReplicaKey.NO_DIRECTORY_ID), false));\n    }\n\n    @Test\n    void testLeaderEndpoints() {\n        Uuid votedDirectoryId = Uuid.randomUuid();\n        VotedState state = newVotedState(votedDirectoryId);\n\n        assertEquals(Endpoints.empty(), state.leaderEndpoints());\n    }\n}",
                "methodCount": 6
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 79,
                        "lineEnd": 81,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method votedKey to class ReplicaKey",
                        "description": "Move method votedKey to org.apache.kafka.raft.internals.ReplicaKey\nRationale: The method votedKey(VotedState state, ReplicaKey votedKey) involves an assertion comparing a ReplicaKey instance to the result of a method call on a VotedState instance. The ReplicaKey class contains significant logic about equality and comparison that is directly relevant to this method. Therefore, it is the most appropriate class to host this method. Having the method in ReplicaKey also adheres to the principle of cohesion, keeping related functionalities together.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "newVotedState",
                            "method_signature": "private newVotedState(\n        Uuid votedDirectoryId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "votedKey",
                            "method_signature": "private votedKey(VotedState state, ReplicaKey votedKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testCanGrantVoteWithoutDirectoryId",
                            "method_signature": "@ParameterizedTest\n    @ValueSource(booleans = {true, false})\n    public testCanGrantVoteWithoutDirectoryId(boolean isLogUpToDate)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "votedKey",
                            "method_signature": "private votedKey(VotedState state, ReplicaKey votedKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "newVotedState",
                            "method_signature": "private newVotedState(\n        Uuid votedDirectoryId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testCanGrantVoteWithoutDirectoryId",
                            "method_signature": "@ParameterizedTest\n    @ValueSource(booleans = {true, false})\n    public testCanGrantVoteWithoutDirectoryId(boolean isLogUpToDate)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "votedKey",
                            "method_signature": "private votedKey(VotedState state, ReplicaKey votedKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "newVotedState",
                            "method_signature": "private newVotedState(\n        Uuid votedDirectoryId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testCanGrantVoteWithoutDirectoryId",
                            "method_signature": "@ParameterizedTest\n    @ValueSource(booleans = {true, false})\n    public testCanGrantVoteWithoutDirectoryId(boolean isLogUpToDate)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private votedKey(VotedState state, ReplicaKey votedKey)": {
                    "first": {
                        "method_name": "votedKey",
                        "method_signature": "private votedKey(VotedState state, ReplicaKey votedKey)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.28817620634373814
                },
                "private newVotedState(\n        Uuid votedDirectoryId\n    )": {
                    "first": {
                        "method_name": "newVotedState",
                        "method_signature": "private newVotedState(\n        Uuid votedDirectoryId\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3672972246896993
                },
                "@ParameterizedTest\n    @ValueSource(booleans = {true, false})\n    public testCanGrantVoteWithoutDirectoryId(boolean isLogUpToDate)": {
                    "first": {
                        "method_name": "testCanGrantVoteWithoutDirectoryId",
                        "method_signature": "@ParameterizedTest\n    @ValueSource(booleans = {true, false})\n    public testCanGrantVoteWithoutDirectoryId(boolean isLogUpToDate)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.43368809959011595
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "private votedKey(VotedState state, ReplicaKey votedKey)"
                ],
                "llm_response_time": 2533
            },
            "targetClassMap": {
                "votedKey": {
                    "target_classes": [
                        {
                            "class_name": "VotedState",
                            "similarity_score": 0.3334822304866716
                        },
                        {
                            "class_name": "ReplicaKey",
                            "similarity_score": 0.33634998607300864
                        },
                        {
                            "class_name": "MockTime",
                            "similarity_score": 0.429766743189736
                        },
                        {
                            "class_name": "LogContext",
                            "similarity_score": 0.3995056298320393
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "ReplicaKey",
                        "LogContext",
                        "MockTime"
                    ],
                    "llm_response_time": 4083,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "8d29bc1fa8ce75b6807b957ea35ad39e8dd9e04a",
        "url": "https://github.com/apache/kafka/commit/8d29bc1fa8ce75b6807b957ea35ad39e8dd9e04a",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public newShareGroupSubscriptionMetadataRecord(groupId String, newSubscriptionMetadata Map<String,TopicMetadata>) : CoordinatorRecord extracted from private shareGroupHeartbeat(groupId String, memberId String, memberEpoch int, rackId String, clientId String, clientHost String, subscribedTopicNames List<String>) : CoordinatorResult<ShareGroupHeartbeatResponseData,CoordinatorRecord> in class org.apache.kafka.coordinator.group.GroupMetadataManager & moved to class org.apache.kafka.coordinator.group.CoordinatorRecordHelpers",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java",
                    "startLine": 2062,
                    "endLine": 2213,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private shareGroupHeartbeat(groupId String, memberId String, memberEpoch int, rackId String, clientId String, clientHost String, subscribedTopicNames List<String>) : CoordinatorResult<ShareGroupHeartbeatResponseData,CoordinatorRecord>"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java",
                    "startLine": 2089,
                    "endLine": 2089,
                    "startColumn": 9,
                    "endColumn": 73,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java",
                    "startLine": 2211,
                    "endLine": 2211,
                    "startColumn": 9,
                    "endColumn": 45,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/CoordinatorRecordHelpers.java",
                    "startLine": 646,
                    "endLine": 688,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public newShareGroupSubscriptionMetadataRecord(groupId String, newSubscriptionMetadata Map<String,TopicMetadata>) : CoordinatorRecord"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/CoordinatorRecordHelpers.java",
                    "startLine": 659,
                    "endLine": 659,
                    "startColumn": 13,
                    "endColumn": 108,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/CoordinatorRecordHelpers.java",
                    "startLine": 662,
                    "endLine": 667,
                    "startColumn": 17,
                    "endColumn": 19,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java",
                    "startLine": 2055,
                    "endLine": 2201,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private shareGroupHeartbeat(groupId String, memberId String, memberEpoch int, rackId String, clientId String, clientHost String, subscribedTopicNames List<String>) : CoordinatorResult<ShareGroupHeartbeatResponseData,CoordinatorRecord>"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java",
                    "startLine": 2143,
                    "endLine": 2143,
                    "startColumn": 29,
                    "endColumn": 99,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "newShareGroupSubscriptionMetadataRecord(groupId,subscriptionMetadata)"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/CoordinatorRecordHelpers.java",
                    "startLine": 657,
                    "endLine": 657,
                    "startColumn": 9,
                    "endColumn": 89,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/CoordinatorRecordHelpers.java",
                    "startLine": 658,
                    "endLine": 675,
                    "startColumn": 9,
                    "endColumn": 12,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/CoordinatorRecordHelpers.java",
                    "startLine": 677,
                    "endLine": 687,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": true
        },
        "ref_id": 631,
        "extraction_results": {
            "success": true,
            "newCommitHash": "66511a520a3e8ea05d6531fa7fe6989099db9164",
            "newBranchName": "extract-newShareGroupSubscriptionMetadataRecord-shareGroupHeartbeat-ad08ec6"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "8d29bc1fa8ce75b6807b957ea35ad39e8dd9e04a",
        "url": "https://github.com/apache/kafka/commit/8d29bc1fa8ce75b6807b957ea35ad39e8dd9e04a",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public newShareGroupSubscriptionMetadataRecord(groupId String, newSubscriptionMetadata Map<String,TopicMetadata>) : CoordinatorRecord extracted from private shareGroupFenceMember(group ShareGroup, member ShareGroupMember, response T) : CoordinatorResult<T,CoordinatorRecord> in class org.apache.kafka.coordinator.group.GroupMetadataManager & moved to class org.apache.kafka.coordinator.group.CoordinatorRecordHelpers",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java",
                    "startLine": 2766,
                    "endLine": 2813,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private shareGroupFenceMember(group ShareGroup, member ShareGroupMember, response T) : CoordinatorResult<T,CoordinatorRecord>"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java",
                    "startLine": 2780,
                    "endLine": 2780,
                    "startColumn": 9,
                    "endColumn": 67,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java",
                    "startLine": 2810,
                    "endLine": 2810,
                    "startColumn": 9,
                    "endColumn": 45,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/CoordinatorRecordHelpers.java",
                    "startLine": 646,
                    "endLine": 688,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public newShareGroupSubscriptionMetadataRecord(groupId String, newSubscriptionMetadata Map<String,TopicMetadata>) : CoordinatorRecord"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/CoordinatorRecordHelpers.java",
                    "startLine": 659,
                    "endLine": 659,
                    "startColumn": 13,
                    "endColumn": 108,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/CoordinatorRecordHelpers.java",
                    "startLine": 662,
                    "endLine": 667,
                    "startColumn": 17,
                    "endColumn": 19,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java",
                    "startLine": 2756,
                    "endLine": 2794,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private shareGroupFenceMember(group ShareGroup, member ShareGroupMember, response T) : CoordinatorResult<T,CoordinatorRecord>"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java",
                    "startLine": 2784,
                    "endLine": 2784,
                    "startColumn": 25,
                    "endColumn": 103,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "newShareGroupSubscriptionMetadataRecord(group.groupId(),subscriptionMetadata)"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/CoordinatorRecordHelpers.java",
                    "startLine": 657,
                    "endLine": 657,
                    "startColumn": 9,
                    "endColumn": 89,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/CoordinatorRecordHelpers.java",
                    "startLine": 658,
                    "endLine": 675,
                    "startColumn": 9,
                    "endColumn": 12,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/CoordinatorRecordHelpers.java",
                    "startLine": 677,
                    "endLine": 687,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": true
        },
        "ref_id": 632,
        "extraction_results": {
            "success": true,
            "newCommitHash": "24a13d857757026051ebda38f04cfbee3cf54b2e",
            "newBranchName": "extract-newShareGroupSubscriptionMetadataRecord-shareGroupFenceMember-ad08ec6"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "932e84096a96e199c0772c81bddf3ea3789377ba",
        "url": "https://github.com/apache/kafka/commit/932e84096a96e199c0772c81bddf3ea3789377ba",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public onClose() : boolean extracted from public acknowledgeOnClose(acknowledgementsMap Map<TopicIdPartition,Acknowledgements>, deadlineMs long) : CompletableFuture<Void> in class org.apache.kafka.clients.consumer.internals.ShareConsumeRequestManager & moved to class org.apache.kafka.clients.consumer.internals.ShareConsumeRequestManager.AcknowledgeRequestState",
            "leftSideLocations": [
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/ShareConsumeRequestManager.java",
                    "startLine": 433,
                    "endLine": 496,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public acknowledgeOnClose(acknowledgementsMap Map<TopicIdPartition,Acknowledgements>, deadlineMs long) : CompletableFuture<Void>"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/ShareConsumeRequestManager.java",
                    "startLine": 457,
                    "endLine": 457,
                    "startColumn": 25,
                    "endColumn": 49,
                    "codeElementType": "IF_STATEMENT_CONDITION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/ShareConsumeRequestManager.java",
                    "startLine": 990,
                    "endLine": 992,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public onClose() : boolean"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/ShareConsumeRequestManager.java",
                    "startLine": 991,
                    "endLine": 991,
                    "startColumn": 13,
                    "endColumn": 64,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/ShareConsumeRequestManager.java",
                    "startLine": 434,
                    "endLine": 502,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public acknowledgeOnClose(acknowledgementsMap Map<TopicIdPartition,Acknowledgements>, deadlineMs long) : CompletableFuture<Void>"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/ShareConsumeRequestManager.java",
                    "startLine": 452,
                    "endLine": 498,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "sessionHandlers.forEach((nodeId,sessionHandler) -> {\n  Node node=cluster.nodeById(nodeId);\n  if (node != null) {\n    Map<TopicIdPartition,Acknowledgements> acknowledgementsMapForNode=new HashMap<>();\n    for (    TopicIdPartition tip : sessionHandler.sessionPartitions()) {\n      Acknowledgements acknowledgements=acknowledgementsMap.getOrDefault(tip,Acknowledgements.empty());\n      if (fetchAcknowledgementsMap.get(tip) != null) {\n        acknowledgements.merge(fetchAcknowledgementsMap.remove(tip));\n      }\n      if (acknowledgements != null && !acknowledgements.isEmpty()) {\n        acknowledgementsMapForNode.put(tip,acknowledgements);\n        metricsManager.recordAcknowledgementSent(acknowledgements.size());\n        log.debug(\"Added closing acknowledge request for partition {} to node {}\",tip.topicPartition(),node.id());\n        resultCount.incrementAndGet();\n      }\n    }\n    acknowledgeRequestStates.putIfAbsent(nodeId,new Pair<>(null,null));\n    if (acknowledgeRequestStates.get(nodeId).getSyncRequest() != null && !acknowledgeRequestStates.get(nodeId).getSyncRequest().isEmpty()) {\n      log.error(\"Attempt to call close() when there is an existing sync request for node {}-{}\",node.id(),acknowledgeRequestStates.get(nodeId).getSyncRequest());\n      closeFuture.completeExceptionally(new IllegalStateException(\"Attempt to call close() when there is an existing sync request for node : \" + node.id()));\n    }\n else {\n      acknowledgeRequestStates.get(nodeId).setSyncRequest(new AcknowledgeRequestState(logContext,ShareConsumeRequestManager.class.getSimpleName() + \":3\",deadlineMs,retryBackoffMs,retryBackoffMaxMs,sessionHandler,nodeId,acknowledgementsMapForNode,this::handleShareAcknowledgeCloseSuccess,this::handleShareAcknowledgeCloseFailure,resultHandler,AcknowledgeRequestType.CLOSE));\n    }\n  }\n}\n)"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/ShareConsumeRequestManager.java",
                    "startLine": 482,
                    "endLine": 494,
                    "startColumn": 21,
                    "endColumn": 23,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "acknowledgeRequestStates.get(nodeId).setSyncRequest(new AcknowledgeRequestState(logContext,ShareConsumeRequestManager.class.getSimpleName() + \":3\",deadlineMs,retryBackoffMs,retryBackoffMaxMs,sessionHandler,nodeId,acknowledgementsMapForNode,this::handleShareAcknowledgeCloseSuccess,this::handleShareAcknowledgeCloseFailure,resultHandler,AcknowledgeRequestType.CLOSE))"
                }
            ],
            "isStatic": false
        },
        "ref_id": 633,
        "extraction_results": {
            "success": true,
            "newCommitHash": "9194b45995ef8c7fbbc4b94df1922f14be6036c8",
            "newBranchName": "extract-onClose-acknowledgeOnClose-f6bfa94"
        },
        "telemetry": {
            "id": "914e6d52-e686-45da-a030-f7d101cb40dd",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1025,
                "lineStart": 63,
                "lineEnd": 1087,
                "bodyLineStart": 63,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ShareConsumeRequestManager.java",
                "sourceCode": "/**\n * {@code ShareConsumeRequestManager} is responsible for generating {@link ShareFetchRequest} and\n * {@link ShareAcknowledgeRequest} to fetch and acknowledge records being delivered for a consumer\n * in a share group.\n */\n@SuppressWarnings(\"NPathComplexity\")\npublic class ShareConsumeRequestManager implements RequestManager, MemberStateListener, Closeable {\n    private final Time time;\n    private final Logger log;\n    private final LogContext logContext;\n    private final String groupId;\n    private final ConsumerMetadata metadata;\n    private final SubscriptionState subscriptions;\n    private final FetchConfig fetchConfig;\n    protected final ShareFetchBuffer shareFetchBuffer;\n    private final BackgroundEventHandler backgroundEventHandler;\n    private final Map<Integer, ShareSessionHandler> sessionHandlers;\n    private final Set<Integer> nodesWithPendingRequests;\n    private final ShareFetchMetricsManager metricsManager;\n    private final IdempotentCloser idempotentCloser = new IdempotentCloser();\n    private Uuid memberId;\n    private boolean fetchMoreRecords = false;\n    private final Map<TopicIdPartition, Acknowledgements> fetchAcknowledgementsMap;\n    private final Map<Integer, Pair<AcknowledgeRequestState>> acknowledgeRequestStates;\n    private final long retryBackoffMs;\n    private final long retryBackoffMaxMs;\n    private boolean closing = false;\n    private final CompletableFuture<Void> closeFuture;\n\n    ShareConsumeRequestManager(final Time time,\n                               final LogContext logContext,\n                               final String groupId,\n                               final ConsumerMetadata metadata,\n                               final SubscriptionState subscriptions,\n                               final FetchConfig fetchConfig,\n                               final ShareFetchBuffer shareFetchBuffer,\n                               final BackgroundEventHandler backgroundEventHandler,\n                               final ShareFetchMetricsManager metricsManager,\n                               final long retryBackoffMs,\n                               final long retryBackoffMaxMs) {\n        this.time = time;\n        this.log = logContext.logger(ShareConsumeRequestManager.class);\n        this.logContext = logContext;\n        this.groupId = groupId;\n        this.metadata = metadata;\n        this.subscriptions = subscriptions;\n        this.fetchConfig = fetchConfig;\n        this.shareFetchBuffer = shareFetchBuffer;\n        this.backgroundEventHandler = backgroundEventHandler;\n        this.metricsManager = metricsManager;\n        this.retryBackoffMs = retryBackoffMs;\n        this.retryBackoffMaxMs = retryBackoffMaxMs;\n        this.sessionHandlers = new HashMap<>();\n        this.nodesWithPendingRequests = new HashSet<>();\n        this.acknowledgeRequestStates = new HashMap<>();\n        this.fetchAcknowledgementsMap = new HashMap<>();\n        this.closeFuture = new CompletableFuture<>();\n    }\n\n    @Override\n    public PollResult poll(long currentTimeMs) {\n        if (memberId == null) {\n            return PollResult.EMPTY;\n        }\n\n        // Send any pending acknowledgements before fetching more records.\n        PollResult pollResult = processAcknowledgements(currentTimeMs);\n        if (pollResult != null) {\n            return pollResult;\n        }\n\n        if (!fetchMoreRecords || closing) {\n            return PollResult.EMPTY;\n        }\n\n        Map<Node, ShareSessionHandler> handlerMap = new HashMap<>();\n        Map<String, Uuid> topicIds = metadata.topicIds();\n        for (TopicPartition partition : partitionsToFetch()) {\n            Optional<Node> leaderOpt = metadata.currentLeader(partition).leader;\n\n            if (!leaderOpt.isPresent()) {\n                log.debug(\"Requesting metadata update for partition {} since current leader node is missing\", partition);\n                metadata.requestUpdate(false);\n                continue;\n            }\n\n            Uuid topicId = topicIds.get(partition.topic());\n            if (topicId == null) {\n                log.debug(\"Requesting metadata update for partition {} since topic ID is missing\", partition);\n                metadata.requestUpdate(false);\n                continue;\n            }\n\n            Node node = leaderOpt.get();\n            if (nodesWithPendingRequests.contains(node.id())) {\n                log.trace(\"Skipping fetch for partition {} because previous fetch request to {} has not been processed\", partition, node.id());\n            } else {\n                // if there is a leader and no in-flight requests, issue a new fetch\n                ShareSessionHandler handler = handlerMap.computeIfAbsent(node,\n                        k -> sessionHandlers.computeIfAbsent(node.id(), n -> new ShareSessionHandler(logContext, n, memberId)));\n\n                TopicIdPartition tip = new TopicIdPartition(topicId, partition);\n                Acknowledgements acknowledgementsToSend = fetchAcknowledgementsMap.get(tip);\n                if (onClose(acknowledgementsToSend)) {\n                    metricsManager.recordAcknowledgementSent(acknowledgementsToSend.size());\n                }\n                handler.addPartitionToFetch(tip, acknowledgementsToSend);\n\n                log.debug(\"Added fetch request for partition {} to node {}\", partition, node.id());\n            }\n        }\n\n        Map<Node, ShareFetchRequest.Builder> builderMap = new LinkedHashMap<>();\n        for (Map.Entry<Node, ShareSessionHandler> entry : handlerMap.entrySet()) {\n            builderMap.put(entry.getKey(), entry.getValue().newShareFetchBuilder(groupId, fetchConfig));\n        }\n\n        List<UnsentRequest> requests = builderMap.entrySet().stream().map(entry -> {\n            Node target = entry.getKey();\n            log.trace(\"Building ShareFetch request to send to node {}\", target.id());\n            ShareFetchRequest.Builder requestBuilder = entry.getValue();\n\n            nodesWithPendingRequests.add(target.id());\n\n            BiConsumer<ClientResponse, Throwable> responseHandler = (clientResponse, error) -> {\n                if (error != null) {\n                    handleShareFetchFailure(target, requestBuilder.data(), error);\n                } else {\n                    handleShareFetchSuccess(target, requestBuilder.data(), clientResponse);\n                }\n            };\n            return new UnsentRequest(requestBuilder, Optional.of(target)).whenComplete(responseHandler);\n        }).collect(Collectors.toList());\n\n        return new PollResult(requests);\n    }\n\n    public void fetch(Map<TopicIdPartition, Acknowledgements> acknowledgementsMap) {\n        if (!fetchMoreRecords) {\n            log.debug(\"Fetch more data\");\n            fetchMoreRecords = true;\n        }\n        acknowledgementsMap.forEach((tip, acks) -> fetchAcknowledgementsMap.merge(tip, acks, Acknowledgements::merge));\n    }\n\n    /**\n     * Process acknowledgeRequestStates and prepares a list of acknowledgements to be sent in the poll().\n     *\n     * @param currentTimeMs the current time in ms.\n     *\n     * @return the PollResult containing zero or more acknowledgements.\n     */\n    private PollResult processAcknowledgements(long currentTimeMs) {\n        List<UnsentRequest> unsentRequests = new ArrayList<>();\n        AtomicBoolean isAsyncDone = new AtomicBoolean();\n        for (Map.Entry<Integer, Pair<AcknowledgeRequestState>> requestStates : acknowledgeRequestStates.entrySet()) {\n            int nodeId = requestStates.getKey();\n\n            if (!isNodeFree(nodeId)) {\n                log.trace(\"Skipping acknowledge request because previous request to {} has not been processed, so acks are not sent\", nodeId);\n            } else {\n                isAsyncDone.set(false);\n                // For commitAsync\n                maybeBuildRequest(requestStates.getValue().getAsyncRequest(), currentTimeMs, true, isAsyncDone).ifPresent(unsentRequests::add);\n                // Check to ensure we start processing commitSync/close only if there are no commitAsync requests left to process.\n                if (!isNodeFree(nodeId)) {\n                    log.trace(\"Skipping acknowledge request because previous request to {} has not been processed, so acks are not sent\", nodeId);\n                } else if (isAsyncDone.get()) {\n                    maybeBuildRequest(requestStates.getValue().getSyncRequest(), currentTimeMs, false, isAsyncDone).ifPresent(unsentRequests::add);\n                }\n            }\n        }\n\n        PollResult pollResult = null;\n        if (!unsentRequests.isEmpty()) {\n            pollResult = new PollResult(unsentRequests);\n        } else if (checkAndRemoveCompletedAcknowledgements()) {\n            // Return empty result until all the acknowledgement request states are processed\n            pollResult = PollResult.EMPTY;\n        } else if (closing) {\n            if (!closeFuture.isDone()) {\n                log.trace(\"Completing acknowledgement on close\");\n                closeFuture.complete(null);\n            }\n            pollResult = PollResult.EMPTY;\n        }\n\n        return pollResult;\n    }\n\n    private boolean isNodeFree(int nodeId) {\n        return !nodesWithPendingRequests.contains(nodeId);\n    }\n\n    private Optional<UnsentRequest> maybeBuildRequest(AcknowledgeRequestState acknowledgeRequestState,\n                                                      long currentTimeMs,\n                                                      boolean onCommitAsync,\n                                                      AtomicBoolean isAsyncDone) {\n        if (acknowledgeRequestState == null || (!acknowledgeRequestState.onClose && acknowledgeRequestState.isEmpty())) {\n            if (onCommitAsync) {\n                isAsyncDone.set(true);\n            }\n            return Optional.empty();\n        } else if (!acknowledgeRequestState.maybeExpire()) {\n            if (acknowledgeRequestState.canSendRequest(currentTimeMs)) {\n                acknowledgeRequestState.onSendAttempt(currentTimeMs);\n                if (onCommitAsync) {\n                    isAsyncDone.set(true);\n                }\n                return Optional.of(acknowledgeRequestState.buildRequest(currentTimeMs));\n            } else {\n                // We wait for the backoff before we can send this request.\n                if (onCommitAsync) {\n                    isAsyncDone.set(false);\n                }\n            }\n        } else {\n            // Fill in TimeoutException\n            for (TopicIdPartition tip : acknowledgeRequestState.incompleteAcknowledgements.keySet()) {\n                metricsManager.recordFailedAcknowledgements(acknowledgeRequestState.getIncompleteAcknowledgementsCount(tip));\n                acknowledgeRequestState.handleAcknowledgeTimedOut(tip);\n            }\n            acknowledgeRequestState.incompleteAcknowledgements.clear();\n            if (onCommitAsync) {\n                isAsyncDone.set(true);\n            }\n        }\n        return Optional.empty();\n    }\n\n    /**\n     * Prunes the empty acknowledgementRequestStates.\n     * Returns true if there are still some acknowledgements left to be processed.\n     */\n    private boolean checkAndRemoveCompletedAcknowledgements() {\n        boolean areAnyAcksLeft = false;\n        Iterator<Map.Entry<Integer, Pair<AcknowledgeRequestState>>> iterator = acknowledgeRequestStates.entrySet().iterator();\n        while (iterator.hasNext()) {\n            Map.Entry<Integer, Pair<AcknowledgeRequestState>> acknowledgeRequestStatePair = iterator.next();\n            if (isRequestStateInProgress(acknowledgeRequestStatePair.getValue().getAsyncRequest()) || isRequestStateInProgress(acknowledgeRequestStatePair.getValue().getSyncRequest())) {\n                areAnyAcksLeft = true;\n            } else if (!closing) {\n                iterator.remove();\n            }\n        }\n        if (!acknowledgeRequestStates.isEmpty()) areAnyAcksLeft = true;\n        return areAnyAcksLeft;\n    }\n\n    private boolean isRequestStateInProgress(AcknowledgeRequestState acknowledgeRequestState) {\n        return acknowledgeRequestState != null && !(acknowledgeRequestState.isEmpty());\n    }\n\n    /**\n     * Enqueue an AcknowledgeRequestState to be picked up on the next poll\n     *\n     * @param acknowledgementsMap The acknowledgements to commit\n     * @param deadlineMs          Time until which the request will be retried if it fails with\n     *                            an expected retriable error.\n     *\n     * @return The future which completes when the acknowledgements finished\n     */\n    public CompletableFuture<Map<TopicIdPartition, Acknowledgements>> commitSync(\n            final Map<TopicIdPartition, Acknowledgements> acknowledgementsMap,\n            final long deadlineMs) {\n        final AtomicInteger resultCount = new AtomicInteger();\n        final CompletableFuture<Map<TopicIdPartition, Acknowledgements>> future = new CompletableFuture<>();\n        final ResultHandler resultHandler = new ResultHandler(resultCount, Optional.of(future));\n\n        final Cluster cluster = metadata.fetch();\n\n        sessionHandlers.forEach((nodeId, sessionHandler) -> {\n            Node node = cluster.nodeById(nodeId);\n            if (node != null) {\n                acknowledgeRequestStates.putIfAbsent(nodeId, new Pair<>(null, null));\n\n                // Ensure there is no commitSync()/close() request already present as they are blocking calls\n                // and only one request can be active at a time.\n                if (acknowledgeRequestStates.get(nodeId).getSyncRequest() != null && !acknowledgeRequestStates.get(nodeId).getSyncRequest().isEmpty()) {\n                    log.error(\"Attempt to call commitSync() when there is an existing sync request for node {}\", node.id());\n                    future.completeExceptionally(\n                            new IllegalStateException(\"Attempt to call commitSync() when there is an existing sync request for node : \" + node.id()));\n                } else {\n                    Map<TopicIdPartition, Acknowledgements> acknowledgementsMapForNode = new HashMap<>();\n                    for (TopicIdPartition tip : sessionHandler.sessionPartitions()) {\n                        Acknowledgements acknowledgements = acknowledgementsMap.get(tip);\n                        if (onClose(acknowledgements)) {\n                            acknowledgementsMapForNode.put(tip, acknowledgements);\n\n                            metricsManager.recordAcknowledgementSent(acknowledgements.size());\n                            log.debug(\"Added sync acknowledge request for partition {} to node {}\", tip.topicPartition(), node.id());\n                            resultCount.incrementAndGet();\n                        }\n                    }\n\n\n                    // There can only be one commitSync()/close() happening at a time. So per node, there will be one acknowledge request state representing commitSync() and close().\n                    acknowledgeRequestStates.get(nodeId).setSyncRequest(new AcknowledgeRequestState(logContext,\n                            ShareConsumeRequestManager.class.getSimpleName() + \":1\",\n                            deadlineMs,\n                            retryBackoffMs,\n                            retryBackoffMaxMs,\n                            sessionHandler,\n                            nodeId,\n                            acknowledgementsMapForNode,\n                            this::handleShareAcknowledgeSuccess,\n                            this::handleShareAcknowledgeFailure,\n                            resultHandler\n                    ));\n                }\n            }\n        });\n\n        resultHandler.completeIfEmpty();\n        return future;\n    }\n\n    /**\n     * Enqueue an AcknowledgeRequestState to be picked up on the next poll.\n     *\n     * @param acknowledgementsMap The acknowledgements to commit\n     */\n    public void commitAsync(final Map<TopicIdPartition, Acknowledgements> acknowledgementsMap) {\n        final Cluster cluster = metadata.fetch();\n        final AtomicInteger resultCount = new AtomicInteger();\n        final ResultHandler resultHandler = new ResultHandler(resultCount, Optional.empty());\n\n        sessionHandlers.forEach((nodeId, sessionHandler) -> {\n            Node node = cluster.nodeById(nodeId);\n            if (node != null) {\n                Map<TopicIdPartition, Acknowledgements> acknowledgementsMapForNode = new HashMap<>();\n\n                acknowledgeRequestStates.putIfAbsent(nodeId, new Pair<>(null, null));\n\n                for (TopicIdPartition tip : sessionHandler.sessionPartitions()) {\n                    Acknowledgements acknowledgements = acknowledgementsMap.get(tip);\n                    if (onClose(acknowledgements)) {\n                        acknowledgementsMapForNode.put(tip, acknowledgements);\n\n                        metricsManager.recordAcknowledgementSent(acknowledgements.size());\n                        log.debug(\"Added async acknowledge request for partition {} to node {}\", tip.topicPartition(), node.id());\n                        resultCount.incrementAndGet();\n                        AcknowledgeRequestState asyncRequestState = acknowledgeRequestStates.get(nodeId).getAsyncRequest();\n                        if (asyncRequestState == null) {\n                            acknowledgeRequestStates.get(nodeId).setAsyncRequest(new AcknowledgeRequestState(logContext,\n                                    ShareConsumeRequestManager.class.getSimpleName() + \":2\",\n                                    Long.MAX_VALUE,\n                                    retryBackoffMs,\n                                    retryBackoffMaxMs,\n                                    sessionHandler,\n                                    nodeId,\n                                    acknowledgementsMapForNode,\n                                    this::handleShareAcknowledgeSuccess,\n                                    this::handleShareAcknowledgeFailure,\n                                    resultHandler\n                            ));\n                        } else {\n                            Acknowledgements prevAcks = asyncRequestState.acknowledgementsToSend.putIfAbsent(tip, acknowledgements);\n                            if (onClose(prevAcks)) {\n                                asyncRequestState.acknowledgementsToSend.get(tip).merge(acknowledgements);\n                            }\n                        }\n                    }\n                }\n            }\n        });\n\n        resultHandler.completeIfEmpty();\n    }\n\n    /**\n     * Enqueue the final AcknowledgeRequestState used to commit the final acknowledgements and\n     * close the share sessions.\n     *\n     * @param acknowledgementsMap The acknowledgements to commit\n     * @param deadlineMs          Time until which the request will be retried if it fails with\n     *                            an expected retriable error.\n     *\n     * @return The future which completes when the acknowledgements finished\n     */\n    public CompletableFuture<Void> acknowledgeOnClose(final Map<TopicIdPartition, Acknowledgements> acknowledgementsMap,\n                                                      final long deadlineMs) {\n        final Cluster cluster = metadata.fetch();\n        final AtomicInteger resultCount = new AtomicInteger();\n        final ResultHandler resultHandler = new ResultHandler(resultCount, Optional.empty());\n\n        closing = true;\n\n        sessionHandlers.forEach((nodeId, sessionHandler) -> {\n            Node node = cluster.nodeById(nodeId);\n            if (node != null) {\n                Map<TopicIdPartition, Acknowledgements> acknowledgementsMapForNode = new HashMap<>();\n                for (TopicIdPartition tip : sessionHandler.sessionPartitions()) {\n                    Acknowledgements acknowledgements = acknowledgementsMap.get(tip);\n                    if (onClose(acknowledgements)) {\n                        acknowledgementsMapForNode.put(tip, acknowledgements);\n\n                        metricsManager.recordAcknowledgementSent(acknowledgements.size());\n                        log.debug(\"Added closing acknowledge request for partition {} to node {}\", tip.topicPartition(), node.id());\n                        resultCount.incrementAndGet();\n                    }\n                }\n\n                acknowledgeRequestStates.putIfAbsent(nodeId, new Pair<>(null, null));\n\n                // Ensure there is no commitSync()/close() request already present as they are blocking calls\n                // and only one request can be active at a time.\n                if (acknowledgeRequestStates.get(nodeId).getSyncRequest() != null && !acknowledgeRequestStates.get(nodeId).getSyncRequest().isEmpty()) {\n                    log.error(\"Attempt to call close() when there is an existing sync request for node {}-{}\", node.id(), acknowledgeRequestStates.get(nodeId).getSyncRequest());\n                    closeFuture.completeExceptionally(\n                            new IllegalStateException(\"Attempt to call close() when there is an existing sync request for node : \" + node.id()));\n                } else {\n                    // There can only be one commitSync()/close() happening at a time. So per node, there will be one acknowledge request state.\n                    acknowledgeRequestStates.get(nodeId).setSyncRequest(new AcknowledgeRequestState(logContext,\n                            ShareConsumeRequestManager.class.getSimpleName() + \":3\",\n                            deadlineMs,\n                            retryBackoffMs,\n                            retryBackoffMaxMs,\n                            sessionHandler,\n                            nodeId,\n                            acknowledgementsMapForNode,\n                            this::handleShareAcknowledgeCloseSuccess,\n                            this::handleShareAcknowledgeCloseFailure,\n                            resultHandler,\n                            true\n                    ));\n\n                }\n            }\n        });\n\n        resultHandler.completeIfEmpty();\n        return closeFuture;\n    }\n\n    private boolean onClose(Acknowledgements acknowledgements) {\n        return acknowledgements != null;\n    }\n\n    private void handleShareFetchSuccess(Node fetchTarget,\n                                         @SuppressWarnings(\"unused\") ShareFetchRequestData requestData,\n                                         ClientResponse resp) {\n        try {\n            log.debug(\"Completed ShareFetch request from node {} successfully\", fetchTarget.id());\n            final ShareFetchResponse response = (ShareFetchResponse) resp.responseBody();\n            final ShareSessionHandler handler = sessionHandler(fetchTarget.id());\n\n            if (handler == null) {\n                log.error(\"Unable to find ShareSessionHandler for node {}. Ignoring ShareFetch response.\",\n                        fetchTarget.id());\n                return;\n            }\n\n            final short requestVersion = resp.requestHeader().apiVersion();\n\n            if (!handler.handleResponse(response, requestVersion)) {\n                if (response.error() == Errors.UNKNOWN_TOPIC_ID) {\n                    metadata.requestUpdate(false);\n                }\n                return;\n            }\n\n            final Map<TopicIdPartition, ShareFetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n\n            response.data().responses().forEach(topicResponse ->\n                    topicResponse.partitions().forEach(partition ->\n                            responseData.put(new TopicIdPartition(topicResponse.topicId(),\n                                    partition.partitionIndex(),\n                                    metadata.topicNames().get(topicResponse.topicId())), partition)));\n\n            final Set<TopicPartition> partitions = responseData.keySet().stream().map(TopicIdPartition::topicPartition).collect(Collectors.toSet());\n            final ShareFetchMetricsAggregator shareFetchMetricsAggregator = new ShareFetchMetricsAggregator(metricsManager, partitions);\n\n            for (Map.Entry<TopicIdPartition, ShareFetchResponseData.PartitionData> entry : responseData.entrySet()) {\n                TopicIdPartition tip = entry.getKey();\n\n                ShareFetchResponseData.PartitionData partitionData = entry.getValue();\n\n                log.debug(\"ShareFetch for partition {} returned fetch data {}\", tip, partitionData);\n\n                Acknowledgements acks = fetchAcknowledgementsMap.remove(tip);\n                if (onClose(acks)) {\n                    if (partitionData.acknowledgeErrorCode() != Errors.NONE.code()) {\n                        metricsManager.recordFailedAcknowledgements(acks.size());\n                    }\n                    acks.setAcknowledgeErrorCode(Errors.forCode(partitionData.acknowledgeErrorCode()));\n                    Map<TopicIdPartition, Acknowledgements> acksMap = Collections.singletonMap(tip, acks);\n                    ShareAcknowledgementCommitCallbackEvent event = new ShareAcknowledgementCommitCallbackEvent(acksMap);\n                    backgroundEventHandler.add(event);\n                }\n\n                ShareCompletedFetch completedFetch = new ShareCompletedFetch(\n                        logContext,\n                        BufferSupplier.create(),\n                        tip,\n                        partitionData,\n                        shareFetchMetricsAggregator,\n                        requestVersion);\n                shareFetchBuffer.add(completedFetch);\n\n                if (!partitionData.acquiredRecords().isEmpty()) {\n                    fetchMoreRecords = false;\n                }\n            }\n\n            metricsManager.recordLatency(resp.requestLatencyMs());\n        } finally {\n            log.debug(\"Removing pending request for node {} - success\", fetchTarget.id());\n            nodesWithPendingRequests.remove(fetchTarget.id());\n        }\n    }\n\n    private void handleShareFetchFailure(Node fetchTarget,\n                                         ShareFetchRequestData requestData,\n                                         Throwable error) {\n        try {\n            log.debug(\"Completed ShareFetch request from node {} unsuccessfully {}\", fetchTarget.id(), Errors.forException(error));\n            final ShareSessionHandler handler = sessionHandler(fetchTarget.id());\n            if (handler != null) {\n                handler.handleError(error);\n            }\n\n            requestData.topics().forEach(topic -> topic.partitions().forEach(partition -> {\n                TopicIdPartition tip = new TopicIdPartition(topic.topicId(),\n                        partition.partitionIndex(),\n                        metadata.topicNames().get(topic.topicId()));\n\n                Acknowledgements acks = fetchAcknowledgementsMap.remove(tip);\n                if (onClose(acks)) {\n                    metricsManager.recordFailedAcknowledgements(acks.size());\n                    acks.setAcknowledgeErrorCode(Errors.forException(error));\n                    Map<TopicIdPartition, Acknowledgements> acksMap = Collections.singletonMap(tip, acks);\n                    ShareAcknowledgementCommitCallbackEvent event = new ShareAcknowledgementCommitCallbackEvent(acksMap);\n                    backgroundEventHandler.add(event);\n                }\n            }));\n        } finally {\n            log.debug(\"Removing pending request for node {} - failed\", fetchTarget.id());\n            nodesWithPendingRequests.remove(fetchTarget.id());\n        }\n    }\n\n    private void handleShareAcknowledgeSuccess(Node fetchTarget,\n                                               ShareAcknowledgeRequestData requestData,\n                                               AcknowledgeRequestState acknowledgeRequestState,\n                                               ClientResponse resp,\n                                               long currentTimeMs) {\n        try {\n            log.debug(\"Completed ShareAcknowledge request from node {} successfully\", fetchTarget.id());\n            final ShareAcknowledgeResponse response = (ShareAcknowledgeResponse) resp.responseBody();\n            final ShareSessionHandler handler = acknowledgeRequestState.sessionHandler();\n\n            final short requestVersion = resp.requestHeader().apiVersion();\n\n            if (!handler.handleResponse(response, requestVersion)) {\n                acknowledgeRequestState.onFailedAttempt(currentTimeMs);\n                if (response.error().exception() instanceof RetriableException && !acknowledgeRequestState.onClose) {\n                    // We retry the request until the timer expires, unless we are closing.\n                    acknowledgeRequestState.retryRequest();\n                } else {\n                    response.data().responses().forEach(shareAcknowledgeTopicResponse -> shareAcknowledgeTopicResponse.partitions().forEach(partitionData -> {\n                        TopicIdPartition tip = new TopicIdPartition(shareAcknowledgeTopicResponse.topicId(),\n                                partitionData.partitionIndex(),\n                                metadata.topicNames().get(shareAcknowledgeTopicResponse.topicId()));\n\n                        acknowledgeRequestState.handleAcknowledgeErrorCode(tip, response.error());\n                        metricsManager.recordLatency(resp.requestLatencyMs());\n                    }));\n                }\n            } else {\n                AtomicBoolean shouldRetry = new AtomicBoolean(false);\n                // Check all partition level error codes\n                response.data().responses().forEach(shareAcknowledgeTopicResponse -> shareAcknowledgeTopicResponse.partitions().forEach(partitionData -> {\n                    Errors partitionError = Errors.forCode(partitionData.errorCode());\n                    TopicIdPartition tip = new TopicIdPartition(shareAcknowledgeTopicResponse.topicId(),\n                            partitionData.partitionIndex(),\n                            metadata.topicNames().get(shareAcknowledgeTopicResponse.topicId()));\n                    if (partitionError.exception() != null) {\n                        if (partitionError.exception() instanceof RetriableException && !acknowledgeRequestState.onClose) {\n                            // Move to incomplete acknowledgements to retry\n                            acknowledgeRequestState.moveToIncompleteAcks(tip);\n                            shouldRetry.set(true);\n                        } else {\n                            metricsManager.recordFailedAcknowledgements(acknowledgeRequestState.getInFlightAcknowledgementsCount(tip));\n                            acknowledgeRequestState.handleAcknowledgeErrorCode(tip, partitionError);\n                        }\n                    } else {\n                        acknowledgeRequestState.handleAcknowledgeErrorCode(tip, partitionError);\n                    }\n                }));\n\n                if (shouldRetry.get()) {\n                    acknowledgeRequestState.onFailedAttempt(currentTimeMs);\n                } else {\n                    acknowledgeRequestState.onSuccessfulAttempt(currentTimeMs);\n                }\n                acknowledgeRequestState.processingComplete();\n            }\n            metricsManager.recordLatency(resp.requestLatencyMs());\n        } finally {\n            log.debug(\"Removing pending request for node {} - success\", fetchTarget.id());\n            nodesWithPendingRequests.remove(fetchTarget.id());\n        }\n    }\n\n    private void handleShareAcknowledgeFailure(Node fetchTarget,\n                                               ShareAcknowledgeRequestData requestData,\n                                               AcknowledgeRequestState acknowledgeRequestState,\n                                               Throwable error,\n                                               long currentTimeMs) {\n        try {\n            log.debug(\"Completed ShareAcknowledge request from node {} unsuccessfully {}\", fetchTarget.id(), Errors.forException(error));\n            acknowledgeRequestState.sessionHandler().handleError(error);\n            acknowledgeRequestState.onFailedAttempt(currentTimeMs);\n\n            requestData.topics().forEach(topic -> topic.partitions().forEach(partition -> {\n                TopicIdPartition tip = new TopicIdPartition(topic.topicId(),\n                        partition.partitionIndex(),\n                        metadata.topicNames().get(topic.topicId()));\n                metricsManager.recordFailedAcknowledgements(acknowledgeRequestState.getInFlightAcknowledgementsCount(tip));\n                acknowledgeRequestState.handleAcknowledgeErrorCode(tip, Errors.forException(error));\n            }));\n        } finally {\n            log.debug(\"Removing pending request for node {} - failed\", fetchTarget.id());\n            nodesWithPendingRequests.remove(fetchTarget.id());\n        }\n    }\n\n    private void handleShareAcknowledgeCloseSuccess(Node fetchTarget,\n                                                    ShareAcknowledgeRequestData requestData,\n                                                    AcknowledgeRequestState acknowledgeRequestState,\n                                                    ClientResponse resp,\n                                                    long currentTimeMs) {\n        try {\n            log.debug(\"Completed ShareAcknowledge on close request from node {} successfully\", fetchTarget.id());\n            final ShareAcknowledgeResponse response = (ShareAcknowledgeResponse) resp.responseBody();\n\n            response.data().responses().forEach(topic -> topic.partitions().forEach(partition -> {\n                TopicIdPartition tip = new TopicIdPartition(topic.topicId(),\n                        partition.partitionIndex(),\n                        metadata.topicNames().get(topic.topicId()));\n                if (partition.errorCode() != Errors.NONE.code()) {\n                    metricsManager.recordFailedAcknowledgements(acknowledgeRequestState.getInFlightAcknowledgementsCount(tip));\n                }\n                acknowledgeRequestState.handleAcknowledgeErrorCode(tip, Errors.forCode(partition.errorCode()));\n            }));\n\n            acknowledgeRequestState.onSuccessfulAttempt(currentTimeMs);\n            metricsManager.recordLatency(resp.requestLatencyMs());\n            acknowledgeRequestState.processingComplete();\n        } finally {\n            log.debug(\"Removing pending request for node {} - success\", fetchTarget.id());\n            nodesWithPendingRequests.remove(fetchTarget.id());\n            sessionHandlers.remove(fetchTarget.id());\n        }\n    }\n\n    private void handleShareAcknowledgeCloseFailure(Node fetchTarget,\n                                                    ShareAcknowledgeRequestData requestData,\n                                                    AcknowledgeRequestState acknowledgeRequestState,\n                                                    Throwable error,\n                                                    long currentTimeMs) {\n        try {\n            log.debug(\"Completed ShareAcknowledge on close request from node {} unsuccessfully {}\", fetchTarget.id(), Errors.forException(error));\n            acknowledgeRequestState.sessionHandler().handleError(error);\n            acknowledgeRequestState.onFailedAttempt(currentTimeMs);\n\n            requestData.topics().forEach(topic -> topic.partitions().forEach(partition -> {\n                TopicIdPartition tip = new TopicIdPartition(topic.topicId(),\n                        partition.partitionIndex(),\n                        metadata.topicNames().get(topic.topicId()));\n                metricsManager.recordFailedAcknowledgements(acknowledgeRequestState.getInFlightAcknowledgementsCount(tip));\n                acknowledgeRequestState.handleAcknowledgeErrorCode(tip, Errors.forException(error));\n            }));\n        } finally {\n            log.debug(\"Removing pending request for node {} - failed\", fetchTarget.id());\n            nodesWithPendingRequests.remove(fetchTarget.id());\n            sessionHandlers.remove(fetchTarget.id());\n        }\n    }\n\n    private List<TopicPartition> partitionsToFetch() {\n        return subscriptions.fetchablePartitions(tp -> true);\n    }\n\n    public ShareSessionHandler sessionHandler(int node) {\n        return sessionHandlers.get(node);\n    }\n\n    boolean hasCompletedFetches() {\n        return !shareFetchBuffer.isEmpty();\n    }\n\n    protected void closeInternal() {\n        Utils.closeQuietly(shareFetchBuffer, \"shareFetchBuffer\");\n    }\n\n    public void close() {\n        idempotentCloser.close(this::closeInternal);\n    }\n\n    @Override\n    public void onMemberEpochUpdated(Optional<Integer> memberEpochOpt, Optional<String> memberIdOpt) {\n        memberIdOpt.ifPresent(s -> memberId = Uuid.fromString(s));\n    }\n\n    /**\n     * Represents a request to acknowledge delivery that can be retried or aborted.\n     */\n    public class AcknowledgeRequestState extends TimedRequestState {\n\n        /**\n         * The share session handler.\n         */\n        private final ShareSessionHandler sessionHandler;\n\n        /**\n         * The node to send the request to.\n         */\n        private final int nodeId;\n\n        /**\n         * The map of acknowledgements to send\n         */\n        private final Map<TopicIdPartition, Acknowledgements> acknowledgementsToSend;\n\n        /**\n         * The map of acknowledgements to be retried in the next attempt.\n         */\n        private final Map<TopicIdPartition, Acknowledgements> incompleteAcknowledgements;\n\n        /**\n         * The in-flight acknowledgements\n         */\n        private final Map<TopicIdPartition, Acknowledgements> inFlightAcknowledgements;\n\n        /**\n         * The handler to call on a successful response from ShareAcknowledge.\n         */\n        private final ResponseHandler<ClientResponse> successHandler;\n\n        /**\n         * The handler to call on a failed response from ShareAcknowledge.\n         */\n        private final ResponseHandler<Throwable> errorHandler;\n\n        /**\n         * This handles completing a future when all results are known.\n         */\n        private final ResultHandler resultHandler;\n\n        /**\n         * Whether this is the final acknowledge request state before the consumer closes.\n         */\n        private final boolean onClose;\n\n        AcknowledgeRequestState(LogContext logContext,\n                                String owner,\n                                long deadlineMs,\n                                long retryBackoffMs,\n                                long retryBackoffMaxMs,\n                                ShareSessionHandler sessionHandler,\n                                int nodeId,\n                                Map<TopicIdPartition, Acknowledgements> acknowledgementsMap,\n                                ResponseHandler<ClientResponse> successHandler,\n                                ResponseHandler<Throwable> errorHandler,\n                                ResultHandler resultHandler) {\n            this(logContext, owner, deadlineMs, retryBackoffMs, retryBackoffMaxMs, sessionHandler, nodeId,\n                    acknowledgementsMap, successHandler, errorHandler, resultHandler, false);\n        }\n\n        AcknowledgeRequestState(LogContext logContext,\n                                String owner,\n                                long deadlineMs,\n                                long retryBackoffMs,\n                                long retryBackoffMaxMs,\n                                ShareSessionHandler sessionHandler,\n                                int nodeId,\n                                Map<TopicIdPartition, Acknowledgements> acknowledgementsMap,\n                                ResponseHandler<ClientResponse> successHandler,\n                                ResponseHandler<Throwable> errorHandler,\n                                ResultHandler resultHandler,\n                                boolean onClose) {\n            super(logContext, owner, retryBackoffMs, retryBackoffMaxMs, deadlineTimer(time, deadlineMs));\n            this.sessionHandler = sessionHandler;\n            this.nodeId = nodeId;\n            this.successHandler = successHandler;\n            this.errorHandler = errorHandler;\n            this.acknowledgementsToSend = acknowledgementsMap;\n            this.resultHandler = resultHandler;\n            this.onClose = onClose;\n            this.inFlightAcknowledgements = new HashMap<>();\n            this.incompleteAcknowledgements = new HashMap<>();\n        }\n\n        UnsentRequest buildRequest(long currentTimeMs) {\n            // If this is the closing request, close the share session by setting the final epoch\n            if (onClose) {\n                sessionHandler.notifyClose();\n            }\n\n            Map<TopicIdPartition, Acknowledgements> finalAcknowledgementsToSend = new HashMap<>(\n                    incompleteAcknowledgements.isEmpty() ? acknowledgementsToSend : incompleteAcknowledgements);\n\n            for (Map.Entry<TopicIdPartition, Acknowledgements> entry : finalAcknowledgementsToSend.entrySet()) {\n                sessionHandler.addPartitionToFetch(entry.getKey(), entry.getValue());\n            }\n\n            ShareAcknowledgeRequest.Builder requestBuilder = sessionHandler.newShareAcknowledgeBuilder(groupId, fetchConfig);\n            Node nodeToSend = metadata.fetch().nodeById(nodeId);\n\n            nodesWithPendingRequests.add(nodeId);\n\n            BiConsumer<ClientResponse, Throwable> responseHandler = (clientResponse, error) -> {\n                if (error != null) {\n                    errorHandler.handle(nodeToSend, requestBuilder.data(), this, error, currentTimeMs);\n                    processingComplete();\n                } else {\n                    successHandler.handle(nodeToSend, requestBuilder.data(), this, clientResponse, currentTimeMs);\n                    if (onClose && !closeFuture.isDone()) {\n                        closeFuture.complete(null);\n                    }\n                }\n            };\n\n            if (requestBuilder == null) {\n                handleSessionErrorCode(Errors.SHARE_SESSION_NOT_FOUND);\n                return null;\n            } else {\n                inFlightAcknowledgements.putAll(finalAcknowledgementsToSend);\n                if (incompleteAcknowledgements.isEmpty()) {\n                    acknowledgementsToSend.clear();\n                } else {\n                    incompleteAcknowledgements.clear();\n                }\n                return new UnsentRequest(requestBuilder, Optional.of(nodeToSend)).whenComplete(responseHandler);\n            }\n        }\n\n        int getInFlightAcknowledgementsCount(TopicIdPartition tip) {\n            Acknowledgements acks = inFlightAcknowledgements.get(tip);\n            if (acks == null) {\n                return 0;\n            } else {\n                return acks.size();\n            }\n        }\n\n        int getIncompleteAcknowledgementsCount(TopicIdPartition tip) {\n            Acknowledgements acks = incompleteAcknowledgements.get(tip);\n            if (acks == null) {\n                return 0;\n            } else {\n                return acks.size();\n            }\n        }\n\n        int getAcknowledgementsToSendCount(TopicIdPartition tip) {\n            Acknowledgements acks = acknowledgementsToSend.get(tip);\n            if (acks == null) {\n                return 0;\n            } else {\n                return acks.size();\n            }\n        }\n\n        boolean isEmpty() {\n            return acknowledgementsToSend.isEmpty() &&\n                    incompleteAcknowledgements.isEmpty() &&\n                    inFlightAcknowledgements.isEmpty();\n        }\n\n        /**\n         * Sets the error code in the acknowledgements and sends the response\n         * through a background event.\n         */\n        void handleAcknowledgeErrorCode(TopicIdPartition tip, Errors acknowledgeErrorCode) {\n            Acknowledgements acks = inFlightAcknowledgements.get(tip);\n            if (onClose(acks)) {\n                acks.setAcknowledgeErrorCode(acknowledgeErrorCode);\n            }\n            resultHandler.complete(tip, acks);\n        }\n\n        /**\n         * Sets the error code for the acknowledgements which were timed out\n         * after some retries.\n         */\n        void handleAcknowledgeTimedOut(TopicIdPartition tip) {\n            Acknowledgements acks = incompleteAcknowledgements.get(tip);\n            if (onClose(acks)) {\n                acks.setAcknowledgeErrorCode(Errors.REQUEST_TIMED_OUT);\n            }\n            resultHandler.complete(tip, acks);\n        }\n\n        /**\n         * Set the error code for all remaining acknowledgements in the event\n         * of a session error which prevents the remains acknowledgements from\n         * being sent.\n         */\n        void handleSessionErrorCode(Errors errorCode) {\n            inFlightAcknowledgements.forEach((tip, acks) -> {\n                if (onClose(acks)) {\n                    acks.setAcknowledgeErrorCode(errorCode);\n                }\n                resultHandler.complete(tip, acks);\n            });\n            processingComplete();\n        }\n\n        ShareSessionHandler sessionHandler() {\n            return sessionHandler;\n        }\n\n        void processingComplete() {\n            inFlightAcknowledgements.clear();\n            resultHandler.completeIfEmpty();\n        }\n\n        void retryRequest() {\n            incompleteAcknowledgements.putAll(inFlightAcknowledgements);\n            inFlightAcknowledgements.clear();\n        }\n\n        boolean maybeExpire() {\n            return numAttempts > 0 && isExpired();\n        }\n\n        public void moveToIncompleteAcks(TopicIdPartition tip) {\n            Acknowledgements acks = inFlightAcknowledgements.remove(tip);\n            if (onClose(acks)) {\n                Acknowledgements existingAcks = incompleteAcknowledgements.putIfAbsent(tip, acks);\n                if (onClose(existingAcks)) {\n                    incompleteAcknowledgements.get(tip).merge(acks);\n                }\n            }\n        }\n    }\n\n    /**\n     * Defines the contract for handling responses from brokers.\n     * @param <T> Type of response, usually either {@link ClientResponse} or {@link Throwable}\n     */\n    @FunctionalInterface\n    private interface ResponseHandler<T> {\n        /**\n         * Handle the response from the given {@link Node target}\n         */\n        void handle(Node target, ShareAcknowledgeRequestData request, AcknowledgeRequestState requestState, T response, long currentTimeMs);\n    }\n\n    /**\n     * Sends a ShareAcknowledgeCommitCallback event to the application when it is done\n     * processing all the remaining acknowledgement request states.\n     * Also manages completing the future for synchronous acknowledgement commit by counting\n     * down the results as they are known and completing the future at the end.\n     */\n    class ResultHandler {\n        private final Map<TopicIdPartition, Acknowledgements> result;\n        private final AtomicInteger remainingResults;\n        private final Optional<CompletableFuture<Map<TopicIdPartition, Acknowledgements>>> future;\n\n        ResultHandler(final AtomicInteger remainingResults,\n                      final Optional<CompletableFuture<Map<TopicIdPartition, Acknowledgements>>> future) {\n            result = new HashMap<>();\n            this.remainingResults = remainingResults;\n            this.future = future;\n        }\n\n        /**\n         * Handle the result of a ShareAcknowledge request sent to one or more nodes and\n         * signal the completion when all results are known.\n         */\n        public void complete(TopicIdPartition partition, Acknowledgements acknowledgements) {\n            if (onClose(acknowledgements)) {\n                result.put(partition, acknowledgements);\n            }\n            if (remainingResults.decrementAndGet() == 0) {\n                ShareAcknowledgementCommitCallbackEvent event = new ShareAcknowledgementCommitCallbackEvent(result);\n                backgroundEventHandler.add(event);\n                future.ifPresent(future -> future.complete(result));\n            }\n        }\n\n        /**\n         * Handles the case where there are no results pending after initialization.\n         */\n        public void completeIfEmpty() {\n            if (remainingResults.get() == 0) {\n                future.ifPresent(future -> future.complete(result));\n            }\n        }\n    }\n\n    static class Pair<V> {\n        private V asyncRequest;\n        private V syncRequest;\n\n        public Pair(V asyncRequest, V syncRequest) {\n            this.asyncRequest = asyncRequest;\n            this.syncRequest = syncRequest;\n        }\n\n        public void setAsyncRequest(V asyncRequest) {\n            this.asyncRequest = asyncRequest;\n        }\n\n        public void setSyncRequest(V second) {\n            this.syncRequest = second;\n        }\n\n        public V getAsyncRequest() {\n            return asyncRequest;\n        }\n\n        public V getSyncRequest() {\n            return syncRequest;\n        }\n    }\n\n    Pair<AcknowledgeRequestState> requestStates(int nodeId) {\n        return acknowledgeRequestStates.get(nodeId);\n    }\n}",
                "methodCount": 49
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 6,
                "candidates": [
                    {
                        "lineStart": 743,
                        "lineEnd": 745,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method partitionsToFetch to class SubscriptionState",
                        "description": "Move method partitionsToFetch to org.apache.kafka.clients.consumer.internals.SubscriptionState\nRationale: The partitionsToFetch() method is closely tied to the subscription logic, particularly dealing with 'fetchable partitions' and interacts with the SubscriptionState's member variables and methods. The SubscriptionState class already has a method with a similar purpose, making it a natural fit for the method. Moving the method here maintains cohesion and keeps the subscription-related logic encapsulated within the SubscriptionState class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 252,
                        "lineEnd": 254,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method isNodeFree to class Time",
                        "description": "Move method isNodeFree to org.apache.kafka.common.utils.Time\nRationale: The method isNodeFree() deals with the state of nodes and whether a node has pending requests. This has no obvious connection to time functionalities, which is the primary focus of the Time class. Given that the Time class deals exclusively with temporal operations, incorporating a method that manages node states would introduce an unrelated responsibility, violating the Single Responsibility Principle. Therefore, it might be more appropriate to consider creating or finding another class that is more aligned with node management for this method.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 747,
                        "lineEnd": 749,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method sessionHandler to class Time",
                        "description": "Move method sessionHandler to org.apache.kafka.common.utils.Time\nRationale: null",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 755,
                        "lineEnd": 757,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method closeInternal to class ShareFetchBuffer",
                        "description": "Move method closeInternal to org.apache.kafka.clients.consumer.internals.ShareFetchBuffer\nRationale: The method closeInternal is responsible for closing a buffer (shareFetchBuffer). The ShareFetchBuffer class is primarily in charge of buffering fetches and already has a close() method implementing AutoCloseable. Since this aligns with the functionality provided by ShareFetchBuffer, it is logical to move closeInternal there for better encapsulation of buffer management. Moreover, ShareFetchBuffer already contains logic related to thread-safety and resource management which further justifies this move.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 311,
                        "lineEnd": 313,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method isRequestStateInProgress to class SubscriptionState",
                        "description": "Move method isRequestStateInProgress to org.apache.kafka.clients.consumer.internals.SubscriptionState\nRationale: The method isRequestStateInProgress() checks for the state of an AcknowledgeRequestState, which appears closely related to subscription management and state checking functionality. SubscriptionState already deals with managing state for subscriptions, making it a logical place for this method. Furthermore, the method's state-checking nature aligns with various similar state-handling methods present in SubscriptionState.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 497,
                        "lineEnd": 499,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method onClose to class SubscriptionState",
                        "description": "Move method onClose to org.apache.kafka.clients.consumer.internals.SubscriptionState\nRationale: The method onClose() is related to checking the state of acknowledgements, which aligns most closely with the state management tasks handled by the SubscriptionState class. The existing functionality of SubscriptionState includes handling various states and transitions, making it an appropriate place to move this method. This method does not appear to be relevant to ConsumerMetadata, which deals with metadata and configuration, or ShareFetchBuffer, which is related to fetching data. Therefore, SubscriptionState is the most suitable target class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "fetch",
                            "method_signature": "public fetch(Map<TopicIdPartition, Acknowledgements> acknowledgementsMap)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "processAcknowledgements",
                            "method_signature": "private processAcknowledgements(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isNodeFree",
                            "method_signature": "private isNodeFree(int nodeId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeBuildRequest",
                            "method_signature": "private maybeBuildRequest(AcknowledgeRequestState acknowledgeRequestState,\n                                                      long currentTimeMs,\n                                                      boolean onCommitAsync,\n                                                      AtomicBoolean isAsyncDone)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "checkAndRemoveCompletedAcknowledgements",
                            "method_signature": "private checkAndRemoveCompletedAcknowledgements()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isRequestStateInProgress",
                            "method_signature": "private isRequestStateInProgress(AcknowledgeRequestState acknowledgeRequestState)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitSync",
                            "method_signature": "public commitSync(\n            final Map<TopicIdPartition, Acknowledgements> acknowledgementsMap,\n            final long deadlineMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitAsync",
                            "method_signature": "public commitAsync(final Map<TopicIdPartition, Acknowledgements> acknowledgementsMap)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "acknowledgeOnClose",
                            "method_signature": "public acknowledgeOnClose(final Map<TopicIdPartition, Acknowledgements> acknowledgementsMap,\n                                                      final long deadlineMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onClose",
                            "method_signature": "private onClose(Acknowledgements acknowledgements)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleShareFetchSuccess",
                            "method_signature": "private handleShareFetchSuccess(Node fetchTarget,\n                                         @SuppressWarnings(\"unused\") ShareFetchRequestData requestData,\n                                         ClientResponse resp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleShareFetchFailure",
                            "method_signature": "private handleShareFetchFailure(Node fetchTarget,\n                                         ShareFetchRequestData requestData,\n                                         Throwable error)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleShareAcknowledgeSuccess",
                            "method_signature": "private handleShareAcknowledgeSuccess(Node fetchTarget,\n                                               ShareAcknowledgeRequestData requestData,\n                                               AcknowledgeRequestState acknowledgeRequestState,\n                                               ClientResponse resp,\n                                               long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleShareAcknowledgeFailure",
                            "method_signature": "private handleShareAcknowledgeFailure(Node fetchTarget,\n                                               ShareAcknowledgeRequestData requestData,\n                                               AcknowledgeRequestState acknowledgeRequestState,\n                                               Throwable error,\n                                               long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleShareAcknowledgeCloseSuccess",
                            "method_signature": "private handleShareAcknowledgeCloseSuccess(Node fetchTarget,\n                                                    ShareAcknowledgeRequestData requestData,\n                                                    AcknowledgeRequestState acknowledgeRequestState,\n                                                    ClientResponse resp,\n                                                    long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleShareAcknowledgeCloseFailure",
                            "method_signature": "private handleShareAcknowledgeCloseFailure(Node fetchTarget,\n                                                    ShareAcknowledgeRequestData requestData,\n                                                    AcknowledgeRequestState acknowledgeRequestState,\n                                                    Throwable error,\n                                                    long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "partitionsToFetch",
                            "method_signature": "private partitionsToFetch()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sessionHandler",
                            "method_signature": "public sessionHandler(int node)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasCompletedFetches",
                            "method_signature": " hasCompletedFetches()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "closeInternal",
                            "method_signature": "protected closeInternal()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "public close()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildRequest",
                            "method_signature": " buildRequest(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getInFlightAcknowledgementsCount",
                            "method_signature": " getInFlightAcknowledgementsCount(TopicIdPartition tip)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getIncompleteAcknowledgementsCount",
                            "method_signature": " getIncompleteAcknowledgementsCount(TopicIdPartition tip)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getAcknowledgementsToSendCount",
                            "method_signature": " getAcknowledgementsToSendCount(TopicIdPartition tip)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isEmpty",
                            "method_signature": " isEmpty()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleAcknowledgeErrorCode",
                            "method_signature": " handleAcknowledgeErrorCode(TopicIdPartition tip, Errors acknowledgeErrorCode)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleAcknowledgeTimedOut",
                            "method_signature": " handleAcknowledgeTimedOut(TopicIdPartition tip)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleSessionErrorCode",
                            "method_signature": " handleSessionErrorCode(Errors errorCode)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "processingComplete",
                            "method_signature": " processingComplete()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "retryRequest",
                            "method_signature": " retryRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeExpire",
                            "method_signature": " maybeExpire()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "moveToIncompleteAcks",
                            "method_signature": "public moveToIncompleteAcks(TopicIdPartition tip)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "complete",
                            "method_signature": "public complete(TopicIdPartition partition, Acknowledgements acknowledgements)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeIfEmpty",
                            "method_signature": "public completeIfEmpty()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "requestStates",
                            "method_signature": " requestStates(int nodeId)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "isEmpty",
                            "method_signature": " isEmpty()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeExpire",
                            "method_signature": " maybeExpire()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isRequestStateInProgress",
                            "method_signature": "private isRequestStateInProgress(AcknowledgeRequestState acknowledgeRequestState)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "closeInternal",
                            "method_signature": "protected closeInternal()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "requestStates",
                            "method_signature": " requestStates(int nodeId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sessionHandler",
                            "method_signature": "public sessionHandler(int node)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "retryRequest",
                            "method_signature": " retryRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "processingComplete",
                            "method_signature": " processingComplete()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "partitionsToFetch",
                            "method_signature": "private partitionsToFetch()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isNodeFree",
                            "method_signature": "private isNodeFree(int nodeId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onClose",
                            "method_signature": "private onClose(Acknowledgements acknowledgements)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "public close()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasCompletedFetches",
                            "method_signature": " hasCompletedFetches()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeIfEmpty",
                            "method_signature": "public completeIfEmpty()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleSessionErrorCode",
                            "method_signature": " handleSessionErrorCode(Errors errorCode)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "isEmpty",
                            "method_signature": " isEmpty()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeExpire",
                            "method_signature": " maybeExpire()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isRequestStateInProgress",
                            "method_signature": "private isRequestStateInProgress(AcknowledgeRequestState acknowledgeRequestState)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "closeInternal",
                            "method_signature": "protected closeInternal()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "requestStates",
                            "method_signature": " requestStates(int nodeId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sessionHandler",
                            "method_signature": "public sessionHandler(int node)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "retryRequest",
                            "method_signature": " retryRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "processingComplete",
                            "method_signature": " processingComplete()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "partitionsToFetch",
                            "method_signature": "private partitionsToFetch()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isNodeFree",
                            "method_signature": "private isNodeFree(int nodeId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onClose",
                            "method_signature": "private onClose(Acknowledgements acknowledgements)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "public close()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasCompletedFetches",
                            "method_signature": " hasCompletedFetches()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeIfEmpty",
                            "method_signature": "public completeIfEmpty()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleSessionErrorCode",
                            "method_signature": " handleSessionErrorCode(Errors errorCode)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                " isEmpty()": {
                    "first": {
                        "method_name": "isEmpty",
                        "method_signature": " isEmpty()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3362733780926663
                },
                " maybeExpire()": {
                    "first": {
                        "method_name": "maybeExpire",
                        "method_signature": " maybeExpire()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.36078377418756
                },
                "private isRequestStateInProgress(AcknowledgeRequestState acknowledgeRequestState)": {
                    "first": {
                        "method_name": "isRequestStateInProgress",
                        "method_signature": "private isRequestStateInProgress(AcknowledgeRequestState acknowledgeRequestState)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3925466435080159
                },
                "protected closeInternal()": {
                    "first": {
                        "method_name": "closeInternal",
                        "method_signature": "protected closeInternal()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.396110053150583
                },
                " requestStates(int nodeId)": {
                    "first": {
                        "method_name": "requestStates",
                        "method_signature": " requestStates(int nodeId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4085638257601413
                },
                "public sessionHandler(int node)": {
                    "first": {
                        "method_name": "sessionHandler",
                        "method_signature": "public sessionHandler(int node)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.40866279625899155
                },
                " retryRequest()": {
                    "first": {
                        "method_name": "retryRequest",
                        "method_signature": " retryRequest()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4278458499904717
                },
                " processingComplete()": {
                    "first": {
                        "method_name": "processingComplete",
                        "method_signature": " processingComplete()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.43151419287126047
                },
                "private partitionsToFetch()": {
                    "first": {
                        "method_name": "partitionsToFetch",
                        "method_signature": "private partitionsToFetch()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.43326650245007275
                },
                "private isNodeFree(int nodeId)": {
                    "first": {
                        "method_name": "isNodeFree",
                        "method_signature": "private isNodeFree(int nodeId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4447008873824991
                },
                "private onClose(Acknowledgements acknowledgements)": {
                    "first": {
                        "method_name": "onClose",
                        "method_signature": "private onClose(Acknowledgements acknowledgements)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4451943000322952
                },
                "public close()": {
                    "first": {
                        "method_name": "close",
                        "method_signature": "public close()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4498588558444224
                },
                " hasCompletedFetches()": {
                    "first": {
                        "method_name": "hasCompletedFetches",
                        "method_signature": " hasCompletedFetches()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.45475159618261185
                },
                "public completeIfEmpty()": {
                    "first": {
                        "method_name": "completeIfEmpty",
                        "method_signature": "public completeIfEmpty()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5304209773409315
                },
                " handleSessionErrorCode(Errors errorCode)": {
                    "first": {
                        "method_name": "handleSessionErrorCode",
                        "method_signature": " handleSessionErrorCode(Errors errorCode)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5351785370955543
                }
            },
            "llmMethodPriority": {
                "priority_method_names": [
                    "public completeIfEmpty()",
                    "private partitionsToFetch()",
                    "private isNodeFree(int nodeId)",
                    "public sessionHandler(int node)",
                    "public close()",
                    "protected closeInternal()",
                    "private isRequestStateInProgress(AcknowledgeRequestState acknowledgeRequestState)",
                    "private onClose(Acknowledgements acknowledgements)"
                ],
                "llm_response_time": 5538
            },
            "targetClassMap": {
                "completeIfEmpty": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3570,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "partitionsToFetch": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.12675747220820874
                        },
                        {
                            "class_name": "SubscriptionState",
                            "similarity_score": 0.4468594180908398
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SubscriptionState",
                        "Time"
                    ],
                    "llm_response_time": 3806,
                    "similarity_computation_time": 5,
                    "similarity_metric": "cosine"
                },
                "isNodeFree": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.1344466022467346
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 2302,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "sessionHandler": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.14055781143976798
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 2626,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "close": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2902,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "closeInternal": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.1306631486813383
                        },
                        {
                            "class_name": "ConsumerMetadata",
                            "similarity_score": 0.3277811406876182
                        },
                        {
                            "class_name": "SubscriptionState",
                            "similarity_score": 0.38502352082351265
                        },
                        {
                            "class_name": "FetchConfig",
                            "similarity_score": 0.066847959055905
                        },
                        {
                            "class_name": "ShareFetchBuffer",
                            "similarity_score": 0.4297123213195926
                        },
                        {
                            "class_name": "ShareFetchMetricsManager",
                            "similarity_score": 0.3574759385653026
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "ShareFetchBuffer",
                        "ShareFetchMetricsManager",
                        "SubscriptionState"
                    ],
                    "llm_response_time": 5711,
                    "similarity_computation_time": 12,
                    "similarity_metric": "cosine"
                },
                "isRequestStateInProgress": {
                    "target_classes": [
                        {
                            "class_name": "AcknowledgeRequestState",
                            "similarity_score": 0.38642756712534854
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.10977519105180875
                        },
                        {
                            "class_name": "ConsumerMetadata",
                            "similarity_score": 0.4479893351905204
                        },
                        {
                            "class_name": "SubscriptionState",
                            "similarity_score": 0.4393354576995261
                        },
                        {
                            "class_name": "FetchConfig",
                            "similarity_score": 0.07020196159785268
                        },
                        {
                            "class_name": "ShareFetchBuffer",
                            "similarity_score": 0.3945361419570049
                        },
                        {
                            "class_name": "ShareFetchMetricsManager",
                            "similarity_score": 0.29902930630809765
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SubscriptionState",
                        "ConsumerMetadata",
                        "ShareFetchBuffer"
                    ],
                    "llm_response_time": 4781,
                    "similarity_computation_time": 4,
                    "similarity_metric": "cosine"
                },
                "onClose": {
                    "target_classes": [
                        {
                            "class_name": "Acknowledgements",
                            "similarity_score": 0.2841361430684012
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.1257187284591675
                        },
                        {
                            "class_name": "LogContext",
                            "similarity_score": 0.35426247089806906
                        },
                        {
                            "class_name": "ConsumerMetadata",
                            "similarity_score": 0.4763139720814412
                        },
                        {
                            "class_name": "SubscriptionState",
                            "similarity_score": 0.4754694727831023
                        },
                        {
                            "class_name": "FetchConfig",
                            "similarity_score": 0.07690239589651064
                        },
                        {
                            "class_name": "ShareFetchBuffer",
                            "similarity_score": 0.42454326127081826
                        },
                        {
                            "class_name": "BackgroundEventHandler",
                            "similarity_score": 0.15275252316519466
                        },
                        {
                            "class_name": "ShareFetchMetricsManager",
                            "similarity_score": 0.3560545574349281
                        },
                        {
                            "class_name": "IdempotentCloser",
                            "similarity_score": 0.14252676107662793
                        },
                        {
                            "class_name": "Uuid",
                            "similarity_score": 0.29414682966578654
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SubscriptionState",
                        "ConsumerMetadata",
                        "ShareFetchBuffer"
                    ],
                    "llm_response_time": 8877,
                    "similarity_computation_time": 6,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "d67c18b4aec66fec5d145c9ee737c7a469b8635c",
        "url": "https://github.com/apache/kafka/commit/d67c18b4aec66fec5d145c9ee737c7a469b8635c",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public forConsumer(requireTimestamp boolean, isolationLevel IsolationLevel) : Builder extracted from private sendListOffsetRequest(node Node, timestampsToSearch Map<TopicPartition,ListOffsetsPartition>, requireTimestamp boolean) : RequestFuture<ListOffsetResult> in class org.apache.kafka.clients.consumer.internals.OffsetFetcher & moved to class org.apache.kafka.common.requests.ListOffsetsRequest.Builder",
            "leftSideLocations": [
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/OffsetFetcher.java",
                    "startLine": 382,
                    "endLine": 407,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private sendListOffsetRequest(node Node, timestampsToSearch Map<TopicPartition,ListOffsetsPartition>, requireTimestamp boolean) : RequestFuture<ListOffsetResult>"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/OffsetFetcher.java",
                    "startLine": 393,
                    "endLine": 395,
                    "startColumn": 9,
                    "endColumn": 93,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/requests/ListOffsetsRequest.java",
                    "startLine": 61,
                    "endLine": 64,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public forConsumer(requireTimestamp boolean, isolationLevel IsolationLevel) : Builder"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/requests/ListOffsetsRequest.java",
                    "startLine": 63,
                    "endLine": 63,
                    "startColumn": 13,
                    "endColumn": 87,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/OffsetFetcher.java",
                    "startLine": 382,
                    "endLine": 407,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private sendListOffsetRequest(node Node, timestampsToSearch Map<TopicPartition,ListOffsetsPartition>, requireTimestamp boolean) : RequestFuture<ListOffsetResult>"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/OffsetFetcher.java",
                    "startLine": 393,
                    "endLine": 394,
                    "startColumn": 46,
                    "endColumn": 63,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ListOffsetsRequest.Builder.forConsumer(requireTimestamp,isolationLevel)"
                }
            ],
            "isStatic": true
        },
        "ref_id": 634,
        "extraction_results": {
            "success": true,
            "newCommitHash": "f843a212e5cfdf88c76494ff4f3dae817d2545f0",
            "newBranchName": "extract-forConsumer-sendListOffsetRequest-3d436f5"
        }
    }
]